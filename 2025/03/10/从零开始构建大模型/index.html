

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ccw">
  <meta name="keywords" content="">
  
    <meta name="description" content="1.Understanding large language models LLM，Large Language Models，大语言模型  LLM 大语言模型（例如 ChatGPT）是一种基于深度学习的神经网络模型，主要用于自然语言处理（NLP）领域。在 LLM 出现之前，传统的 NLP 主要使用简单模型或者人工编写的规则，应用场景有限。大语言模型的出现翻开了新的篇章，它将模型对语言的理解、分析">
<meta property="og:type" content="article">
<meta property="og:title" content="从零开始构建大模型">
<meta property="og:url" content="https://ccw1078.github.io/2025/03/10/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Ccw&#39;s Blogs">
<meta property="og:description" content="1.Understanding large language models LLM，Large Language Models，大语言模型  LLM 大语言模型（例如 ChatGPT）是一种基于深度学习的神经网络模型，主要用于自然语言处理（NLP）领域。在 LLM 出现之前，传统的 NLP 主要使用简单模型或者人工编写的规则，应用场景有限。大语言模型的出现翻开了新的篇章，它将模型对语言的理解、分析">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503100747391.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110634357.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110657249.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110705615.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110710618.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110714311.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110723099.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110742937.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503120727158.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503120738154.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130712013.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130731717.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130738030.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130743620.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130743839.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503140714552.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503150942666.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170519238.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170521323.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170540115.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170617077.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170713612.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170717866.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170756860.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190736390.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190739048.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190802208.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503200652234.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503200655255.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210734863.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210746689.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210805247.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220620677.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220621687.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220636426.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220653964.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220705018.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220758390.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503240635490.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503240713634.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250649329.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250623811.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250634159.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250711998.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503270623437.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503270735653.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310658487.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310708068.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310736036.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504010719065.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504010754996.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504020736917.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504020743900.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030601160.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030616589.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030618948.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030647367.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030701369.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030801842.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504100800737.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504110654926.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140711884.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140716015.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140749483.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504150707074.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504150715928.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504160720141.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504160758463.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504221140198.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504220959894.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504221141181.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504230957036.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423140746067.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423141559889.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423150829158.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423151204567.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423155249464.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423170138827.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240659482.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240702733.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240714853.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424101428126.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424104259443.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424112202606.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424145220641.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424152121301.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153511203.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153903472.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153913526.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424171221160.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424192338714.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424193115615.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424193247958.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424201632645.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424203507229.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424204103461.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424204231827.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424205029005.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425144615939.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425144726686.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425145538280.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425145630819.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425151850612.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425152239695.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425160144252.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425165812254.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504280751970.png">
<meta property="og:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202505080922977.png">
<meta property="article:published_time" content="2025-03-09T22:41:00.000Z">
<meta property="article:modified_time" content="2025-05-11T00:37:28.861Z">
<meta property="article:author" content="ccw">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503100747391.png">
  
  
  
  <title>从零开始构建大模型 - Ccw&#39;s Blogs</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ccw1078.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Ccw's Blogs" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="从零开始构建大模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-10 06:41" pubdate>
          2025年3月10日 早上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          24k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          204 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">从零开始构建大模型</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="1-Understanding-large-language-models"><a href="#1-Understanding-large-language-models" class="headerlink" title="1.Understanding large language models"></a>1.Understanding large language models</h1><blockquote>
<p>LLM，Large Language Models，大语言模型</p>
</blockquote>
<p>LLM 大语言模型（例如 ChatGPT）是一种基于深度学习的神经网络模型，主要用于自然语言处理（NLP）领域。在 LLM 出现之前，传统的 NLP 主要使用简单模型或者人工编写的规则，应用场景有限。大语言模型的出现翻开了新的篇章，它将模型对语言的理解、分析以及复杂任务的处理提升到了一个全新的高度。例如过往的模型是完全无法基于几个关键字，编写一篇意思通顺的文章，而这个任务对现在的大语言模型来说不过是小菜一碟。</p>
<p>大模型给出的答案语义连贯，逻辑清晰，但它输出的每个词，其实都是基于概率计算出来的，暂时还没看出来它具有类似人类的那种“自我意识”。</p>
<p>大模型的突破主要来自在 Transformer 注意力机制的发现，它构成了现在各种大模型的基础，同时配合海量的数据进行训练，这两方面的因素让大模型对人类语言的理解实现了质的飞跃。</p>
<h2 id="什么是大语言模型"><a href="#什么是大语言模型" class="headerlink" title="什么是大语言模型"></a>什么是大语言模型</h2><p>大模型之所以称为“大”，主要是因为模型中的参数非常多，以亿为单位，动不动就百亿、千亿级别。同时也指它训练的数据量很大。这些数据主要来源自过往几十年人类信息化后生产的各种电子文档，例如 wiki 网页、数字图书等。</p>
<p>大模型的基本原理很简单，就是基于句子的前半部分，根据概率大小，预测下一个单词应该是什么。简单的原理配合海量的数据和注意力机制进行训练后，最终得到的结果大大超出了研究人员的预期。模型出现了所谓的“涌现”，即模型对人类语言，当训练的数据量超过一个临界值后，不管是哪个国家的自然语言或者计算机语言，模型突然展示出了对各种语言的惊人理解能力。</p>
<p>由于大模型擅于生成内容，因此有时候也被人们称为 GenAI（生成式 AI），以下是它与各种人工智能概念的关系：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503100747391.png" srcset="/img/loading.gif" lazyload></p>
<p>深度学习是实现 AI 的主流算法，它通过设计训练和验证数据集，让模型自己反复学习和调整自身的参数，以提高预测的准确率。当准确达到某个标准后，即完成了训练。传统的编程方法由人类设计计算规则，深度学习则是给定结果，让模型自己猜测规则。只要猜测和调整的次数足够多，模型就会不断向正确答案靠拢。</p>
<p>但人工智能不只有深度学习一种实现方法，还有其他一些传统的方法，例如特征工程，这些方法在某些特定的应用场景中效果也很好，甚至能够解决一些深度学习算法无法解决的问题。</p>
<h2 id="大语言模型应用场景"><a href="#大语言模型应用场景" class="headerlink" title="大语言模型应用场景"></a>大语言模型应用场景</h2><p>应用场景有很多，例如机器翻译，文案生成，语义分析，摘要总结，代码生成，聊天机器人，虚拟助手，医学或法律等领域的知识检索（对文档进行筛选和总结，以便专业性的技术问题）。</p>
<p>LLM 大模型很可能将重新定义我们与技术的交互方式，让交互过程变得更加符合人类的直觉，同时更加易于使用。</p>
<h2 id="构建和使用大语言模型的步骤"><a href="#构建和使用大语言模型的步骤" class="headerlink" title="构建和使用大语言模型的步骤"></a>构建和使用大语言模型的步骤</h2><p>构建大模型主要由预训练和微调两个步骤组成。在预训练阶段，将给模型输入海量和多样化的数据，以便模型能够习得对各种自然语言的广泛理解。预训练后将得到一个基座模型，之后再基于特定领域的标注数据，对模型进行微调训练，这样模型能够有效提高模型在特定领域上的任务表现。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110634357.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>传统的监督学习方法在预训练阶段即需要使用人工标注数据，因此会有数据标注成本。但大模型的预训练刚好不需要进行人工标注，因为它是基于句子的前半部分去预测下一个单词，那么所有的输入数据，天然就是正确的预标注数据了，因此不需要额外的人工标注，设计非常巧妙。如果不是因为这一点，大模型的训练将是不可能的，因为数据量太庞大了，成本和时间都将是天文数字。</p>
</blockquote>
<h2 id="Transformer-架构介绍"><a href="#Transformer-架构介绍" class="headerlink" title="Transformer 架构介绍"></a>Transformer 架构介绍</h2><p>大部分 LLM 模型都是基于 Transformer 架构，该架构由 2017 年的论文 “Attention is all you need” 中首次提出。它的原理是针对句子中的各个单词给予不同的注意力权重，而不是一视同仁。因此每个单词对整个句子在语义维度的贡献程度是不一样的。通过使用注意力机制，能够让模型更准确的抓住句子的精髓部分。</p>
<p>最早的 Transformer 架构由编码器和解码器两部分组成，当时研究的场景是语言翻译，例如将英语翻译为德语。因此先使用编码器将源语言（如英语）编码成向量，然后再使用解码器，将向量编译成目标语言（如德语）；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110657249.png" srcset="/img/loading.gif" lazyload></p>
<p>它背后的思想大致为，虽然语言之间使用的单词和语法不同，但对于相同意思的句子，在经过注意力机制的筛选后，其对应的向量空间坐标值应该是相同的。经过实践证明，事实也确实如此。而且实验后发现，模型还可以进一步简化，编码器都不需要了，直接使用解码器就可以了，整个模型变得更加简洁和优雅了。</p>
<h2 id="利用大数据"><a href="#利用大数据" class="headerlink" title="利用大数据"></a>利用大数据</h2><p>训练大模型需要的数据量非常惊人，好处是整个模型变得非常通用，经过适当的微调后中，能够有效应对各种细分领域的工作任务。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110705615.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="深入-GPT-架构"><a href="#深入-GPT-架构" class="headerlink" title="深入 GPT 架构"></a>深入 GPT 架构</h2><p>GPT 架构最早在 OpenAI 公司的论文 “Improving Language Understanding by Generative Pre-Training” 中提出，它直译的意思是”生成预训练“，即训练模型，让它能够不断生成句子的下一个单词。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110710618.png" srcset="/img/loading.gif" lazyload></p>
<p>GPT 架构对未标注的数据进行自动标注和自监督学习，是一种自回归的模型，即将上一步的结果，当作生成下一步结果的输入参数，这种模式有助于提升输出的语义连贯性。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110714311.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="构建大语言模型"><a href="#构建大语言模型" class="headerlink" title="构建大语言模型"></a>构建大语言模型</h2><p>大模型的构建主要由几个步骤构成，分别如下：</p>
<ul>
<li>准备数据</li>
<li>使用注意力机制</li>
<li>设计模型的结构</li>
<li>反复迭代训练</li>
<li>模型评估</li>
<li>模型微调</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110723099.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="2-Working-with-text-data"><a href="#2-Working-with-text-data" class="headerlink" title="2.Working with text data"></a>2.Working with text data</h1><h2 id="理解嵌入"><a href="#理解嵌入" class="headerlink" title="理解嵌入"></a>理解嵌入</h2><p>深度学习模型，包括 LLM 大模型，实际上无法直接处理原始文本，因为原始文件无法直接用于数学运算，因此需要先将文本转换成数字，这个转换的过程叫做嵌入（embedding）；所谓的嵌入，就是将非标数据进行标准化的一个过程。</p>
<p>各种类型的数据都需要进行嵌入转换，例如文本、音频、视频等。嵌入一般由大模型的第一层进行处理，也可以由单独的模型进行处理。但不同模型的嵌入格式不同，因此 A 模型生成的嵌入数据，通常无法直接给 B 模型使用。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503110742937.png" srcset="/img/loading.gif" lazyload></p>
<p>本质上，嵌入就是将离散对象，例如文本、图片甚至整个文件，映射到一个连续的向量空间，以方便模型进行处理和计算。虽然最常使用的嵌入是以单词为单位，但其实也可以句子、段落、章节、文档为单位，常用于检索增加生成（Retrival Augmented Generation，简称RAG）场景，例如知识库。 </p>
<p>嵌入有很多种实现算法，例如 Word2Vec，它的原理是训练模型根据单词所在的上下文，生成单词的嵌入。它的思想为在相似上下文中出现的单词，其代表的意思更加相似。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503120727158.png" srcset="/img/loading.gif" lazyload></p>
<p>大模型通常会在输入层中包含嵌入功能，并且在训练过程中该层的参数会进行更新，以便对其不断优化。上面的示例图表只有两个维度，即嵌入的向量空间是二维的，能够表示的分类非常有限。真实的大模型其向量空间的维度是非常多的，例如 120M 个参数的 GPT-2 有 768 维，175B 个参数的 GPT-3 有 12288 维。</p>
<h2 id="文本转-token"><a href="#文本转-token" class="headerlink" title="文本转 token"></a>文本转 token</h2><p>token 主要由单词、标点符号、特殊符号等构成，因此第一步我们需要先将文本拆分（split）成各种单词和标点符号。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503120738154.png" srcset="/img/loading.gif" lazyload></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 下载示例文本 get_file.py</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>):<br>    url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt&quot;</span><br>    file_path = <span class="hljs-string">&quot;the-verdict.txt&quot;</span><br>    urllib.request.urlretrieve(url, file_path)<br></code></pre></td></tr></table></figure>

<h2 id="token-转-ID"><a href="#token-转-ID" class="headerlink" title="token 转 ID"></a>token 转 ID</h2><p>基于拆分后的单词和标点符号，创建一个字典，这样每个单词和标点符号都将为其分配一个 ID，之后我们就可以转 token 转成 ID 了，也可以将 ID 转成文本。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130712013.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    raw_text = f.read()<br>    preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, raw_text)<br>    preprocessed = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip()]<br>    all_words = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(preprocessed))<br>    vocab = &#123;token:integer <span class="hljs-keyword">for</span> integer, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(all_words)&#125;<br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130731717.png" srcset="/img/loading.gif" lazyload></p>
<p>将编码和解码的函数整合在一起，封装成一个对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tokenizer.py</span><br><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV1</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-variable language_">self</span>.str_to_int = vocab<br>        <span class="hljs-variable language_">self</span>.int_to_str = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> vocab.items()&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>        <br>        preprocessed = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip() != <span class="hljs-string">&#x27;&#x27;</span>]<br>        ids = [<span class="hljs-variable language_">self</span>.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([<span class="hljs-variable language_">self</span>.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        text = re.sub(<span class="hljs-string">r&#x27;\s([,.:;?_!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130738030.png" srcset="/img/loading.gif" lazyload></p>
<p>有了封装的对象后，获取文本 ID 就变得很简单了，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer = SimpleTokenizerV1(vocab)<br>text = <span class="hljs-string">&quot;Hello world&quot;</span><br>ids = tokenizer.encode(text)<br><span class="hljs-built_in">print</span>(ids)<br></code></pre></td></tr></table></figure>

<h2 id="添加特殊-token"><a href="#添加特殊-token" class="headerlink" title="添加特殊 token"></a>添加特殊 token</h2><p>特殊 token 通常有两个用途：</p>
<ul>
<li>处理不包括在字典中的未知单词或符号；</li>
<li>让模型对文本的理解变得更容易，例如显式的用一个特殊符号文档的开始和结束，这样模型不用自己判断；</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130743620.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503130743839.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 改进后的版本，以便能够处理未知单词或符号</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV2</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-variable language_">self</span>.str_to_int = vocab<br>        <span class="hljs-variable language_">self</span>.int_to_str = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> vocab.items()&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>        <br>        preprocessed = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip() != <span class="hljs-string">&#x27;&#x27;</span>]<br>        <span class="hljs-comment"># 将未知单词或符号替换为 &#x27;&lt;UNK&gt;&#x27;</span><br>        preprocessed = [item <span class="hljs-keyword">if</span> item <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.str_to_int <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed]<br>        ids = [<span class="hljs-variable language_">self</span>.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([<span class="hljs-variable language_">self</span>.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        text = re.sub(<span class="hljs-string">r&#x27;\s+([,.:;?_!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure>

<p>一些常用的特殊符号：</p>
<ul>
<li>[BOS]，beginning of sequence，序列起点；</li>
<li>[EOS]，end of sequence，序列终点；</li>
<li>[PAD]，padding，占位符（为提高训练速度，训练过程中，允许一次提交多个输入；由于不同句子长度不同，因此需要使用占位符，让不同句子拥有相同的长度，方便模型使用相同的函数进行计算）；</li>
</ul>
<h2 id="字节对编码"><a href="#字节对编码" class="headerlink" title="字节对编码"></a>字节对编码</h2><p>GPT 模型并没有使用 [unk] 符号来处理未知单词，而是使用 BPE 算法（byte pair encoding，字节对编码），将单词进一步拆分为更小的单位，例如子词（subword）或者字母；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503140714552.png" srcset="/img/loading.gif" lazyload></p>
<p>BPE 的工作过程如下：</p>
<ol>
<li>先将训练数据中的所有单词拆分成单个字符，并在单词的末尾添加结束符，以便能够区分不同的单词；这样就得到了初始的词汇表</li>
<li>统计相邻字符对的出现频率；例如单词 low，其中 lo 算一次；ow 也算一次；</li>
<li>根据统计结果，将出现频率最高的字符对，作为一个词汇，添加到词汇表中；</li>
<li>重复第2、3步，直到字符对的最高频率降为 1 或者某个预设的值；</li>
</ol>
<blockquote>
<p>这个方法通过递归的方式找出在各种文档中使用过一次以上的字符组合；</p>
</blockquote>
<p>BPE 的优点是将单词进一步拆分成字符对，有效提升了模型处理未知词汇的能力，以及模型的泛化能力。</p>
<h2 id="数据抽样"><a href="#数据抽样" class="headerlink" title="数据抽样"></a>数据抽样</h2><p>大模型的工作方式是基于句子的已知部分，预测下一个单词。因此，接下来的一个工作，便是生成句子（作为输入）+ 下一个单词（作为目标）的嵌入，作为训练用的数据。在生成嵌入时，会用到一个叫滑动窗口的方法，它跟图像模型的采样方法有点像，区别在于这里的滑动窗口的起始位置不变，示例如下：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503150942666.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tiktoken<br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    raw_text = f.read()<br><br><br>enc_txt = tokenizer.encode(raw_text)<br><span class="hljs-comment"># print(len(enc_txt))</span><br><br>enc_sample = enc_txt[<span class="hljs-number">50</span>:]<br><br>context_size = <span class="hljs-number">4</span> <span class="hljs-comment"># 滑动窗口大小</span><br>x = enc_sample[:context_size]<br>y = enc_sample[<span class="hljs-number">1</span>:context_size+<span class="hljs-number">1</span>]<br><span class="hljs-comment"># print(f&quot;X: &#123;x&#125;&quot;)</span><br><span class="hljs-comment"># print(f&quot;Y: &#123;y&#125;&quot;)</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, context_size+<span class="hljs-number">1</span>):<br>    context = enc_sample[:i]<br>    desired = enc_sample[i]<br>    <span class="hljs-comment"># print(context, &quot;-----&gt;&quot;, desired)</span><br>    <span class="hljs-comment"># print(tokenizer.decode(context), &quot;-----&gt;&quot;, tokenizer.decode([desired]))</span><br></code></pre></td></tr></table></figure>

<p>以上代码的输出如下：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170519238.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170521323.png" srcset="/img/loading.gif" lazyload></p>
<p>在将 token 转成嵌入之前，还有一项最后的工作，即将 token 转换成 tensor（张量，PyTorch 或 Tensorflow 等框架所使用的数据结构）</p>
<blockquote>
<p>tensor 通常以浮点数来表示，以下示例使用原始字符串表示主要是为了方便理解 tensor 所表示的内容；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170540115.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>理论上目标张量只需包含下一个待预测的单词即可，但是这里貌似为了让输出和输出的长度相等，除了待预测的目标单词外，还包含了前面的部分。因此输入和输出存在重叠；</p>
</blockquote>
<p>创建一个类，能够将输入文本，例如一本书，转换成“输入+输出”对，以便用于训练；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTDatasetV1</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, txt, tokenizer, max_length, stride</span>):<br>        <span class="hljs-variable language_">self</span>.input_ids = []<br>        <span class="hljs-variable language_">self</span>.target_ids = []<br><br>        <span class="hljs-comment"># 将 txt 转换成 token_ids</span><br>        token_ids = tokenizer.encode(txt)<br><br>        <span class="hljs-comment"># 使用滑动窗口的方法，按 max_length 将文本切分成多个样本</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(token_ids) - max_length, stride):<br>            input_chunk = token_ids[i : i + max_length]<br>            target_chunk = token_ids[i + <span class="hljs-number">1</span> : i + max_length + <span class="hljs-number">1</span>]<br>            <span class="hljs-variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))<br>            <span class="hljs-variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.input_ids)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.input_ids[idx], <span class="hljs-variable language_">self</span>.target_ids[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, data</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.tokenizer.encode(data)<br></code></pre></td></tr></table></figure>

<p>将“输入+输出”对进行分批。相对单个输入输出对，按批次处理有助于提高训练效率，缩短训练时间；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 基于 PyTorch 的 Dataloader，创建一个生成数据器的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataloader_v1</span>(<span class="hljs-params"></span><br><span class="hljs-params">    txt,</span><br><span class="hljs-params">    batch_size=<span class="hljs-number">4</span>,</span><br><span class="hljs-params">    max_length=<span class="hljs-number">256</span>,</span><br><span class="hljs-params">    stride=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">    shuffle=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    drop_last=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    num_workers=<span class="hljs-number">0</span>,</span><br><span class="hljs-params"></span>):<br>    tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)<br>    dataloader = DataLoader(<br>        dataset,<br>        batch_size=batch_size,<br>        shuffle=shuffle,<br>        drop_last=drop_last,<br>        num_workers=num_workers,<br>    )<br>    <span class="hljs-keyword">return</span> dataloader<br></code></pre></td></tr></table></figure>

<p>使用 create_dataloader_v1 创建一个数据加载器，加载小说 the-verdict.txt </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    raw_text = f.read()<br><br>dataloader = create_dataloader_v1(<br>    raw_text, batch_size=<span class="hljs-number">1</span>, max_length=<span class="hljs-number">4</span>, stride=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span><br>)<br>data_iter = <span class="hljs-built_in">iter</span>(dataloader)<br>first_batch = <span class="hljs-built_in">next</span>(data_iter)<br><span class="hljs-built_in">print</span>(first_batch)<br><br><span class="hljs-comment"># 输出如下（由于 max_length 设置为 4，所以长度只有 4 个token）</span><br><span class="hljs-comment"># [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]</span><br><br><span class="hljs-comment"># 实际训练时，max_length 通常是 256</span><br></code></pre></td></tr></table></figure>

<p>参数 stride &#x3D; 1 用来设置滑动窗口的滑动步伐，1 表示每次滑动一个 token；如果是 4，则每次滑动 4 个 token；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170617077.png" srcset="/img/loading.gif" lazyload></p>
<p>批量参数 batch_size 用来表示每个训练批次的数据量，1 表示每次只输入一个 token 对；4 表示每次输入 4 对；批量小的话，需要的内存也小，但在训练过程中，会带来更多的模型参数波动幅度。合适的批量的大小，需要通过反复实验进行权衡，最终设置一个合理的折中值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 当 batch_size = 8, stride = 4 的效果</span><br>dataloader = create_dataloader_v1(<br>    raw_text, batch_size=<span class="hljs-number">8</span>, max_length=<span class="hljs-number">4</span>, stride=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">False</span><br>)<br>data_iter = <span class="hljs-built_in">iter</span>(dataloader)<br>first_batch = <span class="hljs-built_in">next</span>(data_iter)<br><span class="hljs-built_in">print</span>(first_batch)<br><br><span class="hljs-comment"># [tensor([[   40,   367,  2885,  1464],</span><br><span class="hljs-comment">#         [ 1807,  3619,   402,   271],</span><br><span class="hljs-comment">#         [10899,  2138,   257,  7026],</span><br><span class="hljs-comment">#         [15632,   438,  2016,   257],</span><br><span class="hljs-comment">#         [  922,  5891,  1576,   438],</span><br><span class="hljs-comment">#         [  568,   340,   373,   645],</span><br><span class="hljs-comment">#         [ 1049,  5975,   284,   502],</span><br><span class="hljs-comment">#         [  284,  3285,   326,    11]]), </span><br><span class="hljs-comment">#  tensor([[  367,  2885,  1464,  1807],</span><br><span class="hljs-comment">#         [ 3619,   402,   271, 10899],</span><br><span class="hljs-comment">#         [ 2138,   257,  7026, 15632],</span><br><span class="hljs-comment">#         [  438,  2016,   257,   922],</span><br><span class="hljs-comment">#         [ 5891,  1576,   438,   568],</span><br><span class="hljs-comment">#         [  340,   373,   645,  1049],</span><br><span class="hljs-comment">#         [ 5975,   284,   502,   284],</span><br><span class="hljs-comment">#         [ 3285,   326,    11,   287]])]</span><br><br><span class="hljs-comment"># 当 max_length=4, stride=4 时，每个批次的 token 对之间没有重叠，这样做的好处是可以减少过拟合；</span><br></code></pre></td></tr></table></figure>

<h2 id="创建-token-嵌入"><a href="#创建-token-嵌入" class="headerlink" title="创建 token 嵌入"></a>创建 token 嵌入</h2><p>初始化嵌入层的权重参数。参数值一开始是随机的，但随着训练次数的增加，这些参数会慢慢收敛，并在某个值附近稳定下来。</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 假定词汇表总共只有 6 个单词（BPE 有 50257 个单词）</span><br>vocab_size = <span class="hljs-number">6</span><br><span class="hljs-comment"># 假设嵌入的尺寸为 3（GPT-3 的嵌入尺寸为 12288，尺寸越大，能够存放的参数个数越多，参数越多，模型就越能够记住这个 token 在多种不同场景下的使用方法）</span><br>output_dim = <span class="hljs-number">3</span> <br><br><br>torch.manual_seed(<span class="hljs-number">123</span>) <span class="hljs-comment"># 用于初始化随机数</span><br>embedding = torch.nn.Embedding(vocab_size, output_dim) <span class="hljs-comment"># 生成嵌入层</span><br><span class="hljs-built_in">print</span>(embedding.weight)<br><br><span class="hljs-comment"># 结果如下：</span><br><span class="hljs-comment"># tensor([[ 0.3374, -0.1778, -0.1690],</span><br><span class="hljs-comment">#         [ 0.9178,  1.5810,  1.3010],</span><br><span class="hljs-comment">#         [ 1.2753, -0.2010, -0.1606],</span><br><span class="hljs-comment">#         [-0.4015,  0.9666, -1.1481],</span><br><span class="hljs-comment">#         [-1.1589,  0.3255, -0.6315],</span><br><span class="hljs-comment">#         [-2.8400, -0.7849, -1.4096]], requires_grad=True</span><br><br><span class="hljs-comment"># 词汇表的 6 个token，对应此处的 6 行，每行代表一个token；3 列则对应嵌入的尺</span><br><br><span class="hljs-comment"># 例如取 token_id = 3，print(embedding(torch.tensor([3]))) 结果如下（对应前面的第4行）</span><br><span class="hljs-comment"># tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 当输入的 token_ids 为 [2, 3, 5, 1] 时</span><br>input_ids = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(embedding(input_ids))<br><br><span class="hljs-comment"># tensor([[ 1.2753, -0.2010, -0.1606],</span><br><span class="hljs-comment">#         [-0.4015,  0.9666, -1.1481],</span><br><span class="hljs-comment">#         [-2.8400, -0.7849, -1.4096],</span><br><span class="hljs-comment">#         [ 0.9178,  1.5810,  1.3010]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170713612.png" srcset="/img/loading.gif" lazyload alt="image-20250317071318473"></p>
<h2 id="编码单词位置"><a href="#编码单词位置" class="headerlink" title="编码单词位置"></a>编码单词位置</h2><p>按照上面的方法，每个 token_id 会映射一个 embeding 向量，但是这里有个小问题，即同一个 token_id，不管它出现在文本中的任意位置，它映射后的向量都是一模一样的，这意味着向量中没有包含位置信息。但在真实的语言中，同一个单词，出现在句子中的不同位置时，它的意思是有可能产生变化的。因此，我们需要将单词的位置信息，也添加到嵌入中作为参数，以便让模型能够学到的位置信息的相关知识。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170717866.png" srcset="/img/loading.gif" lazyload></p>
<p>位置有两种表示方法：</p>
<ul>
<li>绝对位置：每个位置用一个数值进行表示，然后添加到原嵌入上；(GPT 模型也是使用绝对位置，并会在训练过程中优化该参数值)；</li>
<li>相对位置：计算 token 之间的相对位置，即 token 之间的相对距离；该方法的优点是有助于模型学习到更好的泛化能力，在处理不同长度的句子时，效果更出色；</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503170756860.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        raw_text = f.read()<br><br>max_length = <span class="hljs-number">4</span> <span class="hljs-comment"># 句子长度为 4 个单词</span><br>batch_size = <span class="hljs-number">8</span> <span class="hljs-comment"># 每批 8 个句子</span><br>dataloader = create_dataloader_v1(<br>    raw_text, batch_size=batch_size, max_length=max_length, stride=max_length, shuffle=<span class="hljs-literal">False</span><br>)<br>data_iter = <span class="hljs-built_in">iter</span>(dataloader)<br>inputs, targets = <span class="hljs-built_in">next</span>(data_iter)<br><span class="hljs-comment"># print(&quot;Token IDs:\n&quot;, inputs)</span><br><span class="hljs-comment"># print(&quot;\nInputs shape:\n&quot;, inputs.shape)</span><br><br><span class="hljs-comment"># Token IDs: 每个批次 8 个句子，每个句子 4 个单词</span><br><span class="hljs-comment">#  tensor([[   40,   367,  2885,  1464],</span><br><span class="hljs-comment">#         [ 1807,  3619,   402,   271],</span><br><span class="hljs-comment">#         [10899,  2138,   257,  7026],</span><br><span class="hljs-comment">#         [15632,   438,  2016,   257],</span><br><span class="hljs-comment">#         [  922,  5891,  1576,   438],</span><br><span class="hljs-comment">#         [  568,   340,   373,   645],</span><br><span class="hljs-comment">#         [ 1049,  5975,   284,   502],</span><br><span class="hljs-comment">#         [  284,  3285,   326,    11]])</span><br><br><span class="hljs-comment"># Inputs shape:</span><br><span class="hljs-comment"># torch.Size([8, 4])</span><br><br><br>vocab_size = <span class="hljs-number">50257</span> <span class="hljs-comment"># 50257 个 token</span><br>output_dim = <span class="hljs-number">256</span> <span class="hljs-comment"># 每个 token 有 256 个参数，用来记住这个 token 的各种用法</span><br>token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)<br><br>token_embeddings = token_embedding_layer(inputs)<br><span class="hljs-built_in">print</span>(token_embeddings.shape)<br><span class="hljs-comment"># torch.Size([8, 4, 256]) 每个 token 有 256 个维度</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>模型需要将文本数据转化成数值类型的向量数据（即嵌入）；文本或图像原本是离散型的数据，通过构建词汇表以及分配 token id，它们被转换成了连续的向量空间中的数值，以便能够输入神经网络模型进行计算；</p>
</blockquote>
<h1 id="3-Coding-attention-mechanisms"><a href="#3-Coding-attention-mechanisms" class="headerlink" title="3.Coding attention mechanisms"></a>3.Coding attention mechanisms</h1><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190736390.png" srcset="/img/loading.gif" lazyload></p>
<p>注意力机制有四种不同的变体，一些变体建立在另外一些变体的基础上，它们分别是：</p>
<ul>
<li>简化的自注意力；</li>
<li>自注意力；</li>
<li>因果（掩码）注意力；</li>
<li>多头注意力（并行关注输入的不同维度）；</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190739048.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="模型化长序列的问题"><a href="#模型化长序列的问题" class="headerlink" title="模型化长序列的问题"></a>模型化长序列的问题</h2><p>在注意力机制出现之前，一般使用 encoder-decoder 机制来处理实现语言翻译，例如将德语翻译为英语。由于不同语言的语法结构不同，显然一个单词一个单词的直接翻译是不可行的。为解决这个剖，encoder-decoder 引入了一个中间层（hidden state）；这个中间有点像是一层嵌入，用来存储句子的核心思想。然后再使用 decoder 将核心思想翻译为目标语言。</p>
<p>RNN 是实现 encoder-decoder 机制的一种架构，它先是逐个单词进行编码，然后将编译后的结果，和下一个单词，作为下一次编码的输入，直到迭代出最终结果，放到 hidden state 隐藏层中。但问题是在迭代的过程中，隐藏层一直在变化，存储的是最新结果，这会导致在处理复杂的长文本时，尤其是相互依赖的内容间隔较远时，RNN 无法记住前面的上下文。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503190802208.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="通过注意力机制捕获数据依赖"><a href="#通过注意力机制捕获数据依赖" class="headerlink" title="通过注意力机制捕获数据依赖"></a>通过注意力机制捕获数据依赖</h2><p>为解决 RNN 因为无法获取前文，导致只因翻译短句的短板，引了一种新的注意力机制 Bahdanau；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503200652234.png" srcset="/img/loading.gif" lazyload></p>
<p>Bahdanau 机制引入几年后，人们意外的发现，其实貌似没有必要使用 encoder + decoder 的机制，直接单独使用 decoder 就够了；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503200655255.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="通过自注意力机制关注输入的不同部分"><a href="#通过自注意力机制关注输入的不同部分" class="headerlink" title="通过自注意力机制关注输入的不同部分"></a>通过自注意力机制关注输入的不同部分</h2><p>自注意力机制的核心，在于它能够识别一个输入序列中不同部分之前的相互关系和依赖，它通过计算权重来表示这种关系。传统的注意力机制则更多是关注两个序列的元素之间的关系（即传统的 encoder 和 decoder 机制）。</p>
<h3 id="不带训练参数的自注意力机制"><a href="#不带训练参数的自注意力机制" class="headerlink" title="不带训练参数的自注意力机制"></a>不带训练参数的自注意力机制</h3><p>计算目标：上下文向量（context vector），用该向量来存储某个 token 的上下文信息，所以也叫增强嵌入向量（enriched embedding vector）；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210734863.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>inputs = torch.tensor(<br>    [<br>        [<span class="hljs-number">0.43</span>, <span class="hljs-number">0.15</span>, <span class="hljs-number">0.89</span>],  <span class="hljs-comment"># Your    (x^1)</span><br>        [<span class="hljs-number">0.55</span>, <span class="hljs-number">0.87</span>, <span class="hljs-number">0.66</span>],  <span class="hljs-comment"># journey (x^2)</span><br>        [<span class="hljs-number">0.57</span>, <span class="hljs-number">0.85</span>, <span class="hljs-number">0.64</span>],  <span class="hljs-comment"># starts  (x^3)</span><br>        [<span class="hljs-number">0.22</span>, <span class="hljs-number">0.58</span>, <span class="hljs-number">0.33</span>],  <span class="hljs-comment"># with    (x^4)</span><br>        [<span class="hljs-number">0.77</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.10</span>],  <span class="hljs-comment"># one     (x^5)</span><br>        [<span class="hljs-number">0.05</span>, <span class="hljs-number">0.80</span>, <span class="hljs-number">0.55</span>],  <span class="hljs-comment"># step    (x^6)</span><br>    ]<br>)<br><br>query = inputs[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 第二个 token &quot;journey&quot;</span><br>attn_scores_2 = torch.empty(inputs.shape[<span class="hljs-number">0</span>])<br><span class="hljs-keyword">for</span> i, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>    attn_scores_2[i] = torch.dot(query, x_i)<br><span class="hljs-built_in">print</span>(attn_scores_2)  <br><br><span class="hljs-comment"># 结果：tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210746689.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算各个 token 注意力权重的占比，以便实现归一化</span><br>attn_weights_2_tmp = attn_scores_2 / attn_scores_2.<span class="hljs-built_in">sum</span>()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Attention weights:&quot;</span>, attn_weights_2_tmp)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Sum:&quot;</span>, attn_weights_2_tmp.<span class="hljs-built_in">sum</span>())<br><br><span class="hljs-comment"># Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])</span><br><span class="hljs-comment"># Sum: tensor(1.0000)</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503210805247.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 为避免过大值或过小值导致计算溢出，改用 PyTorch 内置的归一化函数</span><br>attn_weights_2 = torch.softmax(attn_scores_2, dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Attention weights:&quot;</span>, attn_weights_2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Sum:&quot;</span>, attn_weights_2.<span class="hljs-built_in">sum</span>())<br><span class="hljs-comment"># Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])</span><br><span class="hljs-comment"># Sum: tensor(1.)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 有了权重系数，接下来便是计算上下文向量 context vector</span><br>query = inputs[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 此处取第二个 token，原单词 journey 作为示例</span><br>context_vec_2 = torch.zeros(query.shape)<br><span class="hljs-keyword">for</span> i, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>    context_vec_2 += attn_weights_2[i] * x_i<br><span class="hljs-built_in">print</span>(context_vec_2)<br><span class="hljs-comment"># tensor([0.4419, 0.6515, 0.5683])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220620677.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="计算各输入-token-的注意力权重"><a href="#计算各输入-token-的注意力权重" class="headerlink" title="计算各输入 token 的注意力权重"></a>计算各输入 token 的注意力权重</h3><p>每个 token 相当于其他 token，都会有一个注意力权重系数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算各个 token 之间的注意力分数</span><br>attn_scores = torch.empty(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>)<br><span class="hljs-keyword">for</span> i, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>    <span class="hljs-keyword">for</span> j, x_j <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>        attn_scores[i, j] = torch.dot(x_i, x_j)<br><span class="hljs-built_in">print</span>(attn_scores)<br><br><span class="hljs-comment"># 由于 for 循环的计算性能较弱，以上计算可用优化过的矩阵乘法，计算结果相同</span><br>attn_scores = inputs @ inputs.T<br><span class="hljs-built_in">print</span>(attn_scores)<br><br><span class="hljs-comment"># tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],</span><br><span class="hljs-comment">#         [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],</span><br><span class="hljs-comment">#         [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],</span><br><span class="hljs-comment">#         [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],</span><br><span class="hljs-comment">#         [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],</span><br><span class="hljs-comment">#         [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220621687.png" srcset="/img/loading.gif" lazyload></p>
<p>整个计算过程由以下三步组成：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220636426.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第二步：计算权重系数，dim = -1 表示在最后一维进行归一化计算</span><br>attn_weights = torch.softmax(attn_scores, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights) <span class="hljs-comment"># 6 x 6 的矩阵</span><br><br><span class="hljs-comment"># tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],</span><br><span class="hljs-comment">#         [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],</span><br><span class="hljs-comment">#         [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],</span><br><span class="hljs-comment">#         [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],</span><br><span class="hljs-comment">#         [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],</span><br><span class="hljs-comment">#         [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第三步：计算上下文向量</span><br>all_context_vecs = attn_weights @ inputs<br><span class="hljs-built_in">print</span>(all_context_vecs)<br><br><span class="hljs-comment"># tensor([[0.4421, 0.5931, 0.5790],</span><br><span class="hljs-comment">#         [0.4419, 0.6515, 0.5683],</span><br><span class="hljs-comment">#         [0.4431, 0.6496, 0.5671],</span><br><span class="hljs-comment">#         [0.4304, 0.6298, 0.5510],</span><br><span class="hljs-comment">#         [0.4671, 0.5910, 0.5266],</span><br><span class="hljs-comment">#         [0.4177, 0.6503, 0.5645]])</span><br></code></pre></td></tr></table></figure>

<h2 id="用可训练系数实现自注意力机制"><a href="#用可训练系数实现自注意力机制" class="headerlink" title="用可训练系数实现自注意力机制"></a>用可训练系数实现自注意力机制</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220653964.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="计算注意力参数"><a href="#计算注意力参数" class="headerlink" title="计算注意力参数"></a>计算注意力参数</h3><p>引入三个可训练的注意力权重参数，它们分别是 W<sub>q</sub>，W<sub>k</sub>，W<sub>v</sub>，这三个参数用来将 token 映射成向量 query, key, value；</p>
<blockquote>
<p>有意思的是，input token 是三维的，但映射后 q, k, v 是二维的；但实际的 GPT 模型中，这三个维度通常是一样的。此处出于演示的目的，进行了简化，少了一维；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220705018.png" srcset="/img/loading.gif" lazyload></p>
<p>因为 W 参数是可训练的，所以初始值可先设置为一个随机值；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化训练参数</span><br>d_in = inputs.shape[<span class="hljs-number">1</span>] <span class="hljs-comment"># token 的嵌入此处是 3 维</span><br>d_out = <span class="hljs-number">2</span> <span class="hljs-comment"># 输出设置为 2 维</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br>W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br>W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算第二个 token 的 q，k，v</span><br>x_2 = inputs[<span class="hljs-number">1</span>]<br>query_2 = x_2 @ W_query<br>key_2 = x_2 @ W_key<br>value_2 = x_2 @ W_value<br><span class="hljs-built_in">print</span>(query_2)<br><span class="hljs-comment"># tensor([0.4306, 1.4551])</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>权重参数和注意力参数的区别，前者用于神经网络中的输入层和输出层之间的连接计算，它有点像是一个卷积核，是一种数据变换和信息提取的操作。在训练过程中会不断优化收敛，训练结束后，它的值便固定了；后者用于表示一个句子中，每个 token 跟其他 token 的相互关系。因此它则跟上下文有关。同一个 token，放在不同的上下文中，它跟其他 token 的注意力参数不同；每一次输入，注意力参数都是动态变化的，但权重参数是固定的；</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算所有的q,k,v</span><br>querys = inputs @ W_query<br>keys = inputs @ W_key<br>values = inputs @ W_value<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;querys.shape:&quot;</span>, querys.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;keys.shape:&quot;</span>, keys.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;values.shape:&quot;</span>, values.shape)<br><span class="hljs-comment"># querys.shape: torch.Size([6, 2])</span><br><span class="hljs-comment"># keys.shape: torch.Size([6, 2])</span><br><span class="hljs-comment"># values.shape: torch.Size([6, 2])</span><br></code></pre></td></tr></table></figure>

<p>在前面简化版的注意力机制中，注意力分数是由各个 token 向量相互计算得出的，没有实际的意义。此处改进后的版本则是先使用权重参数对 token 进行三个维度的转换，分别转成 query, key, value；然后再由 query * key 得到注意力参数；</p>
<blockquote>
<p>问：为什么 query * key 的结果可以作为注意力分数？</p>
<p>答：权重 W<sub>q</sub> 应该是对所有的 token 的一种抽象和转换。W<sub>k</sub> 貌似代表一个 token 在某种语境中的重要程度；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503220758390.png" srcset="/img/loading.gif" lazyload></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算第二个 token 的注意力分数，注意此处的 keys.T</span><br>attn_scores_2 = query_2 @ keys.T<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;attn_scores_2: &quot;</span>, attn_scores_2)<br><br><span class="hljs-comment"># 结果是一个 6 维的数组，它表示该 token 相对其他 6 个token 的注意力重要程度</span><br><span class="hljs-comment"># attn_scores_2:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])</span><br></code></pre></td></tr></table></figure>

<p>接下来可以将注意力分数归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">d_k = keys.shape[-<span class="hljs-number">1</span>] <span class="hljs-comment"># keys.shape = [6, 2]，此处取其参数维度 2</span><br><span class="hljs-comment"># 此处使用 softmax 进行权重归一化，但跟之前不同的点在于，额外除以 d_k 平方根，为什么呢？</span><br><span class="hljs-comment"># 答：该操作对归一化的点积值进行了缩放（缩小）。因为点积值过大的话，在反向传播的训练过程中，softmax 更像是阶梯函数，导致梯度接近于零，即没有平滑的过渡，这样会导致训练卡住，难以有效收敛；因此，自注意力机制也被叫做缩放点积注意力机制；</span><br>attn_weights_2 = torch.softmax(attn_scores_2 / d_k**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights_2)<br><span class="hljs-comment"># attn_weights_2:  tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503240635490.png" srcset="/img/loading.gif" lazyload></p>
<p>得到归一化的权重参数后，最后一步便是 value vector 乘以相应的权重参数，得到最终的上下文向量 context vector；</p>
<blockquote>
<p>query * key 归一化后得到 weight，weight * value 得到 context；（此处的 query, key, value 都是使用可训练参数对原 token 嵌入进行了提炼后的结果；但是暂时没有看到 token 位置信息在哪个环节纳入考虑和计算；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503240713634.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">context_vec_2 = attn_weights_2 @ values<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vec_2: &quot;</span>, context_vec_2)<br><span class="hljs-comment"># context_vec_2:  tensor([0.3061, 0.8210])</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>据说此处的 query, key, value 借鉴自数据库的概念，key 类似主键，value 类似值（记录），query 类似搜索查询；但我个人感觉不太像；</p>
</blockquote>
<h3 id="实现一个简单的自注意力-Python-类"><a href="#实现一个简单的自注意力-Python-类" class="headerlink" title="实现一个简单的自注意力 Python 类"></a>实现一个简单的自注意力 Python 类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 使用 torch.rand 生成随机数，特点：纯随机，因此训练时会需要更多的时间收敛</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention_V1</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.W_query = nn.Parameter(torch.rand(d_in, d_out))<br>        <span class="hljs-variable language_">self</span>.W_key = nn.Parameter(torch.rand(d_in, d_out))<br>        <span class="hljs-variable language_">self</span>.W_value = nn.Parameter(torch.rand(d_in, d_out))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        querys = inputs @ <span class="hljs-variable language_">self</span>.W_query<br>        keys = inputs @ <span class="hljs-variable language_">self</span>.W_key<br>        values = inputs @ <span class="hljs-variable language_">self</span>.W_value<br><br>        attn_scores = querys @ keys.T<br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br><br>        context_vecs = attn_weights @ values<br>        <span class="hljs-keyword">return</span> context_vecs<br><br><span class="hljs-comment"># 使用 nn.linear 生成随机数，特点：按一定规律随机，可加快训练收敛过程，避免一开始过于发散</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention_V2</span>(nn.module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        querys = <span class="hljs-variable language_">self</span>.W_query(inputs)<br>        keys = <span class="hljs-variable language_">self</span>.W_key(inputs)<br>        values = <span class="hljs-variable language_">self</span>.W_value(inputs)<br><br>        attn_scores = querys @ keys.T<br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br><br>        context_vecs = attn_weights @ values<br>        <span class="hljs-keyword">return</span> context_vecs<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">789</span>)<br>sa_v2 = SelfAttention_V2(d_in, d_out)<br><span class="hljs-built_in">print</span>(sa_v2(inputs))<br><br><span class="hljs-comment"># tensor([[-0.0739,  0.0713],</span><br><span class="hljs-comment">#         [-0.0748,  0.0703],</span><br><span class="hljs-comment">#         [-0.0749,  0.0702],</span><br><span class="hljs-comment">#         [-0.0760,  0.0685],</span><br><span class="hljs-comment">#         [-0.0763,  0.0679],</span><br><span class="hljs-comment">#         [-0.0754,  0.0693]], grad_fn=&lt;MmBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250649329.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="通过因果注意力机制屏蔽下一个单词"><a href="#通过因果注意力机制屏蔽下一个单词" class="headerlink" title="通过因果注意力机制屏蔽下一个单词"></a>通过因果注意力机制屏蔽下一个单词</h2><p>前面提到的自注意力机制，在计算上下文向量时，是基于整个完整的句子进行计算的。但在实际训练时，需要基于已知的部分内容进行计算，以推测下一个可能的单词，因此需要隐藏部分内容，并对已知内容的权重参数重新做归一化计算。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250623811.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250634159.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 SelfAttention_V2 计算权重参数</span><br>queries = sa_v2.W_query(inputs)<br>keys = sa_v2.W_key(inputs)<br>attn_scores = queries @ keys.T<br>attn_weights = torch.softmax(attn_scores / d_out**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights)<br><br><span class="hljs-comment"># tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],</span><br><span class="hljs-comment">#         [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],</span><br><span class="hljs-comment">#         [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],</span><br><span class="hljs-comment">#         [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],</span><br><span class="hljs-comment">#         [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],</span><br><span class="hljs-comment">#         [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span><br><span class="hljs-comment">#        grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 PyTorch 自带的 tril 函数生成下三角矩阵</span><br>context_length = attn_scores.shape[<span class="hljs-number">0</span>]<br>mask_simple = torch.tril(torch.ones(context_length, context_length))<br><span class="hljs-built_in">print</span>(mask_simple)<br><span class="hljs-comment"># tensor([[1., 0., 0., 0., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 0., 0., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 0., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 1., 0., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 1., 1., 0.],</span><br><span class="hljs-comment">#         [1., 1., 1., 1., 1., 1.]])</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 权重参数 * 下三角矩阵得到掩模后的结果</span><br>masked_simple = attn_weights * mask_simple<br><span class="hljs-built_in">print</span>(masked_simple)<br><span class="hljs-comment"># tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],</span><br><span class="hljs-comment">#         [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span><br><span class="hljs-comment">#        grad_fn=&lt;MulBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对掩模结果重新归一化</span><br>row_sums = masked_simple.<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>masked_simple_norm = masked_simple / row_sums<br><span class="hljs-built_in">print</span>(masked_simple_norm)<br><span class="hljs-comment"># tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],</span><br><span class="hljs-comment">#         [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span><br><span class="hljs-comment">#        grad_fn=&lt;DivBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<p>前述的几个步骤，包含了两次的归一化，事实上可以合并成一次归一化，提高运算速度。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503250711998.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = torch.triu(torch.ones(context_length, context_length), diagonal=<span class="hljs-number">1</span>)<br>masked = attn_scores.masked_fill(mask.<span class="hljs-built_in">bool</span>(), -torch.inf)<br><span class="hljs-built_in">print</span>(masked)<br><span class="hljs-comment"># tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],</span><br><span class="hljs-comment">#         [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],</span><br><span class="hljs-comment">#         [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],</span><br><span class="hljs-comment">#         [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],</span><br><span class="hljs-comment">#         [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],</span><br><span class="hljs-comment">#         [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],</span><br><span class="hljs-comment">#        grad_fn=&lt;MaskedFillBackward0&gt;)</span><br><br>attn_weights = torch.softmax(masked / keys.shape[-<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>, dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights)<br><span class="hljs-comment"># tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],</span><br><span class="hljs-comment">#         [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span><br><span class="hljs-comment">#        grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<h3 id="引入-dropout-避免过拟合"><a href="#引入-dropout-避免过拟合" class="headerlink" title="引入 dropout 避免过拟合"></a>引入 dropout 避免过拟合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br>dropout = torch.nn.Dropout(<span class="hljs-number">0.5</span>)  <span class="hljs-comment"># 50% dropout, 实际训练时一般设置为 0.1 或 0.2</span><br>example = torch.ones(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>)<br><span class="hljs-built_in">print</span>(dropout(example))<br><span class="hljs-comment"># tensor([[2., 2., 2., 2., 2., 2.],</span><br><span class="hljs-comment">#         [0., 2., 0., 0., 0., 0.],</span><br><span class="hljs-comment">#         [0., 0., 2., 0., 2., 0.],</span><br><span class="hljs-comment">#         [2., 2., 0., 0., 0., 2.],</span><br><span class="hljs-comment">#         [2., 0., 0., 0., 0., 2.],</span><br><span class="hljs-comment">#         [0., 2., 0., 0., 0., 0.]])</span><br><span class="hljs-comment"># 由于 dropout 50%，因此有一半的值被置为 0, 一半的值被放大为 2, 以保持合计值不变</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br><span class="hljs-built_in">print</span>(dropout(attn_weights))<br><span class="hljs-comment"># tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],</span><br><span class="hljs-comment">#        grad_fn=&lt;MulBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<h3 id="实现一个紧凑的-Causal-Attention-类"><a href="#实现一个紧凑的-Causal-Attention-类" class="headerlink" title="实现一个紧凑的  Causal Attention 类"></a>实现一个紧凑的  Causal Attention 类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.d_out = d_out<br>        <span class="hljs-variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br>        <span class="hljs-comment"># register_buffer 会将缓存数据自动移动到 CPU 或 GPU 上, 无需再手工检查</span><br>        <span class="hljs-variable language_">self</span>.register_buffer(<br>            <span class="hljs-string">&quot;mask&quot;</span>, torch.triu(torch.ones(context_length, context_length), diagonal=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        b, num_tokens, d_in = inputs.shape  <span class="hljs-comment"># batch size, number of tokens, input dim</span><br>        queries = <span class="hljs-variable language_">self</span>.W_query(inputs)<br>        keys = <span class="hljs-variable language_">self</span>.W_key(inputs)<br>        values = <span class="hljs-variable language_">self</span>.W_value(inputs)<br><br>        attn_scores = queries @ keys.transpose(<br>            <span class="hljs-number">1</span>, <span class="hljs-number">2</span><br>        )  <span class="hljs-comment"># 只转置最后两个维度, 保持第一个维度不变, 因为第一个维度是 batch size</span><br>        attn_scores = attn_scores.masked_fill_(<br>            <span class="hljs-variable language_">self</span>.mask.<span class="hljs-built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf<br>        )<br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br>        attn_weights = <span class="hljs-variable language_">self</span>.dropout(attn_weights)<br>        context_vecs = attn_weights @ values<br>        <span class="hljs-keyword">return</span> context_vecs<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 CausalAttention 来计算输入的上下文向量</span><br>batch = torch.stack((inputs, inputs), dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(batch.shape)<br><span class="hljs-comment"># torch.Size([2, 6, 3])</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>context_length = batch.shape[<span class="hljs-number">1</span>]<br>ca = CausalAttention(d_in, d_out, context_length, <span class="hljs-number">0.0</span>)<br>context_vecs = ca(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape&quot;</span>, context_vecs.shape)<br><span class="hljs-comment"># context_vecs.shape torch.Size([2, 6, 2])</span><br></code></pre></td></tr></table></figure>

<h2 id="将单头注意力扩展为多头注意力"><a href="#将单头注意力扩展为多头注意力" class="headerlink" title="将单头注意力扩展为多头注意力"></a>将单头注意力扩展为多头注意力</h2><p>一套权重参数对应的 Causal Attension 相当于一个单头注意力（single head attention，单个专家），我们可以训练多套权重参数，这样就可以获得多头注意力 multi-head attention（多个专家）。不同专家关注输入数据的不同维度，类似于使用不同的视角来看数据。</p>
<blockquote>
<p>貌似多头注意力有可能和多模态场景结合起来？但又好像有些区别，多头注意力是同一份数据，进行不同的解读；多模态是不同的数据，解读后整合各自的解读结果，作为下一步行动的依据；</p>
</blockquote>
<h3 id="叠加多个单头注意力"><a href="#叠加多个单头注意力" class="headerlink" title="叠加多个单头注意力"></a>叠加多个单头注意力</h3><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503270623437.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个能够聚合多个单头注意力的类</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> CausalAttention <span class="hljs-keyword">import</span> CausalAttention<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttentionWrapper</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.heads = nn.ModuleList(<br>            [<br>                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)<br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads)<br>            ]<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> torch.cat([head(inputs) <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.heads], dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多头类使用示例</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>context_length = batch.shape[<span class="hljs-number">1</span>]<br>d_in, d_out = <span class="hljs-number">3</span>, <span class="hljs-number">2</span><br>mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, <span class="hljs-number">0.0</span>, num_heads=<span class="hljs-number">2</span>)<br>context_vecs = mha(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape&quot;</span>, context_vecs.shape)<br><span class="hljs-built_in">print</span>(context_vecs)<br><span class="hljs-comment"># context_vecs.shape torch.Size([2, 6, 4]) # context_vec 的输出是 2 维，但由于有2个单头叠加，所以是 4 维</span><br><span class="hljs-comment"># tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],</span><br><span class="hljs-comment">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span><br><span class="hljs-comment">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span><br><span class="hljs-comment">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span><br><span class="hljs-comment">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span><br><span class="hljs-comment">#          [-0.5299, -0.1081,  0.5077,  0.3493]],</span><br><br><span class="hljs-comment">#         [[-0.4519,  0.2216,  0.4772,  0.1063],</span><br><span class="hljs-comment">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span><br><span class="hljs-comment">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span><br><span class="hljs-comment">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span><br><span class="hljs-comment">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span><br><span class="hljs-comment">#          [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<h3 id="用-weight-split-实现多头注意力"><a href="#用-weight-split-实现多头注意力" class="headerlink" title="用 weight split 实现多头注意力"></a>用 weight split 实现多头注意力</h3><p>MultiHeadAttentionWrapper 使用 for 循环来叠加多个单头，由于 for 循环是线性计算，因此可考虑使用矩阵的平行计算来提升性能。另外，还可以将 MultiHeadAttentionWrapper 和 CausalAttention 整合到一起，创建一个单独的 MultiHeadAttention 类；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503270735653.png" srcset="/img/loading.gif" lazyload></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> d_out % num_heads == <span class="hljs-number">0</span>  <span class="hljs-comment"># 确保 d_out 可以被 num_heads 整除</span><br>        <span class="hljs-variable language_">self</span>.d_out = d_out<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-variable language_">self</span>.head_dim = d_out // num_heads<br>        <span class="hljs-variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<br>            <span class="hljs-string">&quot;mask&quot;</span>, torch.triu(torch.ones(context_length, context_length), diagonal=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        b, num_tokens, d_in = inputs.shape<br>        queries = <span class="hljs-variable language_">self</span>.W_query(inputs)<br>        keys = <span class="hljs-variable language_">self</span>.W_key(inputs)<br>        values = <span class="hljs-variable language_">self</span>.W_value(inputs)<br>        <span class="hljs-comment"># 将 queries, keys, values 拆分为 num_heads 份</span><br>        queries = queries.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br>        keys = keys.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br>        values = values.view(b, num_tokens, <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim)<br>        <span class="hljs-comment"># 转置 (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span><br>        queries = queries.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        keys = keys.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        values = values.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        attn_scores = queries @ keys.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>        mask_bool = <span class="hljs-variable language_">self</span>.mask.<span class="hljs-built_in">bool</span>()[:num_tokens, :num_tokens]<br><br>        attn_scores.maksed_fill_(mask_bool, -torch.inf)<br><br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br>        attn_weights = <span class="hljs-variable language_">self</span>.dropout(attn_weights)<br><br>        context_vec = (attn_weights @ values).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 使用 view 方法 reshape: self.d_out = self.num_heads * self.head_dim</span><br>        <span class="hljs-comment"># contiguous 方法可调整数据在内存上存储的顺序，与逻辑顺序保持一致，以提高后续的计算性能</span><br>        context_vec = context_vec.contiguous().view(b, num_tokens, <span class="hljs-variable language_">self</span>.d_out)<br><br>        context_vec = <span class="hljs-variable language_">self</span>.out_proj(context_vec)  <span class="hljs-comment"># 输出映射（非必须）</span><br>        <span class="hljs-keyword">return</span> context_vec<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 MultiHeadAttention 计算 context_vecs</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>batch_size, context_length, d_in = batch.shape<br>d_out = <span class="hljs-number">2</span><br>ha = MultiHeadAttention(d_in, d_out, context_length, <span class="hljs-number">0.0</span>, num_heads=<span class="hljs-number">2</span>)<br>context_vecs = mha(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape:&quot;</span>, context_vecs.shape)<br><span class="hljs-built_in">print</span>(context_vecs)<br><span class="hljs-comment"># context_vecs.shape: torch.Size([2, 6, 4])</span><br><span class="hljs-comment"># tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],</span><br><span class="hljs-comment">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span><br><span class="hljs-comment">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span><br><span class="hljs-comment">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span><br><span class="hljs-comment">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span><br><span class="hljs-comment">#          [-0.5299, -0.1081,  0.5077,  0.3493]],</span><br><br><span class="hljs-comment">#         [[-0.4519,  0.2216,  0.4772,  0.1063],</span><br><span class="hljs-comment">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span><br><span class="hljs-comment">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span><br><span class="hljs-comment">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span><br><span class="hljs-comment">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span><br><span class="hljs-comment">#          [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>以上示例中的 MultiHeadAttention，嵌入向量 为 3 维，有 2 个 Head；在实际的 LLM 大模型中，维度和 Head 数量要多得多的多。例如最小的 GPT-2 模型，有 12 个 Head，嵌入向量为 768 维，总共 1.2 亿个参数；最大的 GPT-2 模型，有 25 个 Head，嵌入向量有 1600 维，总共有 15 亿个参数。</p>
</blockquote>
<h1 id="4-Implementing-a-GPT-model-from-scratch-to-generate-text"><a href="#4-Implementing-a-GPT-model-from-scratch-to-generate-text" class="headerlink" title="4.Implementing a GPT model from scratch to generate text"></a>4.Implementing a GPT model from scratch to generate text</h1><p>前面一章主要关注如何实现自注意力机制，这一章则主要关注构建 LLM 架构余下的部分。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310658487.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="编写-LLM-架构"><a href="#编写-LLM-架构" class="headerlink" title="编写 LLM 架构"></a>编写 LLM 架构</h3><p>LLM 架构的核心之一是 Transformer 模块，该模块包含前面实现的掩码多头注意力机制（Masked Multi-Head Attention）；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310708068.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化模型的超参数</span><br>GPT_CONFIG_124M = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,  <span class="hljs-comment"># Vocabulary size，词汇表大小</span><br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,  <span class="hljs-comment"># Context length，上下文长度</span><br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>,  <span class="hljs-comment"># Embedding dimension，嵌入向量的维度</span><br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>,  <span class="hljs-comment"># Number of heads，专家数量</span><br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>,  <span class="hljs-comment"># Number of layers，层数</span><br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.1</span>,  <span class="hljs-comment"># Dropout rate，随机失活率</span><br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">False</span>,  <span class="hljs-comment"># Query/Key/Value bias，是否开启QKV偏置（目的：增强模型灵活性，避免全部依赖权重参数；开启后，会对 QKV 的计算结果进行偏移</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>先搭一个骨架，然后将各个模块组合在一起。其中一个 Transformer 模块由四个子模块组成。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202503310736036.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 预定义一个简单的Transformer Block, 之后会被真正的 TransformerBlock 替换</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyTransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 预定义一个简单的LayerNorm, 之后会被真正的 LayerNorm 替换</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyLayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, normlized_shape, eps=<span class="hljs-number">1e-5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyGPTModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># token embedding 和 position embedding</span><br>        <span class="hljs-variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;context_length&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-comment"># dropout layer</span><br>        <span class="hljs-variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br>        <span class="hljs-comment"># 预定义的 transformer block（此时每个 transformer 都一样，估计与实际的 GPT2 不同）</span><br>        <span class="hljs-variable language_">self</span>.trf_blocks = nn.Sequential(<br>            *[DummyTransformerBlock(cfg) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cfg[<span class="hljs-string">&quot;n_layers&quot;</span>])]<br>        )<br>        <span class="hljs-comment"># 预定义的最终 LayerNorm</span><br>        <span class="hljs-variable language_">self</span>.final_norm = DummyLayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.out_head = nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, in_idx</span>):<br>        batch_size, seq_len = in_idx.shape<br>        tok_embeds = <span class="hljs-variable language_">self</span>.tok_emb(in_idx)<br>        pos_embeds = <span class="hljs-variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))<br>        x = tok_embeds + pos_embeds<br>        x = <span class="hljs-variable language_">self</span>.drop_emb(x)<br>        x = <span class="hljs-variable language_">self</span>.trf_blocks(x)<br>        x = <span class="hljs-variable language_">self</span>.final_norm(x)<br>        logits = <span class="hljs-variable language_">self</span>.out_head(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504010719065.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tiktoken<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> DummyGPTModel <span class="hljs-keyword">import</span> DummyGPTModel<br><br>GPT_CONFIG_124M = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,  <span class="hljs-comment"># Vocabulary size</span><br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,  <span class="hljs-comment"># Context length</span><br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>,  <span class="hljs-comment"># Embedding dimension</span><br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>,  <span class="hljs-comment"># Number of heads</span><br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>,  <span class="hljs-comment"># Number of layers</span><br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.1</span>,  <span class="hljs-comment"># Dropout rate</span><br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">False</span>,  <span class="hljs-comment"># Query, Key, Value bias</span><br>&#125;<br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>batch = []<br>txt1 = <span class="hljs-string">&quot;Every effort moves you&quot;</span><br>txt2 = <span class="hljs-string">&quot;Every day holds a&quot;</span><br><br>batch.append(torch.tensor(tokenizer.encode(txt1)))<br>batch.append(torch.tensor(tokenizer.encode(txt2)))<br>batch = torch.stack(batch, dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(batch)<br><span class="hljs-comment"># tensor([[6109, 3626, 6100,  345],</span><br><span class="hljs-comment">#         [6109, 1110, 6622,  257]])</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model = DummyGPTModel(GPT_CONFIG_124M)<br>logits = model(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, logits.shape)<br><span class="hljs-built_in">print</span>(logits)<br><span class="hljs-comment"># 2 个句子，每个句子 4 个 token，每个 token 有 50257 个维度，刚好对应了 GPT2 的词表大小，之后会经过 softmax 处理，得到每个 token 的概率，最终选择概率最大的 token 作为下一个 token</span><br><span class="hljs-comment"># Output shape: torch.Size([2, 4, 50257])</span><br><span class="hljs-comment"># tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],</span><br><span class="hljs-comment">#          [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],</span><br><span class="hljs-comment">#          [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],</span><br><span class="hljs-comment">#          [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],</span><br><br><span class="hljs-comment">#         [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],</span><br><span class="hljs-comment">#          [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],</span><br><span class="hljs-comment">#          [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],</span><br><span class="hljs-comment">#          [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],</span><br><span class="hljs-comment">#        grad_fn=&lt;UnsafeViewBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<h3 id="使用归一层对激活值进行归一化"><a href="#使用归一层对激活值进行归一化" class="headerlink" title="使用归一层对激活值进行归一化"></a>使用归一层对激活值进行归一化</h3><p>深度神经网络由很多层构成，但层数一多，训练过程中很容易出现梯度消失或者梯度爆炸问题，导致训练结果无法收敛。常见的解决方案是对输出结果进行归一化，以提高训练效率和稳定性。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504010754996.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 添加归一化层的示例</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>batch_example = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>)<br>layer = nn.Sequential(nn.Linear(<span class="hljs-number">5</span>, <span class="hljs-number">6</span>), nn.ReLU())  <span class="hljs-comment"># ReLU 负责将负数置零</span><br>out = layer(batch_example)<br><span class="hljs-built_in">print</span>(out)<br><span class="hljs-comment"># tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],</span><br><span class="hljs-comment">#         [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],</span><br><span class="hljs-comment">#        grad_fn=&lt;ReluBackward0&gt;)</span><br>mean = out.mean(<br>    dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span><br>)  <span class="hljs-comment"># dim=-1 表示最后一个维度，keepdim=True 表示保持整体维度，而不是压缩成一维</span><br>var = out.var(<br>    dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span><br>)  <span class="hljs-comment"># var 表示方差，即各点到均值的距离平方的平均值（如果再开方就是标准差）</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Mean:&quot;</span>, mean)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Variance:&quot;</span>, var)<br><span class="hljs-comment"># Mean: tensor([[0.1324],</span><br><span class="hljs-comment">#         [0.2170]], grad_fn=&lt;MeanBackward1&gt;)</span><br><span class="hljs-comment"># Variance: tensor([[0.0231],</span><br><span class="hljs-comment">#         [0.0398]], grad_fn=&lt;VarBackward0&gt;)</span><br><br><br>out_norm = (out - mean) / torch.sqrt(var)  <span class="hljs-comment"># 归一化，减去均值除以标准差</span><br>mean = out_norm.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>var = out_norm.var(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;out_norm:&quot;</span>, out_norm)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mean:&quot;</span>, mean)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;var:&quot;</span>, var)<br><span class="hljs-comment"># out_norm: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],</span><br><span class="hljs-comment">#         [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],</span><br><span class="hljs-comment">#        grad_fn=&lt;DivBackward0&gt;)</span><br><span class="hljs-comment"># mean: tensor([[9.9341e-09],</span><br><span class="hljs-comment">#         [5.9605e-08]], grad_fn=&lt;MeanBackward1&gt;)</span><br><span class="hljs-comment"># var: tensor([[1.0000],</span><br><span class="hljs-comment">#         [1.0000]], grad_fn=&lt;VarBackward0&gt;)</span><br>torch.set_printoptions(sci_mode=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># 关闭科学计数法</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mean:&quot;</span>, mean)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;var:&quot;</span>, var)<br><span class="hljs-comment"># mean: tensor([[    0.0000],</span><br><span class="hljs-comment">#         [    0.0000]], grad_fn=&lt;MeanBackward1&gt;)</span><br><span class="hljs-comment"># var: tensor([[1.0000],</span><br><span class="hljs-comment">#         [1.0000]], grad_fn=&lt;VarBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用类来封装归一化层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, emb_dim</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.eps = <span class="hljs-number">1e-5</span><br>        <span class="hljs-variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))<br>        <span class="hljs-variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        mean = x.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        var = x.var(<br>            dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>, unbiased=<span class="hljs-literal">False</span><br>        )  <span class="hljs-comment"># unbiased=False 表示方差计算公式为 1/N * sum((x - mean)^2)，否则为 1/(N-1) * sum((x - mean)^2)，因为 embedding 的维度通常很大，所以使用常规的 1/N-1 没有太大影响，但为了和 GPT2 保持一致，这里使用了 1/N 的方式, 也就是 unbiased=False</span><br>        norm_x = (x - mean) / torch.sqrt(var + <span class="hljs-variable language_">self</span>.eps)  <span class="hljs-comment"># 加上 eps 防止分母为 0</span><br>        <span class="hljs-keyword">return</span> (<br>            norm_x * <span class="hljs-variable language_">self</span>.scale + <span class="hljs-variable language_">self</span>.shift<br>        )  <span class="hljs-comment"># scale 和 shift 为可学习参数, 跟图像处理中的缩放和平移很点像</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># LayerNorm 类的使用示例</span><br>ln = LayerNorm(emb_dim=<span class="hljs-number">5</span>)<br>out_ln = ln(batch_example)<br>mean = out_ln.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>var = out_ln.var(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>, unbiased=<span class="hljs-literal">False</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Mean:\n&quot;</span>, mean)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Variance:\n&quot;</span>, var)<br><span class="hljs-comment"># Mean:</span><br><span class="hljs-comment"># tensor([[    -0.0000],</span><br><span class="hljs-comment">#         [     0.0000]], grad_fn=&lt;MeanBackward1&gt;)</span><br><span class="hljs-comment"># Variance:</span><br><span class="hljs-comment"># tensor([[1.0000],</span><br><span class="hljs-comment">#         [1.0000]], grad_fn=&lt;VarBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<blockquote>
<p>相比传统的 batch normalization，使用 layer normalization 的好处是单独对特征维度（最后一维）进行归一化计算，可以不用管 batch 维度，这样在分布式计算中更方便；</p>
</blockquote>
<p>至此已经完成 backbone 和归一化层，接下来将实现 GELU 激活层；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504020736917.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="使用-GELU-激活函数实现前馈网络"><a href="#使用-GELU-激活函数实现前馈网络" class="headerlink" title="使用 GELU 激活函数实现前馈网络"></a>使用 GELU 激活函数实现前馈网络</h3><blockquote>
<p>问：为什么要使用前馈网络？</p>
<p>答：在得到归一化的结果后，需要对结果进行梳理和筛选，淘汰不合格的值，保留合格的值，以供下一步计算使用。此时常用所谓的激活函数来实现梳理和筛选。它会让离散的结果值，变得更加平滑，这样有利于计算值与值之间的变化梯度。进而可以根据变化梯度，判断权重参数的调整幅度；</p>
</blockquote>
<p>传统的深度学习方法中，经常使用 ReLU 作为实现非线性计算的激活函数。但是它在某些场景中过于简单，无法取得最好的效果。因此，在 LLM 中引入了 GELU（Gaussian error linear unit，高斯误差线性单元，它使用基于高斯分布即正态分布的累积分布函数，来实现更平滑的非线性计算，而不是像 ReLU 基于简单的阈值）；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504020743900.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 matplotlib 用图表展示 GELU 更加平滑的过渡效果</span><br><span class="hljs-keyword">from</span> DummyGPTModel <span class="hljs-keyword">import</span> GELU<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>gelu, relu = GELU(), nn.ReLU()<br>x = torch.linspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">100</span>)  <span class="hljs-comment"># 创建 100 个点，范围从 -3 到 3</span><br>y_gelu, y_relu = gelu(x), relu(x)<br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">3</span>))<br><span class="hljs-keyword">for</span> i, (y, label) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>([y_gelu, y_relu], [<span class="hljs-string">&quot;GELU&quot;</span>, <span class="hljs-string">&quot;ReLU&quot;</span>]), <span class="hljs-number">1</span>):<br>    plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, i)<br>    plt.plot(x, y)<br>    plt.title(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;label&#125;</span> activation function&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;x&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;label&#125;</span>(x)&quot;</span>)<br>    plt.grid(<span class="hljs-literal">True</span>)<br><br>plt.tight_layout<br>plt.show()<br></code></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030601160.png" srcset="/img/loading.gif" lazyload></p>
<p>从图上可见 GELU 在有些位置会出现负数，虽然负数的值不大，但它的存在，能够让模型在训练过程中捕获到更细微的参数变化，从而让训练结果的收敛变得更加容易。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现一个简单的前馈网络，它由三个层构成，分别是两个线性层夹一个 GELU 激活层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.Sequential(<br>            nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>] * <span class="hljs-number">4</span>),<br>            GELU(),<br>            nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>] * <span class="hljs-number">4</span>, cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>]),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.layers(x)<br><br>ffn = FeedForward(GPT_CONFIG_124M)<br>x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">768</span>)<br>out = ffn(x)<br><span class="hljs-built_in">print</span>(out.shape)<br><span class="hljs-comment"># torch.Size([2, 3, 768])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030616589.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030618948.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>输入层的每个 token 原本是 768 维，然后线性层将其扩大 4 倍，映射为 768 * 4 &#x3D; 3072 维，据说这么做是为了带来更丰富的表达空间，有待观察其实际效果；</p>
</blockquote>
<p>至此已实现 Transformer block 里面的 GELU 激活层和前馈网络层（示例发下），下一步开始添加快捷连接。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030647367.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="添加跳跃连接"><a href="#添加跳跃连接" class="headerlink" title="添加跳跃连接"></a>添加跳跃连接</h3><p>Shortcut connections，跳跃连接，或者叫残差连接。它的目标是为了解决随着神经网络的深度增加，在反向逐级传播的过程中，出现梯度消失的问题，它的核心思想源自于计算机视觉中的残差网络（residual network，即大名鼎鼎的 ResNet）。其关键在于将输入重新加回到结果值中，这样一来，神经网络实际学习的并不是数据的映射，而是基于原始数据，进行一定程度的平移变换，这样可以保留原来的梯度；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030701369.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构建一个由 5 个 layer 组成的简单神经网络模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleDeepNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, layer_sizes, use_shorcut</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.use_shorcut = use_shorcut<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList(<br>            [<br>                nn.Sequential(<br>                    nn.Linear(layer_sizes[<span class="hljs-number">0</span>], layer_sizes[<span class="hljs-number">1</span>]),<br>                    nn.GELU(),<br>                ),<br>                nn.Sequential(<br>                    nn.Linear(layer_sizes[<span class="hljs-number">1</span>], layer_sizes[<span class="hljs-number">2</span>]),<br>                    nn.GELU(),<br>                ),<br>                nn.Sequential(<br>                    nn.Linear(layer_sizes[<span class="hljs-number">2</span>], layer_sizes[<span class="hljs-number">3</span>]),<br>                    nn.GELU(),<br>                ),<br>                nn.Sequential(<br>                    nn.Linear(layer_sizes[<span class="hljs-number">3</span>], layer_sizes[<span class="hljs-number">4</span>]),<br>                    nn.GELU(),<br>                ),<br>                nn.Sequential(<br>                    nn.Linear(layer_sizes[<span class="hljs-number">4</span>], layer_sizes[<span class="hljs-number">5</span>]),<br>                    nn.GELU(),<br>                ),<br>            ]<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers:<br>            layer_output = layer(x)  <span class="hljs-comment"># 计算当前层的输出</span><br>            <span class="hljs-comment"># 如果输出和输入的维度一样，且 use_shorcut 为 True，则使用跳跃连接</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_shorcut <span class="hljs-keyword">and</span> x.shape == layer_output.shape:<br>                x = x + layer_output<br>            <span class="hljs-keyword">else</span>:<br>                x = layer_output<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ExampleDeepNeuralNetwork 使用示例</span><br>layer_sizes = [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>]<br>sample_input = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>, -<span class="hljs-number">1.0</span>]])  <span class="hljs-comment"># 模拟一个简单的输入</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shorcut=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_gradients</span>(<span class="hljs-params">model, x</span>):<br>    output = model(x)<br>    target = torch.tensor([[<span class="hljs-number">0.0</span>]])  <span class="hljs-comment"># 为简单起见，用 0 作为目标值</span><br>    loss = nn.MSELoss()  <span class="hljs-comment"># 初始化损失函数</span><br>    loss = loss(output, target)  <span class="hljs-comment"># 比较输出和目标值间的差距（即损失）</span><br>    loss.backward()  <span class="hljs-comment"># 反向传播，计算损失的梯度</span><br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;weight&quot;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;name&#125;</span> has gradient mean of <span class="hljs-subst">&#123;param.grad.<span class="hljs-built_in">abs</span>().mean().item()&#125;</span>&quot;</span>)<br><br>print_gradients(model_without_shortcut, sample_input)<br><span class="hljs-comment"># 从下面打印结果可见，随着网络层数的增加，反向传播时，梯度越来越小，逐渐消失</span><br><span class="hljs-comment"># layers.0.0.weight has gradient mean of 0.0002017411752603948</span><br><span class="hljs-comment"># layers.1.0.weight has gradient mean of 0.00012011770741082728</span><br><span class="hljs-comment"># layers.2.0.weight has gradient mean of 0.0007152437465265393</span><br><span class="hljs-comment"># layers.3.0.weight has gradient mean of 0.0013988513965159655</span><br><span class="hljs-comment"># layers.4.0.weight has gradient mean of 0.005049604922533035</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shorcut=<span class="hljs-literal">True</span>)<br>print_gradients(model_with_shortcut, sample_input)<br><span class="hljs-comment"># 从下面打印结果可见，使用跳跃连接后，梯度不会消失，保持稳定</span><br><span class="hljs-comment"># layers.0.0.weight has gradient mean of 0.22186797857284546</span><br><span class="hljs-comment"># layers.1.0.weight has gradient mean of 0.207092747092247</span><br><span class="hljs-comment"># layers.2.0.weight has gradient mean of 0.32923877239227295</span><br><span class="hljs-comment"># layers.3.0.weight has gradient mean of 0.2667771875858307</span><br><span class="hljs-comment"># layers.4.0.weight has gradient mean of 1.3268063068389893</span><br></code></pre></td></tr></table></figure>

<h3 id="在-Transformer-模块中连接注意力层和线性层"><a href="#在-Transformer-模块中连接注意力层和线性层" class="headerlink" title="在 Transformer 模块中连接注意力层和线性层"></a>在 Transformer 模块中连接注意力层和线性层</h3><p>一个 GPT 模型由多个 Transformer 模块组成，而一个 Transformer 模块由以下几个部分组成，它们分别是：</p>
<ul>
<li>Multi-head attention，多头注意力</li>
<li>Normalization layer，归一化</li>
<li>Dropout，随机失活</li>
<li>Feed Forward layer，前向传播</li>
<li>GELU activation，激活函数</li>
</ul>
<p>其中自注意力和多头注意力负责分析输入的各个 token 之间的相互关系，Feed Forward 则负责对单个 token 的数据进行转换。二者结合起来，不仅有助于模型更好的理解和处理输入数据，也有助于找到背后的规律。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504030801842.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个 Transformer 类，它主要由 MultiHeadAttention 和 FeedForward 两部分构成</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerBlock</span>(nn.Modlue):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.att = MultiHeadAttention(<br>            d_in=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            d_out=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            context_length=cfg[<span class="hljs-string">&quot;context_length&quot;</span>],<br>            num_heads=cfg[<span class="hljs-string">&quot;num_heads&quot;</span>],<br>            dropout=cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>],<br>            qkv_bias=cfg[<span class="hljs-string">&quot;qkv_bias&quot;</span>],<br>        )<br>        <span class="hljs-variable language_">self</span>.ff = FeedForward(cfg)<br>        <span class="hljs-variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        <span class="hljs-variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># forward 由两部分构成，先使用多头注意力计算 token 间的相互关系，然后用 FeedForward 进行数据转换（激活），为下一轮计算做准备</span><br>        shortcut = x<br>        x = <span class="hljs-variable language_">self</span>.norm1(x)<br>        x = <span class="hljs-variable language_">self</span>.att(x)  <span class="hljs-comment"># att 即 MultiHeadAttention</span><br>        x = <span class="hljs-variable language_">self</span>.drop_shortcut(x)<br>        x = x + shortcut  <span class="hljs-comment"># 跳跃连接</span><br><br>        shortcut = x<br>        x = <span class="hljs-variable language_">self</span>.norm2(x)<br>        x = <span class="hljs-variable language_">self</span>.ff(x)  <span class="hljs-comment"># ff 即 FeedForward</span><br>        x = <span class="hljs-variable language_">self</span>.drop_shortcut(x)<br>        x = x + shortcut<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># TransformerBlock 使用示例</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">768</span>)<br>block = TransformerBlock(GPT_CONFIG_124M)<br>output = block(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><span class="hljs-comment"># Input shape: torch.Size([2, 4, 768])</span><br><span class="hljs-comment"># Output shape: torch.Size([2, 4, 768])</span><br></code></pre></td></tr></table></figure>

<p>从 Input shape 和 Output shape 可以看出输入和输出的 shape 保持不变，这并非意外，而是有意为之，以便可以在每一层 layer 的计算过程中，保持高效；虽然 shape 不变，但输出的张量已经包含了上下文信息。至此一个完整的 Transformer Block 组件已经形成。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504100800737.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="编写-GPT-模型"><a href="#编写-GPT-模型" class="headerlink" title="编写 GPT 模型"></a>编写 GPT 模型</h3><p>GPT-2 模型的结构如下，分别包含：</p>
<ul>
<li>token 转嵌入层 + 添加位置信息嵌入层；</li>
<li>12 个 Transformer 模块（每个模块有四层，叠加后有 48 层）；</li>
<li>最终归一化层  + 线性输出层；</li>
</ul>
<p>最终结果是一个 shape 为 [4, 50257] 的张量（形状跟输入一样）；模型的最终目前是基于该张量，得到下一个最大概率的 token 是什么。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504110654926.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="生成文本"><a href="#生成文本" class="headerlink" title="生成文本"></a>生成文本</h3><p>最后一步是将模型生成的张量 [batch_size, num_token, vocab_size] 转换成目标语言的文本（例如英文或者中文）。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140711884.png" srcset="/img/loading.gif" lazyload></p>
<p>过程其实非常简单，输出的张量的最后一行，即是基于输入中的最后一个单词与下一个最有可能出现的单词之间的概率值，此时只需取出该行，然后找出最大概率的那个值的索引。然后根据索引，从词汇表里面找到对应的单词即可。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140716015.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_text_simple</span>(<span class="hljs-params">model, idx, max_new_tokens, context_size</span>):<br>    <span class="hljs-comment"># max_new_tokens 用来控制生成的文本长度</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        idx_cond = idx[:, -context_size:]  <span class="hljs-comment"># 根据上下文的长度，截取上下文（从后往前截）</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(idx_cond)<br>        logits = logits[:, -<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># 取最后一个 token 的 logits</span><br>        probas = torch.softmax(<br>            logits, dim=-<span class="hljs-number">1</span><br>        )  <span class="hljs-comment"># 对 logits 进行 softmax 处理，得到每个 token 的概率，但其实这步有点多余，因为最大值不会变，直接计算最大值的索引就可以了</span><br>        idx_next = torch.argmax(probas, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        idx = torch.cat(<br>            (idx, idx_next), dim=-<span class="hljs-number">1</span><br>        )  <span class="hljs-comment"># 将新生成的 token 拼接到 idx 的后面， 为下一轮生成做准备</span><br>    <span class="hljs-keyword">return</span> idx<br><br><br>start_context = <span class="hljs-string">&quot;Hello, I am&quot;</span><br>encoded = tokenizer.encode(start_context)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;encoded:&quot;</span>, encoded)<br>encoded_tensor = torch.tensor(encoded).unsqueeze(<br>    <span class="hljs-number">0</span><br>)  <span class="hljs-comment"># 增加一个维度，变成 (1, 6),模拟批量处理，此时 batch_size=1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;encoded_tensor.shape:&quot;</span>, encoded_tensor.shape)<br><span class="hljs-comment"># encoded: [15496, 11, 314, 716]</span><br><span class="hljs-comment"># encoded_tensor.shape: torch.Size([1, 4])</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504140749483.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 文本生成示例</span><br>model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 将模型设置为评估模式，以便关闭 dropout(训练时才需要开启 dropout)</span><br>out = generate_text_simple(<br>    model=model,<br>    idx=encoded_tensor,<br>    max_new_tokens=<span class="hljs-number">6</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output:&quot;</span>, out)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Ootput length:&quot;</span>, <span class="hljs-built_in">len</span>(out[<span class="hljs-number">0</span>]))<br><span class="hljs-comment"># Output: tensor([[15496,    11,   314,   716, 27018,  7283, 46275, 41426, 33167, 33239]])</span><br><span class="hljs-comment"># Ootput length: 10</span><br><br>decoded_text = tokenizer.decode(out.squeeze(<span class="hljs-number">0</span>).tolist())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Decoded text:&quot;</span>, decoded_text)<br><span class="hljs-comment"># Decoded text: Hello, I am Feature IT snowballProtect youngstersMu</span><br><span class="hljs-comment"># Feature IT snowballProtect youngstersMu 是生成的内容，由于参数还未训练过，所以此时生成的答案约等于胡言乱语</span><br></code></pre></td></tr></table></figure>

<h1 id="5-Pretraining-on-unlabeled-data"><a href="#5-Pretraining-on-unlabeled-data" class="headerlink" title="5.Pretraining on unlabeled data"></a>5.Pretraining on unlabeled data</h1><p>接下来将使用数据集，对模型参数进行训练，并选择合适的指标，对模型训练后的质量进行评估。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504150707074.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="评估文本生成模型"><a href="#评估文本生成模型" class="headerlink" title="评估文本生成模型"></a>评估文本生成模型</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504150715928.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="使用-GPT-生成文本"><a href="#使用-GPT-生成文本" class="headerlink" title="使用 GPT 生成文本"></a>使用 GPT 生成文本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-keyword">from</span> GPTModel <span class="hljs-keyword">import</span> GPTModel, generate_text_simple<br><br>GPT_CONFIG_124M = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">256</span>,  <span class="hljs-comment"># 上下文长度小一些，不然笔记本训练时容易卡死</span><br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>,<br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">False</span>,<br>&#125;<br>torch.manual_seed<br>model = GPTModel(GPT_CONFIG_124M)<br>model.<span class="hljs-built_in">eval</span>()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">text_to_token_ids</span>(<span class="hljs-params">text, tokenizer</span>):<br>    encoded = tokenizer.encode(text, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br>    encoded_tensor = torch.tensor(encoded).unsqueeze(<br>        <span class="hljs-number">0</span><br>    )  <span class="hljs-comment"># 增加一个维度，模拟批量处理，此时 batch_size=1</span><br>    <span class="hljs-keyword">return</span> encoded_tensor<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">token_ids_to_text</span>(<span class="hljs-params">token_ids, tokenizer</span>):<br>    flat = token_ids.squeeze(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> tokenizer.decode(flat.tolist())<br><br><br>start_context = <span class="hljs-string">&quot;Every effort moves you&quot;</span><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>token_ids = generate_text_simple(<br>    model=model,<br>    idx=text_to_token_ids(start_context, tokenizer),<br>    max_new_tokens=<span class="hljs-number">10</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Output text:</span><br><span class="hljs-comment">#  Every effort moves you icatorsthirdwenSave Black energies nationality retreating!!!!uz</span><br><span class="hljs-comment"># 由于还未训练，所以此时模型生成的文本暂时没有意义</span><br></code></pre></td></tr></table></figure>

<h3 id="计算文本生成损失"><a href="#计算文本生成损失" class="headerlink" title="计算文本生成损失"></a>计算文本生成损失</h3><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504160720141.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入，为简单示例起见，上下文长度为3，即句子只有 3 个单词</span><br>inputs = torch.tensor(<br>    [<br>        [<span class="hljs-number">16833</span>, <span class="hljs-number">3626</span>, <span class="hljs-number">6100</span>],  <span class="hljs-comment"># [&quot;Every&quot;, &quot;effort&quot;, &quot;moves&quot;]</span><br>        [<span class="hljs-number">40</span>, <span class="hljs-number">1107</span>, <span class="hljs-number">588</span>],  <span class="hljs-comment"># [&quot;I&quot;, &quot;really&quot;, &quot;like&quot;]</span><br>    ]<br>)<br><br><span class="hljs-comment"># 目标输出，可用于计算实际输出和目标输出之间的损失</span><br>targets = torch.tensor(<br>    [<br>        [<span class="hljs-number">3626</span>, <span class="hljs-number">6100</span>, <span class="hljs-number">345</span>],  <span class="hljs-comment"># [&quot;effort&quot;, &quot;moves&quot;, &quot;you&quot;]</span><br>        [<span class="hljs-number">1107</span>, <span class="hljs-number">588</span>, <span class="hljs-number">11311</span>],  <span class="hljs-comment"># [&quot;really&quot;, &quot;like&quot;, &quot;chocolate&quot;]</span><br>    ]<br>)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    logits = model(inputs)<br>probas = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;probas.shape&quot;</span>, probas.shape)<br><span class="hljs-comment"># probas.shape torch.Size([2, 3, 50257])</span><br><br>token_ids = torch.argmax(probas, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Token IDs:\n&quot;</span>, token_ids)<br><span class="hljs-comment"># Token IDs:</span><br><span class="hljs-comment">#  tensor([[[44689],</span><br><span class="hljs-comment">#          [12200],</span><br><span class="hljs-comment">#          [ 3288]],</span><br><br><span class="hljs-comment">#         [[38526],</span><br><span class="hljs-comment">#          [19969],</span><br><span class="hljs-comment">#          [44901]]])</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Targets batch 1: <span class="hljs-subst">&#123;token_ids_to_text(targets[<span class="hljs-number">0</span>], tokenizer)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Outputs batch 1: <span class="hljs-subst">&#123;token_ids_to_text(token_ids[<span class="hljs-number">0</span>].flatten(), tokenizer)&#125;</span>&quot;</span>)<br><span class="hljs-comment"># 期望的输出</span><br><span class="hljs-comment"># Targets batch 1:  effort moves you</span><br><span class="hljs-comment"># 目前实际的输出，肉眼可见毫不相关</span><br><span class="hljs-comment"># Outputs batch 1:  remem nativesillery</span><br></code></pre></td></tr></table></figure>

<p>接下来将设计一个函数，用来计算实际输出和目标输出之间的差距，然后基于该差距，调整权重参数值。由于目标是预测下一个单词，那么只要 softmax 得到的最大概率值索引，刚好对应词汇表中的预期单词的索引，就算成功了。这个概率值越大，理论上来说，模型预测的准确性越高。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504160758463.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 目标输出的 token 目前在该批次中的概率值，训练的目标，就是让该概率值相对最大化</span><br>text_idx = <span class="hljs-number">0</span><br>target_probas_1 = probas[text_idx, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], targets[text_idx]]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Text 1: &quot;</span>, target_probas_1)<br><span class="hljs-comment"># Text 1:  tensor([3.7347e-05, 3.9809e-05, 2.0112e-05])</span><br><br>text_idx = <span class="hljs-number">1</span><br>target_probas_2 = probas[text_idx, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], targets[text_idx]]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Text 2:&quot;</span>, target_probas_2)<br><span class="hljs-comment"># Text 2: tensor([1.6814e-05, 1.6684e-05, 1.2155e-05])</span><br><br><span class="hljs-comment"># torch.log 用于计算张量中每个元素的自然对数值，即 ln(x)</span><br><span class="hljs-comment"># 相对于直接使用原始概率值进行计算，将其转换成对数后，更方便后续的损失计算处理(计算结果更稳定，避免出现)</span><br>log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;log_probas:&quot;</span>, log_probas)<br><span class="hljs-comment"># log_probas: tensor([-10.0696, -11.0272, -12.2895, -10.4855, -10.3192, -11.6028])</span><br><br><span class="hljs-comment"># 训练的目标，便是让平均值尽量接近 0</span><br>avg_log_probas = torch.mean(log_probas)<br><span class="hljs-built_in">print</span>(avg_log_probas)<br><span class="hljs-comment"># tensor(-11.4343)</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits shape:&quot;</span>, logits.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Targets shape:&quot;</span>, targets.shape)<br><span class="hljs-comment"># Logits shape: torch.Size([2, 3, 50257])</span><br><span class="hljs-comment"># Targets shape: torch.Size([2, 3])</span><br><br>logits_flat = logits.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 只 flat 前两个维度</span><br>targets_flat = targets.flatten()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Flattened logits:&quot;</span>, logits_flat.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Flattened targets:&quot;</span>, targets_flat.shape)<br><span class="hljs-comment"># Flattened logits: torch.Size([6, 50257])</span><br><span class="hljs-comment"># Flattened targets: torch.Size([6])</span><br><br><span class="hljs-comment"># 通过交叉熵计算损失</span><br><span class="hljs-comment"># logits_flat 有 50257 的维度，此处 cross_entropy 函数做了好几个动作，包括：</span><br><span class="hljs-comment"># 根据 target tokenId 索引从 50257 个元素中取到对应的概率值，然后 log 它并计算平均值</span><br>loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)<br><span class="hljs-built_in">print</span>(loss)<br><span class="hljs-comment"># tensor(11.1071)</span><br><br></code></pre></td></tr></table></figure>

<blockquote>
<p>损失函数的计算结果有两种表示形式，一种是交叉熵，还有一种是交叉熵的对数形式，称为 Perplexity（困惑度），它的取值范围在 1 到词汇表大小 N 之间。1 表示结果只有一个单词，没有困惑。50 表示有 50  个候选单词。N 表示词汇表中的所有单词都有可能是候选结果，相当于随机了。</p>
</blockquote>
<h3 id="计算训练集和验证集的损失"><a href="#计算训练集和验证集的损失" class="headerlink" title="计算训练集和验证集的损失"></a>计算训练集和验证集的损失</h3><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504221140198.png" srcset="/img/loading.gif" lazyload></p>
<p>为了方便在有限的空间内进行展示，在以下示意图中，训练的数据集最大长度 max_length 为 6（实际上前面演示的 GPT_CONFIG_124M 用的是 256）; 加载的数据分成两部分，大部分做为训练数据集，小部分做为验证数据集；</p>
<blockquote>
<p>此处将长度设置为固定值进行训练，但实际上也可以作为变量进行训练。这样的好处是可以让模型从文本中学习到更泛化的理解能力；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504220959894.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 the-verdict 文本，对进行训练，训练前，先设计好损失计算的函数</span><br>file_path = <span class="hljs-string">&quot;the-verdict.txt&quot;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> file:<br>    text_data = file.read()<br><br>total_characters = <span class="hljs-built_in">len</span>(text_data)<br>total_tokens = <span class="hljs-built_in">len</span>(tokenizer.encode(text_data))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Characters:&quot;</span>, total_characters)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tokens:&quot;</span>, total_tokens)<br><span class="hljs-comment"># Characters: 20479</span><br><span class="hljs-comment"># Tokens: 5145</span><br><br>train_ratio = <span class="hljs-number">0.90</span><br>split_idx = <span class="hljs-built_in">int</span>(train_ratio * <span class="hljs-built_in">len</span>(text_data))<br>train_data = text_data[:split_idx]<br>val_data = text_data[split_idx:]<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>train_loader = create_dataloader_v1(<br>    train_data,<br>    batch_size=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 此处的 batch size 非常小，实际上训练时取 1024 甚至更大并不罕见</span><br>    max_length=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    stride=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    drop_last=<span class="hljs-literal">True</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">0</span>,<br>)<br>val_loader = create_dataloader_v1(<br>    val_data,<br>    batch_size=<span class="hljs-number">2</span>,<br>    max_length=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],  <span class="hljs-comment"># context_length 为 256</span><br>    stride=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    drop_last=<span class="hljs-literal">True</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">0</span>,<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Train loader:&quot;</span>)<br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_loader:<br>    <span class="hljs-built_in">print</span>(x.shape, y.shape)<br><span class="hljs-comment"># Train loader:</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Validation loader:&quot;</span>)<br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> val_loader:<br>    <span class="hljs-built_in">print</span>(x.shape, y.shape)<br><span class="hljs-comment"># Validation loader:</span><br><span class="hljs-comment"># torch.Size([2, 256]) torch.Size([2, 256])</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_batch</span>(<span class="hljs-params">input_batch, target_batch, model, device</span>):<br>    input_batch = input_batch.to(device)<br>    target_batch = target_batch.to(device)<br>    logits = model(input_batch)<br>    loss = torch.nn.functional.cross_entropy(<br>        logits.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), target_batch.flatten()<br>    )<br>    <span class="hljs-keyword">return</span> loss<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_loader</span>(<span class="hljs-params">data_loader, model, device, num_batches=<span class="hljs-literal">None</span></span>):<br>    total_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(data_loader)) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;nan&quot;</span>)<br>    <span class="hljs-keyword">elif</span> num_batches <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        num_batches = <span class="hljs-built_in">len</span>(data_loader)<br>    <span class="hljs-keyword">else</span>:<br>        num_batches = <span class="hljs-built_in">min</span>(num_batches, <span class="hljs-built_in">len</span>(data_loader))<br>    <span class="hljs-keyword">for</span> i, (input_batch, target_batch) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> i &lt; num_batches:<br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            total_loss += loss.item()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> total_loss / num_batches  <span class="hljs-comment"># 所有批次的总平均损失</span><br><br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model.to(device)<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    train_loss = calc_loss_loader(train_loader, model, device)<br>    val_loss = calc_loss_loader(val_loader, model, device)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training loss:&quot;</span>, train_loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Validation loss:&quot;</span>, val_loss)<br><span class="hljs-comment"># Training loss: 10.99226008521186</span><br><span class="hljs-comment"># Validation loss: 10.980721473693848</span><br></code></pre></td></tr></table></figure>

<p>有了损失计算函数后，那一步就可以开始正式的训练了；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504221141181.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="训练大模型"><a href="#训练大模型" class="headerlink" title="训练大模型"></a>训练大模型</h2><p>训练主要由以下几个步骤组成：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504230957036.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_model</span>(<span class="hljs-params">model, train_loader, val_loader, device, eval_iter</span>):<br>    model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 切换为评估模式，用于临时关闭 dropout，以便得到准确的评估结果</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 临时关闭梯度计算，节省内存和计算资源</span><br>        train_loss = calc_loss_loader(<br>            train_loader, model, device, num_batches=eval_iter<br>        )<br>        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)<br>    model.train()  <span class="hljs-comment"># 恢复训练模式</span><br>    <span class="hljs-keyword">return</span> train_loss, val_loss<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_and_print_sample</span>(<span class="hljs-params">model, tokenizer, device, start_context</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    context_size = model.pos_emb.weight.shape[<span class="hljs-number">0</span>]<br>    encoded = text_to_token_ids(start_context, tokenizer).to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        token_ids = generate_text_simple(<br>            model=model, idx=encoded, max_new_tokens=<span class="hljs-number">50</span>, context_size=context_size<br>        )<br>    decoded_text = token_ids_to_text(token_ids, tokenizer)<br>    <span class="hljs-built_in">print</span>(decoded_text.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot; &quot;</span>))<br>    model.train()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model_simple</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    train_loader,</span><br><span class="hljs-params">    val_loader,</span><br><span class="hljs-params">    optimizer,</span><br><span class="hljs-params">    device,</span><br><span class="hljs-params">    num_epochs,</span><br><span class="hljs-params">    eval_freq,</span><br><span class="hljs-params">    eval_iter,</span><br><span class="hljs-params">    start_context,</span><br><span class="hljs-params">    tokenizer,</span><br><span class="hljs-params"></span>):<br>    train_losses, val_lossed, track_tokens_seen = [], [], []<br>    tokens_seen, global_step = <span class="hljs-number">0</span>, -<span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        model.train()  <span class="hljs-comment"># 强制进入训练模式，避免其他地方不小心将其设置为评估模式</span><br>        <span class="hljs-keyword">for</span> input_batch, target_batch <span class="hljs-keyword">in</span> train_loader:<br>            optimizer.zero_grad()  <span class="hljs-comment"># 清空上一步遗留的梯度值</span><br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            loss.backward()  <span class="hljs-comment"># 反向传播，计算当前梯度值</span><br>            optimizer.step()  <span class="hljs-comment"># 使用梯度值，更新模型参数</span><br>            tokens_seen += input_batch.numel()<br>            global_step += <span class="hljs-number">1</span><br><br>            <span class="hljs-keyword">if</span> global_step % eval_freq == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每 eval_freq 步评估一次</span><br>                train_loss, val_loss = evaluate_model(<br>                    model, train_loader, val_loader, device, eval_iter<br>                )<br>                train_losses.append(train_loss)<br>                val_lossed.append(val_loss)<br>                track_tokens_seen.append(tokens_seen)<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;Ep <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span> (Step <span class="hljs-subst">&#123;global_step:06d&#125;</span>):&quot;</span><br>                    <span class="hljs-string">f&quot; Train loss <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>, &quot;</span><br>                    <span class="hljs-string">f&quot;Val loss <span class="hljs-subst">&#123;val_loss:<span class="hljs-number">.3</span>f&#125;</span>,&quot;</span><br>                )<br><br>        generate_and_print_sample(model, tokenizer, device, start_context)<br>    <span class="hljs-keyword">return</span> train_losses, val_lossed, track_tokens_seen<br><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model = GPTModel(GPT_CONFIG_124M)<br>model.to(device)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.0004</span>, weight_decay=<span class="hljs-number">0.1</span>)<br>num_epochs = <span class="hljs-number">10</span><br>train_losses, val_lossed, tokens_seen = train_model_simple(<br>    model,<br>    train_loader,<br>    val_loader,<br>    optimizer,<br>    device,<br>    num_epochs=num_epochs,<br>    eval_freq=<span class="hljs-number">5</span>,<br>    eval_iter=<span class="hljs-number">5</span>,<br>    start_context=<span class="hljs-string">&quot;Every effort moves you&quot;</span>,<br>    tokenizer=tokenizer,<br>)<br><span class="hljs-comment"># Ep 1 (Step 000000): Train loss 9.781, Val loss 9.923,</span><br><span class="hljs-comment"># Ep 1 (Step 000005): Train loss 8.057, Val loss 8.332,</span><br><span class="hljs-comment"># Every effort moves you,.</span><br><span class="hljs-comment"># Ep 2 (Step 000010): Train loss 6.763, Val loss 7.044,</span><br><span class="hljs-comment"># Ep 2 (Step 000015): Train loss 6.146, Val loss 6.628,</span><br><span class="hljs-comment"># Every effort moves you, and, and, and, and, and, and, and, and,, and,, and, and, and, and, and, and, and,, and, and, and, and, and, and,, and</span><br><span class="hljs-comment"># Ep 3 (Step 000020): Train loss 13.849, Val loss 14.409,</span><br><span class="hljs-comment"># Ep 3 (Step 000025): Train loss 5.536, Val loss 6.441,</span><br><span class="hljs-comment"># Every effort moves you, and to to to the to to the to the to the to the to the to the to the to the</span><br><span class="hljs-comment"># Ep 4 (Step 000030): Train loss 5.181, Val loss 6.360,</span><br><span class="hljs-comment"># Ep 4 (Step 000035): Train loss 5.026, Val loss 6.373,</span><br><span class="hljs-comment"># Every effort moves you of the picture to the picture to the picture to the picture to the picture to the picture to the picture to the picture to the picture to the the picture to the picture to the my to the picture to the picture to the of the picture to the</span><br><span class="hljs-comment"># Ep 5 (Step 000040): Train loss 4.689, Val loss 6.335,</span><br><span class="hljs-comment"># Every effort moves you know it was not to have to have to have to have to have to have to have--and, and I was, and I had been the picture--as Jack himself, and I had been to have to have to have to have to have</span><br><span class="hljs-comment"># Ep 6 (Step 000045): Train loss 4.133, Val loss 6.177,</span><br><span class="hljs-comment"># Ep 6 (Step 000050): Train loss 3.686, Val loss 6.150,</span><br><span class="hljs-comment"># Every effort moves you know it was not to have to have to see the fact of the last word.</span><br><span class="hljs-comment"># Ep 7 (Step 000055): Train loss 3.395, Val loss 6.097,</span><br><span class="hljs-comment"># Ep 7 (Step 000060): Train loss 2.701, Val loss 6.093,</span><br><span class="hljs-comment"># Every effort moves you know it was not that the picture--I had the fact the fact of the donkey, I had been--I</span><br><span class="hljs-comment"># Ep 8 (Step 000065): Train loss 2.494, Val loss 6.123,</span><br><span class="hljs-comment"># Ep 8 (Step 000070): Train loss 2.166, Val loss 6.153,</span><br><span class="hljs-comment"># Every effort moves you know it was not that the picture for nothing--I told Mrs.</span><br><span class="hljs-comment"># Ep 9 (Step 000075): Train loss 1.793, Val loss 6.197,</span><br><span class="hljs-comment"># Ep 9 (Step 000080): Train loss 1.471, Val loss 6.180,</span><br><span class="hljs-comment"># Every effort moves you know,&quot; was not that my hostess was &quot;interesting&quot;: on the last word.</span><br><span class="hljs-comment"># Ep 10 (Step 000085): Train loss 1.071, Val loss 6.233,</span><br><span class="hljs-comment"># Every effort moves you know,&quot; was not that my hostess was &quot;interesting&quot;: on that point I could have given Miss Croft the</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib.ticker <span class="hljs-keyword">import</span> MaxNLocator<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_losses</span>(<span class="hljs-params">epochs_seen, tokens_seen, train_losses, val_losses</span>):<br>    fig, ax1 = plt.subplots(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>))<br>    ax1.plot(epochs_seen, train_losses, label=<span class="hljs-string">&quot;Trainning loss&quot;</span>)<br>    ax1.plot(epochs_seen, val_losses, label=<span class="hljs-string">&quot;Validation loss&quot;</span>, linestyle=<span class="hljs-string">&quot;-.&quot;</span>)<br>    ax1.set_xlabel(<span class="hljs-string">&quot;Epochs&quot;</span>)<br>    ax1.set_ylabel(<span class="hljs-string">&quot;Loss&quot;</span>)<br>    ax1.legend(loc=<span class="hljs-string">&quot;upper right&quot;</span>)<br>    ax1.xaxis.set_major_locator(MaxNLocator(integer=<span class="hljs-literal">True</span>))<br>    ax2 = ax1.twiny()<br>    ax2.plot(tokens_seen, train_losses, alpha=<span class="hljs-number">0</span>)<br>    ax2.set_xlabel(<span class="hljs-string">&quot;Tokens seen&quot;</span>)<br>    fig.tight_layout()<br>    plt.show()<br></code></pre></td></tr></table></figure>

<p>训练过程中，一开始 val_loss 下降的挺快，但很快进入了平台期，之后 train_loss 仍然继续下降。二者开始出现明显的分离，说明训练参数与训练数据之间开始出现过拟合了。</p>
<blockquote>
<p>由于训练数据集非常小，而且还训练了 10 轮，所以很容易出现过拟合。反之，如果训练集很大，例如几万本书，而且只训练一轮的话，那么就不会出现过拟合了；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423140746067.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="文本生成策略的随机性控制"><a href="#文本生成策略的随机性控制" class="headerlink" title="文本生成策略的随机性控制"></a>文本生成策略的随机性控制</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423141559889.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="温度缩放"><a href="#温度缩放" class="headerlink" title="温度缩放"></a>温度缩放</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用模型生成输出</span><br>model.to(<span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment"># 将模型移回 CPU，推理时不需要 GPU</span><br>model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 切换为评估模式，避免 dropout 等操作影响推理结果</span><br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>token_ids = generate_text_simple(<br>    model=model,<br>    idx=text_to_token_ids(<span class="hljs-string">&quot;Every effort moves you&quot;</span>, tokenizer),<br>    max_new_tokens=<span class="hljs-number">25</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><br><span class="hljs-comment"># Every effort moves you know,&quot; was not that my hostess was &quot;interesting&quot;: on that point I could have given Miss Croft the</span><br></code></pre></td></tr></table></figure>

<p>这里发现一个问题，即对于训练好的模型，相同的输入，每次调用时，总是会得到相同的输出。这是因为模型在训练时，dropout 的概率值是固定的，所以每次调用时，模型的行为是确定的。</p>
<p>为了更好的控制生成文本的随机性，以应对不同的使用场景，此处引入一个新的参数 temperature；</p>
<ul>
<li>调低温度时，即 T &lt; 1，减少模型生成结果的随机性，以便应对需要严谨结果的场景，例如生成代码；</li>
<li>调高温度时，即 T &gt; 1，增加模型生成结果的随机性，以便应对需要引入创意的场景，例如诗歌；</li>
</ul>
<p>它的原理是模型在输出 token 的概率分布前，对这个概率进行缩放。放大概率，意味着结果更有确定性。缩小概率，变得平滑，则结果会更加富有多样性。</p>
<blockquote>
<p>new_logit &#x3D; old_logit &#x2F; temperature， 所以 T 越小，logit 越大；T 越大，logit 越小</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>vocab = &#123;<br>    <span class="hljs-string">&quot;closer&quot;</span>: <span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;every&quot;</span>: <span class="hljs-number">1</span>,<br>    <span class="hljs-string">&quot;effort&quot;</span>: <span class="hljs-number">2</span>,<br>    <span class="hljs-string">&quot;forward&quot;</span>: <span class="hljs-number">3</span>,<br>    <span class="hljs-string">&quot;inches&quot;</span>: <span class="hljs-number">4</span>,<br>    <span class="hljs-string">&quot;moves&quot;</span>: <span class="hljs-number">5</span>,<br>    <span class="hljs-string">&quot;pizza&quot;</span>: <span class="hljs-number">6</span>,<br>    <span class="hljs-string">&quot;toward&quot;</span>: <span class="hljs-number">7</span>,<br>    <span class="hljs-string">&quot;you&quot;</span>: <span class="hljs-number">8</span>,<br>&#125;<br><br>inverse_vocab = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> vocab.items()&#125;<br><br>next_token_logits = torch.tensor(<br>    [<span class="hljs-number">4.51</span>, <span class="hljs-number">0.89</span>, -<span class="hljs-number">1.90</span>, <span class="hljs-number">6.75</span>, <span class="hljs-number">1.63</span>, -<span class="hljs-number">1.62</span>, -<span class="hljs-number">1.89</span>, <span class="hljs-number">6.28</span>, <span class="hljs-number">1.79</span>]<br>)<br><br>probas = torch.softmax(next_token_logits, dim=<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_sampled_tokens</span>(<span class="hljs-params">probas</span>):<br>    torch.manual_seed(<span class="hljs-number">123</span>)<br>    sample = [torch.multinomial(probas, num_samples=<span class="hljs-number">1</span>).item() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br>    sampled_ids = torch.bincount(torch.tensor(sample))<br>    <span class="hljs-keyword">for</span> i, freq <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sampled_ids):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;inverse_vocab[i]&#125;</span> : <span class="hljs-subst">&#123;freq&#125;</span>&quot;</span>)<br><br><br>print_sampled_tokens(probas)<br><span class="hljs-comment"># 通过 multinomial 引入了随机性</span><br><span class="hljs-comment"># closer: 71</span><br><span class="hljs-comment"># every: 2</span><br><span class="hljs-comment"># effort: 0</span><br><span class="hljs-comment"># forward: 544</span><br><span class="hljs-comment"># inches: 2</span><br><span class="hljs-comment"># moves: 1</span><br><span class="hljs-comment"># pizza: 0</span><br><span class="hljs-comment"># toward: 376</span><br><span class="hljs-comment"># you: 4</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用不同的 temperature 参数输出不同的结果</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_with_temperature</span>(<span class="hljs-params">logits, temperature=<span class="hljs-number">1.0</span></span>):<br>    scaled_logits = logits / temperature<br>    <span class="hljs-keyword">return</span> torch.softmax(scaled_logits, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423150829158.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="Top-K-取样"><a href="#Top-K-取样" class="headerlink" title="Top-K 取样"></a>Top-K 取样</h3><p>较大的 temperature 虽然会增加多样性，但同时也意味着生成的结果存在不可控的情况，即有可能会生成一个没有意义的文本。因此，可搭配 Top-K 策略一起使用。Top-K 从名字可以看出来，就是取前 K 个最大概率，其他选项则忽略，这样可以减少无意义的情况出现。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423151204567.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">top_k = <span class="hljs-number">3</span><br>top_logits, top_pos = torch.topk(next_token_logits, top_k) <span class="hljs-comment"># 结果是降序排列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top logits:&quot;</span>, top_logits)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top positions:&quot;</span>, top_pos)<br><span class="hljs-comment"># Top logits: tensor([6.7500, 6.2800, 4.5100])</span><br><span class="hljs-comment"># Top positions: tensor([3, 7, 0])</span><br><br>new_logits = torch.where(<br>    condition=next_token_logits &lt; top_logits[-<span class="hljs-number">1</span>],  <span class="hljs-comment"># top_logits 已是降序排列</span><br>    <span class="hljs-built_in">input</span>=torch.tensor(<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)),<br>    other=next_token_logits,<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;New logits:&quot;</span>, new_logits)<br><span class="hljs-comment"># New logits: tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])</span><br></code></pre></td></tr></table></figure>

<h3 id="修改文本生成函数"><a href="#修改文本生成函数" class="headerlink" title="修改文本生成函数"></a>修改文本生成函数</h3><p>将 temperature 和 top-k 结合起来使用；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model, idx, max_new_tokens, context_size, temperature=<span class="hljs-number">0.0</span>, top_k=<span class="hljs-literal">None</span>, eos_id=<span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        idx_cond = idx[:, -context_size:]<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(idx_cond)<br>        logits = logits[:, -<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># 只取最后一步的 logits</span><br><br>        <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            top_logits, _ = torch.topk(logits, top_k)<br>            min_val = top_logits[:, -<span class="hljs-number">1</span>]<br>            logits = torch.where(<br>                logits &lt; min_val, torch.tensor(<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)).to(logits.device), logits<br>            )<br><br>        <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0.0</span>:<br>            logits = logits / temperature<br>            probs = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br>            idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            idx_next = torch.argmax(logits, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-keyword">if</span> idx_next == eos_id:<br>            <span class="hljs-keyword">break</span><br><br>        idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 拼接新的 token</span><br><br>    <span class="hljs-keyword">return</span> idx<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br>token_ids = generate(<br>    model=model,<br>    idx=text_to_token_ids(<span class="hljs-string">&quot;Every effort moves you&quot;</span>, tokenizer),<br>    max_new_tokens=<span class="hljs-number">15</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    top_k=<span class="hljs-number">25</span>,<br>    temperature=<span class="hljs-number">1.4</span>,<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Output text:</span><br><span class="hljs-comment"># Every effort moves you stand to work on surprise, a one of us had gone</span><br><span class="hljs-comment"># with randomness, and the other side of the world. I was</span><br></code></pre></td></tr></table></figure>

<h2 id="加载和保存模型参数"><a href="#加载和保存模型参数" class="headerlink" title="加载和保存模型参数"></a>加载和保存模型参数</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423155249464.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 直接调用 save 方法，即可保存模型的参数</span><br>torch.save(model.state_dict(), <span class="hljs-string">&quot;model.pth&quot;</span>)<br><br>model = GPTModel(GPT_CONFIG_124M)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.0004</span>, weight_decay=<span class="hljs-number">0.1</span>)<br><br>model.load_state_dict(torch.load(<span class="hljs-string">&quot;model.pth&quot;</span>), map_location=device)<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 可同时保存模型和优化器的参数</span><br>torch.save(<br>    &#123;<br>        <span class="hljs-string">&quot;model_state_dict&quot;</span>: model.state_dict(),<br>        <span class="hljs-string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),<br>    &#125;,<br>    <span class="hljs-string">&quot;model_and_optimizer.pth&quot;</span>,<br>)<br><br><span class="hljs-comment"># 加载模型和优化器的参数</span><br>checkpoint = torch.load(<span class="hljs-string">&quot;model_and_optimizer.pth&quot;</span>, map_location=device)<br>model = GPTModel(GPT_CONFIG_124M)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&quot;model_state_dict&quot;</span>])<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.0004</span>, weight_decay=<span class="hljs-number">0.1</span>)<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&quot;optimizer_state_dict&quot;</span>])<br>model.train()<br></code></pre></td></tr></table></figure>

<h2 id="加载预训练参数"><a href="#加载预训练参数" class="headerlink" title="加载预训练参数"></a>加载预训练参数</h2><p>加载预训练的模型参数前，需要先下载它们，以下是下载模型参数的代码；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 下载模型参数</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download_and_load_gpt2</span>(<span class="hljs-params">model_size, models_dir</span>):<br>    <span class="hljs-comment"># 有多个不同参数数量的版本</span><br>    allowed_sizes = (<span class="hljs-string">&quot;124M&quot;</span>, <span class="hljs-string">&quot;355M&quot;</span>, <span class="hljs-string">&quot;774M&quot;</span>, <span class="hljs-string">&quot;1558M&quot;</span>)<br>    <span class="hljs-keyword">if</span> model_size <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> allowed_sizes:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Model size not in <span class="hljs-subst">&#123;allowed_sizes&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># 待下载的文件</span><br>    model_dir = os.path.join(models_dir, model_size)<br>    base_url = <span class="hljs-string">&quot;https://openaipublic.blob.core.windows.net/gpt-2/models&quot;</span><br>    backup_base_url = <span class="hljs-string">&quot;https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2&quot;</span><br>    filenames = [<br>        <span class="hljs-string">&quot;checkpoint&quot;</span>, <span class="hljs-string">&quot;encoder.json&quot;</span>, <span class="hljs-string">&quot;hparams.json&quot;</span>,<br>        <span class="hljs-string">&quot;model.ckpt.data-00000-of-00001&quot;</span>, <span class="hljs-string">&quot;model.ckpt.index&quot;</span>,<br>        <span class="hljs-string">&quot;model.ckpt.meta&quot;</span>, <span class="hljs-string">&quot;vocab.bpe&quot;</span><br>    ]<br><br>    <span class="hljs-comment"># 下载文件</span><br>    os.makedirs(model_dir, exist_ok=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>        file_url = os.path.join(base_url, model_size, filename)<br>        backup_url = os.path.join(backup_base_url, model_size, filename)<br>        file_path = os.path.join(model_dir, filename)<br>        download_file(file_url, file_path, backup_url)<br><br>    <span class="hljs-comment"># 加载配置和参数</span><br>    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)<br>    settings = json.load(<span class="hljs-built_in">open</span>(os.path.join(model_dir, <span class="hljs-string">&quot;hparams.json&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>))<br>    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)<br><br>    <span class="hljs-keyword">return</span> settings, params<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download_file</span>(<span class="hljs-params">url, destination, backup_url=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_attempt_download</span>(<span class="hljs-params">download_url</span>):<br>        <span class="hljs-keyword">with</span> urllib.request.urlopen(download_url) <span class="hljs-keyword">as</span> response:<br>            <span class="hljs-comment"># Get the total file size from headers, defaulting to 0 if not present</span><br>            file_size = <span class="hljs-built_in">int</span>(response.headers.get(<span class="hljs-string">&quot;Content-Length&quot;</span>, <span class="hljs-number">0</span>))<br><br>            <span class="hljs-comment"># Check if file exists and has the same size</span><br>            <span class="hljs-keyword">if</span> os.path.exists(destination):<br>                file_size_local = os.path.getsize(destination)<br>                <span class="hljs-keyword">if</span> file_size == file_size_local:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File already exists and is up-to-date: <span class="hljs-subst">&#123;destination&#125;</span>&quot;</span>)<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>  <span class="hljs-comment"># Indicate success without re-downloading</span><br><br>            block_size = <span class="hljs-number">1024</span>  <span class="hljs-comment"># 1 Kilobyte</span><br><br>            <span class="hljs-comment"># Initialize the progress bar with total file size</span><br>            progress_bar_description = os.path.basename(download_url)<br>            <span class="hljs-keyword">with</span> tqdm(total=file_size, unit=<span class="hljs-string">&quot;iB&quot;</span>, unit_scale=<span class="hljs-literal">True</span>, desc=progress_bar_description) <span class="hljs-keyword">as</span> progress_bar:<br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(destination, <span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> file:<br>                    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>                        chunk = response.read(block_size)<br>                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> chunk:<br>                            <span class="hljs-keyword">break</span><br>                        file.write(chunk)<br>                        progress_bar.update(<span class="hljs-built_in">len</span>(chunk))<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">if</span> _attempt_download(url):<br>            <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">except</span> (urllib.error.HTTPError, urllib.error.URLError):<br>        <span class="hljs-keyword">if</span> backup_url <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Primary URL (<span class="hljs-subst">&#123;url&#125;</span>) failed. Attempting backup URL: <span class="hljs-subst">&#123;backup_url&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">if</span> _attempt_download(backup_url):<br>                    <span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">except</span> urllib.error.HTTPError:<br>                <span class="hljs-keyword">pass</span><br><br>        <span class="hljs-comment"># If we reach here, both attempts have failed</span><br>        error_message = (<br>            <span class="hljs-string">f&quot;Failed to download from both primary URL (<span class="hljs-subst">&#123;url&#125;</span>)&quot;</span><br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-string">&#x27; and backup URL (&#x27;</span> + backup_url + <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-keyword">if</span> backup_url <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span>&#125;</span>.&quot;</span><br>            <span class="hljs-string">&quot;\nCheck your internet connection or the file availability.\n&quot;</span><br>            <span class="hljs-string">&quot;For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273&quot;</span><br>        )<br>        <span class="hljs-built_in">print</span>(error_message)<br>    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;An unexpected error occurred: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br><br><br><span class="hljs-comment"># Alternative way using `requests`</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">def download_file(url, destination):</span><br><span class="hljs-string">    # Send a GET request to download the file in streaming mode</span><br><span class="hljs-string">    response = requests.get(url, stream=True)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # Get the total file size from headers, defaulting to 0 if not present</span><br><span class="hljs-string">    file_size = int(response.headers.get(&quot;content-length&quot;, 0))</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # Check if file exists and has the same size</span><br><span class="hljs-string">    if os.path.exists(destination):</span><br><span class="hljs-string">        file_size_local = os.path.getsize(destination)</span><br><span class="hljs-string">        if file_size == file_size_local:</span><br><span class="hljs-string">            print(f&quot;File already exists and is up-to-date: &#123;destination&#125;&quot;)</span><br><span class="hljs-string">            return</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # Define the block size for reading the file</span><br><span class="hljs-string">    block_size = 1024  # 1 Kilobyte</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # Initialize the progress bar with total file size</span><br><span class="hljs-string">    progress_bar_description = url.split(&quot;/&quot;)[-1]  # Extract filename from URL</span><br><span class="hljs-string">    with tqdm(total=file_size, unit=&quot;iB&quot;, unit_scale=True, desc=progress_bar_description) as progress_bar:</span><br><span class="hljs-string">        # Open the destination file in binary write mode</span><br><span class="hljs-string">        with open(destination, &quot;wb&quot;) as file:</span><br><span class="hljs-string">            # Iterate over the file data in chunks</span><br><span class="hljs-string">            for chunk in response.iter_content(block_size):</span><br><span class="hljs-string">                progress_bar.update(len(chunk))  # Update progress bar</span><br><span class="hljs-string">                file.write(chunk)  # Write the chunk to the file</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_gpt2_params_from_tf_ckpt</span>(<span class="hljs-params">ckpt_path, settings</span>):<br>    <span class="hljs-comment"># Initialize parameters dictionary with empty blocks for each layer</span><br>    params = &#123;<span class="hljs-string">&quot;blocks&quot;</span>: [&#123;&#125; <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(settings[<span class="hljs-string">&quot;n_layer&quot;</span>])]&#125;<br><br>    <span class="hljs-comment"># Iterate over each variable in the checkpoint</span><br>    <span class="hljs-keyword">for</span> name, _ <span class="hljs-keyword">in</span> tf.train.list_variables(ckpt_path):<br>        <span class="hljs-comment"># Load the variable and remove singleton dimensions</span><br>        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))<br><br>        <span class="hljs-comment"># Process the variable name to extract relevant parts</span><br>        variable_name_parts = name.split(<span class="hljs-string">&quot;/&quot;</span>)[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># Skip the &#x27;model/&#x27; prefix</span><br><br>        <span class="hljs-comment"># Identify the target dictionary for the variable</span><br>        target_dict = params<br>        <span class="hljs-keyword">if</span> variable_name_parts[<span class="hljs-number">0</span>].startswith(<span class="hljs-string">&quot;h&quot;</span>):<br>            layer_number = <span class="hljs-built_in">int</span>(variable_name_parts[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>:])<br>            target_dict = params[<span class="hljs-string">&quot;blocks&quot;</span>][layer_number]<br><br>        <span class="hljs-comment"># Recursively access or create nested dictionaries</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> variable_name_parts[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>]:<br>            target_dict = target_dict.setdefault(key, &#123;&#125;)<br><br>        <span class="hljs-comment"># Assign the variable array to the last key</span><br>        last_key = variable_name_parts[-<span class="hljs-number">1</span>]<br>        target_dict[last_key] = variable_array<br><br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 下载 124M 个参数的 gpt2 模型</span><br><span class="hljs-keyword">from</span> gpt_download <span class="hljs-keyword">import</span> download_and_load_gpt2<br><br>settings, params = download_and_load_gpt2(model_size=<span class="hljs-string">&quot;124M&quot;</span>, models_dir=<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Settings:&quot;</span>, settings)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Parameter dictionary keys:&quot;</span>, params.keys())<br></code></pre></td></tr></table></figure>

<p>GPT-2 模型有好几个，它们的差别主要在于参数规模不同，但模型结构是差不多的。不同的地方包括：</p>
<ul>
<li>参数的数量；</li>
<li>嵌入维度的数量；</li>
<li>Transformer Block 的数量；</li>
<li>Multi-head 多头注意力的数量；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">model_configs = &#123;<br>    <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-medium(355M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-large(774M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1280</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">36</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">20</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-xl(1558M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1600</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">48</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">25</span>&#125;,<br>&#125;<br></code></pre></td></tr></table></figure>



<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250423170138827.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python">GPT_CONFIG_124M = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">256</span>,<br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>,<br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.1</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">False</span>,<br>&#125;<br><br>model_name = <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span><br>NEW_CONFIG = GPT_CONFIG_124M.copy()<br>NEW_CONFIG.update(model_configs[model_name])<br>NEW_CONFIG.update(&#123;<span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>&#125;)<br>NEW_CONFIG.update(&#123;<span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br><br>gpt = GPTModel(NEW_CONFIG)<br>gpt.<span class="hljs-built_in">eval</span>()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">assign</span>(<span class="hljs-params">left, right</span>):<br>    <span class="hljs-keyword">if</span> left.shape != right.shape:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Shape mismatch: <span class="hljs-subst">&#123;left.shape&#125;</span> vs <span class="hljs-subst">&#123;right.shape&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> torch.nn.Parameter(torch.tensor(right))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_weights_into_gpt</span>(<span class="hljs-params">gpt, params</span>):<br>    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[<span class="hljs-string">&quot;wpe&quot;</span>])<br>    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[<span class="hljs-string">&quot;wte&quot;</span>])<br><br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params[<span class="hljs-string">&quot;blocks&quot;</span>])):<br>        q_w, k_w, v_w = np.split(<br>            (params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_attn&quot;</span>])[<span class="hljs-string">&quot;w&quot;</span>], <span class="hljs-number">3</span>, axis=-<span class="hljs-number">1</span><br>        )<br>        gpt.trf_blocks[b].att.W_query.weight = assign(<br>            gpt.trf_blocks[b].att.W_query.weight, q_w.T<br>        )<br>        gpt.trf_blocks[b].att.W_key.weight = assign(<br>            gpt.trf_blocks[b].att.W_key.weight, k_w.T<br>        )<br>        gpt.trf_blocks[b].att.W_value.weight = assign(<br>            gpt.trf_blocks[b].att.W_value.weight, v_w.T<br>        )<br><br>        q_b, k_b, v_b = np.split(<br>            (params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_attn&quot;</span>])[<span class="hljs-string">&quot;b&quot;</span>], <span class="hljs-number">3</span>, axis=-<span class="hljs-number">1</span><br>        )<br>        gpt.trf_blocks[b].att.W_query.bias = assign(<br>            gpt.trf_blocks[b].att.W_query.bias, q_b<br>        )<br>        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)<br>        gpt.trf_blocks[b].att.W_value.bias = assign(<br>            gpt.trf_blocks[b].att.W_value.bias, v_b<br>        )<br><br>        gpt.trf_blocks[b].att.out_proj.weight = assign(<br>            gpt.trf_blocks[b].att.out_proj.weight,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T,<br>        )<br>        gpt.trf_blocks[b].att.out_proj.bias = assign(<br>            gpt.trf_blocks[b].att.out_proj.bias,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>],<br>        )<br><br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].weight = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].weight,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_fc&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T,<br>        )<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].bias = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].bias,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_fc&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>],<br>        )<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].weight = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].weight,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T,<br>        )<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].bias = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].bias,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>],<br>        )<br><br>        gpt.trf_blocks[b].norm1.scale = assign(<br>            gpt.trf_blocks[b].norm1.scale,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_1&quot;</span>][<span class="hljs-string">&quot;g&quot;</span>],<br>        )<br>        gpt.trf_blocks[b].norm1.shift = assign(<br>            gpt.trf_blocks[b].norm1.shift,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_1&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>],<br>        )<br>        gpt.trf_blocks[b].norm2.scale = assign(<br>            gpt.trf_blocks[b].norm2.scale,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_2&quot;</span>][<span class="hljs-string">&quot;g&quot;</span>],<br>        )<br>        gpt.trf_blocks[b].norm2.shift = assign(<br>            gpt.trf_blocks[b].norm2.shift,<br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_2&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>],<br>        )<br><br>    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[<span class="hljs-string">&quot;g&quot;</span>])<br>    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[<span class="hljs-string">&quot;b&quot;</span>])<br>    gpt.out_head.weight = assign(gpt.out_head.weight, params[<span class="hljs-string">&quot;wte&quot;</span>])<br><br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>load_weights_into_gpt(gpt, params)<br>gpt.to(device)<br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>token_ids = generate(<br>    model=gpt,<br>    idx=text_to_token_ids(<span class="hljs-string">&quot;Every efforts moves you&quot;</span>, tokenizer).to(device),<br>    max_new_tokens=<span class="hljs-number">25</span>,<br>    context_size=NEW_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    top_k=<span class="hljs-number">50</span>,<br>    temperature=<span class="hljs-number">1.5</span>,<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Every efforts moves you have made a movement a moves a moved move was moved</span><br></code></pre></td></tr></table></figure>

<h1 id="6-Fine-tuning-for-classification"><a href="#6-Fine-tuning-for-classification" class="headerlink" title="6.Fine-tuning for classification"></a>6.Fine-tuning for classification</h1><p>目标：微调模型，以便其能够完成分类任务。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240659482.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="不同的微调类型"><a href="#不同的微调类型" class="headerlink" title="不同的微调类型"></a>不同的微调类型</h2><p>有两种常见的微调，一种是用于完成分类任务，一种是用于遵循指令输出结果。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240702733.png" srcset="/img/loading.gif" lazyload></p>
<p>指令型微调能够处理更加复杂和通用的任务，但同时它也需要更大的数据集进行训练，以及更多的算力。反之，分类型微调只需可较少的数据集即可完成训练。</p>
<h2 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h2><p>进行微调训练前，需要先准备好数据集。此处为了方便，直接从网上下载一份现成的数据集，其中包括垃圾短信和正常短信（也可以用于垃圾邮件的分类训练，原理一样）；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504240714853.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request<br><span class="hljs-keyword">import</span> zipfile<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># 下载数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download_and_unzip_spam_data</span>(<span class="hljs-params">url, zip_path, extracted_path, data_file_path</span>):<br>    <span class="hljs-keyword">if</span> data_file_path.exists():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_file_path&#125;</span> already exists. Skipping download and extraction&quot;</span>)<br>        <span class="hljs-keyword">return</span><br><br>    <span class="hljs-keyword">with</span> urllib.request.urlopen(url) <span class="hljs-keyword">as</span> response:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(zip_path, <span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> out_file:<br>            out_file.write(response.read())<br><br>    <span class="hljs-keyword">with</span> zipfile.ZipFile(zip_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> zip_ref:<br>        zip_ref.extractall(extracted_path)<br><br>    original_file_path = Path(extracted_path) / <span class="hljs-string">&quot;SMSSpamCollection&quot;</span><br>    os.rename(original_file_path, data_file_path)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;File downloaded and saved as <span class="hljs-subst">&#123;data_file_path&#125;</span>&quot;</span>)<br><br><br>url = <span class="hljs-string">&quot;https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip&quot;</span><br>data_dir = <span class="hljs-string">&quot;data&quot;</span><br>zip_path = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_dir&#125;</span>/sms_spam_coleectioni.zip&quot;</span><br><span class="hljs-comment"># print(&quot;zip_path&quot;, zip_path)</span><br>extracted_path = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_dir&#125;</span>/sms_spam_collection&quot;</span><br>data_file_path = Path(extracted_path) / <span class="hljs-string">&quot;SMSSpamCollection.tsv&quot;</span><br>download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)<br><br>df = pd.read_csv(data_file_path, sep=<span class="hljs-string">&quot;\t&quot;</span>, header=<span class="hljs-literal">None</span>, names=[<span class="hljs-string">&quot;Label&quot;</span>, <span class="hljs-string">&quot;Text&quot;</span>])<br><span class="hljs-built_in">print</span>(df[<span class="hljs-string">&quot;Label&quot;</span>].value_counts())<br><span class="hljs-comment"># ham     4825</span><br><span class="hljs-comment"># spam     747</span><br><span class="hljs-comment"># Name: count, dtype: int64</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_balanced_dataset</span>(<span class="hljs-params">df</span>):<br>    num_spam = df[df[<span class="hljs-string">&quot;Label&quot;</span>] == <span class="hljs-string">&quot;spam&quot;</span>].shape[<span class="hljs-number">0</span>]<br>    ham_subset = df[df[<span class="hljs-string">&quot;Label&quot;</span>] == <span class="hljs-string">&quot;ham&quot;</span>].sample(num_spam, random_state=<span class="hljs-number">42</span>)<br>    balanced_df = pd.concat([ham_subset, df[df[<span class="hljs-string">&quot;Label&quot;</span>] == <span class="hljs-string">&quot;spam&quot;</span>]])<br>    <span class="hljs-keyword">return</span> balanced_df<br><br><br>balanced_df = create_balanced_dataset(df)<br><span class="hljs-built_in">print</span>(balanced_df[<span class="hljs-string">&quot;Label&quot;</span>].value_counts())<br><span class="hljs-comment"># Label</span><br><span class="hljs-comment"># ham     747</span><br><span class="hljs-comment"># spam    747</span><br><span class="hljs-comment"># Name: count, dtype: int64</span><br><br>balanced_df[<span class="hljs-string">&quot;Label&quot;</span>] = balanced_df[<span class="hljs-string">&quot;Label&quot;</span>].<span class="hljs-built_in">map</span>(&#123;<span class="hljs-string">&quot;ham&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;spam&quot;</span>: <span class="hljs-number">1</span>&#125;)<br><span class="hljs-comment"># print(balanced_df[&quot;Label&quot;])</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_split</span>(<span class="hljs-params">df, train_frac, validation_frac</span>):<br>    df = df.sample(frac=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">123</span>).reset_index(drop=<span class="hljs-literal">True</span>)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(df) * train_frac)<br>    validation_end = train_end + <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(df) * validation_frac)<br><br>    train_df = df[:train_end]<br>    validation_df = df[train_end:validation_end]<br>    test_df = df[validation_end:]<br><br>    <span class="hljs-keyword">return</span> train_df, validation_df, test_df<br><br>train_df, validation_df, test_df = random_split(balanced_df, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>)<br><br>train_df.to_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_dir&#125;</span>/train.csv&quot;</span>, index=<span class="hljs-literal">None</span>)<br>validation_df.to_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_dir&#125;</span>/validation.csv&quot;</span>, index=<span class="hljs-literal">None</span>)<br>test_df.to_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;data_dir&#125;</span>/test.csv&quot;</span>, index=<span class="hljs-literal">None</span>)<br><br></code></pre></td></tr></table></figure>

<h2 id="创建数据加载器"><a href="#创建数据加载器" class="headerlink" title="创建数据加载器"></a>创建数据加载器</h2><p>由于每个句子长短不一，因此需要对较短的句子做 padding 填充，以便所有句子的长度相同。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424101428126.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SpamDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-comment"># 模型的上下文长度为 1024，如果单句文本长度超过 1024，则需要传入 max_length 参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, csv_file, tokenizer, max_length=<span class="hljs-literal">None</span>, pad_token_id=<span class="hljs-number">50256</span></span>):<br>        <span class="hljs-variable language_">self</span>.data = pd.read_csv(csv_file)<br>        <span class="hljs-variable language_">self</span>.encoded_texts = [tokenizer.encode(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.data[<span class="hljs-string">&quot;Text&quot;</span>]]<br>        <span class="hljs-keyword">if</span> max_length <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-variable language_">self</span>.max_length = <span class="hljs-variable language_">self</span>._longest_encoded_length()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.max_length = max_length<br>            <span class="hljs-variable language_">self</span>.encoded_texts = [<br>                encoded_text[: <span class="hljs-variable language_">self</span>.max_length] <span class="hljs-keyword">for</span> encoded_text <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.encoded_texts<br>            ]<br>        <span class="hljs-variable language_">self</span>.encoded_texts = [<br>            encoded_text + [pad_token_id] * (<span class="hljs-variable language_">self</span>.max_length - <span class="hljs-built_in">len</span>(encoded_text))<br>            <span class="hljs-keyword">for</span> encoded_text <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.encoded_texts<br>        ]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        encoded = <span class="hljs-variable language_">self</span>.encoded_texts[idx]<br>        label = <span class="hljs-variable language_">self</span>.data.iloc[idx][<span class="hljs-string">&quot;Label&quot;</span>]<br>        <span class="hljs-keyword">return</span> [<br>            torch.tensor(encoded, dtype=torch.long),<br>            torch.tensor(label, dtype=torch.long),<br>        ]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_longest_encoded_length</span>(<span class="hljs-params">self</span>):<br>        max_length = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> encoded_text <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.encoded_texts:<br>            max_length = <span class="hljs-built_in">max</span>(max_length, <span class="hljs-built_in">len</span>(encoded_text))<br>        <span class="hljs-keyword">return</span> max_length<br><br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>train_dataset = SpamDataset(<br>    csv_file=<span class="hljs-string">&quot;data/train.csv&quot;</span>, max_length=<span class="hljs-literal">None</span>, tokenizer=tokenizer<br>)<br><span class="hljs-comment"># print(train_dataset.max_length)</span><br><span class="hljs-comment"># 120</span><br><br>val_dataset = SpamDataset(<br>    csv_file=<span class="hljs-string">&quot;data/validation.csv&quot;</span>,<br>    max_length=train_dataset.max_length,<br>    tokenizer=tokenizer,<br>)<br>test_dataset = SpamDataset(<br>    csv_file=<span class="hljs-string">&quot;data/test.csv&quot;</span>,<br>    max_length=train_dataset.max_length,<br>    tokenizer=tokenizer,<br>)<br></code></pre></td></tr></table></figure>

<p>分类任务跟之前的模型训练有一个很大的不同，即它不再是预测下一个单词是什么，而是预测标签（label），例如判断是否为垃圾消息，即是（0）或者否（1）</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424104259443.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python">num_workers = <span class="hljs-number">0</span><br>batch_size = <span class="hljs-number">8</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br>train_loader = DataLoader(<br>    dataset=train_dataset,<br>    batch_size=batch_size,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=num_workers,<br>    drop_last=<span class="hljs-literal">True</span>,<br>)<br>val_loader = DataLoader(<br>    dataset=val_dataset,<br>    batch_size=batch_size,<br>    num_workers=num_workers,<br>    drop_last=<span class="hljs-literal">True</span>,<br>)<br>test_loader = DataLoader(<br>    dataset=test_dataset,<br>    batch_size=batch_size,<br>    num_workers=num_workers,<br>    drop_last=<span class="hljs-literal">True</span>,<br>)<br><br><span class="hljs-keyword">for</span> input_batch, target_batch <span class="hljs-keyword">in</span> train_loader:<br>    <span class="hljs-keyword">pass</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input batch dimension:&quot;</span>, input_batch.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Label batch dimension:&quot;</span>, target_batch.shape)<br><span class="hljs-comment"># Input batch dimension: torch.Size([8, 120])</span><br><span class="hljs-comment"># Label batch dimension: torch.Size([8])</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(train_loader)&#125;</span> training batches&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(val_loader)&#125;</span> valication batches&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(test_loader)&#125;</span> test batches&quot;</span>)<br><span class="hljs-comment"># 130 training batches</span><br><span class="hljs-comment"># 18 valication batches</span><br><span class="hljs-comment"># 37 test batches</span><br></code></pre></td></tr></table></figure>

<h2 id="用预训练参数初始化模型"><a href="#用预训练参数初始化模型" class="headerlink" title="用预训练参数初始化模型"></a>用预训练参数初始化模型</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424112202606.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化模型</span><br>CHOOSE_MODEL = <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span><br>INPUT_PROMPT = <span class="hljs-string">&quot;Every effort moves&quot;</span><br>BASE_CONFIG = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.0</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>,<br>&#125;<br>model_configs = &#123;<br>    <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-medium(355M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-large(774M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1280</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">36</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">20</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-xl(1558M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1600</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">48</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">25</span>&#125;,<br>&#125;<br>BASE_CONFIG.update(model_configs[CHOOSE_MODEL])<br><br>model_size = get_model_size(CHOOSE_MODEL)<br>settings, params = download_and_load_gpt2(model_size=model_size, models_dir=<span class="hljs-string">&quot;data/gpt2&quot;</span>)<br>model = GPTModel(BASE_CONFIG)<br>load_weights_into_gpt(model, params)<br>model.<span class="hljs-built_in">eval</span>()<br><br>text_1 = <span class="hljs-string">&quot;Every effort moves you&quot;</span><br>token_ids = generate_text_simple(<br>    model=model,<br>    idx=text_to_token_ids(text_1, tokenizer),<br>    max_new_tokens=<span class="hljs-number">15</span>,<br>    context_size=BASE_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>)<br><span class="hljs-built_in">print</span>(token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Every effort moves you.</span><br><br>text_2 = (<br>    <span class="hljs-string">&quot;Is the following text &#x27;spam&#x27;? Answer with &#x27;yes&#x27; or &#x27;no&#x27;:&quot;</span><br>    <span class="hljs-string">&quot; &#x27;You are a winner you have been specially&quot;</span><br>    <span class="hljs-string">&quot; selected to receive $1000 cash or a $2000 award.&#x27;&quot;</span><br>)<br>token_ids = generate_text_simple(<br>    model=model,<br>    idx=text_to_token_ids(text_2, tokenizer),<br>    max_new_tokens=<span class="hljs-number">23</span>,<br>    context_size=BASE_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>)<br><span class="hljs-built_in">print</span>(token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Is the following text &#x27;spam&#x27;? Answer with &#x27;yes&#x27; or &#x27;no&#x27;: &#x27;You are a winner you have been specially selected to receive $1000 cash or a $2000 award.&#x27; Answer a cash award a cash award cash&#x27; cash cash cash a cash a cash&#x27; cash a cash&#x27; cash&#x27;</span><br></code></pre></td></tr></table></figure>

<p>虽然输入显式的指示模型最后输出 Yes 或者 No 作为结果，但显然模型最后的输出跟预期不同。这是因为模型之前的训练并没有专门针对分类任务进行设计。</p>
<h2 id="添加分类头"><a href="#添加分类头" class="headerlink" title="添加分类头"></a>添加分类头</h2><p>为了更好的完成分类任务，需要对模型进行微调。其中一种办法就是替换模型最后的输出层，原本是输出 vocab 中各个单词的概率，现在改为输出 yes 和 no 两个 label 的概率即可，分别用数值 1 和 0 表示。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424145220641.png" srcset="/img/loading.gif" lazyload></p>
<p>以下是模型的结构，中间有 12 个 transformer block，最后有一个 out_head，这个 out_head 负责输出 50257 个单词的概率；通过替换 out_head 层，即可实现将输出转换成 yes&#x2F;no 两个结果；</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs routeros">GPTModel(<br>  (tok_emb): Embedding(50257, 768)<br>  (pos_emb): Embedding(1024, 768)<br>  (drop_emb): Dropout(<span class="hljs-attribute">p</span>=0.0, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>  (trf_blocks): Sequential(<br>    (0): TransformerBlock(<br>      (att): MultiHeadAttention(<br>        (W_query): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (W_key): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (W_value): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (out_proj): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (dropout): Dropout(<span class="hljs-attribute">p</span>=0.0, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>      )<br>      (ff): FeedForward(<br>        (layers): Sequential(<br>          (0): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=3072, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>          (1): GELU()<br>          (2): Linear(<span class="hljs-attribute">in_features</span>=3072, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        )<br>      )<br>      (norm1): LayerNorm()<br>      (norm2): LayerNorm()<br>      (drop_shortcut): Dropout(<span class="hljs-attribute">p</span>=0.0, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>    )<br>    <span class="hljs-built_in">..</span>. (为方便查看，此处省略了从 1 到 10 等 10 个 transformer block)<br>    (11): TransformerBlock(<br>      (att): MultiHeadAttention(<br>        (W_query): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (W_key): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (W_value): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (out_proj): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        (dropout): Dropout(<span class="hljs-attribute">p</span>=0.0, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>      )<br>      (ff): FeedForward(<br>        (layers): Sequential(<br>          (0): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=3072, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>          (1): GELU()<br>          (2): Linear(<span class="hljs-attribute">in_features</span>=3072, <span class="hljs-attribute">out_features</span>=768, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>        )<br>      )<br>      (norm1): LayerNorm()<br>      (norm2): LayerNorm()<br>      (drop_shortcut): Dropout(<span class="hljs-attribute">p</span>=0.0, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br>    )<br>  )<br>  (final_norm): LayerNorm()<br>  (out_head): Linear(<span class="hljs-attribute">in_features</span>=768, <span class="hljs-attribute">out_features</span>=50257, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将模型参数的 requires_grad 属性设置为 False，以便冻结模型参数，这样在训练时不会更新模型参数</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>num_classes = <span class="hljs-number">2</span><br>model.out_head = torch.nn.Linear(<br>    in_features=BASE_CONFIG[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>    out_features=num_classes,<br>)  <span class="hljs-comment"># 新添加的层 requires_grad 默认是 True</span><br><br><span class="hljs-comment"># 上面只替换最后的 out_head 层，但基于过往经验，对最后几层也进行训练，模型最终的效果会更好，因此下面将 final_norm 层和最后一个 transformer 的参数放开，允许在训练过程中更新</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.trf_blocks[-<span class="hljs-number">1</span>].parameters():<br>    param.requires_grad = <span class="hljs-literal">True</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.final_norm.parameters():<br>    param.requires_grad = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424152121301.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 调整后的模型，用法仍然跟以前相同，区别在于最后的输出有点点不一样</span><br>inputs = tokenizer.encode(<span class="hljs-string">&quot;Do you have time&quot;</span>)<br>inputs = torch.tensor(inputs).unsqueeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Inputs:&quot;</span>, inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Inputs dimensions:&quot;</span>, inputs.shape)<br><span class="hljs-comment"># Inputs: tensor([[5211,  345,  423,  640]])</span><br><span class="hljs-comment"># Inputs dimensions: torch.Size([1, 4])</span><br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    outputs = model(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outputs:\n&quot;</span>, outputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Outputs dimensions:&quot;</span>, outputs.shape)<br><span class="hljs-comment"># Outputs:</span><br><span class="hljs-comment">#  tensor([[[-0.5966,  0.6271],</span><br><span class="hljs-comment">#          [-2.8117,  6.8164],</span><br><span class="hljs-comment">#          [-2.0197,  5.6612],</span><br><span class="hljs-comment">#          [-3.3210,  4.7156]]])</span><br><span class="hljs-comment"># Outputs dimensions: torch.Size([1, 4, 2])</span><br><span class="hljs-comment"># 未调整前的模型，最后的输出是 50257 维的，现在只剩下 2 维了</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Last output token:&quot;</span>, outputs[:, -<span class="hljs-number">1</span>, :])<br><span class="hljs-comment"># Last output token: tensor([[-3.3210,  4.7156]])</span><br><span class="hljs-comment"># 在预测最后的输出时，只取读取最后一个 token 所在行的概率进行计算即可</span><br><span class="hljs-comment"># 因为该 token 包含前面所有 token 完整的相关信息</span><br></code></pre></td></tr></table></figure>

<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153511203.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="计算分类损失和精度"><a href="#计算分类损失和精度" class="headerlink" title="计算分类损失和精度"></a>计算分类损失和精度</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153903472.png" srcset="/img/loading.gif" lazyload></p>
<p>之前在预测 token 时，通过 softmax + argmax 找出最大概率的索引值即可。在分类任务，原理仍然不变，只是之前有 50267 个索引，现在只剩下 2 个了；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424153913526.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 之前在预测 token 时，通过 softmax + argmax 找出最大概率的索引值即可</span><br><span class="hljs-comment"># 在分类任务，原理仍然不变，只是之前有 50267 个索引，现在只剩下 2 个了；</span><br><br>probas = torch.softmax(outputs[:, -<span class="hljs-number">1</span>, :], dim=-<span class="hljs-number">1</span>)<br>label = torch.argmax(probas)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Class label:&quot;</span>, label.item())<br><span class="hljs-comment"># Class label: 1</span><br><br><br><span class="hljs-comment"># 计算预测的准确率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_accuracy_loader</span>(<span class="hljs-params">data_loader, model, device, num_batches=<span class="hljs-literal">None</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    correct_predictions, num_examples = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">if</span> num_batches <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        num_batches = <span class="hljs-built_in">len</span>(data_loader)<br>    <span class="hljs-keyword">else</span>:<br>        num_batches = <span class="hljs-built_in">min</span>(num_batches, <span class="hljs-built_in">len</span>(data_loader))<br>    <span class="hljs-keyword">for</span> i, (input_batch, target_batch) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> i &lt; num_batches:<br>            input_batch = input_batch.to(device)<br>            target_batch = target_batch.to(device)<br><br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                logits = model(input_batch)[:, -<span class="hljs-number">1</span>, :]<br>            predicted_labels = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br><br>            num_examples += predicted_labels.shape[<span class="hljs-number">0</span>]<br>            correct_predictions += (predicted_labels == target_batch).<span class="hljs-built_in">sum</span>().item()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> correct_predictions / num_examples<br><br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model.to(device)<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>train_accuracy = calc_accuracy_loader(<br>    data_loader=train_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br>val_accuracy = calc_accuracy_loader(<br>    data_loader=val_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br>test_accuracy = calc_accuracy_loader(<br>    data_loader=test_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training accuracy: <span class="hljs-subst">&#123;train_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Validation accuracy: <span class="hljs-subst">&#123;val_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Test accuracy: <span class="hljs-subst">&#123;test_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-comment"># Training accuracy: 56.25%</span><br><span class="hljs-comment"># Validation accuracy: 62.50%</span><br><span class="hljs-comment"># Test accuracy: 55.00%</span><br><span class="hljs-comment"># 从以上结果可见，目前预测准确率都很低，因为此时模型暂时没有学会如何区分垃圾短信和正常短信</span><br><br><br><span class="hljs-comment"># 计算单批损失</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_batch</span>(<span class="hljs-params">input_batch, target_batch, model, device</span>):<br>    input_batch = input_batch.to(device)<br>    target_batch = target_batch.to(device)<br>	<span class="hljs-comment"># 计算模型的输出，并取出输出中的最后一个 token 的 logits 即可</span><br>    logits = model(input_batch)[:, -<span class="hljs-number">1</span>, :]<br>    loss = torch.nn.functional.cross_entropy(logits, target_batch)<br>    <span class="hljs-keyword">return</span> loss<br><br><br><span class="hljs-comment"># 计算所有批次的损失，取平均值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_loader</span>(<span class="hljs-params">data_loader, model, device, num_batches=<span class="hljs-literal">None</span></span>):<br>    total_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_loader) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;nan&quot;</span>)<br>    <span class="hljs-keyword">elif</span> num_batches <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        num_batches = <span class="hljs-built_in">len</span>(data_loader)<br>    <span class="hljs-keyword">else</span>:<br>        num_batches = <span class="hljs-built_in">min</span>(num_batches, <span class="hljs-built_in">len</span>(data_loader))<br>    <span class="hljs-keyword">for</span> i, (input_batch, target_batch) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> i &lt; num_batches:<br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            total_loss += loss.item()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> total_loss / num_batches<br><br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    train_loss = calc_loss_loader(train_loader, model, device, num_batches=<span class="hljs-number">5</span>)<br>    val_loss = calc_loss_loader(val_loader, model, device, num_batches=<span class="hljs-number">5</span>)<br>    test_loss = calc_loss_loader(test_loader, model, device, num_batches=<span class="hljs-number">5</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training loss: <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Validation loss: <span class="hljs-subst">&#123;val_loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Test loss: <span class="hljs-subst">&#123;test_loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br><span class="hljs-comment"># Training loss: 2.213</span><br><span class="hljs-comment"># Validation loss: 2.050</span><br><span class="hljs-comment"># Test loss: 1.408</span><br><span class="hljs-comment"># 从以上结果可见，目前模型的损失函数值都很高</span><br></code></pre></td></tr></table></figure>

<h2 id="使用监督数据微调模型"><a href="#使用监督数据微调模型" class="headerlink" title="使用监督数据微调模型"></a>使用监督数据微调模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 分类器训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_classifier_simple</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter</span><br><span class="hljs-params"></span>):<br>    train_losses, val_losses, train_accs, val_accs = [], [], [], []<br>    examples_seen, global_step = <span class="hljs-number">0</span>, -<span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        model.train()<br><br>        <span class="hljs-keyword">for</span> input_batch, target_batch <span class="hljs-keyword">in</span> train_loader:<br>            optimizer.zero_grad()  <span class="hljs-comment"># 清空上一步遗留的梯度值</span><br>            <span class="hljs-comment"># calc_loss_batch 将预测输出，并返回损失值</span><br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            loss.backward()  <span class="hljs-comment"># 反向传播，计算当前梯度值</span><br>            optimizer.step()  <span class="hljs-comment"># 使用梯度值，更新模型参数</span><br>            examples_seen += input_batch.shape[<span class="hljs-number">0</span>]<br>            global_step += <span class="hljs-number">1</span><br><br>            <span class="hljs-keyword">if</span> global_step % eval_freq == <span class="hljs-number">0</span>:<br>                train_loss, val_loss = evaluate_model(<br>                    model, train_loader, val_loader, device, eval_iter<br>                )<br>                train_losses.append(train_loss)<br>                val_losses.append(val_loss)<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;Ep <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span> (Step <span class="hljs-subst">&#123;global_step: 06d&#125;</span>):&quot;</span><br>                    <span class="hljs-string">f&quot;Train loss <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>,&quot;</span><br>                    <span class="hljs-string">f&quot; Val loss <span class="hljs-subst">&#123;val_loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span><br>                )<br><br>        train_accuracy = calc_accuracy_loader(<br>            train_loader, model, device, num_batches=eval_iter<br>        )<br>        val_accuracy = calc_accuracy_loader(<br>            val_loader, model, device, num_batches=eval_iter<br>        )<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training accuracy: <span class="hljs-subst">&#123;train_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>% | &quot;</span>, end=<span class="hljs-string">&quot;&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Validation accuracy: <span class="hljs-subst">&#123;val_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br>        train_accs.append(train_accuracy)<br>        val_accs.append(val_accuracy)<br><br>    <span class="hljs-keyword">return</span> train_losses, val_losses, train_accs, val_accs, examples_seen<br><br><br><span class="hljs-comment"># 评估模型准确率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_model</span>(<span class="hljs-params">model, train_loader, val_loader, device, eval_iter</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        train_loss = calc_loss_loader(<br>            train_loader, model, device, num_batches=eval_iter<br>        )<br>        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)<br>    model.train()<br>    <span class="hljs-keyword">return</span> train_loss, val_loss<br><br><br>start_time = time.time()<br>torch.manual_seed(<span class="hljs-number">123</span>)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>, weight_decay=<span class="hljs-number">0.01</span>)<br>num_epochs = <span class="hljs-number">5</span><br><br>train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(<br>    model,<br>    train_loader,<br>    val_loader,<br>    optimizer,<br>    device,<br>    num_epochs=num_epochs,<br>    eval_freq=<span class="hljs-number">50</span>,<br>    eval_iter=<span class="hljs-number">5</span>,<br>)<br>end_time = time.time()<br>execution_time_minutes = (end_time - start_time) / <span class="hljs-number">60</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training completed in <span class="hljs-subst">&#123;execution_time_minutes:<span class="hljs-number">.2</span>f&#125;</span> minutes&quot;</span>)<br><span class="hljs-comment"># Last output token: tensor([[-3.3210,  4.7156]])</span><br><span class="hljs-comment"># Ep 1 (Step  00000):Train loss 1.369, Val loss 1.781</span><br><span class="hljs-comment"># Ep 1 (Step  00050):Train loss 0.488, Val loss 0.349</span><br><span class="hljs-comment"># Ep 1 (Step  00100):Train loss 0.216, Val loss 0.335</span><br><span class="hljs-comment"># Training accuracy: 95.00% | Validation accuracy: 95.00%</span><br><span class="hljs-comment"># Ep 2 (Step  00150):Train loss 0.257, Val loss 0.209</span><br><span class="hljs-comment"># Ep 2 (Step  00200):Train loss 0.086, Val loss 0.191</span><br><span class="hljs-comment"># Ep 2 (Step  00250):Train loss 0.138, Val loss 0.128</span><br><span class="hljs-comment"># Training accuracy: 100.00% | Validation accuracy: 97.50%</span><br><span class="hljs-comment"># Ep 3 (Step  00300):Train loss 0.127, Val loss 0.211</span><br><span class="hljs-comment"># Ep 3 (Step  00350):Train loss 0.187, Val loss 0.105</span><br><span class="hljs-comment"># Training accuracy: 92.50% | Validation accuracy: 97.50%</span><br><span class="hljs-comment"># Ep 4 (Step  00400):Train loss 0.108, Val loss 0.089</span><br><span class="hljs-comment"># Ep 4 (Step  00450):Train loss 0.027, Val loss 0.094</span><br><span class="hljs-comment"># Ep 4 (Step  00500):Train loss 0.203, Val loss 0.048</span><br><span class="hljs-comment"># Training accuracy: 100.00% | Validation accuracy: 97.50%</span><br><span class="hljs-comment"># Ep 5 (Step  00550):Train loss 0.073, Val loss 0.044</span><br><span class="hljs-comment"># Ep 5 (Step  00600):Train loss 0.047, Val loss 0.074</span><br><span class="hljs-comment"># Training accuracy: 100.00% | Validation accuracy: 97.50%</span><br><span class="hljs-comment"># Training completed in 14.17 minutes</span><br></code></pre></td></tr></table></figure>

<p>训练设置了 5 轮，但其实 2 轮之后，预测的准确率就很高了。因此，实际需要训练的轮次，跟数据集的大小和任务的复杂度有关。如果数据集足够大，任务也较为简单，有可能 1~2 轮就足够了，这样也可以很多时间和算力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">train_accuracy = calc_accuracy_loader(<br>    data_loader=train_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br>val_accuracy = calc_accuracy_loader(<br>    data_loader=val_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br>test_accuracy = calc_accuracy_loader(<br>    data_loader=test_loader, model=model, device=device, num_batches=<span class="hljs-number">10</span><br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training accuracy: <span class="hljs-subst">&#123;train_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Validation accuracy: <span class="hljs-subst">&#123;val_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Test accuracy: <span class="hljs-subst">&#123;test_accuracy*<span class="hljs-number">100</span>:<span class="hljs-number">.2</span>f&#125;</span>%&quot;</span>)<br><span class="hljs-comment"># Training accuracy: 100%</span><br><span class="hljs-comment"># Validation accuracy: 97.5%</span><br><span class="hljs-comment"># Test accuracy: 95.67%</span><br><span class="hljs-comment"># 可见在训练过后，相对之前约 50% 的准确率，现在的准确率得到了明显的提高</span><br></code></pre></td></tr></table></figure>

<h2 id="将模型用于垃圾消息分类"><a href="#将模型用于垃圾消息分类" class="headerlink" title="将模型用于垃圾消息分类"></a>将模型用于垃圾消息分类</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424171221160.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_review</span>(<span class="hljs-params"></span><br><span class="hljs-params">    text, model, tokenizer, device, max_length=<span class="hljs-literal">None</span>, pad_token_id=<span class="hljs-number">50256</span></span><br><span class="hljs-params"></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    input_ids = tokenizer.encode(text)<br>    supported_context_length = model.pos_emb.weight.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># 限制输入长度</span><br>    input_ids = input_ids[: <span class="hljs-built_in">min</span>(max_length, supported_context_length)]<br>    <span class="hljs-comment"># 如果输入长度小于最大长度，则用 pad_token_id 填充</span><br>    input_ids += [pad_token_id] * (max_length - <span class="hljs-built_in">len</span>(input_ids))<br><br>    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        logits = model(input_tensor)[:, -<span class="hljs-number">1</span>, :]<br>    predicted_labels = torch.argmax(logits, dim=-<span class="hljs-number">1</span>).item()<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;spam&quot;</span> <span class="hljs-keyword">if</span> predicted_labels == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;not_spam&quot;</span><br><br><br>text_1 = <span class="hljs-string">&quot;You are a winner you have been specially selected to receive $1000 cash or a $2000 award.&quot;</span><br><br><span class="hljs-built_in">print</span>(<br>    classify_review(<br>        text_1, model, tokenizer, device, max_length=train_dataset.max_length<br>    )<br>)<br><br>text_2 = <span class="hljs-string">&quot;Hey, just wanted to check if we&#x27;re still on for dinner tonight? Let me know!&quot;</span><br><span class="hljs-built_in">print</span>(<br>    classify_review(<br>        text_2, model, tokenizer, device, max_length=train_dataset.max_length<br>    )<br>)<br><span class="hljs-comment"># 保存模型参数</span><br>torch.save(model.state_dict(), <span class="hljs-string">&quot;data/review_classifier.pth&quot;</span>)<br><br><span class="hljs-comment"># 模型的参数保存后，可以在下次使用时直接加载</span><br><span class="hljs-comment"># model_state_dict = torch.load(&quot;data/review_classifier.pth, map_location=device&quot;)</span><br><span class="hljs-comment"># model.load_state_dict(model_state_dict)</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 直接加载模型进行分类</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> GPTModel <span class="hljs-keyword">import</span> GPTModel<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> get_model_size<br><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_review</span>(<span class="hljs-params"></span><br><span class="hljs-params">    text, model, tokenizer, device, max_length=<span class="hljs-literal">None</span>, pad_token_id=<span class="hljs-number">50256</span></span><br><span class="hljs-params"></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    input_ids = tokenizer.encode(text)<br>    supported_context_length = model.pos_emb.weight.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># 限制输入长度</span><br>    input_ids = input_ids[: <span class="hljs-built_in">min</span>(max_length, supported_context_length)]<br>    <span class="hljs-comment"># 如果输入长度小于最大长度，则用 pad_token_id 填充</span><br>    input_ids += [pad_token_id] * (max_length - <span class="hljs-built_in">len</span>(input_ids))<br><br>    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        logits = model(input_tensor)[:, -<span class="hljs-number">1</span>, :]<br>    predicted_labels = torch.argmax(logits, dim=-<span class="hljs-number">1</span>).item()<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;spam&quot;</span> <span class="hljs-keyword">if</span> predicted_labels == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;not_spam&quot;</span><br><br><br>CHOOSE_MODEL = <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span><br>BASE_CONFIG = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.0</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>,<br>&#125;<br>model_configs = &#123;<br>    <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-medium(355M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-large(774M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1280</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">36</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">20</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-xl(1558M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1600</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">48</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">25</span>&#125;,<br>&#125;<br>BASE_CONFIG.update(model_configs[CHOOSE_MODEL])<br>model_size = get_model_size(CHOOSE_MODEL)<br>model = GPTModel(BASE_CONFIG)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model.to(device)<br>torch.manual_seed(<span class="hljs-number">123</span>)<br>num_classes = <span class="hljs-number">2</span><br>model.out_head = torch.nn.Linear(<br>    in_features=BASE_CONFIG[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>    out_features=num_classes,<br>)<br><br><span class="hljs-comment"># 模型的参数保存后，可以在下次使用时直接加载</span><br>model_state_dict = torch.load(<span class="hljs-string">&quot;data/review_classifier.pth&quot;</span>, map_location=device)<br>model.load_state_dict(model_state_dict)<br><br>text_1 = <span class="hljs-string">&quot;You are a winner you have been specially selected to receive $1000 cash or a $2000 award.&quot;</span><br><br><span class="hljs-built_in">print</span>(classify_review(text_1, model, tokenizer, device, max_length=<span class="hljs-number">120</span>))<br><br>text_2 = <span class="hljs-string">&quot;Hey, just wanted to check if we&#x27;re still on for dinner tonight? Let me know!&quot;</span><br><span class="hljs-built_in">print</span>(classify_review(text_2, model, tokenizer, device, max_length=<span class="hljs-number">120</span>))<br></code></pre></td></tr></table></figure>

<h1 id="7-Fine-tuning-to-follow-instructions"><a href="#7-Fine-tuning-to-follow-instructions" class="headerlink" title="7.Fine-tuning to follow instructions"></a>7.Fine-tuning to follow instructions</h1><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424192338714.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="指令微调介绍"><a href="#指令微调介绍" class="headerlink" title="指令微调介绍"></a>指令微调介绍</h2><p>根据特定的指令来完成特定的任务，示例如下：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424193115615.png" srcset="/img/loading.gif" lazyload></p>
<p>指令微调同样由三个步骤组成，分别为：</p>
<ul>
<li>准备数据集</li>
<li>微调模型</li>
<li>评估模型</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424193247958.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="准备数据集-1"><a href="#准备数据集-1" class="headerlink" title="准备数据集"></a>准备数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> urllib.request<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">download_and_load_file</span>(<span class="hljs-params">file_path, url</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(file_path):<br>        <span class="hljs-keyword">with</span> urllib.request.urlopen(url) <span class="hljs-keyword">as</span> response:<br>            text_data = response.read().decode(<span class="hljs-string">&quot;utf-8&quot;</span>)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            f.write(text_data)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            text_data = f.read()<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        data = json.load(f)<br>    <span class="hljs-keyword">return</span> data<br><br><br>file_path = <span class="hljs-string">&quot;data/instruction-data.json&quot;</span><br>url = (<br>    <span class="hljs-string">&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch&quot;</span><br>    <span class="hljs-string">&quot;/main/ch07/01_main-chapter-code/instruction-data.json&quot;</span><br>)<br><br>data = download_and_load_file(file_path, url)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of entries:&quot;</span>, <span class="hljs-built_in">len</span>(data))<br><span class="hljs-comment"># Number of entries: 1100</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Example entry:\n&quot;</span>, data[<span class="hljs-number">50</span>])<br><span class="hljs-comment"># Example entry:</span><br><span class="hljs-comment">#  &#123;&#x27;instruction&#x27;: &#x27;Identify the correct spelling of the following word.&#x27;, &#x27;input&#x27;: &#x27;Ocassion&#x27;, &#x27;output&#x27;: &quot;The correct spelling is &#x27;Occasion.&#x27;&quot;&#125;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Another example entry:\n&quot;</span>, data[<span class="hljs-number">999</span>])<br><span class="hljs-comment"># &#123;&#x27;instruction&#x27;: &quot;What is an antonym of &#x27;complicated&#x27;?&quot;, &#x27;input&#x27;: &#x27;&#x27;, &#x27;output&#x27;: &quot;An antonym of &#x27;complicated&#x27; is &#x27;simple&#x27;.&quot;&#125;</span><br></code></pre></td></tr></table></figure>

<p>按指令微调模型，其训练数据集是一个包含指令、输入和输出的 JSON 文件。数据可以有多种不同的格式化模板，例如 Alpaca 和 Phi-3 两种常见的类型。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424201632645.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 格式化输入的数据，以便用于训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_input</span>(<span class="hljs-params">entry</span>):<br>    instruction_text = (<br>        <span class="hljs-string">f&quot;Below is an instruction that describes a task.&quot;</span><br>        <span class="hljs-string">f&quot;Write a response that appropriately completes the request.&quot;</span><br>        <span class="hljs-string">f&quot;\n\n### Instruction:\n<span class="hljs-subst">&#123;entry[<span class="hljs-string">&#x27;instruction&#x27;</span>]&#125;</span>&quot;</span><br>    )<br><br>    input_text = <span class="hljs-string">f&quot;\n\n### Input:\n<span class="hljs-subst">&#123;entry[<span class="hljs-string">&#x27;input&#x27;</span>]&#125;</span>&quot;</span> <span class="hljs-keyword">if</span> entry[<span class="hljs-string">&quot;input&quot;</span>] <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;&quot;</span><br><br>    <span class="hljs-keyword">return</span> instruction_text + input_text<br><br><br>model_input = format_input(data[<span class="hljs-number">50</span>])<br>desired_response = <span class="hljs-string">f&quot;\n\n### Response:\n<span class="hljs-subst">&#123;data[<span class="hljs-number">50</span>][<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span><br><span class="hljs-built_in">print</span>(model_input + desired_response)<br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.</span><br><br><span class="hljs-comment"># ### Instruction:</span><br><span class="hljs-comment"># Identify the correct spelling of the following word.</span><br><br><span class="hljs-comment"># ### Input:</span><br><span class="hljs-comment"># Ocassion</span><br><br><span class="hljs-comment"># ### Response:</span><br><span class="hljs-comment"># The correct spelling is &#x27;Occasion.&#x27;</span><br><br><br>model_input = format_input(data[<span class="hljs-number">999</span>])<br>desired_response = <span class="hljs-string">f&quot;\n\n### Response:\n<span class="hljs-subst">&#123;data[<span class="hljs-number">999</span>][<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span><br><span class="hljs-built_in">print</span>(model_input + desired_response)<br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.</span><br><br><span class="hljs-comment"># ### Instruction:</span><br><span class="hljs-comment"># What is an antonym of &#x27;complicated&#x27;?</span><br><br><span class="hljs-comment"># ### Response:</span><br><span class="hljs-comment"># An antonym of &#x27;complicated&#x27; is &#x27;simple&#x27;.</span><br></code></pre></td></tr></table></figure>

<h2 id="将数据分批"><a href="#将数据分批" class="headerlink" title="将数据分批"></a>将数据分批</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424203507229.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">train_portion = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) * <span class="hljs-number">0.85</span>)<br>test_portion = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) * <span class="hljs-number">0.1</span>)<br>val_portion = <span class="hljs-built_in">len</span>(data) - train_portion - test_portion<br><br>train_data = data[:train_portion]<br>test_data = data[train_portion : train_portion + test_portion]<br>val_data = data[train_portion + test_portion :]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training set length:&quot;</span>, <span class="hljs-built_in">len</span>(train_data))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Validation set length:&quot;</span>, <span class="hljs-built_in">len</span>(val_data))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Test set length:&quot;</span>, <span class="hljs-built_in">len</span>(test_data))<br><span class="hljs-comment"># Training set length: 935</span><br><span class="hljs-comment"># Validation set length: 55</span><br><span class="hljs-comment"># Test set length: 110</span><br></code></pre></td></tr></table></figure>

<p>数据分批涉及多个动作，包括：</p>
<ul>
<li>格式化</li>
<li>token 化</li>
<li>填充，保证长度相同</li>
<li>创建目标 token ids</li>
<li>用占位符替换填充值</li>
</ul>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424204103461.png" srcset="/img/loading.gif" lazyload></p>
<p>以下是格式化 和 token 化的示例：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424204231827.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对数据集进行格式化</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InstructionDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data, tokenizer</span>):<br>        <span class="hljs-variable language_">self</span>.data = data<br>        <span class="hljs-variable language_">self</span>.encoded_texts = []<br>        <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> data:<br>            instruction_plus_input = format_input(entry)<br>            response_text = <span class="hljs-string">f&quot;\n\n### Response:\n<span class="hljs-subst">&#123;entry[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span><br>            full_text = instruction_plus_input + response_text<br>            <span class="hljs-variable language_">self</span>.encoded_texts.append(tokenizer.encode(full_text))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.encoded_texts[index]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br></code></pre></td></tr></table></figure>

<p>填充长度可按需调整，不同 batch 大小可以会有所不同，单个 batch 内部的长度相同即可</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250424205029005.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 自定义的 collate_fn 函数，用于对每个 batch 内部的长度进行 padding，</span><br><span class="hljs-comment"># 用于替换 DataLoader 默认的 collate_fn</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_collate_draft_1</span>(<span class="hljs-params">batch, pad_token_id=<span class="hljs-number">50256</span>, device=<span class="hljs-string">&quot;cpu&quot;</span></span>):<br>    batch_max_length = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(item) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch)<br>    inputs_lst = []<br><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>        new_item = item.copy()<br>        new_item += [pad_token_id]  <span class="hljs-comment"># 添加一个结束标记</span><br>        <span class="hljs-comment"># 填充</span><br>        padded = new_item + [pad_token_id] * (batch_max_length - <span class="hljs-built_in">len</span>(new_item))<br>        inputs = torch.tensor(padded[:-<span class="hljs-number">1</span>])<br>        inputs_lst.append(inputs)<br><br>    inputs_tensor = torch.stack(inputs_lst).to(device)<br>    <span class="hljs-keyword">return</span> inputs_tensor<br><br><br>inputs_1 = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>inputs_2 = [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br>inputs_3 = [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]<br>batch = (inputs_1, inputs_2, inputs_3)<br><span class="hljs-built_in">print</span>(custom_collate_draft_1(batch))<br><span class="hljs-comment"># tensor([[    0,     1,     2,     3,     4],</span><br><span class="hljs-comment">#         [    5,     6, 50256, 50256, 50256],</span><br><span class="hljs-comment">#         [    7,     8,     9, 50256, 50256]])</span><br></code></pre></td></tr></table></figure>



<p>除了需要将 inputs 转成 token id，还需要将 response（即预期输出）也转成 token id，以便提交给模型进行训练学习；</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425144615939.png" srcset="/img/loading.gif" lazyload></p>
<p>response 和 inputs 的区别在于往右移动了一位，示例如下：</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425144726686.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 改进版的 collate_fn 函数，增加了 targets 目标输出</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_collate_draft_2</span>(<span class="hljs-params">batch, pad_token_id=<span class="hljs-number">50256</span>, device=<span class="hljs-string">&quot;cpu&quot;</span></span>):<br>    batch_max_length = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(item) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch)<br>    inputs_lst = []<br>    target_lst = []<br><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>        new_item = item.copy()<br>        new_item += [pad_token_id]  <span class="hljs-comment"># 添加一个结束标记</span><br>        <span class="hljs-comment"># 填充</span><br>        padded = new_item + [pad_token_id] * (batch_max_length - <span class="hljs-built_in">len</span>(new_item))<br>        inputs = torch.tensor(padded[:-<span class="hljs-number">1</span>])<br>        targets = torch.tensor(padded[<span class="hljs-number">1</span>:])<br>        inputs_lst.append(inputs)<br>        target_lst.append(targets)<br><br>    inputs_tensor = torch.stack(inputs_lst).to(device)<br>    targets_tensor = torch.stack(target_lst).to(device)<br>    <span class="hljs-keyword">return</span> inputs_tensor, targets_tensor<br><br>inputs, targets = custom_collate_draft_2(batch)<br><span class="hljs-built_in">print</span>(inputs)<br><span class="hljs-built_in">print</span>(targets)<br><span class="hljs-comment"># tensor([[    0,     1,     2,     3,     4],</span><br><span class="hljs-comment">#         [    5,     6, 50256, 50256, 50256],</span><br><span class="hljs-comment">#         [    7,     8,     9, 50256, 50256]])</span><br><span class="hljs-comment"># tensor([[    1,     2,     3,     4, 50256],</span><br><span class="hljs-comment">#         [    6, 50256, 50256, 50256, 50256],</span><br><span class="hljs-comment">#         [    8,     9, 50256, 50256, 50256]])</span><br></code></pre></td></tr></table></figure>

<p>接下来需要将多余的 end-of-text 标记替换成 -100，以便在计算损失函数时，能够将其排除，避免影响计算；仅保留第一个 end-of-text，因为我们需要模型学习任何结束输出。</p>
<blockquote>
<p>end-of-text 之所以替换为 -100，是因为在框架中，使用写死的 -100 做为判断；</p>
</blockquote>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425145538280.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425145630819.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_collate_fn</span>(<span class="hljs-params"></span><br><span class="hljs-params">    batch, pad_token_id=<span class="hljs-number">50256</span>, ignore_index=-<span class="hljs-number">100</span>, allowed_max_length=<span class="hljs-literal">None</span>, device=<span class="hljs-string">&quot;cpu&quot;</span></span><br><span class="hljs-params"></span>):<br>    batch_max_length = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(item) + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch)<br>    inputs_lst, targets_lst = [], []<br><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> batch:<br>        new_item = item.copy()<br>        new_item += [pad_token_id]<br>        padded = new_item + [pad_token_id] * (batch_max_length - <span class="hljs-built_in">len</span>(new_item))<br>        inputs = torch.tensor(padded[:-<span class="hljs-number">1</span>])<br>        targets = torch.tensor(padded[<span class="hljs-number">1</span>:])<br>        mask = targets == pad_token_id<br>        indices = torch.nonzero(mask).squeeze()<br>        <span class="hljs-keyword">if</span> indices.numel() &gt; <span class="hljs-number">1</span>:  <span class="hljs-comment"># numel 方法会返回元素的总数量</span><br>            <span class="hljs-comment"># 将多余的 pad_token_id 替换为 ignore_index，即 -100</span><br>            <span class="hljs-comment"># 这样交叉熵函数计算损失时，会忽略该值；该值是内置的参数，有点像个魔术数字</span><br>            targets[indices[<span class="hljs-number">1</span>:]] = ignore_index<br>        <span class="hljs-comment"># 通过 allowed_max_length 限制长度（如需）</span><br>        <span class="hljs-keyword">if</span> allowed_max_length <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            inputs = inputs[:allowed_max_length]<br>            targets = targets[:allowed_max_length]<br><br>        inputs_lst.append(inputs)<br>        targets_lst.append(targets)<br><br>    inputs_tensor = torch.stack(inputs_lst).to(device)<br>    targets_tensor = torch.stack(targets_lst).to(device)<br>    <span class="hljs-keyword">return</span> inputs_tensor, targets_tensor<br><br><br>inputs, targets = custom_collate_fn(batch)<br><span class="hljs-built_in">print</span>(inputs)<br><span class="hljs-built_in">print</span>(targets)<br><span class="hljs-comment"># tensor([[    0,     1,     2,     3,     4],</span><br><span class="hljs-comment">#         [    5,     6, 50256, 50256, 50256],</span><br><span class="hljs-comment">#         [    7,     8,     9, 50256, 50256]])</span><br><span class="hljs-comment"># tensor([[    1,     2,     3,     4, 50256],</span><br><span class="hljs-comment">#         [    6, 50256,  -100,  -100,  -100],</span><br><span class="hljs-comment">#         [    8,     9, 50256,  -100,  -100]])</span><br></code></pre></td></tr></table></figure>

<p>接下来还可以考虑进一步 mask 掉输出中与指令重叠的部分，以便得到纯输出。但这个做法目前存在争议，因为有部分研究发现，如果不 mask 重叠的部分，模型的性能更好。</p>
<p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425151850612.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="创建数据加载器-1"><a href="#创建数据加载器-1" class="headerlink" title="创建数据加载器"></a>创建数据加载器</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425152239695.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python">device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Device:&quot;</span>, device)<br><span class="hljs-comment"># Device: cpu</span><br><br><span class="hljs-comment"># 使用 partial 封装函数和具体的 device 参数</span><br>customized_collate_fn = partial(<br>    custom_collate_fn, device=device, allowed_max_length=<span class="hljs-number">1024</span><br>)<br><br>num_workers = <span class="hljs-number">0</span><br>batch_size = <span class="hljs-number">8</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>train_dataset = InstructionDataset(train_data, tokenizer)<br>train_loader = DataLoader(<br>    train_dataset,<br>    batch_size=batch_size,<br>    collate_fn=customized_collate_fn,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    drop_last=<span class="hljs-literal">True</span>,<br>    num_workers=num_workers,<br>)<br><br>val_dataset = InstructionDataset(val_data, tokenizer)<br>val_loader = DataLoader(<br>    val_dataset,<br>    batch_size=batch_size,<br>    collate_fn=customized_collate_fn,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    drop_last=<span class="hljs-literal">True</span>,<br>    num_workers=num_workers,<br>)<br><br>test_dataset = InstructionDataset(test_data, tokenizer)<br>test_loader = DataLoader(<br>    test_dataset,<br>    batch_size=batch_size,<br>    collate_fn=customized_collate_fn,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    drop_last=<span class="hljs-literal">True</span>,<br>    num_workers=num_workers,<br>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Train loader:&quot;</span>)<br><span class="hljs-keyword">for</span> inputs, targets <span class="hljs-keyword">in</span> train_loader:<br>    <span class="hljs-built_in">print</span>(inputs.shape, targets.shape)<br><span class="hljs-comment"># Train loader:</span><br><span class="hljs-comment"># torch.Size([8, 61]) torch.Size([8, 61])</span><br><span class="hljs-comment"># torch.Size([8, 76]) torch.Size([8, 76])</span><br><span class="hljs-comment"># torch.Size([8, 73]) torch.Size([8, 73])</span><br><span class="hljs-comment"># ...</span><br><span class="hljs-comment"># torch.Size([8, 74]) torch.Size([8, 74])</span><br><span class="hljs-comment"># torch.Size([8, 69]) torch.Size([8, 69])</span><br><span class="hljs-comment"># 第一维 8 是批量大小，第二维是每个批次的长度</span><br></code></pre></td></tr></table></figure>

<h2 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425160144252.png" srcset="/img/loading.gif" lazyload></p>
<p>之前做分类任务微调时，使用的是 124M 参数的小模型。但对于指令型任务，这个模型性能一般，因此接下来改用 355M 参数的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python">BASE_CONFIG = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.0</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>,<br>&#125;<br>model_configs = &#123;<br>    <span class="hljs-string">&quot;gpt2-small(124M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-medium(355M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-large(774M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1280</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">36</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">20</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-xl(1558M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1600</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">48</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">25</span>&#125;,<br>&#125;<br>CHOOSE_MODEL = <span class="hljs-string">&quot;gpt2-medium(355M)&quot;</span><br>BASE_CONFIG.update(model_configs[CHOOSE_MODEL])<br>model_size = get_model_size(CHOOSE_MODEL)<br><br><br>settings, params = download_and_load_gpt2(<br>    model_size=model_size,<br>    models_dir=<span class="hljs-string">&quot;data/gpt2m&quot;</span>,<br>)<br><br>model = GPTModel(BASE_CONFIG)<br>load_weights_into_gpt(model, params)<br>model.<span class="hljs-built_in">eval</span>()<br><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>input_text = format_input(val_data[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(input_text)<br><br><br>token_ids = generate(<br>    model=model,<br>    idx=text_to_token_ids(input_text, tokenizer),<br>    max_new_tokens=<span class="hljs-number">35</span>,<br>    context_size=BASE_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    eos_id=<span class="hljs-number">50256</span>,<br>)<br>generated_text = token_ids_to_text(token_ids, tokenizer)<br><br>response_text = generated_text[<span class="hljs-built_in">len</span>(input_text) :].strip()<br><span class="hljs-built_in">print</span>(response_text)<br><span class="hljs-comment"># ###</span><br><span class="hljs-comment"># ### the active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The active</span><br><span class="hljs-comment"># The</span><br><br><span class="hljs-comment"># 以上生成的内容完全没有实现预期中的效果，毕竟还没有进行微调训练，情有可原</span><br></code></pre></td></tr></table></figure>

<h2 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/20250425165812254.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 未训练前的预测损失</span><br>model.to(device)<br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    train_loss = calc_loss_loader(train_loader, model, device, num_batches=<span class="hljs-number">5</span>)<br>    val_loss = calc_loss_loader(val_loader, model, device, num_batches=<span class="hljs-number">5</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Traning loss:&quot;</span>, train_loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Validation loss:&quot;</span>, val_loss)<br><span class="hljs-comment"># Traning loss: 4.88994665145874</span><br><span class="hljs-comment"># Validation loss: 5.035206127166748</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 开始训练</span><br>start_time = time.time()<br>torch.manual_seed(<span class="hljs-number">123</span>)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.00005</span>, weight_decay=<span class="hljs-number">0.1</span>)<br><br>num_epochs = <span class="hljs-number">2</span><br>train_losses, val_losses, tokens_seen = train_model_simple(<br>    model,<br>    train_loader,<br>    val_loader,<br>    optimizer,<br>    device,<br>    num_epochs=num_epochs,<br>    eval_freq=<span class="hljs-number">5</span>,<br>    eval_iter=<span class="hljs-number">5</span>,<br>    start_context=format_input(val_data[<span class="hljs-number">0</span>]),<br>    tokenizer=tokenizer,<br>)<br>end_time = time.time()<br>executed_time_minutes = (end_time - start_time) / <span class="hljs-number">60</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training completed in <span class="hljs-subst">&#123;executed_time_minutes:<span class="hljs-number">.2</span>f&#125;</span> minutes.&quot;</span>)<br><br><span class="hljs-comment"># 保存训练结果，方便下次直接复用，无需重新训练一次</span><br>torch.save(model.state_dict(), <span class="hljs-string">&quot;data/instruction.pth&quot;</span>)<br><br><span class="hljs-comment"># Ep 1 (Step 000000): Train loss 3.258, Val loss 3.389,</span><br><span class="hljs-comment"># Ep 1 (Step 000005): Train loss 1.878, Val loss 1.920,</span><br><span class="hljs-comment"># Ep 1 (Step 000010): Train loss 1.472, Val loss 1.480,</span><br><span class="hljs-comment"># Ep 1 (Step 000015): Train loss 1.302, Val loss 1.353,</span><br><span class="hljs-comment"># Ep 1 (Step 000020): Train loss 1.195, Val loss 1.181,</span><br><span class="hljs-comment"># Ep 1 (Step 000025): Train loss 1.023, Val loss 1.142,</span><br><span class="hljs-comment"># Ep 1 (Step 000030): Train loss 1.020, Val loss 1.092,</span><br><span class="hljs-comment"># Ep 1 (Step 000035): Train loss 0.991, Val loss 1.062,</span><br><span class="hljs-comment"># Ep 1 (Step 000040): Train loss 0.892, Val loss 0.959,</span><br><span class="hljs-comment"># Ep 1 (Step 000045): Train loss 0.919, Val loss 0.905,</span><br><span class="hljs-comment"># Ep 1 (Step 000050): Train loss 0.905, Val loss 0.873,</span><br><span class="hljs-comment"># Ep 1 (Step 000055): Train loss 0.816, Val loss 0.849,</span><br><span class="hljs-comment"># Ep 1 (Step 000060): Train loss 0.783, Val loss 0.895,</span><br><span class="hljs-comment"># Ep 1 (Step 000065): Train loss 0.794, Val loss 0.818,</span><br><span class="hljs-comment"># Ep 1 (Step 000070): Train loss 0.705, Val loss 0.757,</span><br><span class="hljs-comment"># Ep 1 (Step 000075): Train loss 0.678, Val loss 0.777,</span><br><span class="hljs-comment"># Ep 1 (Step 000080): Train loss 0.700, Val loss 0.699,</span><br><span class="hljs-comment"># Ep 1 (Step 000085): Train loss 0.603, Val loss 0.673,</span><br><span class="hljs-comment"># Ep 1 (Step 000090): Train loss 0.533, Val loss 0.606,</span><br><span class="hljs-comment"># Ep 1 (Step 000095): Train loss 0.572, Val loss 0.633,</span><br><span class="hljs-comment"># Ep 1 (Step 000100): Train loss 0.432, Val loss 0.589,</span><br><span class="hljs-comment"># Ep 1 (Step 000105): Train loss 0.497, Val loss 0.620,</span><br><span class="hljs-comment"># Ep 1 (Step 000110): Train loss 0.515, Val loss 0.591,</span><br><span class="hljs-comment"># Ep 1 (Step 000115): Train loss 0.464, Val loss 0.576,</span><br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: &#x27;The chef cooks the meal every day.&#x27; ### Response: &#x27;The chef.&#x27; ### Response: &#x27;The chef.&#x27; ### Response: &#x27;### Response: &#x27;### Response: &#x27;### Response: &#x27;### Response: &#x27;### Response: &#x27;### Response: &#x27;### Response: &#x27;###</span><br><span class="hljs-comment"># Ep 2 (Step 000120): Train loss 0.486, Val loss 0.598,</span><br><span class="hljs-comment"># Ep 2 (Step 000125): Train loss 0.436, Val loss 0.605,</span><br><span class="hljs-comment"># Ep 2 (Step 000130): Train loss 0.360, Val loss 0.516,</span><br><span class="hljs-comment"># Ep 2 (Step 000135): Train loss 0.358, Val loss 0.465,</span><br><span class="hljs-comment"># Ep 2 (Step 000140): Train loss 0.531, Val loss 0.552,</span><br><span class="hljs-comment"># Ep 2 (Step 000145): Train loss 0.379, Val loss 0.546,</span><br><span class="hljs-comment"># Ep 2 (Step 000150): Train loss 0.324, Val loss 0.478,</span><br><span class="hljs-comment"># Ep 2 (Step 000155): Train loss 0.418, Val loss 0.467,</span><br><span class="hljs-comment"># Ep 2 (Step 000160): Train loss 0.394, Val loss 0.438,</span><br><span class="hljs-comment"># Ep 2 (Step 000165): Train loss 0.307, Val loss 0.495,</span><br><span class="hljs-comment"># Ep 2 (Step 000170): Train loss 0.358, Val loss 0.425,</span><br><span class="hljs-comment"># Ep 2 (Step 000175): Train loss 0.294, Val loss 0.425,</span><br><span class="hljs-comment"># Ep 2 (Step 000180): Train loss 0.270, Val loss 0.464,</span><br><span class="hljs-comment"># Ep 2 (Step 000185): Train loss 0.353, Val loss 0.413,</span><br><span class="hljs-comment"># Ep 2 (Step 000190): Train loss 0.372, Val loss 0.383,</span><br><span class="hljs-comment"># Ep 2 (Step 000195): Train loss 0.246, Val loss 0.368,</span><br><span class="hljs-comment"># Ep 2 (Step 000200): Train loss 0.334, Val loss 0.396,</span><br><span class="hljs-comment"># Ep 2 (Step 000205): Train loss 0.339, Val loss 0.424,</span><br><span class="hljs-comment"># Ep 2 (Step 000210): Train loss 0.263, Val loss 0.388,</span><br><span class="hljs-comment"># Ep 2 (Step 000215): Train loss 0.197, Val loss 0.375,</span><br><span class="hljs-comment"># Ep 2 (Step 000220): Train loss 0.241, Val loss 0.321,</span><br><span class="hljs-comment"># Ep 2 (Step 000225): Train loss 0.198, Val loss 0.349,</span><br><span class="hljs-comment"># Ep 2 (Step 000230): Train loss 0.235, Val loss 0.400,</span><br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: &#x27;The chef cooks the meal every day.&#x27; The active sentence to the active sentence to the active sentence to the active sentence to the active sentence to the active sentence to the active sentence active sentence active sentence active sentence active sentence active sentence active sentence active sentence active sentence active sentence active sentence active sentence</span><br><span class="hljs-comment"># Training completed in 44.29 minutes.</span><br><br><span class="hljs-comment"># 使用微调后的模型进行测试</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br><span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> test_data[:<span class="hljs-number">3</span>]:<br>    input_text = format_input(entry)<br>    token_ids = generate(<br>        model=model,<br>        idx=text_to_token_ids(input_text, tokenizer).to(device),<br>        max_new_tokens=<span class="hljs-number">256</span>,<br>        context_size=BASE_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>        eos_id=<span class="hljs-number">50256</span>,<br>    )<br>    generated_text = token_ids_to_text(token_ids, tokenizer)<br><br>    response_text = (<br>        generated_text[<span class="hljs-built_in">len</span>(input_text) :].replace(<span class="hljs-string">&quot;### Response:&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).strip()<br>    )<br>    <span class="hljs-built_in">print</span>(input_text)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\nCorrect response:\n&gt;&gt; <span class="hljs-subst">&#123;entry[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\nModel response:\n&gt;&gt; <span class="hljs-subst">&#123;response_text.strip()&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;------------------------------------------&quot;</span>)<br><br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.</span><br><br><span class="hljs-comment"># ### Instruction:</span><br><span class="hljs-comment"># Rewrite the sentence using a simile.</span><br><br><span class="hljs-comment"># ### Input:</span><br><span class="hljs-comment"># The car is very fast.</span><br><br><span class="hljs-comment"># Correct response:</span><br><span class="hljs-comment"># &gt;&gt; The car is as fast as lightning.</span><br><br><span class="hljs-comment"># Model response:</span><br><span class="hljs-comment"># &gt;&gt; The car is very fast.</span><br><span class="hljs-comment"># ------------------------------------------</span><br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.</span><br><br><span class="hljs-comment"># ### Instruction:</span><br><span class="hljs-comment"># What type of cloud is typically associated with thunderstorms?</span><br><br><span class="hljs-comment"># Correct response:</span><br><span class="hljs-comment"># &gt;&gt; The type of cloud typically associated with thunderstorms is cumulonimbus.</span><br><br><span class="hljs-comment"># Model response:</span><br><span class="hljs-comment"># &gt;&gt; What type of cloud associated with type of cloud associated with associated with associated with associated with associated with associated with associated with associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud with cloud with cloud with cloud with cloud with cloud with cloud with cloud with cloud with cloud with cloud with associated with associated with cloud with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud with associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated cloud associated with associated with associated with associated with associated with associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud associated with cloud with cloud with cloud with cloud with cloud with cloud</span><br><span class="hljs-comment"># ------------------------------------------</span><br><span class="hljs-comment"># Below is an instruction that describes a task.Write a response that appropriately completes the request.</span><br><br><span class="hljs-comment"># ### Instruction:</span><br><span class="hljs-comment"># Name the author of &#x27;Pride and Prejudice&#x27;.</span><br><br><span class="hljs-comment"># Correct response:</span><br><span class="hljs-comment"># &gt;&gt; Jane Austen.</span><br><br><span class="hljs-comment"># Model response:</span><br><span class="hljs-comment"># &gt;&gt; Name the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of the author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author of author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the author describes the</span><br><br><span class="hljs-comment"># 训练结果远不如预期，很菜，待查找一下原因</span><br></code></pre></td></tr></table></figure>

<h2 id="读取并保存响应"><a href="#读取并保存响应" class="headerlink" title="读取并保存响应"></a>读取并保存响应</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202504280751970.png" srcset="/img/loading.gif" lazyload></p>
<p>有多种方法可用来评估微调后的模型质量，包括：</p>
<ul>
<li>简答题和多选题测试，例如 MMMU<ul>
<li>优点：可自动化；</li>
<li>缺点：结果会受到题目设计的影响，例如选择题中选项的顺序；</li>
</ul>
</li>
<li>与其他模型相比的测试，例如 LMSYS（需人工介入进行主观评估，例如进行手工数据标注）；<ul>
<li>优点：不容易出错</li>
<li>缺点：费时费力，而且每个人有自己不同的偏好；</li>
</ul>
</li>
<li>自动化对话测试，例如用其他更高级的模型，对生成结果进行评分，例如 AlpacaEval；<ul>
<li>可自动化</li>
<li>缺点：会受其他模型局限性的影响</li>
</ul>
</li>
</ul>
<p>每种测试方法都有其优缺点，因此更好的方式是将它们组合起来使用，而不是只使用其中一种。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> tiktoken<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> GPTModel <span class="hljs-keyword">import</span> GPTModel<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> (<br>    download_and_load_file,<br>    format_input,<br>    generate,<br>    text_to_token_ids,<br>    token_ids_to_text,<br>)<br><br>file_path = <span class="hljs-string">&quot;data/instruction-data.json&quot;</span><br>url = (<br>    <span class="hljs-string">&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch&quot;</span><br>    <span class="hljs-string">&quot;/main/ch07/01_main-chapter-code/instruction-data.json&quot;</span><br>)<br><br>data = download_and_load_file(file_path, url)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of entries:&quot;</span>, <span class="hljs-built_in">len</span>(data))<br><br>train_portion = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) * <span class="hljs-number">0.85</span>)<br>test_portion = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) * <span class="hljs-number">0.1</span>)<br>val_portion = <span class="hljs-built_in">len</span>(data) - train_portion - test_portion<br><br>train_data = data[:train_portion]<br>test_data = data[train_portion : train_portion + test_portion]<br>val_data = data[train_portion + test_portion :]<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>BASE_CONFIG = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,<br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.0</span>,<br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>,<br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>,<br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>,<br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>,<br>&#125;<br><br><span class="hljs-comment"># 加载微调后的模型</span><br>model = GPTModel(BASE_CONFIG)<br>checkpoint = torch.load(<span class="hljs-string">&quot;data/instruction.pth&quot;</span>, map_location=device)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&quot;model_state_dict&quot;</span>])<br><br><span class="hljs-comment"># 生成文本</span><br><span class="hljs-keyword">for</span> i, entry <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(test_data), total=<span class="hljs-built_in">len</span>(test_data)):<br>    input_text = format_input(entry)<br><br>    token_ids = generate(<br>        model=model,<br>        idx=text_to_token_ids(input_text, tokenizer).to(device),<br>        max_new_tokens=<span class="hljs-number">256</span>,<br>        context_size=BASE_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>        eos_id=<span class="hljs-number">50256</span>,<br>    )<br>    generated_text = token_ids_to_text(token_ids, tokenizer)<br>    response_text = (<br>        generated_text[<span class="hljs-built_in">len</span>(input_text) :].replace(<span class="hljs-string">&quot;### Response:&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).strip()<br>    )<br>    test_data[i][<span class="hljs-string">&quot;model_response&quot;</span>] = response_text<br><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;data/instruction-data-with-response.json&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    json.dump(test_data, f, indent=<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># 检查生成的文本是否保存成功</span><br><span class="hljs-built_in">print</span>(test_data[<span class="hljs-number">0</span>])<br><br></code></pre></td></tr></table></figure>

<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p><img src="https://lhwccw.oss-cn-shenzhen.aliyuncs.com/202505080922977.png" srcset="/img/loading.gif" lazyload></p>
<p>人工审核模型的输出过于费时费力，一种简单高效的办法是使用另外一个训练好的模型，对当前模型的输出结果进行评估和打分。</p>
<blockquote>
<p>Ollama 是一个专门用来加载和运行大模型的库，但不能用来训练或微调；可在官网上下载：<a target="_blank" rel="noopener" href="https://ollama.com/">https://ollama.com</a></p>
<p>我们可使用该库直接下载另外一个大模型，例如 llama3，然后用它来评估我们自己训练的模型；下载完后，让其在后台运行，然后就可以在命令行中调用它了，例如：运行 ollama run llama3 命令，会自动下载 llama3，并进入对话状态；</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下载并运行 ollama3 模型</span><br>PS C:\Users\ccw&gt; ollama run llama3<br>&gt;&gt;&gt; what <span class="hljs-keyword">do</span> llamas eat?<br>Llamas are herbivores, <span class="hljs-built_in">which</span> means they primarily eat plants and plant-based foods. Their diet typically consists of:<br><br>1. Grasses: Llamas love to graze on grasses, including various species like timothy grass, orchard grass, and brome.<br>2. Hay: They enjoy eating hay, such as alfalfa or oat hay, <span class="hljs-built_in">which</span> is high <span class="hljs-keyword">in</span> fiber and protein.<br>3. Grains: Llamas might eat grains like oats, barley, or corn, but these should not make up more than 10% of their diet.<br>4. Fruits and vegetables: Many llamas enjoy fruits and veggies like apples, carrots, sweet potatoes, and peas as treats or supplements.<br>5. Minerals: Llamas need access to mineral blocks or loose minerals that provide essential nutrients like calcium, phosphorus, and salt.<br><br>In the wild, llamas might also eat:<br><br>1. Leaves: They<span class="hljs-string">&#x27;ll munch on leaves from trees and shrubs, such as willow, alder, and juniper.</span><br><span class="hljs-string">2. Bark: In some cases, they might eat bark, especially during times of food scarcity.</span><br><span class="hljs-string">3. Mushrooms: Llamas have been known to eat certain types of mushrooms, like those in the genus Boletus.</span><br><span class="hljs-string"></span><br><span class="hljs-string">In captivity or on farms, llama owners usually provide a diet that consists mainly of hay and grains, with occasional treats like fruits and veggies.</span><br><span class="hljs-string">It&#x27;</span>s essential to ensure they receive a balanced diet and access to clean water at all <span class="hljs-built_in">times</span>.<br><br><span class="hljs-comment"># 退出模型对话</span><br>&gt;&gt;&gt; /bye  <br></code></pre></td></tr></table></figure>

<p>手动使用 Llama3 对之前的微调模型进行打分</p>
<blockquote>
<p>从打分结果来看，效果很差，Llama3 给模型打了 100 分，但实际输入的是多个重复无意义的 Input 单词。另外使用通义千问对相同输入打分，通义的结果是 0 分。显然通义正确理解了输入的内容，而 Llama3 完全没理解输入的内容；</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">PS C:\Users\ccw&gt; ollama run llama3<br><span class="hljs-meta">&gt;&gt;&gt; </span>Given the <span class="hljs-built_in">input</span> `Below <span class="hljs-keyword">is</span> an instruction that describe a task. Write a response that appropriately completes the request.<br>...<br><span class="hljs-meta">... </span><span class="hljs-comment">### Instruction:</span><br><span class="hljs-meta">... </span>Rewrite the following sentence so that it <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> active voice.<br>...<br><span class="hljs-meta">... </span><span class="hljs-comment">### Input:</span><br><span class="hljs-meta">... </span>The cake was baked by Sarah.` <span class="hljs-keyword">and</span> correct output `Sarah baked the cake.`, score the model response `Input: Input: Input: Input: Input: Input: Input` on<br><span class="hljs-meta">... </span> a scale <span class="hljs-keyword">from</span> <span class="hljs-number">0</span> to <span class="hljs-number">100</span>, where <span class="hljs-number">100</span> <span class="hljs-keyword">is</span> the best score.<br>A nice <span class="hljs-keyword">and</span> simple instruction!<br><br>To rewrite the sentence <span class="hljs-keyword">in</span> active voice, I<span class="hljs-string">&#x27;ll make sure that the subject of the sentence performs the action described by the verb.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Original sentence: The cake was baked by Sarah.</span><br><span class="hljs-string">Rewritten sentence: Sarah baked the cake.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Score: 100</span><br><span class="hljs-string"></span><br><span class="hljs-string">Why? Because the rewritten sentence follows the structure of an active voice sentence, where the subject (Sarah) performs the action (baked) on the</span><br><span class="hljs-string">object (the cake). The original sentence is in passive voice, which can sometimes make it harder to identify who performed the action.</span><br></code></pre></td></tr></table></figure>

<p>自动批量打分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_model_scores</span>(<span class="hljs-params">json_data, json_key, model=<span class="hljs-string">&quot;llama3&quot;</span></span>):<br>    scores = []<br>    <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> tqdm(json_data, desc=<span class="hljs-string">&quot;Scoring entries&quot;</span>):<br>        prompt = (<br>            <span class="hljs-string">f&quot;Given the input `<span class="hljs-subst">&#123;format_input(entry)&#125;</span>` &quot;</span><br>            <span class="hljs-string">f&quot;and correct output `<span class="hljs-subst">&#123;entry[<span class="hljs-string">&#x27;output&#x27;</span>]&#125;</span>`, &quot;</span><br>            <span class="hljs-string">f&quot;score the model response `<span class="hljs-subst">&#123;entry[json_key]&#125;</span>`&quot;</span><br>            <span class="hljs-string">f&quot; on a scale from 0 to 100, where 100 is the best score. &quot;</span><br>            <span class="hljs-string">f&quot;Respond with the integer number only.&quot;</span><br>        )<br>        score = query_model(prompt, model)<br>        <span class="hljs-keyword">try</span>:<br>            scores.append(<span class="hljs-built_in">int</span>(score))<br>        <span class="hljs-keyword">except</span> ValueError:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Count not convert score: <span class="hljs-subst">&#123;score&#125;</span>&quot;</span>)<br>            <span class="hljs-keyword">continue</span><br>    <span class="hljs-keyword">return</span> scores<br><br><br>scores = generate_model_scores(test_data, <span class="hljs-string">&quot;model_response&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of scores: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(scores)&#125;</span> of <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(test_data)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Average score: <span class="hljs-subst">&#123;<span class="hljs-built_in">sum</span>(scores) / <span class="hljs-built_in">len</span>(scores):<span class="hljs-number">.2</span>f&#125;</span>\n&quot;</span>)<br><span class="hljs-comment"># Number of scores: 110 of 110</span><br><span class="hljs-comment"># Average score: 50.32</span><br></code></pre></td></tr></table></figure>

<p>通过打分，可以比较不同模型的性能，同时也可以用来调整方法重新训练模型，例如：</p>
<ul>
<li>调整微调过程中的相关参数，例如学习率 learning rate，批量大小 batch siz，迭代次数 num of epochs 等；</li>
<li>增加训练数据集中的数据量，或者让内容覆盖更多的主题和风格；</li>
<li>使用不同的 prompt 或 instruction 格式，以便让模型更准确的理解意图；</li>
<li>使用更强大的预训练模型进行微调，以便能够理解更复杂的输入内容；</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" class="category-chain-item">计算机</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>从零开始构建大模型</div>
      <div>https://ccw1078.github.io/2025/03/10/从零开始构建大模型/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>ccw</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年3月10日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/02/19/%E9%87%91%E9%92%B1%E5%BF%83%E7%90%86%E5%AD%A6/" title="金钱心理学">
                        <span class="hidden-mobile">金钱心理学</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
